Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=60, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
59511
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.9311267652951525, grad_norm: 4.941419257086488, ic: -0.03907055451704089
train 0, step: 500, loss: 0.8647736906556142, grad_norm: 0.026269981496414147, ic: 0.03622010234578963
train 0, step: 1000, loss: 1.9478798163748172, grad_norm: 0.5072789571846923, ic: 0.035782384185455464
train 0, step: 1500, loss: 0.9576308902544466, grad_norm: 0.054439974534898496, ic: 0.0304004724736186
train 0, step: 2000, loss: 1.001249353176746, grad_norm: 0.15393224050292947, ic: 0.007779578350609568
Epoch 0: 2022-04-28 14:20:40.929756: train loss: 1.648675619057575
Eval step 0: eval loss: 0.8361719933400289
Eval: 2022-04-28 14:21:13.221073: total loss: 1.079295262359885, mse:4.822919830099826, ic :0.008589574701513123, sharpe5:8.239294842481613, irr5:232.3008270263672, ndcg5:0.8528519431052917, pnl5:2.58039927482605 
train 1, step: 0, loss: 2.77197501890121, grad_norm: 0.8636258048049184, ic: 0.05759575449638967
train 1, step: 500, loss: 1.7546697080022815, grad_norm: 0.7544368861990439, ic: 0.11957323651527771
train 1, step: 1000, loss: 0.8775615378773617, grad_norm: 0.17389846775067525, ic: 0.06769023658449247
train 1, step: 1500, loss: 1.7129590966235633, grad_norm: 0.20786211801025778, ic: -0.014657210510147463
train 1, step: 2000, loss: 2.178447265625, grad_norm: 0.8865367745595222, ic: -0.037028784469609506
Epoch 1: 2022-04-28 14:29:33.390484: train loss: 1.6467071890983203
Eval step 0: eval loss: 0.8344052538856691
Eval: 2022-04-28 14:30:03.751635: total loss: 1.0789720035318093, mse:4.823638683528131, ic :0.00802955852563131, sharpe5:7.841895369887352, irr5:222.6210479736328, ndcg5:0.8589998888350817, pnl5:2.564917802810669 
train 2, step: 0, loss: 2.1423579545454543, grad_norm: 0.009692993341611085, ic: 0.10854503579146407
train 2, step: 500, loss: 3.2988510490023475, grad_norm: 0.27914802996624133, ic: 0.08959319435319718
train 2, step: 1000, loss: 2.0727395010177205, grad_norm: 0.00017478684400490507, ic: 0.1693742642792411
train 2, step: 1500, loss: 1.4847386949844943, grad_norm: 0.05927519541034277, ic: -0.04644972007101113
train 2, step: 2000, loss: 3.2339547025240383, grad_norm: 0.7801884980153005, ic: 0.15081566191218726
Epoch 2: 2022-04-28 14:38:01.696381: train loss: 1.646501735392995
Eval step 0: eval loss: 0.8358690685919388
Eval: 2022-04-28 14:38:31.679622: total loss: 1.0794154642905167, mse:4.820800811112737, ic :0.022752420048131456, sharpe5:10.510945846438407, irr5:331.1249084472656, ndcg5:0.8437314219040682, pnl5:3.206212043762207 
train 3, step: 0, loss: 1.5226219115218496, grad_norm: 0.5202472568605505, ic: -0.022985004588151774
train 3, step: 500, loss: 1.4984253216461922, grad_norm: 0.3369670686008933, ic: 0.11599829966476616
train 3, step: 1000, loss: 3.679302946891192, grad_norm: 0.6999767502051751, ic: -0.045346331362426
train 3, step: 1500, loss: 1.9859093698854338, grad_norm: 1.3354214135142233, ic: -0.05777092601539583
train 3, step: 2000, loss: 0.9007272091427365, grad_norm: 0.00395870984521526, ic: 0.004289090406791083
Epoch 3: 2022-04-28 14:46:54.446796: train loss: 1.6463431588814732
Eval step 0: eval loss: 0.8346057244673669
Eval: 2022-04-28 14:47:26.125203: total loss: 1.079053184173786, mse:4.817453299723839, ic :0.04035562157016544, sharpe5:11.00819439947605, irr5:352.3132629394531, ndcg5:0.8507870467288771, pnl5:3.057093620300293 
train 4, step: 0, loss: 1.4325432477678572, grad_norm: 0.04556456742195293, ic: 0.1265413042911449
train 4, step: 500, loss: 1.6501157265009843, grad_norm: 0.563767679065803, ic: 0.00864146161375454
train 4, step: 1000, loss: 2.9485432694597944, grad_norm: 0.746960423298063, ic: -0.016197804826748617
train 4, step: 1500, loss: 2.13162682126846, grad_norm: 0.5202234167211653, ic: -0.03032921984010891
train 4, step: 2000, loss: 1.0718663810385887, grad_norm: 0.4143156601909387, ic: 0.22009976989202737
Epoch 4: 2022-04-28 14:55:39.228698: train loss: 1.6408830419273315
Eval step 0: eval loss: 0.8405939871945798
Eval: 2022-04-28 14:56:09.234938: total loss: 1.078724552801036, mse:4.71474220223695, ic :0.13807725387781436, sharpe5:12.064472751617432, irr5:402.5730895996094, ndcg5:0.8415602995739202, pnl5:3.405900001525879 
train 5, step: 0, loss: 1.3074412096266779, grad_norm: 0.17192032855564784, ic: 0.3696169383069946
train 5, step: 500, loss: 0.8802715788560667, grad_norm: 0.016932404646534817, ic: 0.9071510513921122
train 5, step: 1000, loss: 0.9842698605124521, grad_norm: 0.20687065810302768, ic: -0.012188228011011827
train 5, step: 1500, loss: 1.5235790118324082, grad_norm: 0.16377783088330305, ic: 0.04643267824087889
train 5, step: 2000, loss: 1.0943973833336353, grad_norm: 0.03079137571495666, ic: 0.20691493484816478
Epoch 5: 2022-04-28 15:04:18.683139: train loss: 1.6322020233211088
Eval step 0: eval loss: 0.8324568882129214
Eval: 2022-04-28 15:04:51.155475: total loss: 1.071482214865203, mse:4.6848087436371415, ic :0.1650045424700436, sharpe5:17.153594706058502, irr5:561.7031860351562, ndcg5:0.8428965663888849, pnl5:7.3915228843688965 
train 6, step: 0, loss: 1.3210543643154407, grad_norm: 0.4660809032591736, ic: 0.1812237929484127
train 6, step: 500, loss: 1.0084191128086715, grad_norm: 0.47938851499789675, ic: 0.009045500079289937
train 6, step: 1000, loss: 1.1110581778754751, grad_norm: 0.28104760501733894, ic: 0.7388596775446938
train 6, step: 1500, loss: 1.5724809126420454, grad_norm: 0.7297526218209732, ic: 0.16080352372321077
train 6, step: 2000, loss: 0.8032511947057646, grad_norm: 0.05916789683640044, ic: 0.35625639952493016
Epoch 6: 2022-04-28 15:13:03.626631: train loss: 1.6280384382424196
Eval step 0: eval loss: 0.8259490870324024
Eval: 2022-04-28 15:13:36.236556: total loss: 1.0689877080387993, mse:4.68389056608683, ic :0.16920058627501794, sharpe5:16.776794657707214, irr5:564.2412719726562, ndcg5:0.8501577630520312, pnl5:4.160772800445557 
train 7, step: 0, loss: 0.9851575851440431, grad_norm: 0.6810267988476659, ic: 0.14348607595158447
train 7, step: 500, loss: 0.6458597386136968, grad_norm: 0.35130020710025206, ic: 0.08911321646719937
train 7, step: 1000, loss: 1.027647949590632, grad_norm: 0.2879867831420922, ic: 0.0628947210575836
train 7, step: 1500, loss: 2.2494298151962755, grad_norm: 0.7161884666989246, ic: 0.4489930115715344
train 7, step: 2000, loss: 0.9174747497854403, grad_norm: 0.09061933837730853, ic: -0.048972584634032694
Epoch 7: 2022-04-28 15:21:55.859222: train loss: 1.6274389179503304
Eval step 0: eval loss: 0.8294761344178082
Eval: 2022-04-28 15:22:28.254630: total loss: 1.0709430169750442, mse:4.682308826491636, ic :0.16857348464496016, sharpe5:17.16938031435013, irr5:551.4851684570312, ndcg5:0.8610034205956292, pnl5:7.615969181060791 
train 8, step: 0, loss: 3.600473774343297, grad_norm: 1.0452441026921664, ic: 0.16061959555779337
train 8, step: 500, loss: 2.7660771053975113, grad_norm: 1.3082764707148429, ic: 0.037308620007988375
train 8, step: 1000, loss: 3.057736781023551, grad_norm: 0.891669400283317, ic: 0.11947130420496382
train 8, step: 1500, loss: 0.7141710838760018, grad_norm: 0.029809741916065405, ic: 0.4437603334356015
train 8, step: 2000, loss: 1.0880464691086382, grad_norm: 0.5388518703361402, ic: 0.5128433990650813
Epoch 8: 2022-04-28 15:30:37.448708: train loss: 1.6269184096140104
Eval step 0: eval loss: 0.8235824793985445
Eval: 2022-04-28 15:31:09.645903: total loss: 1.068204687618688, mse:4.675913184079722, ic :0.17188556455768403, sharpe5:16.43657166481018, irr5:556.6256103515625, ndcg5:0.8484228885434814, pnl5:5.211929798126221 
train 9, step: 0, loss: 5.4076626669657095, grad_norm: 0.7868781380529196, ic: 0.08003692874771083
train 9, step: 500, loss: 1.3286146162636707, grad_norm: 1.508072504276944, ic: 0.34765515990952006
train 9, step: 1000, loss: 0.9277768690989326, grad_norm: 0.06537245908781285, ic: 0.09491762527124012
train 9, step: 1500, loss: 1.08632061021493, grad_norm: 0.024682253834180753, ic: 0.4426422564827661
train 9, step: 2000, loss: 1.0711407458322768, grad_norm: 0.3425620989319823, ic: 0.2463136416258378
Epoch 9: 2022-04-28 15:39:24.942976: train loss: 1.62560804867961
Eval step 0: eval loss: 0.8258874730390542
Eval: 2022-04-28 15:39:57.897003: total loss: 1.0701708245199177, mse:4.67545825874557, ic :0.1702522451128063, sharpe5:16.447603050470352, irr5:545.4996948242188, ndcg5:0.8412237983067453, pnl5:7.29289436340332 
train 10, step: 0, loss: 7.090622437591108, grad_norm: 2.9515225109044527, ic: 0.2538784303883085
train 10, step: 500, loss: 1.127906787339335, grad_norm: 0.1537523535617843, ic: 0.07272380117833872
train 10, step: 1000, loss: 2.376771142877684, grad_norm: 1.1570622733513827, ic: 0.12119408107541835
train 10, step: 1500, loss: 1.1159432151100852, grad_norm: 0.3550448449153283, ic: -0.006295264020570766
train 10, step: 2000, loss: 2.7308711367899887, grad_norm: 1.2572344663650963, ic: 0.47586559657573213
Epoch 10: 2022-04-28 15:48:08.609398: train loss: 1.6270543682943273
Eval step 0: eval loss: 0.823912223598854
Eval: 2022-04-28 15:48:40.750738: total loss: 1.0685961369651307, mse:4.669397964875305, ic :0.17395987648905678, sharpe5:16.81256767630577, irr5:565.084716796875, ndcg5:0.8554474352939176, pnl5:10.740656852722168 
train 11, step: 0, loss: 1.2566269168720998, grad_norm: 0.10478448238150223, ic: 0.1987321286212242
train 11, step: 500, loss: 0.6577351746833796, grad_norm: 0.06187709412517305, ic: 0.5593014351575696
train 11, step: 1000, loss: 0.9418697750283388, grad_norm: 0.371508183810341, ic: 0.019570574111516328
train 11, step: 1500, loss: 1.0546916493198328, grad_norm: 0.10000882599684616, ic: 0.17195523888845732
train 11, step: 2000, loss: 0.7852386426398351, grad_norm: 0.014893813767469885, ic: 0.12353918680042893
Epoch 11: 2022-04-28 15:56:59.956959: train loss: 1.6245437276573547
Eval step 0: eval loss: 0.8303651638855044
Eval: 2022-04-28 15:57:31.775608: total loss: 1.0688054783216359, mse:4.618092086981571, ic :0.18418413235344977, sharpe5:16.666227293014526, irr5:541.8547973632812, ndcg5:0.8471703206093841, pnl5:5.6828389167785645 
train 12, step: 0, loss: 0.9649367332458496, grad_norm: 0.10416571106700426, ic: 0.3875203403890778
train 12, step: 500, loss: 0.9266108482417077, grad_norm: 0.10560841668026884, ic: 0.15848885744793903
train 12, step: 1000, loss: 2.938645672646298, grad_norm: 0.5309967013893206, ic: 0.2066700694314617
train 12, step: 1500, loss: 0.9713687940308454, grad_norm: 2.913875957611448, ic: -0.0959547249554504
train 12, step: 2000, loss: 0.8744313219882931, grad_norm: 0.008052432410352466, ic: 0.21104927488035338
Epoch 12: 2022-04-28 16:05:41.096323: train loss: 1.6215192581695155
Eval step 0: eval loss: 0.8285648518794454
Eval: 2022-04-28 16:06:13.657164: total loss: 1.0667990180643072, mse:4.591930625398718, ic :0.18325852674583928, sharpe5:15.48203329861164, irr5:515.514892578125, ndcg5:0.8447967691301262, pnl5:4.158973217010498 
train 13, step: 0, loss: 2.0652423927121832, grad_norm: 1.0259260807140462, ic: 0.43313445259154293
train 13, step: 500, loss: 0.8055968655407998, grad_norm: 0.28796216713799533, ic: 0.6030559582494555
train 13, step: 1000, loss: 0.9481786126258389, grad_norm: 0.425181771059542, ic: 0.5955035832066479
train 13, step: 1500, loss: 2.402644304071526, grad_norm: 0.5641130753978332, ic: 0.009506883067996862
train 13, step: 2000, loss: 1.4610441588180811, grad_norm: 0.10301431123354181, ic: 0.21047362955479648
Epoch 13: 2022-04-28 16:14:31.760381: train loss: 1.621193867335144
Eval step 0: eval loss: 0.826043373164186
Eval: 2022-04-28 16:15:04.940191: total loss: 1.0659878735414985, mse:4.604660113221258, ic :0.18449419611436566, sharpe5:16.966598435640336, irr5:559.484619140625, ndcg5:0.8458261490337832, pnl5:6.593116760253906 
train 14, step: 0, loss: 4.495750201101794, grad_norm: 2.9005140776062595, ic: 0.209380375716534
train 14, step: 500, loss: 0.828407310929138, grad_norm: 0.009052119697525337, ic: 0.11132308505818439
train 14, step: 1000, loss: 1.7961045759776006, grad_norm: 0.5594275320894362, ic: 0.46615433947057017
train 14, step: 1500, loss: 1.1271612385939953, grad_norm: 0.08381392581107612, ic: -0.05673268957563694
train 14, step: 2000, loss: 1.1558825145464762, grad_norm: 0.34031173390947056, ic: 0.07298710302476558
Epoch 14: 2022-04-28 16:23:30.257300: train loss: 1.619668546302348
Eval step 0: eval loss: 0.831250205808746
Eval: 2022-04-28 16:24:02.639168: total loss: 1.0669024017903987, mse:4.5891399520561835, ic :0.18852469108365633, sharpe5:16.78745524048805, irr5:549.4933471679688, ndcg5:0.8485583557277675, pnl5:5.152950286865234 
train 15, step: 0, loss: 3.3796232824659533, grad_norm: 3.433855006869133, ic: 0.12525560343320297
train 15, step: 500, loss: 1.2519683457372195, grad_norm: 0.04372811857893777, ic: 0.09078033668150051
train 15, step: 1000, loss: 1.314896746379573, grad_norm: 0.1501496408102499, ic: 0.015738440620089538
train 15, step: 1500, loss: 0.8526028850885826, grad_norm: 0.5020132529563904, ic: 0.07110267548774346
train 15, step: 2000, loss: 1.4624337950382447, grad_norm: 0.6494604371960406, ic: 0.0641491254904693
Epoch 15: 2022-04-28 16:32:17.887723: train loss: 1.6198708308476524
Eval step 0: eval loss: 0.8330057544125394
Eval: 2022-04-28 16:32:50.044313: total loss: 1.0696694250877823, mse:4.589364677203579, ic :0.18521132414079083, sharpe5:16.348097341060637, irr5:525.1349487304688, ndcg5:0.867324637813035, pnl5:4.48456335067749 
train 16, step: 0, loss: 0.6848425302633857, grad_norm: 0.5312116153326244, ic: -0.047342486504103304
train 16, step: 500, loss: 1.583587985748081, grad_norm: 1.459199167263485, ic: 0.18368221240273008
train 16, step: 1000, loss: 0.8813245368726326, grad_norm: 0.008311343420179716, ic: -0.08473634218951479
train 16, step: 1500, loss: 0.8404563140267928, grad_norm: 0.523922657404674, ic: 0.1412492789687514
train 16, step: 2000, loss: 3.3392260399463254, grad_norm: 1.3245010210130146, ic: -0.03670231884486831
Epoch 16: 2022-04-28 16:41:09.983445: train loss: 1.6195547111475581
Eval step 0: eval loss: 0.8290750646239462
Eval: 2022-04-28 16:41:43.508768: total loss: 1.0671222666067681, mse:4.595755437434061, ic :0.18365467633336768, sharpe5:16.680546716451644, irr5:543.1585693359375, ndcg5:0.865037190768111, pnl5:6.316961765289307 
train 17, step: 0, loss: 1.2779502807940981, grad_norm: 0.32138967149049097, ic: -0.06149625611502692
train 17, step: 500, loss: 1.7509913829607047, grad_norm: 0.5764939897562209, ic: 0.18628943674952425
train 17, step: 1000, loss: 1.2766372179049577, grad_norm: 0.11054151776490348, ic: 0.15606957421713347
train 17, step: 1500, loss: 4.536880481952608, grad_norm: 2.6539994083749363, ic: 0.19301459388039172
train 17, step: 2000, loss: 1.2791851957164877, grad_norm: 1.1134025330614348, ic: 0.1212673789630094
Epoch 17: 2022-04-28 16:50:07.694069: train loss: 1.6189778456821144
Eval step 0: eval loss: 0.8306318148421035
Eval: 2022-04-28 16:50:40.230362: total loss: 1.0666679372138643, mse:4.586490453606905, ic :0.19191097337162097, sharpe5:17.120965944528578, irr5:558.277587890625, ndcg5:0.850262264506669, pnl5:6.983332633972168 
train 18, step: 0, loss: 1.409043498905881, grad_norm: 1.0862091622222432, ic: 0.23134013985196245
train 18, step: 500, loss: 1.5040126551150121, grad_norm: 0.7419613605191518, ic: -0.008559110475446028
train 18, step: 1000, loss: 0.6564603622645547, grad_norm: 0.0453647445891776, ic: 0.5730854997176582
train 18, step: 1500, loss: 1.423130294712326, grad_norm: 0.5924193492617705, ic: 0.2123396692130038
train 18, step: 2000, loss: 0.9116286745496617, grad_norm: 0.0230319666033313, ic: -0.008538836258182197
Epoch 18: 2022-04-28 16:59:05.710596: train loss: 1.6187329685341962
Eval step 0: eval loss: 0.8238021802349512
Eval: 2022-04-28 16:59:37.891015: total loss: 1.0646988110963713, mse:4.591697692857318, ic :0.19069757817714159, sharpe5:17.385953789949415, irr5:564.5930786132812, ndcg5:0.8248938364081448, pnl5:7.610891342163086 
train 19, step: 0, loss: 1.4977886866009424, grad_norm: 0.8564516971042822, ic: 0.014308613475841017
train 19, step: 500, loss: 0.8646891558611834, grad_norm: 0.03567754209526908, ic: 0.22036983406725072
train 19, step: 1000, loss: 0.9519180147233296, grad_norm: 0.016508743308407868, ic: 0.21450674563769268
train 19, step: 1500, loss: 3.960378876054111, grad_norm: 1.2621068205321455, ic: 0.1448282475266487
train 19, step: 2000, loss: 1.0083573091947116, grad_norm: 0.13099238838999475, ic: 0.21228575051857698
Epoch 19: 2022-04-28 17:07:57.690721: train loss: 1.6187313003025596
Eval step 0: eval loss: 0.8280950934165898
Eval: 2022-04-28 17:08:30.768613: total loss: 1.065749277386276, mse:4.587516546223295, ic :0.19261573542423463, sharpe5:16.9056183886528, irr5:556.8914184570312, ndcg5:0.8511222292080083, pnl5:8.843671798706055 
train 20, step: 0, loss: 2.289949512104743, grad_norm: 1.0609831355636097, ic: 0.04834654364048477
train 20, step: 500, loss: 3.22587109375, grad_norm: 0.8107243325187634, ic: 0.08543426738408981
train 20, step: 1000, loss: 0.9708419799804688, grad_norm: 0.29801957730643847, ic: 0.16941615213984218
train 20, step: 1500, loss: 1.7612005587193722, grad_norm: 4.636693258692407, ic: 0.24537120188626382
train 20, step: 2000, loss: 1.0226726189717579, grad_norm: 0.03340396366905744, ic: 0.038413634003991344
Epoch 20: 2022-04-28 17:16:48.388298: train loss: 1.6179327371662522
Eval step 0: eval loss: 0.8287630071127502
Eval: 2022-04-28 17:17:20.174714: total loss: 1.0663303540702418, mse:4.580515195068548, ic :0.19407318313012092, sharpe5:17.489833459854125, irr5:584.0515747070312, ndcg5:0.8460089977463117, pnl5:6.180999755859375 
train 21, step: 0, loss: 1.01928096395881, grad_norm: 0.3090887195235563, ic: 0.042091949974837076
train 21, step: 500, loss: 0.7653017297255255, grad_norm: 0.016325581807970255, ic: 0.2062663710341362
train 21, step: 1000, loss: 0.9322113572505482, grad_norm: 1.0160498860168987, ic: 0.19091373231118275
train 21, step: 1500, loss: 0.9976076327956264, grad_norm: 0.36464756641621415, ic: 0.3082289192253741
train 21, step: 2000, loss: 0.9362033239894795, grad_norm: 0.17697401681860453, ic: 0.08177928574635213
Epoch 21: 2022-04-28 17:25:41.797622: train loss: 1.6183775186783875
Eval step 0: eval loss: 0.8267604880136986
Eval: 2022-04-28 17:26:15.054224: total loss: 1.0657517067275761, mse:4.588833780658257, ic :0.18949373471958506, sharpe5:17.02243832230568, irr5:563.1764526367188, ndcg5:0.8611988727182582, pnl5:9.844260215759277 
train 22, step: 0, loss: 1.0427539200432556, grad_norm: 0.025912215145817097, ic: 0.21614052282328627
train 22, step: 500, loss: 3.2509521484375, grad_norm: 1.8538450508727613, ic: -0.22904284162437671
train 22, step: 1000, loss: 1.1950395705382948, grad_norm: 0.030238646888217067, ic: 0.45970581705173075
train 22, step: 1500, loss: 0.976245418595679, grad_norm: 0.2305664936074706, ic: 0.0816489863721056
train 22, step: 2000, loss: 1.7182168765943877, grad_norm: 1.192808955907894, ic: 0.19606682309000467
Epoch 22: 2022-04-28 17:34:37.882788: train loss: 1.618024573337331
Eval step 0: eval loss: 0.8251243084826132
Eval: 2022-04-28 17:35:11.114527: total loss: 1.0651457090268424, mse:4.588630901115699, ic :0.19175625668804241, sharpe5:17.041502509117127, irr5:562.3411865234375, ndcg5:0.845120222851799, pnl5:4.574873924255371 
train 23, step: 0, loss: 0.980634230907781, grad_norm: 0.051009464058832214, ic: 0.17252924050402124
train 23, step: 500, loss: 1.420863656084429, grad_norm: 0.6058819256905177, ic: 0.1084870368774597
train 23, step: 1000, loss: 1.6510550944010418, grad_norm: 0.08324411854067149, ic: 0.24479217225169503
train 23, step: 1500, loss: 1.1116031813603222, grad_norm: 1.467614163889401, ic: 0.08606889874197286
train 23, step: 2000, loss: 1.8874190332707852, grad_norm: 4.861691148818006, ic: 0.4482206715310094
Epoch 23: 2022-04-28 17:43:24.598587: train loss: 1.6174146209599574
Eval step 0: eval loss: 0.8306052526508166
Eval: 2022-04-28 17:43:57.310933: total loss: 1.0657538767174928, mse:4.581945042014122, ic :0.1877999368998979, sharpe5:16.277521823644637, irr5:535.7406616210938, ndcg5:0.8462508004129529, pnl5:4.560803413391113 
train 24, step: 0, loss: 2.206713903790292, grad_norm: 0.10960912672418477, ic: 0.12380266927246772
train 24, step: 500, loss: 1.2113172954158562, grad_norm: 3.093071057312054, ic: 0.16770123515304136
train 24, step: 1000, loss: 0.907265137048447, grad_norm: 0.4256024281030293, ic: 0.5174639854403565
train 24, step: 1500, loss: 2.638685561366447, grad_norm: 10.876437514900218, ic: 0.024324184480528865
train 24, step: 2000, loss: 0.9243449589636817, grad_norm: 0.04444533035174621, ic: 0.11460321296391363
Epoch 24: 2022-04-28 17:52:19.344343: train loss: 1.6152322208684144
Eval step 0: eval loss: 0.8226941573984128
Eval: 2022-04-28 17:52:50.984093: total loss: 1.0664418702764964, mse:4.604554773586073, ic :0.19076522867590226, sharpe5:17.63998970746994, irr5:578.1517333984375, ndcg5:0.8407989740858519, pnl5:9.26820182800293 
train 25, step: 0, loss: 0.8299634778821791, grad_norm: 0.18591627315752818, ic: 0.6220970935168035
train 25, step: 500, loss: 0.8682566294137085, grad_norm: 0.0052497861225020955, ic: 0.23767458847939094
train 25, step: 1000, loss: 2.0872059899800077, grad_norm: 0.18189471171659877, ic: 0.2611876297791247
train 25, step: 1500, loss: 1.1404362783057764, grad_norm: 0.4837784967350914, ic: 0.5303472203880575
train 25, step: 2000, loss: 1.034860265494882, grad_norm: 3.266419426612235, ic: 0.6063980040839231
Epoch 25: 2022-04-28 18:01:13.958373: train loss: 1.6182945465032763
Eval step 0: eval loss: 0.8302732574173471
Eval: 2022-04-28 18:01:46.288716: total loss: 1.0653172830269324, mse:4.582172442496622, ic :0.19280116473690195, sharpe5:16.561530343294145, irr5:555.0322875976562, ndcg5:0.8583332066126639, pnl5:5.10825777053833 
train 26, step: 0, loss: 8.329597643769969, grad_norm: 236.7132421555447, ic: -0.20104018809456192
train 26, step: 500, loss: 3.8498078943510556, grad_norm: 0.8192382980802471, ic: 0.36426853777200063
train 26, step: 1000, loss: 1.2704729352678572, grad_norm: 1.414604244148659, ic: 0.01846294135646116
train 26, step: 1500, loss: 0.8481289966332248, grad_norm: 8.916067313938088, ic: 0.2892004404416929
train 26, step: 2000, loss: 0.9463135533966857, grad_norm: 0.13123037470541907, ic: 0.20222041009556047
Epoch 26: 2022-04-28 18:10:13.672919: train loss: 1.624588705653067
Eval step 0: eval loss: 0.8280725830849908
Eval: 2022-04-28 18:10:46.431564: total loss: 1.0669286513819431, mse:4.590377192163992, ic :0.18635392270006312, sharpe5:17.42626914381981, irr5:571.7523803710938, ndcg5:0.836923015631017, pnl5:5.707761764526367 
train 27, step: 0, loss: 0.829280886182598, grad_norm: 0.07083599090432804, ic: 0.08556710383316596
train 27, step: 500, loss: 0.8925734204646298, grad_norm: 1.6718173696707699, ic: 0.30290056496482093
train 27, step: 1000, loss: 0.7602459759280743, grad_norm: 0.5792881155830465, ic: 0.11230022783066237
train 27, step: 1500, loss: 0.6576022043321545, grad_norm: 0.5738414906902679, ic: 0.20504204885543198
train 27, step: 2000, loss: 1.3839306900151862, grad_norm: 0.05044607909274113, ic: 0.018209520517385218
Epoch 27: 2022-04-28 18:19:01.719618: train loss: 1.620078865427541
Eval step 0: eval loss: 0.8299883409345363
Eval: 2022-04-28 18:19:32.378890: total loss: 1.0681484402403902, mse:4.6034558795021985, ic :0.18526995813829963, sharpe5:16.936432864665985, irr5:562.6573486328125, ndcg5:0.8519491500267358, pnl5:4.592935562133789 
train 28, step: 0, loss: 1.5298434106539576, grad_norm: 0.8834204628599887, ic: 0.2725816316058429
train 28, step: 500, loss: 1.3808842121122136, grad_norm: 1.170159869529528, ic: 0.1846657904348975
train 28, step: 1000, loss: 0.9098967265307418, grad_norm: 0.4375220176221336, ic: 0.5775038365852959
train 28, step: 1500, loss: 1.0383876513026904, grad_norm: 0.06489297292985037, ic: 0.011864178000429715
train 28, step: 2000, loss: 1.0461147753007574, grad_norm: 1.082276716045094, ic: 0.07275301318390841
Epoch 28: 2022-04-28 18:27:59.223675: train loss: 1.6185342365681186
Eval step 0: eval loss: 0.8260065848508298
Eval: 2022-04-28 18:28:31.230933: total loss: 1.0753242117195845, mse:4.659277052121238, ic :0.17933457292871907, sharpe5:17.691029967069625, irr5:574.2352905273438, ndcg5:0.8367093638515427, pnl5:8.151426315307617 
train 29, step: 0, loss: 0.9087450838703564, grad_norm: 0.08790794580940425, ic: 0.08948532450380706
train 29, step: 500, loss: 1.1089280061839402, grad_norm: 0.22012962380407522, ic: 0.6137516987081594
train 29, step: 1000, loss: 1.0634434920316098, grad_norm: 0.8826415676251038, ic: -0.014324423956719567
train 29, step: 1500, loss: 2.357389284799473, grad_norm: 1.8246177638893366, ic: -0.08882530147540832
train 29, step: 2000, loss: 4.372862262490354, grad_norm: 20.993746621232162, ic: 0.23281188629295127
Epoch 29: 2022-04-28 18:37:02.857305: train loss: 1.6177537271403235
Eval step 0: eval loss: 0.8283978252189804
Eval: 2022-04-28 18:37:34.356584: total loss: 1.0669193493542106, mse:4.589981216947108, ic :0.18936858830264447, sharpe5:17.761747373342512, irr5:578.59228515625, ndcg5:0.8369959286358237, pnl5:14.569883346557617 
train 30, step: 0, loss: 1.0137905736277857, grad_norm: 0.12982399932923952, ic: 0.5050106895487492
train 30, step: 500, loss: 1.40211382921452, grad_norm: 3.738294826609472, ic: 0.0755796804328528
train 30, step: 1000, loss: 0.9779622395833333, grad_norm: 0.15091311864957221, ic: -0.03537250141780013
train 30, step: 1500, loss: 1.4980331445417112, grad_norm: 2.1395966984783343, ic: 0.14900429513318317
train 30, step: 2000, loss: 1.8434879270873181, grad_norm: 2.2187381252327665, ic: 0.09226215593156616
Epoch 30: 2022-04-28 18:45:50.096566: train loss: 1.6173915406672519
Eval step 0: eval loss: 0.8300504694497497
Eval: 2022-04-28 18:46:23.592291: total loss: 1.0707482393834744, mse:4.673576744277646, ic :0.18861479761085848, sharpe5:17.149956675767896, irr5:575.6749877929688, ndcg5:0.847807542194432, pnl5:7.6910200119018555 
train 31, step: 0, loss: 1.0528429188264785, grad_norm: 0.5184045109715861, ic: 0.3566023640090252
train 31, step: 500, loss: 1.5042144900977366, grad_norm: 0.7634997938028304, ic: 0.018199002491968607
train 31, step: 1000, loss: 4.407039025907494, grad_norm: 11.31595506224522, ic: 0.4723195228711871
train 31, step: 1500, loss: 0.7653159231701482, grad_norm: 0.0320357253191767, ic: 0.7119170422510269
train 31, step: 2000, loss: 1.2172734686668882, grad_norm: 0.9967368691062225, ic: 0.18208464497706237
Epoch 31: 2022-04-28 18:54:38.028654: train loss: 1.6158185799478402
Eval step 0: eval loss: 0.8308569824733272
Eval: 2022-04-28 18:55:11.001137: total loss: 1.066360041732527, mse:4.583496709137689, ic :0.19112024223061327, sharpe5:17.202241773605344, irr5:566.2738037109375, ndcg5:0.8439818840448057, pnl5:5.666811943054199 
train 32, step: 0, loss: 1.136695249324094, grad_norm: 0.03190253235386342, ic: 0.16509897355778305
train 32, step: 500, loss: 1.4955973948080707, grad_norm: 0.46471011874733503, ic: 0.040685604595467405
train 32, step: 1000, loss: 1.034538353918804, grad_norm: 0.17227538010285084, ic: 0.5108154492866093
train 32, step: 1500, loss: 0.9839209841008772, grad_norm: 2.025792381395145, ic: 0.07176265368998253
train 32, step: 2000, loss: 0.9456083755446099, grad_norm: 0.08177352539504712, ic: 0.5463034680637984
Epoch 32: 2022-04-28 19:03:30.093147: train loss: 1.6179168892410742
Eval step 0: eval loss: 0.823368438302654
Eval: 2022-04-28 19:04:02.885970: total loss: 1.0652748384738195, mse:4.605627452595763, ic :0.18954010576672956, sharpe5:17.49451768398285, irr5:572.1093139648438, ndcg5:0.8589371322266037, pnl5:6.062632083892822 
train 33, step: 0, loss: 1.2708081986253599, grad_norm: 0.4552286773257539, ic: 0.199822028578878
train 33, step: 500, loss: 0.9865077334200726, grad_norm: 0.03707939693068387, ic: 0.18275200570896094
train 33, step: 1000, loss: 1.05657690769862, grad_norm: 15.337144549058145, ic: 0.22984482460584188
train 33, step: 1500, loss: 0.8971571373628411, grad_norm: 0.9981634758010527, ic: 0.5538937879704278
train 33, step: 2000, loss: 0.804286176920315, grad_norm: 0.031131851794667098, ic: 0.2857767551762
Epoch 33: 2022-04-28 19:12:11.271609: train loss: 1.6167993974848536
Eval step 0: eval loss: 0.8277627122917215
Eval: 2022-04-28 19:12:44.635341: total loss: 1.065091557144546, mse:4.580523563632047, ic :0.19381434496109062, sharpe5:17.78824275255203, irr5:586.3145751953125, ndcg5:0.8573704365494899, pnl5:6.96934175491333 
train 34, step: 0, loss: 1.0131255658776457, grad_norm: 0.7844146325268346, ic: 0.6022423617337594
train 34, step: 500, loss: 0.7919785619146598, grad_norm: 0.15970037021597822, ic: 0.26891083552225625
train 34, step: 1000, loss: 3.1777276245679724, grad_norm: 3.092133896476386, ic: 0.35213819656272694
train 34, step: 1500, loss: 0.8135621135652562, grad_norm: 0.23734153438676153, ic: 0.6886135273112771
train 34, step: 2000, loss: 6.585560793474052, grad_norm: 14.911415438804385, ic: 0.4139923557656403
Epoch 34: 2022-04-28 19:21:11.255259: train loss: 1.616055816458308
Eval step 0: eval loss: 0.8262617876959298
Eval: 2022-04-28 19:21:44.054716: total loss: 1.0648290007948642, mse:4.586517680288896, ic :0.19424575304243402, sharpe5:17.388172931671143, irr5:572.790771484375, ndcg5:0.8569530487202811, pnl5:5.463253498077393 
train 35, step: 0, loss: 1.2253762637867647, grad_norm: 0.8408032565045169, ic: 0.5557938514975961
train 35, step: 500, loss: 1.1687908032246526, grad_norm: 0.5746174979749263, ic: 0.13631929238472068
train 35, step: 1000, loss: 1.796199596105361, grad_norm: 17.131338559586947, ic: -0.013061447934437188
train 35, step: 1500, loss: 1.6161373281837408, grad_norm: 1.230542683782282, ic: 0.044221991682073136
train 35, step: 2000, loss: 0.7834814551042363, grad_norm: 0.19204504770348435, ic: 0.5617617527786007
Epoch 35: 2022-04-28 19:29:59.768429: train loss: 1.6154648740950959
Eval step 0: eval loss: 0.8297404700260141
Eval: 2022-04-28 19:30:33.629841: total loss: 1.065797368877653, mse:4.5878796132761845, ic :0.1914200851189082, sharpe5:17.238202085494994, irr5:564.5469970703125, ndcg5:0.8443005005620087, pnl5:6.8788323402404785 
train 36, step: 0, loss: 1.8387042993279044, grad_norm: 3.2939491728047834, ic: 0.1064877244744534
train 36, step: 500, loss: 0.8394127042393125, grad_norm: 0.03163525801073625, ic: 0.12957874937012762
train 36, step: 1000, loss: 1.6782860440340908, grad_norm: 5.330908017661736, ic: 0.23966885462854376
train 36, step: 1500, loss: 0.7649620606420565, grad_norm: 0.0997416967180016, ic: 0.3908112023569795
train 36, step: 2000, loss: 1.1342851004021048, grad_norm: 0.5293668228322319, ic: 0.7686858289013281
Epoch 36: 2022-04-28 19:38:58.256478: train loss: 1.613468512543192
Eval step 0: eval loss: 0.8298772042116701
Eval: 2022-04-28 19:39:31.159138: total loss: 1.0653506233192085, mse:4.586711323503086, ic :0.19265381661397388, sharpe5:17.58445440888405, irr5:583.41357421875, ndcg5:0.8608064578720479, pnl5:7.523015022277832 
train 37, step: 0, loss: 2.02228633769621, grad_norm: 2.192633127566176, ic: 0.19472431110090094
train 37, step: 500, loss: 2.339659947555567, grad_norm: 1.1328043250236015, ic: -0.06921361123552613
train 37, step: 1000, loss: 1.0693535884584284, grad_norm: 0.09087662244162316, ic: 0.06274766304785323
train 37, step: 1500, loss: 2.0440206243254515, grad_norm: 1.501772930554325, ic: 0.07205708578514974
train 37, step: 2000, loss: 1.3157918429453903, grad_norm: 0.21508729217091543, ic: 0.20895258452753457
Epoch 37: 2022-04-28 19:47:02.729546: train loss: 1.6105788966014816
Eval step 0: eval loss: 0.8242243454252831
Eval: 2022-04-28 19:47:28.198141: total loss: 1.0687543099824197, mse:4.609244447504997, ic :0.19060509204064063, sharpe5:17.95092116475105, irr5:602.574951171875, ndcg5:0.8396196719035512, pnl5:6.5081915855407715 
train 38, step: 0, loss: 1.315216064453125, grad_norm: 0.8429074844592545, ic: -0.03985433139260963
train 38, step: 500, loss: 0.915053680796682, grad_norm: 0.08923024742424121, ic: 0.26054021356940704
train 38, step: 1000, loss: 0.9095399155447135, grad_norm: 0.1755122764511221, ic: 0.058071498589254256
train 38, step: 1500, loss: 0.9499752707977411, grad_norm: 0.022265478526239717, ic: 0.2085946616564208
train 38, step: 2000, loss: 2.2752239307968276, grad_norm: 1.696731047044687, ic: 0.009743508925793605
Epoch 38: 2022-04-28 19:53:13.872402: train loss: 1.6140895052805895
Eval step 0: eval loss: 0.8258573091947115
Eval: 2022-04-28 19:53:38.886700: total loss: 1.0636581637516076, mse:4.58384840482916, ic :0.19660703938898388, sharpe5:17.582918225526807, irr5:594.551025390625, ndcg5:0.8358107983535575, pnl5:8.30751895904541 
train 39, step: 0, loss: 0.9684487439076835, grad_norm: 0.005530754727556904, ic: 0.06661917484413192
train 39, step: 500, loss: 0.8888206863110107, grad_norm: 0.08257420109109478, ic: 0.27714604304757207
train 39, step: 1000, loss: 0.9932177994692498, grad_norm: 0.8900818662723455, ic: 0.15640571974921508
train 39, step: 1500, loss: 2.0768880466273774, grad_norm: 1.0293409996615455, ic: 0.24649591958835337
train 39, step: 2000, loss: 0.615656125037026, grad_norm: 0.09662868542104563, ic: 0.12874222253001322
Epoch 39: 2022-04-28 19:59:22.928647: train loss: 1.622666077645154
Eval step 0: eval loss: 0.8281287302835221
Eval: 2022-04-28 19:59:48.386750: total loss: 1.0663268494834919, mse:4.6137664443535344, ic :0.18966741646431565, sharpe5:17.092247639894484, irr5:579.91943359375, ndcg5:0.8680532283975085, pnl5:5.075555801391602 
train 40, step: 0, loss: 0.8825089661001462, grad_norm: 0.09101550963161581, ic: 0.23107950619079984
train 40, step: 500, loss: 1.0910565325322044, grad_norm: 0.04056330692030185, ic: 0.46710238368509793
train 40, step: 1000, loss: 1.3284219881383383, grad_norm: 1.8594354479957953, ic: 0.10968901466971845
train 40, step: 1500, loss: 2.700708932892298, grad_norm: 3.510376211099265, ic: -0.07902276202125587
train 40, step: 2000, loss: 1.0507493652714504, grad_norm: 1.9211074862272295, ic: 0.10052167883419937
Epoch 40: 2022-04-28 20:05:34.793941: train loss: 1.6164299539590206
Eval step 0: eval loss: 0.8318076259343716
Eval: 2022-04-28 20:06:00.248099: total loss: 1.0682543017188744, mse:4.588357415973377, ic :0.1945184101032918, sharpe5:17.521565483808516, irr5:587.9280395507812, ndcg5:0.8604749965590264, pnl5:5.157375812530518 
train 41, step: 0, loss: 1.6609772079292384, grad_norm: 1.0229936469141334, ic: 0.41357618981334815
train 41, step: 500, loss: 1.2176257761139517, grad_norm: 8.81252163837247, ic: 0.24479495621862052
train 41, step: 1000, loss: 1.051163488489981, grad_norm: 1.4051606536098278, ic: 0.2284563682872487
train 41, step: 1500, loss: 3.310048065309703, grad_norm: 18.4058329309298, ic: 0.009160312903448773
train 41, step: 2000, loss: 1.0504259020314857, grad_norm: 0.4558272380115036, ic: 0.14359824799814516
Epoch 41: 2022-04-28 20:11:48.883658: train loss: 1.6135683790108588
Eval step 0: eval loss: 0.8283569850459365
Eval: 2022-04-28 20:12:14.360164: total loss: 1.0652219983926037, mse:4.594015837131548, ic :0.19397621453318034, sharpe5:17.591196967363356, irr5:588.2533569335938, ndcg5:0.8471837600630684, pnl5:8.468437194824219 
train 42, step: 0, loss: 2.074391799868295, grad_norm: 0.6083661451122746, ic: 0.09125649303886554
train 42, step: 500, loss: 1.4917465365297082, grad_norm: 2.316116927340469, ic: 0.2159546784657459
train 42, step: 1000, loss: 3.257516341854588, grad_norm: 7.970624817965492, ic: 0.10195655725969655
train 42, step: 1500, loss: 1.200837696697695, grad_norm: 0.0798928675799439, ic: 0.563155493212674
train 42, step: 2000, loss: 1.1788534704660405, grad_norm: 0.9828996071070981, ic: 0.47569026397358594
Epoch 42: 2022-04-28 20:18:04.705476: train loss: 1.6140771288676532
Eval step 0: eval loss: 0.8321467601587197
Eval: 2022-04-28 20:18:30.319152: total loss: 1.072805800523742, mse:4.631272649067397, ic :0.1716515863810744, sharpe5:16.480494779348373, irr5:538.5936279296875, ndcg5:0.8659863629261783, pnl5:5.490304470062256 
train 43, step: 0, loss: 0.832972176467316, grad_norm: 0.6369795207929814, ic: 0.05979661435823383
train 43, step: 500, loss: 0.9239118686837716, grad_norm: 0.09976344734662068, ic: 0.2900172628230733
train 43, step: 1000, loss: 1.7141008030431106, grad_norm: 6.633009071342683, ic: -0.043982573925599094
train 43, step: 1500, loss: 1.3952068279741148, grad_norm: 0.4124746705383104, ic: 0.13716845535098418
train 43, step: 2000, loss: 1.7042618879183071, grad_norm: 7.705369543923777, ic: -0.1002910926307871
Epoch 43: 2022-04-28 20:24:16.860534: train loss: 1.623086140532406
Eval step 0: eval loss: 0.8246242575449486
Eval: 2022-04-28 20:24:42.400279: total loss: 1.0667993969203777, mse:4.601188787467293, ic :0.1850466466101491, sharpe5:16.684029614925382, irr5:556.4234008789062, ndcg5:0.8582505148327895, pnl5:4.105655670166016 
train 44, step: 0, loss: 1.0470183281844105, grad_norm: 12.927976893336997, ic: 0.012221315493793636
train 44, step: 500, loss: 2.0804264041633935, grad_norm: 1.8664542419189494, ic: 0.05592478575505212
train 44, step: 1000, loss: 1.7987526069253177, grad_norm: 1.4122514416592717, ic: 0.15338447805083877
train 44, step: 1500, loss: 1.0273700016801075, grad_norm: 0.30757215492302853, ic: 0.1594074552571479
train 44, step: 2000, loss: 0.9622012407351763, grad_norm: 1.3875540749102884, ic: 0.6734081237018088
Epoch 44: 2022-04-28 20:30:27.672942: train loss: 1.62019845955507
Eval step 0: eval loss: 0.8235367512677818
Eval: 2022-04-28 20:30:53.430662: total loss: 1.0680164305341886, mse:4.630789217170192, ic :0.17000524579015935, sharpe5:14.491895018219948, irr5:502.73248291015625, ndcg5:0.8567271896542945, pnl5:3.399486780166626 
train 45, step: 0, loss: 1.6557462993421053, grad_norm: 529.8841090865931, ic: 0.0566146405230115
train 45, step: 500, loss: 0.9618952770398055, grad_norm: 0.2500149100144147, ic: 0.4796118266187884
train 45, step: 1000, loss: 1.5134083888421157, grad_norm: 2.1060251723328243, ic: 0.8996587677101443
train 45, step: 1500, loss: 1.0213558168949306, grad_norm: 0.4232719281636625, ic: 0.12693664436365887
train 45, step: 2000, loss: 1.735108762254902, grad_norm: 0.3019307927041928, ic: 0.4323360734943853
Epoch 45: 2022-04-28 20:36:42.333629: train loss: 1.6179330499319098
Eval step 0: eval loss: 0.8332739489347338
Eval: 2022-04-28 20:37:08.049903: total loss: 1.0745659603464057, mse:4.679135153337202, ic :0.18696479115841744, sharpe5:16.81445196032524, irr5:547.961669921875, ndcg5:0.8487512344795297, pnl5:3.9979248046875 
train 46, step: 0, loss: 2.0987982233132008, grad_norm: 1.1716041628905802, ic: 0.038920630620050356
train 46, step: 500, loss: 2.004168359375, grad_norm: 0.7292001281573921, ic: 0.13501011365308638
train 46, step: 1000, loss: 0.8699128725459978, grad_norm: 0.3948491354364498, ic: 0.08395021825550643
train 46, step: 1500, loss: 1.2509726247479838, grad_norm: 0.5866677404969322, ic: 0.971845920680845
train 46, step: 2000, loss: 2.781579940239527, grad_norm: 244.4541133924135, ic: 0.303509754452137
Epoch 46: 2022-04-28 20:42:55.571492: train loss: 1.6236653870798687
Eval step 0: eval loss: 0.8254457560178476
Eval: 2022-04-28 20:43:21.022050: total loss: 1.0684755259027698, mse:4.608358911333995, ic :0.1791975613361564, sharpe5:16.01591093301773, irr5:519.7099609375, ndcg5:0.856659372060705, pnl5:4.596864223480225 
train 47, step: 0, loss: 1.0754568715106583, grad_norm: 0.20615580780320655, ic: 0.21168824485342233
train 47, step: 500, loss: 1.5617258923412858, grad_norm: 0.7091020414954872, ic: 0.1259394118081785
train 47, step: 1000, loss: 2.8781925499854766, grad_norm: 12.992243588089172, ic: 0.5209863501239442
train 47, step: 1500, loss: 1.666791962414253, grad_norm: 1.1052893145358902, ic: -0.027934358695704032
train 47, step: 2000, loss: 1.2462502553610342, grad_norm: 0.9399104258414758, ic: -0.0016492178369843097
Epoch 47: 2022-04-28 20:49:04.872834: train loss: 1.6211043093176147
Eval step 0: eval loss: 0.8270154335978661
Eval: 2022-04-28 20:49:30.719978: total loss: 1.068279558180113, mse:4.620716406496966, ic :0.17241407243046017, sharpe5:15.372782662510872, irr5:499.16424560546875, ndcg5:0.8580864775905264, pnl5:4.697183609008789 
train 48, step: 0, loss: 1.0895412071078432, grad_norm: 0.49176168365837775, ic: 0.1225800689892112
train 48, step: 500, loss: 1.2865969251498455, grad_norm: 106.8282842281523, ic: 0.2032439365985803
train 48, step: 1000, loss: 1.55688902281558, grad_norm: 0.843467279013391, ic: 0.1323411537651192
train 48, step: 1500, loss: 1.1345153307719078, grad_norm: 1.822616902196384, ic: 0.5200522966205474
train 48, step: 2000, loss: 2.4092757078477787, grad_norm: 2.161952378907735, ic: 0.545071090750139
Epoch 48: 2022-04-28 20:55:15.926839: train loss: 1.6201656933237272
Eval step 0: eval loss: 0.8396856631569415
Eval: 2022-04-28 20:55:40.548503: total loss: 1.0737275135203845, mse:4.612321026297554, ic :0.18036831637581885, sharpe5:17.392249871492385, irr5:554.7797241210938, ndcg5:0.8454460714199216, pnl5:7.0056633949279785 
train 49, step: 0, loss: 0.9065333912207646, grad_norm: 0.7131221616103147, ic: 0.10130884250286258
train 49, step: 500, loss: 1.4974462972073082, grad_norm: 0.020789474151106466, ic: 0.03382243912921931
train 49, step: 1000, loss: 1.7323686599731447, grad_norm: 0.3232752855470406, ic: 0.11056378908627978
train 49, step: 1500, loss: 1.6000044894858625, grad_norm: 0.17562691446221573, ic: 0.45982785665676446
train 49, step: 2000, loss: 0.9645488969431627, grad_norm: 0.5651989766636778, ic: 0.5964073797348766
Epoch 49: 2022-04-28 21:01:29.281633: train loss: 1.6194852048022752
Eval step 0: eval loss: 0.8264824532608337
Eval: 2022-04-28 21:01:55.084916: total loss: 1.0653495968624243, mse:4.598195242488673, ic :0.18813483028033917, sharpe5:17.069303265810014, irr5:566.0977172851562, ndcg5:0.8535759389963359, pnl5:6.951484680175781 
train 50, step: 0, loss: 1.3817997270403288, grad_norm: 4.788345199881021, ic: 0.15040156114175174
train 50, step: 500, loss: 2.87098451395751, grad_norm: 1463.024732376038, ic: 0.3209391905438435
train 50, step: 1000, loss: 0.8641468732190349, grad_norm: 0.31578972941221534, ic: 0.1072550392017858
train 50, step: 1500, loss: 1.4507098120850173, grad_norm: 4.1644914291812825, ic: 0.36375643365238236
train 50, step: 2000, loss: 9.58039725304399, grad_norm: 29.20955628493761, ic: 0.06602871939522459
Epoch 50: 2022-04-28 21:07:36.838575: train loss: 1.6170306292720833
Eval step 0: eval loss: 0.8250358107218124
Eval: 2022-04-28 21:08:02.615632: total loss: 1.0700428097220036, mse:4.6307145555718465, ic :0.1727762261434865, sharpe5:15.856418398618697, irr5:517.72900390625, ndcg5:0.8445068698783172, pnl5:3.7339820861816406 
train 51, step: 0, loss: 3.3760100285146573, grad_norm: 2.9725483487870106, ic: 0.010499635426556193
train 51, step: 500, loss: 1.43872014615019, grad_norm: 0.6293990894856728, ic: 0.09768674207013542
train 51, step: 1000, loss: 1.5133495784221473, grad_norm: 0.40075876483818224, ic: 0.8793795423416233
train 51, step: 1500, loss: 1.0183747034200432, grad_norm: 1.8776125088154614, ic: 0.1726806914118818
train 51, step: 2000, loss: 2.3557163349325725, grad_norm: 0.29621858424439756, ic: 0.09733016519803037
Epoch 51: 2022-04-28 21:13:44.442548: train loss: 1.6209975027385137
Eval step 0: eval loss: 0.8313468716041557
Eval: 2022-04-28 21:14:10.089724: total loss: 1.0705154476447016, mse:4.622967889146673, ic :0.1806597587720207, sharpe5:17.349034309387207, irr5:562.6527709960938, ndcg5:0.8535448564213043, pnl5:7.336370944976807 
train 52, step: 0, loss: 1.2194725941803495, grad_norm: 5.998974071968903, ic: 0.11839896877045052
train 52, step: 500, loss: 1.660087060901691, grad_norm: 11.024630845525188, ic: 0.16953134003087797
train 52, step: 1000, loss: 1.1896713623205304, grad_norm: 0.2750487788001153, ic: 0.586055323197941
train 52, step: 1500, loss: 1.0363835110082502, grad_norm: 0.20854608439225863, ic: -0.0011900979362123188
train 52, step: 2000, loss: 1.3877435469699848, grad_norm: 0.8238823746751807, ic: 0.15452827033932817
Epoch 52: 2022-04-28 21:19:58.246310: train loss: 1.6227404828153607
Eval step 0: eval loss: 0.8273309641316516
Eval: 2022-04-28 21:20:23.694914: total loss: 1.0663031873351283, mse:4.603169178233641, ic :0.18217767449344574, sharpe5:16.066050671339035, irr5:510.77716064453125, ndcg5:0.8349085909590035, pnl5:7.5915913581848145 
train 53, step: 0, loss: 3.1173108962029326, grad_norm: 14.26939426511818, ic: 0.06662974407669374
train 53, step: 500, loss: 1.0735694761655468, grad_norm: 0.23428862263951206, ic: 0.4862283240852184
train 53, step: 1000, loss: 1.2432844766310855, grad_norm: 0.10271376608020828, ic: 0.1765177419515657
train 53, step: 1500, loss: 1.295564718236347, grad_norm: 2.2722358191921233, ic: 0.1727063766175799
train 53, step: 2000, loss: 3.1102092271710267, grad_norm: 23.513143786730577, ic: 0.15802804682439614
Epoch 53: 2022-04-28 21:26:13.264281: train loss: 1.615866359475002
Eval step 0: eval loss: 0.8248027323169125
Eval: 2022-04-28 21:26:38.609593: total loss: 1.067508874274042, mse:4.598720552127947, ic :0.18400994737005538, sharpe5:16.987408801317216, irr5:557.1616821289062, ndcg5:0.857683315487834, pnl5:5.41820764541626 
train 54, step: 0, loss: 2.054566204737103, grad_norm: 2.448054357010738, ic: -0.0018551493090927362
train 54, step: 500, loss: 0.8671581167035398, grad_norm: 0.21747675740054168, ic: 0.540561509633677
train 54, step: 1000, loss: 2.218178111561767, grad_norm: 0.20124762430402882, ic: 0.17432913474907163
train 54, step: 1500, loss: 1.881862979597309, grad_norm: 9.134051128516617, ic: 0.1661309445796896
train 54, step: 2000, loss: 2.14966217289126, grad_norm: 3.7327837612970685, ic: 5.705810826604589e-05
Epoch 54: 2022-04-28 21:32:23.175169: train loss: 1.6151588383502302
Eval step 0: eval loss: 0.8244824424558745
Eval: 2022-04-28 21:32:48.327915: total loss: 1.0720611062709071, mse:4.640333999092592, ic :0.17603144571442497, sharpe5:17.882205337285995, irr5:562.9075927734375, ndcg5:0.8571388402498248, pnl5:7.056576728820801 
train 55, step: 0, loss: 1.7989369726255888, grad_norm: 1.0105656647853998, ic: 0.13334790762694862
train 55, step: 500, loss: 0.8802917568037124, grad_norm: 1.5703493631597258, ic: 0.31706140132622473
train 55, step: 1000, loss: 1.0196975632593699, grad_norm: 2.2099984303659275, ic: 0.07537821820606597
train 55, step: 1500, loss: 1.089024204706225, grad_norm: 0.9481158913159321, ic: 0.5607153281176369
train 55, step: 2000, loss: 2.2509154338749044, grad_norm: 1.1025331477464146, ic: 0.08734495826465034
Epoch 55: 2022-04-28 21:38:29.673465: train loss: 1.6183033712483945
Eval step 0: eval loss: 0.826273300122662
Eval: 2022-04-28 21:38:55.199207: total loss: 1.0681506613086875, mse:4.596694200662343, ic :0.19060431003531728, sharpe5:17.49428309440613, irr5:584.8436889648438, ndcg5:0.840859313540138, pnl5:8.069907188415527 
train 56, step: 0, loss: 1.0133594628347302, grad_norm: 0.9809295359216579, ic: 0.06680147708783186
train 56, step: 500, loss: 0.894626851992314, grad_norm: 17.038047282641642, ic: 0.02255889385405776
train 56, step: 1000, loss: 5.033880368383228, grad_norm: 17.112922420820976, ic: -0.005174306970856791
train 56, step: 1500, loss: 1.1828142624589828, grad_norm: 0.1118974803566422, ic: 0.06062808355656844
train 56, step: 2000, loss: 0.9832876832420417, grad_norm: 0.39329565572895236, ic: 0.4308670282371504
Epoch 56: 2022-04-28 21:44:44.666647: train loss: 1.6170251287484076
Eval step 0: eval loss: 0.8272434954145811
Eval: 2022-04-28 21:45:10.299347: total loss: 1.0660425743841038, mse:4.596411510230509, ic :0.18794910801182857, sharpe5:17.036555317640303, irr5:550.5819702148438, ndcg5:0.8470352893786336, pnl5:5.738386631011963 
train 57, step: 0, loss: 1.0219999098206458, grad_norm: 0.3389420108114146, ic: 0.14632538363219624
train 57, step: 500, loss: 0.8747722842504899, grad_norm: 0.42060544522858356, ic: 0.5777915106744963
train 57, step: 1000, loss: 2.260330166311991, grad_norm: 1.0855658090841034, ic: 0.09059272165024482
train 57, step: 1500, loss: 1.1282990435663245, grad_norm: 0.20398243178856953, ic: 0.0602448550731548
train 57, step: 2000, loss: 1.0808643041264396, grad_norm: 0.8218345061844337, ic: 0.6046985816678255
Epoch 57: 2022-04-28 21:50:56.570916: train loss: 1.6136003911257997
Eval step 0: eval loss: 0.8267178470141266
Eval: 2022-04-28 21:51:21.899125: total loss: 1.065655085220931, mse:4.595154877752118, ic :0.18751585800566822, sharpe5:17.72578516960144, irr5:568.8898315429688, ndcg5:0.8528533079522449, pnl5:7.347562789916992 
train 58, step: 0, loss: 1.5189016845511942, grad_norm: 0.5477911582364543, ic: 0.15420731574197577
train 58, step: 500, loss: 1.5697017485347564, grad_norm: 25.68300378621066, ic: 0.10729990956016457
train 58, step: 1000, loss: 1.5020089285714286, grad_norm: 4.028166846353525, ic: 0.08766286627129763
train 58, step: 1500, loss: 2.2660844781425564, grad_norm: 1.1566694038807768, ic: 0.40879320046920253
train 58, step: 2000, loss: 1.0120355304335995, grad_norm: 0.39099171496270596, ic: 0.26345002994247346
Epoch 58: 2022-04-28 21:57:15.484136: train loss: 1.615016854796121
Eval step 0: eval loss: 0.826536285110972
Eval: 2022-04-28 21:57:40.866664: total loss: 1.065959186684617, mse:4.5850762886029415, ic :0.19328177642130737, sharpe5:17.509232504367827, irr5:588.6182861328125, ndcg5:0.8304612128387606, pnl5:5.0282182693481445 
train 59, step: 0, loss: 1.1856418600863141, grad_norm: 0.28897722961206135, ic: 0.03919108796695229
train 59, step: 500, loss: 0.7373676572095385, grad_norm: 0.26496721174998317, ic: 0.1731058155891007
train 59, step: 1000, loss: 5.120656887123911, grad_norm: 31.93840700252899, ic: -0.14918624149075593
train 59, step: 1500, loss: 1.37488464647503, grad_norm: 1.1753910549868223, ic: 0.11751730001952455
train 59, step: 2000, loss: 0.993297980813419, grad_norm: 0.09845661117803974, ic: 0.055100870581478986
Epoch 59: 2022-04-28 22:03:24.932124: train loss: 1.6141942334521597
Eval step 0: eval loss: 0.8230940695180782
Eval: 2022-04-28 22:03:50.605116: total loss: 1.066427102193338, mse:4.5948029117601505, ic :0.18538575781700858, sharpe5:16.293173110485075, irr5:527.050048828125, ndcg5:0.8485348749901972, pnl5:4.759097099304199 
