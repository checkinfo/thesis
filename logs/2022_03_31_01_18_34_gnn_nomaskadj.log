Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjTimeDataset', dout=0.3, epochs=20, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=3, mask_adj=False, mask_type='soft', model_type='GNNModel', num_days=1, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
20838
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
    (1): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.8711797440440289, grad_norm: 0.00031956484387691387, ic: 0.07367429081551674
train 0, step: 500, loss: 0.9887768848832832, grad_norm: 0.1385080930139056, ic: 0.05146675725241032
train 0, step: 1000, loss: 1.0685467454033324, grad_norm: 0.021659186609947607, ic: -0.011561406975857048
train 0, step: 1500, loss: 0.9562451655321782, grad_norm: 0.10671323633251462, ic: 0.20349839429488475
train 0, step: 2000, loss: 1.0721424171656597, grad_norm: 0.33318751348565073, ic: 0.12453069505098399
Epoch 0: train loss: 1.6475303997158688
Eval step 0: eval loss: 1.0131017635721102
Eval: total loss: 1.091426325295144, mse:4.888495908680578, ic :0.011385765826239144, sharpe5:6.255923212766647, irr5:185.82435607910156, ndcg5:0.8562884456698534 
train 1, step: 0, loss: 0.9413731638953281, grad_norm: 0.07678923230861268, ic: 0.12001501717377616
train 1, step: 500, loss: 1.6817684376185509, grad_norm: 0.11000988132183795, ic: -0.061968779003093825
train 1, step: 1000, loss: 1.1102656885282716, grad_norm: 0.40957359964532225, ic: 0.08093434477229076
train 1, step: 1500, loss: 0.8964011907969006, grad_norm: 0.13636518945114232, ic: -0.09014686944502098
train 1, step: 2000, loss: 4.70510141284695, grad_norm: 1.62371697295488, ic: 0.058964727901507225
Epoch 1: train loss: 1.6455444647402286
Eval step 0: eval loss: 1.001417018414297
Eval: total loss: 1.089213859331328, mse:4.877737022723544, ic :0.05004524719591932, sharpe5:5.2896922588348385, irr5:121.3299331665039, ndcg5:0.8493949068987132 
train 2, step: 0, loss: 0.9630292188985266, grad_norm: 0.12019989778798715, ic: 0.01011650153543381
train 2, step: 500, loss: 1.308419038643648, grad_norm: 0.09682109801640251, ic: 0.19706116881497027
train 2, step: 1000, loss: 1.069362387653449, grad_norm: 0.011968857198043114, ic: 0.001973935929077549
train 2, step: 1500, loss: 0.9691971793026737, grad_norm: 0.33337946043263106, ic: -0.028371344043577017
train 2, step: 2000, loss: 1.718193432800752, grad_norm: 0.0867914746995805, ic: 0.11280604340973362
Epoch 2: train loss: 1.644483437530383
Eval step 0: eval loss: 0.9937582151584715
Eval: total loss: 1.0896583982820172, mse:4.894373528142066, ic :0.046823131596404916, sharpe5:4.5797934570908545, irr5:86.13580322265625, ndcg5:0.8383443325021672 
train 3, step: 0, loss: 0.8435865385791895, grad_norm: 0.0656649645398334, ic: 0.2653241050737786
train 3, step: 500, loss: 1.222061180957183, grad_norm: 0.004382983897689727, ic: 0.10217191577676642
train 3, step: 1000, loss: 0.9040515921966374, grad_norm: 0.03148368244617933, ic: 0.22482142801373217
train 3, step: 1500, loss: 4.379786573720398, grad_norm: 0.7479584346020799, ic: 0.09849504255817582
train 3, step: 2000, loss: 1.664470841399336, grad_norm: 0.3976865876369504, ic: -0.027327562693359682
Epoch 3: train loss: 1.6435989368146833
Eval step 0: eval loss: 1.0006971956868747
Eval: total loss: 1.0905064121215273, mse:4.878582673740813, ic :0.05156449838049333, sharpe5:6.867853907346725, irr5:181.6890106201172, ndcg5:0.8612665302748465 
train 4, step: 0, loss: 1.301009783461758, grad_norm: 0.2199060923176002, ic: 0.030740947764692324
train 4, step: 500, loss: 0.8621437440327555, grad_norm: 0.04356410942807228, ic: -0.019032824201782043
train 4, step: 1000, loss: 1.9366841376582278, grad_norm: 0.18729129660270624, ic: -0.05222219234606437
train 4, step: 1500, loss: 1.1698450644482865, grad_norm: 0.014940540445631433, ic: 0.19964424323112867
train 4, step: 2000, loss: 6.633404467479292, grad_norm: 1.0736845395048338, ic: -0.03203984124787812
Epoch 4: train loss: 1.6440306757198837
Eval step 0: eval loss: 0.998736871173973
Eval: total loss: 1.0891321014509203, mse:4.876635937421591, ic :0.05333047802305255, sharpe5:4.442360856235027, irr5:73.05699920654297, ndcg5:0.8520695400924828 
train 5, step: 0, loss: 3.7562861016456117, grad_norm: 0.6411458819310446, ic: 0.1416804466327488
train 5, step: 500, loss: 3.4369039653260027, grad_norm: 0.7372033989491078, ic: -0.04903932300730183
train 5, step: 1000, loss: 1.042062512036944, grad_norm: 0.04608758166644386, ic: 0.06483148637511109
train 5, step: 1500, loss: 1.5291253657982589, grad_norm: 1.1099738142824955, ic: -0.04508449699356975
train 5, step: 2000, loss: 1.7971612023057115, grad_norm: 0.454724208102522, ic: 0.024146413112636426
Epoch 5: train loss: 1.6445466108080848
Eval step 0: eval loss: 1.0005480629196286
Eval: total loss: 1.0902664027108648, mse:4.86977928469462, ic :0.06766008491881317, sharpe5:7.1613136437535285, irr5:214.20578002929688, ndcg5:0.8427549920910405 
train 6, step: 0, loss: 1.1313886318987574, grad_norm: 0.0262841187714504, ic: 0.043644760242220054
train 6, step: 500, loss: 1.840317943728494, grad_norm: 0.10823867478065831, ic: 0.19207452156615196
train 6, step: 1000, loss: 0.8754242664247047, grad_norm: 0.188905170639133, ic: 0.0725352976827161
train 6, step: 1500, loss: 0.8503066460959383, grad_norm: 0.0029062016053077212, ic: 0.08670294949945105
train 6, step: 2000, loss: 1.9410061940225856, grad_norm: 0.24454778486334872, ic: 0.05543407769182808
Epoch 6: train loss: 1.639763009506541
Eval step 0: eval loss: 0.9956983553227027
Eval: total loss: 1.0847130848096864, mse:4.728691661191749, ic :0.12340247959751341, sharpe5:8.431516406536101, irr5:241.7630157470703, ndcg5:0.8512710431671694 
train 7, step: 0, loss: 0.8199196047124393, grad_norm: 0.07715872455876621, ic: 0.5076072288041769
train 7, step: 500, loss: 1.1388361435317178, grad_norm: 0.019569954354480196, ic: 0.02088090107854225
train 7, step: 1000, loss: 1.0979723926743745, grad_norm: 0.025496451810559116, ic: -0.06574460896513654
train 7, step: 1500, loss: 1.109698194327961, grad_norm: 0.07759599744139142, ic: 0.43822122434226435
train 7, step: 2000, loss: 0.7088180798545085, grad_norm: 0.012125514415465341, ic: -0.03629875161737309
Epoch 7: train loss: 1.6374691956032572
Eval step 0: eval loss: 1.0033142186265798
Eval: total loss: 1.0981064666684215, mse:4.81994449605748, ic :0.10440599780383736, sharpe5:8.126062805056572, irr5:232.22434997558594, ndcg5:0.8522862563519056 
train 8, step: 0, loss: 2.0174701372275385, grad_norm: 0.3506141167704341, ic: -0.09508621355409214
train 8, step: 500, loss: 0.9458594996818928, grad_norm: 0.04817102331157946, ic: 0.4852686881462102
train 8, step: 1000, loss: 0.7334460583361951, grad_norm: 0.035154621161950575, ic: 0.09995563107768113
train 8, step: 1500, loss: 0.8501203069950087, grad_norm: 0.038768535183509506, ic: 0.15862801824890227
train 8, step: 2000, loss: 1.4669396579499707, grad_norm: 0.8237642828837619, ic: -0.0027966224606582706
Epoch 8: train loss: 1.6374133809423221
Eval step 0: eval loss: 1.0014058334567535
Eval: total loss: 1.0861957270073657, mse:4.714696160658549, ic :0.12351664038488842, sharpe5:8.37954819381237, irr5:237.3428955078125, ndcg5:0.8641725552924293 
train 9, step: 0, loss: 0.7851904095936213, grad_norm: 0.01408279825657179, ic: 0.5124486775334082
train 9, step: 500, loss: 1.770875513311127, grad_norm: 0.06265237273852668, ic: -0.10284863025567409
train 9, step: 1000, loss: 0.7820404862935564, grad_norm: 0.00878583229483356, ic: 0.11391658789939291
train 9, step: 1500, loss: 1.0647731862297751, grad_norm: 0.05010375163179079, ic: -0.08791636224855633
train 9, step: 2000, loss: 4.488500306372549, grad_norm: 2.1729716914166755, ic: 0.24032176391409127
Epoch 9: train loss: 1.6370195054258279
Eval step 0: eval loss: 1.0102052452565493
Eval: total loss: 1.1040773192101256, mse:4.850258439444276, ic :0.11063054059982733, sharpe5:8.312238011956214, irr5:233.7632598876953, ndcg5:0.8564446020892813 
train 10, step: 0, loss: 0.7995224834794357, grad_norm: 0.0007532273019474061, ic: 0.09169499082228541
train 10, step: 500, loss: 0.7927413518249753, grad_norm: 0.07595249233986497, ic: -0.030800681369146883
train 10, step: 1000, loss: 1.0183356768924121, grad_norm: 0.3871557112483488, ic: -0.04159203657018561
train 10, step: 1500, loss: 1.3344947876969004, grad_norm: 0.14310664214335558, ic: -0.10511754911112967
train 10, step: 2000, loss: 1.2628788136585798, grad_norm: 0.4392594280049548, ic: -0.18712561901369926
Epoch 10: train loss: 1.635781800513044
Eval step 0: eval loss: 0.9996338533438651
Eval: total loss: 1.0842544484171537, mse:4.7327094288514795, ic :0.13004055746251753, sharpe5:10.53006300508976, irr5:322.09124755859375, ndcg5:0.8375246255974257 
train 11, step: 0, loss: 1.5406071689225054, grad_norm: 0.041912527326156176, ic: 0.17886467251482296
train 11, step: 500, loss: 1.3217725877637987, grad_norm: 0.25324929537506297, ic: 0.15899903645824792
train 11, step: 1000, loss: 2.2508333958410267, grad_norm: 0.7921555433783152, ic: 0.2653768029845459
train 11, step: 1500, loss: 1.3760284989423852, grad_norm: 1.0290758201410113, ic: 0.04401689846286881
train 11, step: 2000, loss: 3.733978532318376, grad_norm: 2.7145745777123698, ic: 0.22411843127496978
Epoch 11: train loss: 1.6296989826854105
Eval step 0: eval loss: 1.0131428393644681
Eval: total loss: 1.0854762722241578, mse:4.688245101146212, ic :0.15732326394994184, sharpe5:14.284986065626144, irr5:468.06402587890625, ndcg5:0.8446778136149314 
train 12, step: 0, loss: 1.0006442174642436, grad_norm: 0.06570244350597432, ic: 0.08598982749610662
train 12, step: 500, loss: 1.2121271177779795, grad_norm: 0.1345194778350155, ic: 0.10600866737376219
train 12, step: 1000, loss: 1.1995917667556346, grad_norm: 0.9900259681980204, ic: 0.6079351691780571
train 12, step: 1500, loss: 0.7642026047381415, grad_norm: 0.08801896894618715, ic: 0.6040172845170771
train 12, step: 2000, loss: 2.7664194709042773, grad_norm: 0.7100078651954568, ic: -0.06359333190028346
Epoch 12: train loss: 1.6293520681486044
Eval step 0: eval loss: 1.0026737191038047
Eval: total loss: 1.082511559078855, mse:4.684424249286734, ic :0.1587769842645593, sharpe5:14.39784635424614, irr5:465.25054931640625, ndcg5:0.845184355420435 
train 13, step: 0, loss: 1.3526164638440603, grad_norm: 0.6079540739764555, ic: 0.13715845360052709
train 13, step: 500, loss: 1.4426093950663526, grad_norm: 0.18345839053888208, ic: -0.09551142861984518
train 13, step: 1000, loss: 2.205907160816913, grad_norm: 0.8499771768898924, ic: 0.04866810814845308
train 13, step: 1500, loss: 1.5755743374482045, grad_norm: 0.8847776631430353, ic: -0.10716171903639944
train 13, step: 2000, loss: 0.7167099890850179, grad_norm: 0.02655993048890817, ic: -0.05694322087608718
Epoch 13: train loss: 1.6275862213112662
Eval step 0: eval loss: 1.006791454795287
Eval: total loss: 1.0832773936197262, mse:4.686483221903308, ic :0.15755623724133655, sharpe5:14.250977198481559, irr5:452.1287841796875, ndcg5:0.8552502047493031 
train 14, step: 0, loss: 2.016678533848288, grad_norm: 1.432198013240421, ic: 0.18760555654815275
train 14, step: 500, loss: 2.7604159200592506, grad_norm: 1.8936731336200445, ic: -0.11837358680827316
train 14, step: 1000, loss: 1.0622729240096307, grad_norm: 0.09391928542293825, ic: 0.16638712596358377
train 14, step: 1500, loss: 3.0126168387276784, grad_norm: 0.8419145914599138, ic: 0.1583460929893607
train 14, step: 2000, loss: 0.8026475280295802, grad_norm: 0.031631895316357524, ic: -0.03672440811226636
Epoch 14: train loss: 1.628444600858761
Eval step 0: eval loss: 1.003711991714389
Eval: total loss: 1.0819748014914272, mse:4.679579927826682, ic :0.16651610434918526, sharpe5:14.852288931012152, irr5:492.5583801269531, ndcg5:0.8531480982676591 
train 15, step: 0, loss: 1.2377466020662367, grad_norm: 0.2560160758014404, ic: 0.5302956330463935
train 15, step: 500, loss: 1.8025375658524905, grad_norm: 1.1547795778135332, ic: -0.012116667861568507
train 15, step: 1000, loss: 1.744449522437119, grad_norm: 0.5101708983223235, ic: 0.18466200582303363
train 15, step: 1500, loss: 1.45459913493168, grad_norm: 0.4697378702375252, ic: -0.09270365726151779
train 15, step: 2000, loss: 3.118384472504374, grad_norm: 0.05015145774355134, ic: 0.1931421665898046
Epoch 15: train loss: 1.6260507464328013
Eval step 0: eval loss: 1.001911727800816
Eval: total loss: 1.0833366553188166, mse:4.689051771383339, ic :0.16054521404734182, sharpe5:14.510519349575041, irr5:477.1458740234375, ndcg5:0.8513549313253493 
train 16, step: 0, loss: 1.7988866128895664, grad_norm: 0.47504411034579014, ic: 0.1472247711847474
train 16, step: 500, loss: 1.3427738177813473, grad_norm: 1.0817807610005368, ic: 0.34961986703429426
train 16, step: 1000, loss: 0.91482349565834, grad_norm: 0.01615489587062853, ic: 0.016148937607249773
train 16, step: 1500, loss: 1.3812003269759372, grad_norm: 1.2224082076515903, ic: 0.22966148151144394
train 16, step: 2000, loss: 1.2538508478386265, grad_norm: 0.1011481818675679, ic: 0.22800623550102594
Epoch 16: train loss: 1.6260927169965864
Eval step 0: eval loss: 1.004928966520208
Eval: total loss: 1.0827220624134768, mse:4.681195744354402, ic :0.16580350292179596, sharpe5:14.592427057027816, irr5:480.73675537109375, ndcg5:0.8404777059965721 
train 17, step: 0, loss: 1.566262245901251, grad_norm: 0.9962027499666113, ic: 0.12363965868227508
train 17, step: 500, loss: 2.245242135515684, grad_norm: 1.2313440710107366, ic: 0.11238505140795568
train 17, step: 1000, loss: 0.9583449279425549, grad_norm: 0.005753337476838275, ic: 0.1987211902261895
train 17, step: 1500, loss: 0.8617880094984456, grad_norm: 0.13035261817363833, ic: 0.2894051570756065
train 17, step: 2000, loss: 3.1792679798753953, grad_norm: 1.60356786544303, ic: 0.037474390870318224
Epoch 17: train loss: 1.6258224229602272
Eval step 0: eval loss: 1.011384615463731
Eval: total loss: 1.0836388603948846, mse:4.687063948765506, ic :0.1633288227522954, sharpe5:14.495257784128189, irr5:469.0680236816406, ndcg5:0.8497068525479976 
train 18, step: 0, loss: 1.0802116176769732, grad_norm: 0.6445070360984835, ic: 0.005728422385399218
train 18, step: 500, loss: 0.8071809188179347, grad_norm: 0.09573298562303358, ic: -0.02290198251300783
train 18, step: 1000, loss: 1.1756091114243745, grad_norm: 0.8069134531039914, ic: 0.07608429965319065
train 18, step: 1500, loss: 0.9386233923570165, grad_norm: 0.012140737506434122, ic: 0.13164585683386454
train 18, step: 2000, loss: 1.7528043225562284, grad_norm: 1.0623110370622966, ic: 0.4797509751536329
Epoch 18: train loss: 1.625644767536915
Eval step 0: eval loss: 0.9972091602488151
Eval: total loss: 1.0831903210807665, mse:4.719562962123411, ic :0.15603163296851072, sharpe5:15.056151056885719, irr5:490.78375244140625, ndcg5:0.8480750300005504 
train 19, step: 0, loss: 1.1210880223607038, grad_norm: 1.1505229784261284, ic: 0.023262562683986764
train 19, step: 500, loss: 2.2572111391894456, grad_norm: 0.8784780038733631, ic: 0.20987398721985828
train 19, step: 1000, loss: 1.306238939314324, grad_norm: 0.09544695098270997, ic: 0.5564167028026534
train 19, step: 1500, loss: 1.5129623294432835, grad_norm: 0.24244995118745932, ic: 0.4450426900505326
train 19, step: 2000, loss: 1.151600357601866, grad_norm: 0.3107943052760928, ic: 0.21964429502870714
Epoch 19: train loss: 1.6263256087522318
Eval step 0: eval loss: 1.0098252781068984
Eval: total loss: 1.085372532379858, mse:4.702182201162178, ic :0.15385596708410837, sharpe5:13.297725291252135, irr5:428.795166015625, ndcg5:0.8512914795538357 
