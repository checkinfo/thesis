Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=8, dataset_type='TimeDataset', dout=0.3, epochs=40, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=False, label_cnt=3, lr=0.001, lstm_layers=3, mask_adj=False, mask_type='soft', model_type='BaseLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
16749
BaseLSTM(
  (rnn1): LSTM(9, 128, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (predict): Linear(in_features=128, out_features=1, bias=True)
  (relu): PReLU(num_parameters=1)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 2.8342588139248646, grad_norm: 0.023691743471715907, ic: -0.008258967738426857
Epoch 0: train loss: 1.6303653910324685
Eval step 0: eval loss: 1.1492664956252878
Eval: total loss: 1.0789395262673787, mse:0.6080303831650329, ic :-0.0027634280899382553, sharpe5:-2.03389919757843, irr5:-40.47715759277344, ndcg5:0.8381641285010146 
train 1, step: 0, loss: 2.165224800367239, grad_norm: 0.02635850880309862, ic: -0.007883617159955808
Epoch 1: train loss: 1.6301748398176212
Eval step 0: eval loss: 1.1466285369794749
Eval: total loss: 1.0785403517406484, mse:0.6082190557075082, ic :0.009245104695197367, sharpe5:7.470111092627048, irr5:212.76235961914062, ndcg5:0.8518401992233897 
train 2, step: 0, loss: 1.9461228668514559, grad_norm: 0.005799274301065761, ic: 0.015481660622010835
Epoch 2: train loss: 1.632578860424173
Eval step 0: eval loss: 1.1495309210742712
Eval: total loss: 1.0789748070022527, mse:0.6080043487831494, ic :0.006322514506502318, sharpe5:7.360992413163185, irr5:207.2159881591797, ndcg5:0.8545734363417442 
train 3, step: 0, loss: 1.6384330724825964, grad_norm: 0.016834684060784533, ic: 0.054322329117864145
Epoch 3: train loss: 1.6296019843078697
Eval step 0: eval loss: 1.1518633385550292
Eval: total loss: 1.0794901720958145, mse:0.6079071087428185, ic :0.0100863747947245, sharpe5:7.435356836020946, irr5:211.91685485839844, ndcg5:0.8599953908891808 
train 4, step: 0, loss: 1.9661917472612118, grad_norm: 0.009325621395776683, ic: 0.02216597069547426
Epoch 4: train loss: 1.6295118165708513
Eval step 0: eval loss: 1.1511734939313203
Eval: total loss: 1.079338146711926, mse:0.6079592712864814, ic :0.007755054417681316, sharpe5:7.211013999581336, irr5:205.14031982421875, ndcg5:0.8520135467480089 
train 5, step: 0, loss: 1.5976909871629374, grad_norm: 0.08175517646570286, ic: 0.0739264092416087
Epoch 5: train loss: 1.6291469881960088
Eval step 0: eval loss: 1.1512219333843168
Eval: total loss: 1.0793404187180315, mse:0.6079570491995835, ic :0.01136720129154191, sharpe5:7.163422585129737, irr5:202.71652221679688, ndcg5:0.8619579498278209 
train 6, step: 0, loss: 1.622828851744186, grad_norm: 0.016980520786981642, ic: 0.11249049418507501
Epoch 6: train loss: 1.6256874543328872
Eval step 0: eval loss: 1.1519981210117753
Eval: total loss: 1.0795050681390768, mse:0.6079159246666462, ic :0.009721316901559918, sharpe5:7.204812510609626, irr5:203.71958923339844, ndcg5:0.8547584131572143 
train 7, step: 0, loss: 2.1540945037663275, grad_norm: 0.1381058106381402, ic: 0.058278107895358554
Epoch 7: train loss: 1.6278162187202887
Eval step 0: eval loss: 1.1480511407555423
Eval: total loss: 1.078639388373062, mse:0.6080273519022275, ic :0.011407289455900695, sharpe5:7.265143652558327, irr5:203.90859985351562, ndcg5:0.8522331915563622 
train 8, step: 0, loss: 2.0310060423889618, grad_norm: 0.014838609309409546, ic: 0.04431932913201431
Epoch 8: train loss: 1.628660444101165
Eval step 0: eval loss: 1.149548652226827
Eval: total loss: 1.078887223425814, mse:0.6079347627031135, ic :0.010861738574693948, sharpe5:7.046455560028552, irr5:199.2496337890625, ndcg5:0.8497393616960732 
train 9, step: 0, loss: 2.292418028369406, grad_norm: 0.017168682128641, ic: 0.020420607355472506
Epoch 9: train loss: 1.6273419657606119
Eval step 0: eval loss: 1.1459667024373394
Eval: total loss: 1.0783077081389405, mse:0.608181321633777, ic :0.01424022153920928, sharpe5:6.944283286035061, irr5:196.53292846679688, ndcg5:0.8528996475462275 
train 10, step: 0, loss: 1.1300638679402972, grad_norm: 0.005704495973048454, ic: 0.07077040095272044
Epoch 10: train loss: 1.6258627350903925
Eval step 0: eval loss: 1.1488124239359252
Eval: total loss: 1.0784598313275295, mse:0.6075898167780073, ic :0.03057194629432703, sharpe5:7.200425780117511, irr5:199.50631713867188, ndcg5:0.8353879089469212 
train 11, step: 0, loss: 1.5968995249896738, grad_norm: 0.013388794413966003, ic: 0.11344351586301787
Epoch 11: train loss: 1.6278991374139955
Eval step 0: eval loss: 1.1438503994391815
Eval: total loss: 1.0775822210759813, mse:0.6079846528838635, ic :0.03278621704276983, sharpe5:6.944834760725498, irr5:192.73326110839844, ndcg5:0.8536949098958719 
train 12, step: 0, loss: 1.5165338833347917, grad_norm: 0.0804161718667574, ic: 0.14560780124946257
Epoch 12: train loss: 1.6271695458784252
Eval step 0: eval loss: 1.1560829674034603
Eval: total loss: 1.0808508192382975, mse:0.6077776395333132, ic :0.03691600613113574, sharpe5:6.944396797120571, irr5:194.4770965576172, ndcg5:0.8383163231224362 
train 13, step: 0, loss: 1.4545299206127535, grad_norm: 0.015036165134797719, ic: 0.06605407723507133
Epoch 13: train loss: 1.6270408466927722
Eval step 0: eval loss: 1.1476942049453982
Eval: total loss: 1.0781594380593034, mse:0.6075678998002255, ic :0.03802817075442893, sharpe5:6.8756081333756445, irr5:191.6705780029297, ndcg5:0.8594481406829511 
train 14, step: 0, loss: 1.7358640092408146, grad_norm: 0.043578786069482525, ic: 0.08716488376105773
Epoch 14: train loss: 1.6276060585321237
Eval step 0: eval loss: 1.1477486832691928
Eval: total loss: 1.0781119606153642, mse:0.6075108081101395, ic :0.04028024560995376, sharpe5:6.716978756189346, irr5:184.6845703125, ndcg5:0.8556027189698218 
train 15, step: 0, loss: 1.6296801036321005, grad_norm: 0.0014739265952168716, ic: 8.007764883027207e-05
Epoch 15: train loss: 1.625636338213359
Eval step 0: eval loss: 1.1503655701105189
Eval: total loss: 1.0783870704315273, mse:0.607093495126962, ic :0.04453759103474953, sharpe5:4.9093932312726976, irr5:123.49879455566406, ndcg5:0.8371445463006579 
train 16, step: 0, loss: 1.4947072425341492, grad_norm: 0.013068246676218141, ic: 0.03490809426848314
Epoch 16: train loss: 1.6287406120929038
Eval step 0: eval loss: 1.1478432494161568
Eval: total loss: 1.0778621155893269, mse:0.6073224616821026, ic :0.041731923416074, sharpe5:1.491751807630062, irr5:19.2402400970459, ndcg5:0.8460532452081058 
train 17, step: 0, loss: 1.5144092958061701, grad_norm: 0.003646057428046458, ic: 0.04237374917189847
Epoch 17: train loss: 1.6217831577352237
Eval step 0: eval loss: 1.1455850971975527
Eval: total loss: 1.077681440386909, mse:0.6077099006461364, ic :0.03616211991481629, sharpe5:-0.7511972680315375, irr5:-10.141576766967773, ndcg5:0.8447244130078352 
train 18, step: 0, loss: 1.2604637524477806, grad_norm: 0.20320074605143876, ic: 0.026133710427244362
Epoch 18: train loss: 1.6304943673336516
Eval step 0: eval loss: 1.147222787563318
Eval: total loss: 1.0778326307647248, mse:0.6074843159259877, ic :0.038718146725655246, sharpe5:-0.495552183073014, irr5:-7.25245475769043, ndcg5:0.8583431847900664 
train 19, step: 0, loss: 1.707843731112934, grad_norm: 0.019461213362708802, ic: 0.07807265918594769
Epoch 19: train loss: 1.6273466191897812
Eval step 0: eval loss: 1.147307588727715
Eval: total loss: 1.0779686121124024, mse:0.607580629599054, ic :0.037451061466193654, sharpe5:2.081649054586887, irr5:34.9620361328125, ndcg5:0.8547233231077659 
train 20, step: 0, loss: 1.5819833224993904, grad_norm: 0.08740781814595232, ic: 0.006381485751984399
Epoch 20: train loss: 1.6272554295483428
Eval step 0: eval loss: 1.1471320760147359
Eval: total loss: 1.0777948996531919, mse:0.6074560725731956, ic :0.03889043607561915, sharpe5:-0.6314624642953276, irr5:-9.261720657348633, ndcg5:0.8482405232341159 
train 21, step: 0, loss: 2.2610378099962833, grad_norm: 0.004614837818354531, ic: 0.014741964253881152
Epoch 21: train loss: 1.6289858825829366
Eval step 0: eval loss: 1.147140813104401
Eval: total loss: 1.0777749325545491, mse:0.6075375890979687, ic :0.04244503571265479, sharpe5:1.1727351685613394, irr5:16.956438064575195, ndcg5:0.8330883943442198 
train 22, step: 0, loss: 1.7652834177274235, grad_norm: 0.00836546062654981, ic: -0.010578289375314031
Epoch 22: train loss: 1.626206397984102
Eval step 0: eval loss: 1.1489699485231233
Eval: total loss: 1.0780303742540278, mse:0.6072713313963272, ic :0.04713755982474818, sharpe5:8.743278386592864, irr5:251.76239013671875, ndcg5:0.8333845871314904 
train 23, step: 0, loss: 1.8355767027673462, grad_norm: 0.05023801265822016, ic: 0.10896679913487224
Epoch 23: train loss: 1.62350989019746
Eval step 0: eval loss: 1.1430294984704952
Eval: total loss: 1.0833872330325016, mse:0.6130701751104591, ic :0.03816768914597076, sharpe5:-0.6854892478510737, irr5:-9.925824165344238, ndcg5:0.8522702059679931 
train 24, step: 0, loss: 1.6144340169363232, grad_norm: 0.0729305402679127, ic: 0.02268785594417964
Epoch 24: train loss: 1.6248981842507966
Eval step 0: eval loss: 1.1467245164791788
Eval: total loss: 1.079344631865891, mse:0.6095327998083518, ic :0.042962947463841494, sharpe5:6.894440568387508, irr5:194.74952697753906, ndcg5:0.8445742659245824 
train 25, step: 0, loss: 2.504141479126256, grad_norm: 0.10740731124848353, ic: 0.08711193311433985
Epoch 25: train loss: 1.6255559196635578
Eval step 0: eval loss: 1.1494811967551475
Eval: total loss: 1.0780887379555475, mse:0.6065463359155366, ic :0.05892395918300628, sharpe5:9.643194272518157, irr5:279.7367858886719, ndcg5:0.8470335292567508 
train 26, step: 0, loss: 1.4555252817319098, grad_norm: 0.002781539921402087, ic: 0.14294224887706936
Epoch 26: train loss: 1.622699362550602
Eval step 0: eval loss: 1.1342155738109334
Eval: total loss: 1.0765214302215136, mse:0.5979045653527023, ic :0.13592366877536435, sharpe5:11.515406388044356, irr5:381.89483642578125, ndcg5:0.8484099089890418 
train 27, step: 0, loss: 1.0661189522576107, grad_norm: 0.00369599436043925, ic: 0.268402043706443
Epoch 27: train loss: 1.6186355603927407
Eval step 0: eval loss: 1.1402264345273336
Eval: total loss: 1.072264553945304, mse:0.5876178355635479, ic :0.15852140709802842, sharpe5:11.505424033999443, irr5:393.3563537597656, ndcg5:0.8424230407065781 
train 28, step: 0, loss: 1.2567380487542072, grad_norm: 0.02283895729887825, ic: 0.4211300211994441
Epoch 28: train loss: 1.6128940744065698
Eval step 0: eval loss: 1.137973678746464
Eval: total loss: 1.072316729999998, mse:0.5832077122072451, ic :0.15721350310586887, sharpe5:11.72023335814476, irr5:401.4925537109375, ndcg5:0.8572157673575127 
train 29, step: 0, loss: 1.339519189759314, grad_norm: 0.022245918719934005, ic: 0.18292803414571926
Epoch 29: train loss: 1.6121888021004636
Eval step 0: eval loss: 1.1328315160186828
Eval: total loss: 1.0693901248656397, mse:0.5818805394434697, ic :0.16270074713474392, sharpe5:11.567120146751403, irr5:387.81072998046875, ndcg5:0.8510650138193474 
train 30, step: 0, loss: 1.6088544176596864, grad_norm: 0.027635863308380307, ic: 0.16284660010768107
Epoch 30: train loss: 1.6141742533972048
Eval step 0: eval loss: 1.135684946714032
Eval: total loss: 1.070792254414189, mse:0.5828513066090387, ic :0.16130427674516082, sharpe5:12.227534263730048, irr5:401.27850341796875, ndcg5:0.8526597046243488 
train 31, step: 0, loss: 1.3434945964707512, grad_norm: 0.02737848349297049, ic: 0.32432764175611417
Epoch 31: train loss: 1.6110873105029462
Eval step 0: eval loss: 1.1335371644957568
Eval: total loss: 1.070651830021017, mse:0.5835181483933527, ic :0.15941860495511784, sharpe5:11.478635417819023, irr5:387.51483154296875, ndcg5:0.844863469249101 
train 32, step: 0, loss: 1.5516159124452487, grad_norm: 0.001145372265806753, ic: 0.11586977119672806
Epoch 32: train loss: 1.6107682762014222
Eval step 0: eval loss: 1.1712916707782381
Eval: total loss: 1.0861741816952892, mse:0.5871357284312412, ic :0.15481097689726403, sharpe5:11.555335804224013, irr5:394.43499755859375, ndcg5:0.8364121749320961 
train 33, step: 0, loss: 2.898581499111153, grad_norm: 0.8712486627661011, ic: 0.1674363307161569
Epoch 33: train loss: 1.610537866596285
Eval step 0: eval loss: 1.1321721227221893
Eval: total loss: 1.069725162708343, mse:0.5827986625722488, ic :0.1623785044753943, sharpe5:11.807127985954285, irr5:398.7179260253906, ndcg5:0.8446622773015915 
train 34, step: 0, loss: 1.5256365083549643, grad_norm: 0.14629291839989686, ic: 0.22442606664096526
Epoch 34: train loss: 1.6093841745687065
Eval step 0: eval loss: 1.137799322412999
Eval: total loss: 1.0704598294128107, mse:0.5821041325711924, ic :0.16512334511695742, sharpe5:12.035186891555785, irr5:395.97332763671875, ndcg5:0.8503023547873793 
train 35, step: 0, loss: 1.9794050526648537, grad_norm: 0.01618145914704746, ic: 0.14924386408689652
Epoch 35: train loss: 1.611301248248071
Eval step 0: eval loss: 1.141227088267219
Eval: total loss: 1.0708703633345926, mse:0.5813444245350339, ic :0.16629757801308126, sharpe5:12.385761149525642, irr5:420.4342956542969, ndcg5:0.847840571191404 
train 36, step: 0, loss: 1.5503727743680116, grad_norm: 0.0023013389766287804, ic: 0.07817962062770868
Epoch 36: train loss: 1.6088820039511573
Eval step 0: eval loss: 1.1378791125995
Eval: total loss: 1.0708948872599626, mse:0.5821531460215045, ic :0.16252677332609822, sharpe5:11.659871000647545, irr5:409.02618408203125, ndcg5:0.8628264913094237 
train 37, step: 0, loss: 1.2880185825988362, grad_norm: 0.004209874564268372, ic: 0.34714802922907295
Epoch 37: train loss: 1.6089680221427338
Eval step 0: eval loss: 1.1342758340323005
Eval: total loss: 1.0697151988927485, mse:0.5820394397220913, ic :0.1652831858808605, sharpe5:11.965166632533073, irr5:402.76806640625, ndcg5:0.8516549847162522 
train 38, step: 0, loss: 0.9908879329318045, grad_norm: 0.0009680504675464687, ic: 0.23281899748813434
Epoch 38: train loss: 1.6118601190452886
Eval step 0: eval loss: 1.1349777563976053
Eval: total loss: 1.0703992276424605, mse:0.5826822767562202, ic :0.16555010740804757, sharpe5:12.191786785125732, irr5:414.0122985839844, ndcg5:0.8519570043289632 
train 39, step: 0, loss: 2.139325294703636, grad_norm: 0.010931210226411943, ic: 0.2693551306687054
Epoch 39: train loss: 1.6123990440956055
Eval step 0: eval loss: 1.1358530072034734
Eval: total loss: 1.0705537107721281, mse:0.5824498781047879, ic :0.16327831683193544, sharpe5:12.40201593697071, irr5:417.0733947753906, ndcg5:0.8443173645170244 
