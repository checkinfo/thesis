Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjSeqTimeDataset', dout=0.3, epochs=40, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=1, mask_adj=True, mask_type='soft', model_type='BiGLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
1759
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8208717040345277, grad_norm: 0.1316871457142797, ic: -0.023056277385782552
train 0, step: 500, loss: 1.1523095117064117, grad_norm: 0.01445686112948877, ic: 0.07168302434464488
train 0, step: 1000, loss: 0.7122008824112391, grad_norm: 9.759013352711705e-05, ic: 0.09299909253219638
train 0, step: 1500, loss: 0.8560693570882967, grad_norm: 0.167268809704455, ic: 0.05925676859476529
train 0, step: 2000, loss: 2.3866707398261107, grad_norm: 0.7219572414981602, ic: 0.10022224157041229
Epoch 0: train loss: 1.6485100792867546
Eval step 0: eval loss: 0.8352980136368875
Eval: total loss: 1.079026522828802, mse:4.8236588719164635, ic :0.00986851030125589, sharpe5:7.612329604625701, irr5:213.43630981445312, ndcg5:0.8384544504654008 
train 1, step: 0, loss: 2.3076739378168876, grad_norm: 0.04311782230103413, ic: 0.05288412704194468
train 1, step: 500, loss: 0.626067350444624, grad_norm: 0.04336418872078897, ic: -0.08590259861175135
train 1, step: 1000, loss: 1.1951736257046084, grad_norm: 0.09872686437016291, ic: 0.049702723640925636
train 1, step: 1500, loss: 2.2701421006234668, grad_norm: 0.8626175200482775, ic: 0.07182836575912016
train 1, step: 2000, loss: 1.1662651823742076, grad_norm: 0.012003912468532454, ic: 0.003908315043571767
Epoch 1: train loss: 1.6468337819148717
Eval step 0: eval loss: 0.8362396529652923
Eval: total loss: 1.0792808592739989, mse:4.822889666545611, ic :0.02440011031250044, sharpe5:11.000116193890571, irr5:317.1817932128906, ndcg5:0.8480706711890423 
train 2, step: 0, loss: 2.00597146116657, grad_norm: 0.23803381270656565, ic: 0.08319080889465559
train 2, step: 500, loss: 1.1481237906749335, grad_norm: 0.42273409544872736, ic: 0.4972991816941
train 2, step: 1000, loss: 1.133373891310884, grad_norm: 0.05770680281583218, ic: 0.26514867445826873
train 2, step: 1500, loss: 1.2786982989439784, grad_norm: 0.2713751586298224, ic: 0.5709920491489989
train 2, step: 2000, loss: 2.4719292493180514, grad_norm: 1.9933192751681945, ic: 0.13073434000469059
Epoch 2: train loss: 1.6467540343005902
Eval step 0: eval loss: 0.8386674243858666
Eval: total loss: 1.0800268220837612, mse:4.814973199967311, ic :0.06428159560301147, sharpe5:11.199767058491707, irr5:356.0056457519531, ndcg5:0.8409755133090608 
train 3, step: 0, loss: 0.8486542920345416, grad_norm: 0.14700942097282654, ic: 0.39611075489073994
train 3, step: 500, loss: 1.2525166967828798, grad_norm: 0.0706370229452277, ic: 0.03642129864581328
train 3, step: 1000, loss: 1.394325402280987, grad_norm: 0.3497694687969605, ic: 0.005615254166932042
train 3, step: 1500, loss: 1.7152374710537623, grad_norm: 0.28100392985156775, ic: 0.22807481780316374
train 3, step: 2000, loss: 1.0937539215339271, grad_norm: 0.2738716329405249, ic: -0.07228598364389603
Epoch 3: train loss: 1.642441105528147
Eval step 0: eval loss: 0.8402073883281743
Eval: total loss: 1.0797238136786784, mse:4.738008885988182, ic :0.12619400874391565, sharpe5:11.162303670644759, irr5:389.58648681640625, ndcg5:0.8433078343676477 
train 4, step: 0, loss: 1.1085081444630385, grad_norm: 0.3681426205904308, ic: -0.005471353088766418
train 4, step: 500, loss: 1.023057700943773, grad_norm: 0.05687877790820186, ic: 0.5225698698437742
train 4, step: 1000, loss: 1.1291078671402153, grad_norm: 0.014448973414604084, ic: 0.39419229187913285
train 4, step: 1500, loss: 1.2869885483505583, grad_norm: 0.28447777699569665, ic: 0.036843077792807793
train 4, step: 2000, loss: 1.3726830575980393, grad_norm: 0.14707696535482834, ic: 0.3778200928313582
Epoch 4: train loss: 1.6408086795989638
Eval step 0: eval loss: 0.8321795609276211
Eval: total loss: 1.0757678141574174, mse:4.711328278278756, ic :0.13433020561276193, sharpe5:11.845462571382521, irr5:402.47967529296875, ndcg5:0.8550423997211729 
train 5, step: 0, loss: 2.2791619551809212, grad_norm: 0.0038014625337582223, ic: 0.05135273023265367
train 5, step: 500, loss: 1.797560293463212, grad_norm: 0.6900789834911839, ic: 0.008924316842020146
train 5, step: 1000, loss: 4.465253751797269, grad_norm: 0.7890952343074478, ic: 0.046908510745789236
train 5, step: 1500, loss: 0.9331233129152824, grad_norm: 0.027990193599972586, ic: 0.11199222367607779
train 5, step: 2000, loss: 2.2524703564600643, grad_norm: 0.6801940082392033, ic: 0.02161253880880183
Epoch 5: train loss: 1.6374224566480269
Eval step 0: eval loss: 0.835047248542874
Eval: total loss: 1.0732121963577903, mse:4.61989457267449, ic :0.14375175457175834, sharpe5:11.609228029847145, irr5:396.36761474609375, ndcg5:0.8447702450357301 
train 6, step: 0, loss: 1.9796273896188448, grad_norm: 0.04233348297986875, ic: 0.25990178066143743
train 6, step: 500, loss: 1.0147038096005154, grad_norm: 8.538949247415329e-05, ic: 0.014138770882662414
train 6, step: 1000, loss: 1.5701036590414459, grad_norm: 0.5547625197240782, ic: 0.023969685787311307
train 6, step: 1500, loss: 1.09343574535162, grad_norm: 0.01618648455013562, ic: 0.48152512578301193
train 6, step: 2000, loss: 1.0973028509173548, grad_norm: 0.16338687619634243, ic: 0.004248728328991258
Epoch 6: train loss: 1.6340056189426841
Eval step 0: eval loss: 0.8304743068361432
Eval: total loss: 1.071415528961928, mse:4.618070275135271, ic :0.15093934300978365, sharpe5:11.865941863059996, irr5:395.4502258300781, ndcg5:0.8644360665019722 
train 7, step: 0, loss: 1.1898833250265164, grad_norm: 0.1773752955348275, ic: 0.5866231959551831
train 7, step: 500, loss: 1.4145973279319952, grad_norm: 0.035630666727317614, ic: 0.11193165258724054
train 7, step: 1000, loss: 1.0662795804920526, grad_norm: 0.11658746001229522, ic: 0.016049471638016986
train 7, step: 1500, loss: 1.9748322692098497, grad_norm: 0.8016307400434666, ic: -0.1116604558360811
train 7, step: 2000, loss: 2.99794155213891, grad_norm: 0.7473967924274619, ic: 0.055882719570550785
Epoch 7: train loss: 1.6338798725265586
Eval step 0: eval loss: 0.8447998818657797
Eval: total loss: 1.078236456246374, mse:4.6334379063696, ic :0.1488649486037124, sharpe5:12.413280020356178, irr5:412.2792663574219, ndcg5:0.8563079788169952 
train 8, step: 0, loss: 1.2643124872979628, grad_norm: 0.0003311257808698818, ic: 0.0464296974227358
train 8, step: 500, loss: 0.6355011421602615, grad_norm: 0.08905078914396686, ic: -0.00540722793460951
train 8, step: 1000, loss: 1.928354217787721, grad_norm: 0.6255952579280146, ic: 0.44325128693128574
train 8, step: 1500, loss: 1.4260601898756289, grad_norm: 0.25730388142901484, ic: 0.11993229921609497
train 8, step: 2000, loss: 4.062437803711745, grad_norm: 1.1115856620135174, ic: -0.06033714550276825
Epoch 8: train loss: 1.6340130501203267
Eval step 0: eval loss: 0.8362482068913
Eval: total loss: 1.0738540231375808, mse:4.651842279384839, ic :0.11419895932252529, sharpe5:7.22631387501955, irr5:198.3899688720703, ndcg5:0.8693492725887552 
train 9, step: 0, loss: 0.7978794715568537, grad_norm: 0.006177620354444891, ic: 0.054710965585294986
train 9, step: 500, loss: 1.0592936726359579, grad_norm: 0.19718927110558537, ic: -0.01564010849274336
train 9, step: 1000, loss: 0.8034764879532442, grad_norm: 0.03210325328062638, ic: -0.015598017047833713
train 9, step: 1500, loss: 1.0461068914456944, grad_norm: 0.10273123938792529, ic: 0.12631311462788222
train 9, step: 2000, loss: 6.666595946485622, grad_norm: 0.2519870775862569, ic: 0.11802819309418906
Epoch 9: train loss: 1.6330652402439327
Eval step 0: eval loss: 0.8329874888863277
Eval: total loss: 1.0712866948094135, mse:4.615214587267602, ic :0.15942004897455206, sharpe5:12.131900228261948, irr5:400.4891357421875, ndcg5:0.8607889858885751 
train 10, step: 0, loss: 0.9109181396194767, grad_norm: 0.0185447076842381, ic: 0.5254171732973087
train 10, step: 500, loss: 3.9743722904624277, grad_norm: 1.0393134561286503, ic: 0.10340451647651018
train 10, step: 1000, loss: 1.3086658313961848, grad_norm: 0.5233157144804021, ic: 0.2717569560476646
train 10, step: 1500, loss: 1.404745117932399, grad_norm: 0.00757099859912207, ic: 0.14707231227985829
train 10, step: 2000, loss: 1.299413388293282, grad_norm: 0.676290666034213, ic: 0.14198548333958608
Epoch 10: train loss: 1.6330353188664386
Eval step 0: eval loss: 0.8282419250938488
Eval: total loss: 1.0692603256942879, mse:4.615000762103802, ic :0.16412078871772395, sharpe5:12.549330623745918, irr5:417.2326354980469, ndcg5:0.8535288140166325 
train 11, step: 0, loss: 2.2940540840955284, grad_norm: 0.1096508949999765, ic: -0.1262392383709081
train 11, step: 500, loss: 1.8875411865881961, grad_norm: 0.4089549474225488, ic: 0.1304964964711644
train 11, step: 1000, loss: 2.9861136917519358, grad_norm: 1.3216926881296107, ic: 0.07642774383028511
train 11, step: 1500, loss: 1.0259949763295728, grad_norm: 0.13345927930268914, ic: -0.006087809922164157
train 11, step: 2000, loss: 1.1893993532174845, grad_norm: 0.5125832284459212, ic: 0.6799501195943825
Epoch 11: train loss: 1.6324952957327543
Eval step 0: eval loss: 0.8286193268819151
Eval: total loss: 1.0693420723386777, mse:4.61808882598755, ic :0.1622959753232646, sharpe5:12.705813222527503, irr5:419.1873474121094, ndcg5:0.8379071220881512 
train 12, step: 0, loss: 1.4031460106516922, grad_norm: 0.7751998867917549, ic: -0.16234286009199517
train 12, step: 500, loss: 1.9398794320913462, grad_norm: 0.30530457980208137, ic: 0.12277200592865394
train 12, step: 1000, loss: 1.3517669275876258, grad_norm: 0.14469693737585693, ic: 0.5001909380119945
train 12, step: 1500, loss: 1.1242631121379574, grad_norm: 0.26218288880235185, ic: 0.20123399953832907
train 12, step: 2000, loss: 2.8416570998330153, grad_norm: 0.6445603368373529, ic: 0.03905278355039999
Epoch 12: train loss: 1.6318718299371497
Eval step 0: eval loss: 0.8294436552250724
Eval: total loss: 1.0694001753226012, mse:4.634794052669003, ic :0.16009886943914994, sharpe5:12.568694669008254, irr5:407.0388488769531, ndcg5:0.8501524161956397 
train 13, step: 0, loss: 1.9006879485867205, grad_norm: 0.06643690844214546, ic: 0.08791600820469825
train 13, step: 500, loss: 2.864262058423913, grad_norm: 0.9032650617974196, ic: 0.2639550566840853
train 13, step: 1000, loss: 1.0055863533862381, grad_norm: 0.0442152740539215, ic: 0.38533141389378384
train 13, step: 1500, loss: 0.9670245171317389, grad_norm: 0.0012170749541746124, ic: 0.11717265586213627
train 13, step: 2000, loss: 0.804596622101505, grad_norm: 0.05534311967877635, ic: 0.9253600073116365
Epoch 13: train loss: 1.6321937389494174
Eval step 0: eval loss: 0.8256362577384088
Eval: total loss: 1.0701411243536365, mse:4.65379689874331, ic :0.1610098750671232, sharpe5:12.00233394563198, irr5:407.8826904296875, ndcg5:0.8553651603093444 
train 14, step: 0, loss: 2.2313031475360576, grad_norm: 0.5598616426294895, ic: 0.09351497299327324
train 14, step: 500, loss: 0.7740081206927281, grad_norm: 0.0016708089716905507, ic: 0.024032361543201305
train 14, step: 1000, loss: 3.940845892881106, grad_norm: 0.9356195801333632, ic: 0.20935682064394623
train 14, step: 1500, loss: 1.4408819417189953, grad_norm: 0.5686372590705029, ic: 0.1986670326612457
train 14, step: 2000, loss: 1.3218170335423276, grad_norm: 0.2827899390606019, ic: 0.19130945936638316
Epoch 14: train loss: 1.6304299415995176
Eval step 0: eval loss: 0.8308433476439014
Eval: total loss: 1.0693435343994326, mse:4.608307530534999, ic :0.1638752716212882, sharpe5:12.654455131888389, irr5:421.2125244140625, ndcg5:0.8244105839705368 
train 15, step: 0, loss: 0.9168438959882983, grad_norm: 0.10919174235693674, ic: 0.5457504845111648
train 15, step: 500, loss: 2.2938798612433358, grad_norm: 0.4326572577162205, ic: 0.45097807015728736
train 15, step: 1000, loss: 1.1268993719032085, grad_norm: 0.39595914054383663, ic: 0.5197072395737394
train 15, step: 1500, loss: 0.8579366852084916, grad_norm: 0.0036195405048899615, ic: 0.10605443260498443
train 15, step: 2000, loss: 2.983651524329527, grad_norm: 1.076037375890543, ic: 0.15661590840122863
Epoch 15: train loss: 1.6287656335934642
Eval step 0: eval loss: 0.8276332457274104
Eval: total loss: 1.0694153652648863, mse:4.599458575506506, ic :0.17126014609984108, sharpe5:15.516485804915428, irr5:509.54803466796875, ndcg5:0.8412452136621777 
train 16, step: 0, loss: 0.9217903928741172, grad_norm: 0.011138879086130836, ic: 0.047151775220079094
train 16, step: 500, loss: 1.4344999480156844, grad_norm: 0.01252357291912315, ic: 0.04083046643957637
train 16, step: 1000, loss: 0.984494756183353, grad_norm: 0.14732513265362351, ic: 0.023337999314032604
train 16, step: 1500, loss: 2.749802353240106, grad_norm: 4.386598717324637, ic: 0.06594330576700445
train 16, step: 2000, loss: 1.9404775885087024, grad_norm: 0.7141510553778148, ic: -0.10245439657531091
Epoch 16: train loss: 1.6278877093781725
Eval step 0: eval loss: 0.8226309355242359
Eval: total loss: 1.0710865634590376, mse:4.780163120539581, ic :0.11950302291652544, sharpe5:15.365357145667076, irr5:485.20770263671875, ndcg5:0.8500773845814246 
train 17, step: 0, loss: 0.7641520375593824, grad_norm: 0.5239406484426743, ic: 0.1550426757774304
train 17, step: 500, loss: 1.7867795460561144, grad_norm: 0.36470206423624285, ic: 0.1353173088091177
train 17, step: 1000, loss: 2.0570682586185516, grad_norm: 2.318356358449833, ic: 0.015154120832382819
train 17, step: 1500, loss: 1.1201721841704717, grad_norm: 0.14270123567187604, ic: 0.030807850765392143
train 17, step: 2000, loss: 1.624270042413933, grad_norm: 1.02545446153495, ic: 0.11970374862589497
Epoch 17: train loss: 1.6309048568112063
Eval step 0: eval loss: 0.8437689086785432
Eval: total loss: 1.0758660156046613, mse:4.615945248200159, ic :0.17505335716725914, sharpe5:15.66321685552597, irr5:505.7190856933594, ndcg5:0.8495519015803207 
train 18, step: 0, loss: 1.0331829357293263, grad_norm: 0.20802954037253213, ic: 0.054558519162719664
train 18, step: 500, loss: 1.5972144717261905, grad_norm: 0.28370811082273384, ic: 0.20334611358107402
train 18, step: 1000, loss: 1.7603658458299096, grad_norm: 0.7936695115254042, ic: 0.07074242478511918
train 18, step: 1500, loss: 0.79047384794822, grad_norm: 0.027377567589609922, ic: 0.015066431180311663
train 18, step: 2000, loss: 3.262963092453214, grad_norm: 1.06287687942588, ic: 0.017495581844944475
Epoch 18: train loss: 1.6260672629227373
Eval step 0: eval loss: 0.8346554401425842
Eval: total loss: 1.0707343201201938, mse:4.593709342820699, ic :0.18095036752303104, sharpe5:15.744642140865325, irr5:507.6167907714844, ndcg5:0.8439710558435535 
train 19, step: 0, loss: 2.380527427738956, grad_norm: 0.5761273195000758, ic: 0.07270183341372154
train 19, step: 500, loss: 9.214473223929694, grad_norm: 1.4245056862022314, ic: 0.012172119980390216
train 19, step: 1000, loss: 1.2698921857230856, grad_norm: 0.5383111440954763, ic: 0.04520978029909431
train 19, step: 1500, loss: 0.7353365114705466, grad_norm: 0.032684673278663756, ic: 0.21275910815432514
train 19, step: 2000, loss: 1.3072994702482876, grad_norm: 0.31441060450688896, ic: 0.020663955863476264
Epoch 19: train loss: 1.6257523230420745
Eval step 0: eval loss: 0.8291996432305386
Eval: total loss: 1.0691829263623263, mse:4.58940710523608, ic :0.18129883975848057, sharpe5:17.010305879116057, irr5:531.851806640625, ndcg5:0.8652654543907191 
train 20, step: 0, loss: 1.3168738941636762, grad_norm: 0.5408229342453803, ic: 0.17700972692166342
train 20, step: 500, loss: 0.8112296389571162, grad_norm: 0.08115247787924525, ic: -0.018216304956069435
train 20, step: 1000, loss: 1.0200281126427944, grad_norm: 0.05164613319777604, ic: 0.5200415113942705
train 20, step: 1500, loss: 1.3637677553419316, grad_norm: 0.23740482299986496, ic: 0.16353242251102054
train 20, step: 2000, loss: 1.2859084503560128, grad_norm: 0.04850263804611597, ic: 0.46017586174831904
Epoch 20: train loss: 1.6258697143695031
Eval step 0: eval loss: 0.8328441945468914
Eval: total loss: 1.070115272854215, mse:4.590366137281212, ic :0.18639672927935816, sharpe5:16.64273617386818, irr5:540.6651611328125, ndcg5:0.8365556302217986 
train 21, step: 0, loss: 1.2974542373336801, grad_norm: 0.12898787819572466, ic: 0.25913695616052296
train 21, step: 500, loss: 2.854851264458198, grad_norm: 0.7995774097545758, ic: 0.16959795363893262
train 21, step: 1000, loss: 1.4071136102443789, grad_norm: 0.19178639481882742, ic: -0.012554140299825637
train 21, step: 1500, loss: 0.7770716667175294, grad_norm: 0.002328463626479341, ic: 0.14900309658348274
train 21, step: 2000, loss: 1.1023456790904307, grad_norm: 2.2508958626064564, ic: 0.4334337253032153
Epoch 21: train loss: 1.6254309209412365
Eval step 0: eval loss: 0.8293178546290503
Eval: total loss: 1.069088634102087, mse:4.5990224304844185, ic :0.17914606384073772, sharpe5:15.476777735352515, irr5:513.8733520507812, ndcg5:0.8536700937436509 
train 22, step: 0, loss: 2.888529505979499, grad_norm: 0.9703095270025284, ic: -0.04146645194962216
train 22, step: 500, loss: 2.1191417406464206, grad_norm: 0.2052753760344723, ic: 0.23259661591553288
train 22, step: 1000, loss: 1.7346665002349626, grad_norm: 0.878894910180587, ic: 0.0508819443313476
train 22, step: 1500, loss: 1.0537887351385804, grad_norm: 0.2506911310511327, ic: 0.10623388702842661
train 22, step: 2000, loss: 2.301410264418389, grad_norm: 0.25442157194403836, ic: 0.058353472242915434
Epoch 22: train loss: 1.6286677852645257
Eval step 0: eval loss: 0.8753250491882902
Eval: total loss: 1.0954059634774276, mse:4.784634342522885, ic :0.11213347243104546, sharpe5:12.393134640455246, irr5:405.3008728027344, ndcg5:0.8545137804076748 
train 23, step: 0, loss: 1.8521016903300989, grad_norm: 0.801512418616297, ic: 0.18146050402003955
train 23, step: 500, loss: 1.1230491216928682, grad_norm: 0.13373282529622077, ic: 0.0637373240100784
train 23, step: 1000, loss: 1.669073551756317, grad_norm: 1.3113410079353323, ic: 0.06347101132098919
train 23, step: 1500, loss: 0.8912512313487918, grad_norm: 0.0366575032036625, ic: 0.19437181880578336
train 23, step: 2000, loss: 1.0378886047221008, grad_norm: 0.4594348020496201, ic: 0.16159131033168778
Epoch 23: train loss: 1.6364217322835608
Eval step 0: eval loss: 0.8266979092918532
Eval: total loss: 1.0680913768929299, mse:4.6023052558687425, ic :0.1792982458567257, sharpe5:15.753419385552405, irr5:517.0770263671875, ndcg5:0.8641448379339133 
train 24, step: 0, loss: 1.7976801963601534, grad_norm: 0.9082362708193117, ic: 0.10006882820491787
train 24, step: 500, loss: 0.9501435584628705, grad_norm: 23.308370787730027, ic: 0.3701808447787934
train 24, step: 1000, loss: 0.9888375586564843, grad_norm: 0.37145821136528545, ic: 0.20810687375993336
train 24, step: 1500, loss: 0.9387066587864461, grad_norm: 0.5103118495181014, ic: 0.06032522085748809
train 24, step: 2000, loss: 1.7438856915729803, grad_norm: 0.9472304403013485, ic: 0.05579681957269832
Epoch 24: train loss: 1.6279328117820304
Eval step 0: eval loss: 0.8279089651318822
Eval: total loss: 1.0683952027583832, mse:4.5962160535378915, ic :0.18083951041084126, sharpe5:15.634502334594726, irr5:504.56866455078125, ndcg5:0.8438389772519905 
train 25, step: 0, loss: 1.7935233892396438, grad_norm: 0.34768010594496546, ic: 0.07416338707172905
train 25, step: 500, loss: 0.8940587511846568, grad_norm: 0.4988652949070354, ic: 0.1797366887740335
train 25, step: 1000, loss: 1.2381134751827414, grad_norm: 0.7267779659749694, ic: -0.028084593801747073
train 25, step: 1500, loss: 1.105876895680147, grad_norm: 0.22937630174978707, ic: 0.09269893870741348
train 25, step: 2000, loss: 1.0095357597072259, grad_norm: 0.954311678075459, ic: 0.5663725159405021
Epoch 25: train loss: 1.6302133052654308
Eval step 0: eval loss: 0.8343810713580083
Eval: total loss: 1.0715447661312996, mse:4.593370072212722, ic :0.18421779287971343, sharpe5:16.302995603084565, irr5:513.9470825195312, ndcg5:0.8354765085704702 
train 26, step: 0, loss: 1.0614946854016012, grad_norm: 0.1902804205830546, ic: 0.20336473320580717
train 26, step: 500, loss: 0.6970039847847465, grad_norm: 0.09693844002964735, ic: 0.2141809164998642
train 26, step: 1000, loss: 1.3792892663814298, grad_norm: 0.3997411509737479, ic: 0.052935687587513894
train 26, step: 1500, loss: 2.1826368819835573, grad_norm: 0.9175136420042529, ic: 0.20508815984632045
train 26, step: 2000, loss: 0.8140404759138786, grad_norm: 0.22653550357504326, ic: 0.5904648359178924
Epoch 26: train loss: 1.626165690622297
Eval step 0: eval loss: 0.8238286781110049
Eval: total loss: 1.06923696498105, mse:4.6188290473803155, ic :0.17489434265771683, sharpe5:15.379747513532637, irr5:497.3629150390625, ndcg5:0.8514904770142319 
train 27, step: 0, loss: 1.3421545391576293, grad_norm: 0.2288327968500188, ic: 0.031434956593886135
train 27, step: 500, loss: 1.5581968365293561, grad_norm: 0.3740634672428301, ic: 0.24049751338765024
train 27, step: 1000, loss: 0.9576411073507085, grad_norm: 0.048893967976870414, ic: 0.2283829238880917
train 27, step: 1500, loss: 0.7951810376538255, grad_norm: 0.005502693811632622, ic: 0.07762349387600637
train 27, step: 2000, loss: 0.8807560025471827, grad_norm: 0.28022862125468884, ic: 0.16713951945229158
Epoch 27: train loss: 1.6327018831501534
Eval step 0: eval loss: 0.8336896826017518
Eval: total loss: 1.0699865751833755, mse:4.614825217451688, ic :0.16482341964969704, sharpe5:11.88836314022541, irr5:395.63043212890625, ndcg5:0.846690244303021 
train 28, step: 0, loss: 3.6407867687309596, grad_norm: 1.183691676022746, ic: 0.017688062581740306
train 28, step: 500, loss: 0.9064245334774287, grad_norm: 0.004593627362355608, ic: 0.06009323735480752
train 28, step: 1000, loss: 1.4123836726302363, grad_norm: 0.08123087422948824, ic: 0.1806440156217654
train 28, step: 1500, loss: 1.2596373527280746, grad_norm: 0.05538595836453007, ic: 0.6531499021654515
train 28, step: 2000, loss: 1.1622979326210474, grad_norm: 0.03387291968678498, ic: -0.005435062395713173
Epoch 28: train loss: 1.6269295940534803
Eval step 0: eval loss: 0.8374570116981691
Eval: total loss: 1.0720325474435388, mse:4.592165055474763, ic :0.18696050060071895, sharpe5:17.134882403612135, irr5:540.3353881835938, ndcg5:0.8254513774313975 
train 29, step: 0, loss: 1.0646352972094801, grad_norm: 0.24250768330998135, ic: 0.06747464996621719
train 29, step: 500, loss: 0.8998101700183957, grad_norm: 0.06202416770294199, ic: 0.19417678308664085
train 29, step: 1000, loss: 0.7047045397269062, grad_norm: 0.06530660455627663, ic: 0.3698949322522086
train 29, step: 1500, loss: 2.0070580748555056, grad_norm: 1.4270272669017656, ic: 0.0060068469312743785
train 29, step: 2000, loss: 1.090662339154412, grad_norm: 0.02464730892687668, ic: 0.12476239120580772
Epoch 29: train loss: 1.6302545072164756
Eval step 0: eval loss: 0.8270464335402397
Eval: total loss: 1.0700888925959615, mse:4.66672633523075, ic :0.168787485311091, sharpe5:15.102873165607452, irr5:471.0849914550781, ndcg5:0.8443485197610209 
train 30, step: 0, loss: 1.0682192422510162, grad_norm: 0.343953186655801, ic: 0.12438528812117554
train 30, step: 500, loss: 1.200337304988212, grad_norm: 0.01023416832032816, ic: 0.04907769798946105
train 30, step: 1000, loss: 1.0493274948610023, grad_norm: 0.2172746969796463, ic: 0.23907568546685098
train 30, step: 1500, loss: 1.3328808482944876, grad_norm: 0.30582334346673967, ic: 0.026404554215238178
train 30, step: 2000, loss: 1.1384012924343445, grad_norm: 0.07951237606372852, ic: 0.12825378899532333
Epoch 30: train loss: 1.6250604892115377
Eval step 0: eval loss: 0.839855712633364
Eval: total loss: 1.0729256357908135, mse:4.616463552716166, ic :0.17676601740370526, sharpe5:16.038768281936644, irr5:508.6485900878906, ndcg5:0.8490241699312063 
train 31, step: 0, loss: 1.5653909454934845, grad_norm: 0.7459625023314367, ic: 0.11242961735090146
train 31, step: 500, loss: 0.9824416909845352, grad_norm: 0.26972242135698365, ic: -0.07860499733582207
train 31, step: 1000, loss: 3.805853866949314, grad_norm: 5.4745768608153735, ic: 0.11998225186446246
train 31, step: 1500, loss: 3.8869068807454035, grad_norm: 1.1088912138420808, ic: 0.04432469270354679
train 31, step: 2000, loss: 1.0017474541950862, grad_norm: 0.021925150847126772, ic: 0.11214423336126897
Epoch 31: train loss: 1.6238892049429803
Eval step 0: eval loss: 0.8286749595585814
Eval: total loss: 1.0686415158518632, mse:4.595389287959527, ic :0.18243256559224222, sharpe5:15.85790161013603, irr5:507.6755676269531, ndcg5:0.8591978550504856 
train 32, step: 0, loss: 8.010236608041465, grad_norm: 1.3383816318662707, ic: 0.26620267872365927
train 32, step: 500, loss: 1.2392200951888317, grad_norm: 0.45535306098450723, ic: 0.09818940104542337
train 32, step: 1000, loss: 0.7898732323643993, grad_norm: 0.00869461999899949, ic: 0.11012777576028179
train 32, step: 1500, loss: 1.364425566702178, grad_norm: 0.7379515100477656, ic: -0.21247208426570857
train 32, step: 2000, loss: 1.8955100835755814, grad_norm: 0.3492801011852561, ic: 0.07660443398598582
Epoch 32: train loss: 1.6258041972129769
Eval step 0: eval loss: 0.8382375413675579
Eval: total loss: 1.0707812843906337, mse:4.578535791142793, ic :0.18907900654138124, sharpe5:16.962932027578354, irr5:539.5877075195312, ndcg5:0.8438704429413979 
train 33, step: 0, loss: 1.438162452450886, grad_norm: 0.35133313362796637, ic: 0.01810299825957086
train 33, step: 500, loss: 0.862651736237282, grad_norm: 0.0422399881938133, ic: 0.5145681781034414
train 33, step: 1000, loss: 1.3650770253350788, grad_norm: 0.08565601256554889, ic: -0.05063080159503722
train 33, step: 1500, loss: 0.7268983587165111, grad_norm: 0.08836467089468565, ic: 0.5999308014697833
train 33, step: 2000, loss: 1.2236966875834288, grad_norm: 0.19763306540648135, ic: -0.04419365292366774
Epoch 33: train loss: 1.6243624912335672
Eval step 0: eval loss: 0.833345596104452
Eval: total loss: 1.0702078594181432, mse:4.6103610787561475, ic :0.17330776643933984, sharpe5:16.10144721150398, irr5:505.5973815917969, ndcg5:0.848442127892 
train 34, step: 0, loss: 1.2160097947761193, grad_norm: 0.3492408864082245, ic: 0.5989124667798721
train 34, step: 500, loss: 1.3686007931935702, grad_norm: 0.377024212090648, ic: 0.5439950037355742
train 34, step: 1000, loss: 3.036522160947712, grad_norm: 8.78091236039101, ic: 0.21778201172047137
train 34, step: 1500, loss: 0.913697354868488, grad_norm: 0.2163441497336723, ic: 0.5017117612807738
train 34, step: 2000, loss: 1.7381631926033936, grad_norm: 1.325020023953349, ic: 0.36020371788950684
Epoch 34: train loss: 1.6242490240809098
Eval step 0: eval loss: 0.8337956741059668
Eval: total loss: 1.0698649960033169, mse:4.592767263462442, ic :0.18193170560017424, sharpe5:16.54383585691452, irr5:534.4193115234375, ndcg5:0.8531873638952764 
train 35, step: 0, loss: 1.8459352903728243, grad_norm: 0.9564820537572731, ic: 0.09211440264021971
train 35, step: 500, loss: 0.9352531780758078, grad_norm: 0.1909523193351874, ic: 0.15937694944263153
train 35, step: 1000, loss: 0.7881177357049188, grad_norm: 0.15743160377871038, ic: 0.16372788232527372
train 35, step: 1500, loss: 1.2288414981286075, grad_norm: 0.3176426729791243, ic: 0.2594547021283625
train 35, step: 2000, loss: 1.7732994367598074, grad_norm: 0.6811506184435843, ic: 0.17480097964951102
Epoch 35: train loss: 1.623071509446755
Eval step 0: eval loss: 0.8320091255597998
Eval: total loss: 1.0681166056760538, mse:4.5944443492254186, ic :0.18774250293209277, sharpe5:16.399141384363173, irr5:537.4024047851562, ndcg5:0.8532315638950341 
train 36, step: 0, loss: 0.7279041828748511, grad_norm: 0.08835884870098322, ic: 0.6022237933106915
train 36, step: 500, loss: 1.3819763691066704, grad_norm: 0.523268140211384, ic: 0.2422137925119108
train 36, step: 1000, loss: 1.1756750936158276, grad_norm: 0.745575105474191, ic: 0.3920238891575103
train 36, step: 1500, loss: 1.9257622213924632, grad_norm: 0.840346838396657, ic: 0.22797015123884506
train 36, step: 2000, loss: 1.242170963838946, grad_norm: 0.34621049517207203, ic: 0.09070023488189867
Epoch 36: train loss: 1.6252706760656266
Eval step 0: eval loss: 0.8350045432280689
Eval: total loss: 1.070684372927361, mse:4.5937924722795636, ic :0.18320660264281424, sharpe5:15.644091183543205, irr5:511.3173828125, ndcg5:0.8437053435376007 
train 37, step: 0, loss: 1.8228007673652253, grad_norm: 0.8750756563545755, ic: 0.5177308053063066
train 37, step: 500, loss: 1.7034072635301656, grad_norm: 1.0169305014788157, ic: 0.02612385962475628
train 37, step: 1000, loss: 4.949799031732625, grad_norm: 2.6787857548730645, ic: 0.09654027502687541
train 37, step: 1500, loss: 1.0535076449376455, grad_norm: 0.05218402724914717, ic: 0.21235398724482407
train 37, step: 2000, loss: 1.6343895965715098, grad_norm: 0.13654410931925953, ic: 0.11592705137808823
Epoch 37: train loss: 1.6230964190794435
Eval step 0: eval loss: 0.8191236972306375
Eval: total loss: 1.0665328480783947, mse:4.6083016615362595, ic :0.18651224710669698, sharpe5:16.67424117565155, irr5:543.7493286132812, ndcg5:0.8485993165623307 
train 38, step: 0, loss: 0.8326370804398149, grad_norm: 0.22868920217874922, ic: 0.16279278088570534
train 38, step: 500, loss: 0.7723691003141645, grad_norm: 0.05482059742934844, ic: 0.7109641835114835
train 38, step: 1000, loss: 5.520232675964929, grad_norm: 2.191320396102074, ic: 0.10559610096926145
train 38, step: 1500, loss: 2.1881736575787776, grad_norm: 0.45931166429305664, ic: 0.20080772770659341
train 38, step: 2000, loss: 1.3556947921694016, grad_norm: 1.5584442639109493, ic: 0.5970699237882167
Epoch 38: train loss: 1.6242852919483135
Eval step 0: eval loss: 0.8323874277611301
Eval: total loss: 1.070006504701179, mse:4.59785794806778, ic :0.1811593027077818, sharpe5:16.373323287963867, irr5:506.1471252441406, ndcg5:0.8549262768980506 
train 39, step: 0, loss: 0.9294629225649019, grad_norm: 0.0015515382927310697, ic: 0.05451828642713724
train 39, step: 500, loss: 1.375765434014515, grad_norm: 0.6547961810343028, ic: 0.07538715377740846
train 39, step: 1000, loss: 0.7350298024615618, grad_norm: 0.020296881628988385, ic: 0.341071005195717
train 39, step: 1500, loss: 1.3459702851440238, grad_norm: 0.4392181063610773, ic: 0.22304313645321136
train 39, step: 2000, loss: 3.082322510024523, grad_norm: 1.340215586998527, ic: -0.09000534320161552
Epoch 39: train loss: 1.6237634203623112
Eval step 0: eval loss: 0.8248941242632046
Eval: total loss: 1.0667115640093738, mse:4.600353445170838, ic :0.186103572055246, sharpe5:16.80188060760498, irr5:542.2230224609375, ndcg5:0.8451331182613219 
