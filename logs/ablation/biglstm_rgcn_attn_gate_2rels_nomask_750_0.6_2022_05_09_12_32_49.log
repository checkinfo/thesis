Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=False, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
60595
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795778944121384, grad_norm: 4.817658377942883, ic: 0.022458513930623418
train 0, step: 500, loss: 0.8631304789056613, grad_norm: 0.026563994022124476, ic: 0.04523978734958649
train 0, step: 1000, loss: 1.950086904775512, grad_norm: 0.5128287216550775, ic: 0.015688447743498563
train 0, step: 1500, loss: 0.9563700438488142, grad_norm: 0.04898932044593732, ic: 0.015595302503716352
train 0, step: 2000, loss: 1.0022332100150737, grad_norm: 0.15819944025161586, ic: 0.025619369092713007
Epoch 0: 2022-05-09 12:40:19.822330: train loss: 1.6484912163076408
Eval step 0: eval loss: 0.8363100781455808
Eval: 2022-05-09 12:40:40.881121: total loss: 1.0793433508062125, mse:4.822845311792515, ic :0.0085121825567249, sharpe5:7.9187035918235775, irr5:224.28302001953125, ndcg5:0.8594152147053226, pnl5:2.6084134578704834 
train 1, step: 0, loss: 2.7747637348790324, grad_norm: 0.8786347498327713, ic: 0.057035801915921384
train 1, step: 500, loss: 1.7554637411992606, grad_norm: 0.7705417466776576, ic: 0.09620242637257266
train 1, step: 1000, loss: 0.8777560080520947, grad_norm: 0.17539855330635212, ic: 0.08006859567587618
train 1, step: 1500, loss: 1.7132606243265085, grad_norm: 0.20669848926744824, ic: -0.03170224378848118
train 1, step: 2000, loss: 2.17727578125, grad_norm: 0.8880295650735819, ic: -0.045796855292330935
Epoch 1: 2022-05-09 12:46:48.854322: train loss: 1.6467353734950543
Eval step 0: eval loss: 0.8346058530978332
Eval: 2022-05-09 12:47:10.034916: total loss: 1.0789709222330692, mse:4.823388067015644, ic :0.008612674650705726, sharpe5:7.627396782338619, irr5:214.6351318359375, ndcg5:0.8375663615809751, pnl5:2.528751850128174 
train 2, step: 0, loss: 2.141786221590909, grad_norm: 0.009292774051876182, ic: 0.1303543558250294
train 2, step: 500, loss: 3.3000003056533647, grad_norm: 0.2847317412341909, ic: 0.0485079160575196
train 2, step: 1000, loss: 2.0723007962164752, grad_norm: 0.00020848695980903932, ic: 0.193030724359117
train 2, step: 1500, loss: 1.4855513478053435, grad_norm: 0.059768323071297536, ic: -0.039750523121475714
train 2, step: 2000, loss: 3.2351551231971154, grad_norm: 0.7876367971785165, ic: 0.20517303767313833
Epoch 2: 2022-05-09 12:53:17.205233: train loss: 1.646496983119928
Eval step 0: eval loss: 0.8359059212205281
Eval: 2022-05-09 12:53:38.333935: total loss: 1.0793904426195338, mse:4.822523999652592, ic :0.012873841019284342, sharpe5:7.5055005383491515, irr5:214.56234741210938, ndcg5:0.8592174790831038, pnl5:2.77880859375 
train 3, step: 0, loss: 1.5229438595655487, grad_norm: 0.5250099774816224, ic: -0.0006378468949814146
train 3, step: 500, loss: 1.5013342657963, grad_norm: 0.34227139195330025, ic: 0.09469295101331784
train 3, step: 1000, loss: 3.679821868703224, grad_norm: 0.7054448973883538, ic: -0.050137433840611215
train 3, step: 1500, loss: 1.9822516558408347, grad_norm: 1.2455424564721105, ic: -0.069105018179874
train 3, step: 2000, loss: 0.8991418127111487, grad_norm: 0.0012790971761314244, ic: 0.010399180704099995
Epoch 3: 2022-05-09 12:59:40.043858: train loss: 1.6459747699516114
Eval step 0: eval loss: 0.8351965885142254
Eval: 2022-05-09 13:00:00.930207: total loss: 1.0789668476262997, mse:4.8223489082112465, ic :0.020255142267274134, sharpe5:8.008284648656845, irr5:228.93409729003906, ndcg5:0.8402967289501297, pnl5:2.850538969039917 
train 4, step: 0, loss: 1.431822285554847, grad_norm: 0.04470664668566931, ic: 0.12161057976833513
train 4, step: 500, loss: 1.6492764210137796, grad_norm: 0.5616727060486, ic: 0.026718607119606788
train 4, step: 1000, loss: 2.9722249100847944, grad_norm: 0.7658976424977095, ic: 0.0738789402137549
train 4, step: 1500, loss: 2.1490036590189874, grad_norm: 0.47852787537690344, ic: 0.010620432593240473
train 4, step: 2000, loss: 1.0866642265260498, grad_norm: 0.39199405378121666, ic: 0.25615055511793206
Epoch 4: 2022-05-09 13:06:01.273691: train loss: 1.6454730090980907
Eval step 0: eval loss: 0.8524908260751448
Eval: 2022-05-09 13:06:22.333985: total loss: 1.0856251264467578, mse:4.821207032752694, ic :0.07044175181500015, sharpe5:10.274328204989432, irr5:186.53701782226562, ndcg5:0.8425957352207237, pnl5:1.8298701047897339 
train 5, step: 0, loss: 1.3560549332791527, grad_norm: 0.16215203116647384, ic: 0.15247221889324955
train 5, step: 500, loss: 0.8912212181242566, grad_norm: 0.011037651673859304, ic: 0.10116203417520304
train 5, step: 1000, loss: 0.9777989179238507, grad_norm: 0.14708860557319642, ic: -0.0023655960719340847
train 5, step: 1500, loss: 1.5402446939605667, grad_norm: 0.18901700455164386, ic: 0.04745464242569809
train 5, step: 2000, loss: 1.1024343397667125, grad_norm: 0.038916938265755714, ic: 0.18231352233260675
Epoch 5: 2022-05-09 13:12:27.070656: train loss: 1.6415512692603245
Eval step 0: eval loss: 0.8307398644337789
Eval: 2022-05-09 13:12:48.082586: total loss: 1.0740313223286746, mse:4.761919493318724, ic :0.11761372146964376, sharpe5:17.70868472456932, irr5:525.669189453125, ndcg5:0.8413620483989065, pnl5:5.669888496398926 
train 6, step: 0, loss: 1.33943908788751, grad_norm: 0.4438599260962164, ic: 0.18369337066078162
train 6, step: 500, loss: 1.0071187742271248, grad_norm: 0.04462006436512523, ic: 0.06268934468580395
train 6, step: 1000, loss: 1.1210148452352662, grad_norm: 0.08786027960815634, ic: 0.13304829023970335
train 6, step: 1500, loss: 1.568166887267562, grad_norm: 0.705835014860979, ic: 0.14765169495895442
train 6, step: 2000, loss: 0.8029525671318304, grad_norm: 0.060955215523481134, ic: 0.3580837366421545
Epoch 6: 2022-05-09 13:18:50.286977: train loss: 1.6307049705364582
Eval step 0: eval loss: 0.8258455395070469
Eval: 2022-05-09 13:19:11.426801: total loss: 1.070795751513659, mse:4.746128298738419, ic :0.13278291551440363, sharpe5:17.32654965519905, irr5:552.9708862304688, ndcg5:0.8439183541555619, pnl5:6.630305767059326 
train 7, step: 0, loss: 0.994631862640381, grad_norm: 0.09753667047403929, ic: 0.027789126994720706
train 7, step: 500, loss: 0.6489916398532485, grad_norm: 0.03189276644782825, ic: 0.09234693343582723
train 7, step: 1000, loss: 1.026529792965775, grad_norm: 0.22499611628419425, ic: 0.06797997437828204
train 7, step: 1500, loss: 2.249499158460611, grad_norm: 0.7849187202125787, ic: 0.440748449526007
train 7, step: 2000, loss: 0.9115476346362581, grad_norm: 0.05266691114571584, ic: -0.030776318891215574
Epoch 7: 2022-05-09 13:25:18.316635: train loss: 1.6284765004048765
Eval step 0: eval loss: 0.8317956633010076
Eval: 2022-05-09 13:25:39.439732: total loss: 1.074286878085779, mse:4.718983553282521, ic :0.15221606086546471, sharpe5:17.467218267917634, irr5:561.827880859375, ndcg5:0.8552137985363754, pnl5:10.034881591796875 
train 8, step: 0, loss: 3.599083588088768, grad_norm: 1.1039216514331247, ic: 0.16942169235548243
train 8, step: 500, loss: 2.765491798655965, grad_norm: 0.851628903139012, ic: 0.033061011299965165
train 8, step: 1000, loss: 3.0502781080163044, grad_norm: 0.8598301866571162, ic: 0.11641468162417717
train 8, step: 1500, loss: 0.7136055867563194, grad_norm: 0.04245698852139541, ic: 0.4607067837186174
train 8, step: 2000, loss: 1.0810604426760235, grad_norm: 0.3505842744564176, ic: 0.5363971150385802
Epoch 8: 2022-05-09 13:31:47.446735: train loss: 1.62829549591097
Eval step 0: eval loss: 0.8246319110576923
Eval: 2022-05-09 13:32:08.569909: total loss: 1.0698226650116034, mse:4.680660417848203, ic :0.17097312073573068, sharpe5:17.535595453977585, irr5:569.3712768554688, ndcg5:0.8362685415257054, pnl5:11.556220054626465 
train 9, step: 0, loss: 5.439534894587321, grad_norm: 0.8846629685616106, ic: 0.1404609986672934
train 9, step: 500, loss: 1.3481335173872182, grad_norm: 1.405096595110385, ic: 0.3299813201812283
train 9, step: 1000, loss: 0.9298680999204638, grad_norm: 0.21779004053372963, ic: 0.049680833258220064
train 9, step: 1500, loss: 1.0867406651267948, grad_norm: 0.0181687610389817, ic: 0.4096302586842053
train 9, step: 2000, loss: 1.0630594270305105, grad_norm: 0.20381334499182235, ic: 0.25987016192964424
Epoch 9: 2022-05-09 13:38:12.404123: train loss: 1.6269259305533494
Eval step 0: eval loss: 0.8290558986844704
Eval: 2022-05-09 13:38:33.537752: total loss: 1.0747674080056302, mse:4.69336672687871, ic :0.15787194005852767, sharpe5:16.029344969987868, irr5:558.6639404296875, ndcg5:0.8405888550399585, pnl5:3.038334369659424 
train 10, step: 0, loss: 7.161374817784257, grad_norm: 2.586446817946194, ic: 0.2841947709959646
train 10, step: 500, loss: 1.1271138093909439, grad_norm: 0.07016595867401595, ic: 0.08622362199830494
train 10, step: 1000, loss: 2.396713537672546, grad_norm: 0.7855085746994952, ic: 0.1602605235896633
train 10, step: 1500, loss: 1.131740817775974, grad_norm: 0.5125144488797444, ic: 0.010430514263639641
train 10, step: 2000, loss: 2.736546107700893, grad_norm: 1.2548596968176535, ic: 0.43198540960062204
Epoch 10: 2022-05-09 13:44:37.581437: train loss: 1.62911846367323
Eval step 0: eval loss: 0.8237640413016991
Eval: 2022-05-09 13:44:58.542383: total loss: 1.0695669311251543, mse:4.675492596785449, ic :0.1759254362157842, sharpe5:17.840189208984373, irr5:604.1172485351562, ndcg5:0.8536800328581666, pnl5:6.421427249908447 
train 11, step: 0, loss: 1.2546382942406227, grad_norm: 0.06791543538764341, ic: 0.20192566261627287
train 11, step: 500, loss: 0.6612898689603574, grad_norm: 0.02156018226176781, ic: 0.5265909909298643
train 11, step: 1000, loss: 0.9273601132587078, grad_norm: 0.10962492152650408, ic: 0.07441665735038763
train 11, step: 1500, loss: 1.0565823337488007, grad_norm: 0.059108869285238695, ic: 0.1773730474145861
train 11, step: 2000, loss: 0.7876847735701767, grad_norm: 0.004625973608295762, ic: 0.12799140460548913
Epoch 11: 2022-05-09 13:51:05.941386: train loss: 1.6264655056070743
Eval step 0: eval loss: 0.8275942706961275
Eval: 2022-05-09 13:51:26.986148: total loss: 1.0696565485777418, mse:4.670342309656663, ic :0.1765907608344175, sharpe5:17.83114994287491, irr5:592.57177734375, ndcg5:0.8413200082918658, pnl5:5.981510162353516 
train 12, step: 0, loss: 0.9559458096822102, grad_norm: 0.22083645130569515, ic: 0.3993345255902263
train 12, step: 500, loss: 0.9302287683635341, grad_norm: 0.06757957770610568, ic: 0.18995405375156266
train 12, step: 1000, loss: 2.957361889492934, grad_norm: 0.36093308541459435, ic: 0.3379910814013338
train 12, step: 1500, loss: 0.9348174281821211, grad_norm: 0.11829355308277582, ic: -0.13795480857948678
train 12, step: 2000, loss: 0.8732052713722621, grad_norm: 0.005430205438416278, ic: 0.22086939167095665
Epoch 12: 2022-05-09 13:57:31.495451: train loss: 1.6247845234318892
Eval step 0: eval loss: 0.8249402382853661
Eval: 2022-05-09 13:57:52.613081: total loss: 1.0684952243804036, mse:4.666340252687082, ic :0.18026961474510678, sharpe5:17.539415102005005, irr5:582.7564086914062, ndcg5:0.8372289493896337, pnl5:6.954673767089844 
train 13, step: 0, loss: 2.0513448100910767, grad_norm: 0.7325184438435421, ic: 0.3066198872413738
train 13, step: 500, loss: 0.8230678251300307, grad_norm: 0.029139289422746656, ic: 0.5317682027975194
train 13, step: 1000, loss: 0.967346601038171, grad_norm: 0.34842757193172863, ic: 0.5041071162807249
train 13, step: 1500, loss: 2.37609243876854, grad_norm: 0.290150870302148, ic: -0.07795835940796772
train 13, step: 2000, loss: 1.4874290131531465, grad_norm: 0.12307235926260773, ic: 0.16723548252783832
Epoch 13: 2022-05-09 14:03:57.128292: train loss: 1.6258022765856612
Eval step 0: eval loss: 0.8218575448457257
Eval: 2022-05-09 14:04:18.240179: total loss: 1.0701874550397075, mse:4.668943669394984, ic :0.17674194513679417, sharpe5:18.541018002033233, irr5:615.6292114257812, ndcg5:0.8396489576608666, pnl5:19.472692489624023 
train 14, step: 0, loss: 4.542814495782957, grad_norm: 1.3617548886519246, ic: 0.1867433550166565
train 14, step: 500, loss: 0.8276873014024274, grad_norm: 0.0032192106505637636, ic: 0.1424026613970368
train 14, step: 1000, loss: 1.8550190270263858, grad_norm: 0.22678878798184787, ic: 0.3826598744386606
train 14, step: 1500, loss: 1.1288874699519231, grad_norm: 0.07156967379077434, ic: -0.06985443714963098
train 14, step: 2000, loss: 1.1622763365991153, grad_norm: 0.24069095815048586, ic: 0.05744584361272852
Epoch 14: 2022-05-09 14:10:23.138157: train loss: 1.6232371751250994
Eval step 0: eval loss: 0.8301989090078371
Eval: 2022-05-09 14:10:44.387327: total loss: 1.073223388064929, mse:4.721694206072257, ic :0.15718387558287275, sharpe5:18.609124653339386, irr5:592.1542358398438, ndcg5:0.8488866377896334, pnl5:8.756904602050781 
train 15, step: 0, loss: 3.4109192607003895, grad_norm: 1.1178091004238706, ic: 0.12487145891039797
train 15, step: 500, loss: 1.2613284902976933, grad_norm: 0.04265029560272744, ic: 0.03345004498762849
train 15, step: 1000, loss: 1.3240037871570123, grad_norm: 0.13879019971308815, ic: 0.028189899109247976
train 15, step: 1500, loss: 0.8513385442298228, grad_norm: 0.20463160242400918, ic: 0.06850332809185794
train 15, step: 2000, loss: 1.464671947337963, grad_norm: 0.6381329343150429, ic: 0.05483618643197059
Epoch 15: 2022-05-09 14:16:46.029157: train loss: 1.6209825946531287
Eval step 0: eval loss: 0.8429271509072049
Eval: 2022-05-09 14:17:07.070980: total loss: 1.0702572262129115, mse:4.579677764108209, ic :0.19686052899186982, sharpe5:17.858511790037156, irr5:590.670166015625, ndcg5:0.850800552579367, pnl5:7.580511569976807 
train 16, step: 0, loss: 0.6982132405019051, grad_norm: 0.3916140860798061, ic: -0.06399027681121029
train 16, step: 500, loss: 1.6200047458145845, grad_norm: 0.4999419290382809, ic: 0.1617349755790382
train 16, step: 1000, loss: 0.8836817885890151, grad_norm: 0.0118670968088855, ic: -0.12276862631282312
train 16, step: 1500, loss: 0.8448862447054768, grad_norm: 0.23729689869976792, ic: 0.1444794028628846
train 16, step: 2000, loss: 3.328490706931255, grad_norm: 1.0785155431896203, ic: 0.057990574041908424
Epoch 16: 2022-05-09 14:23:14.031165: train loss: 1.6192747360176616
Eval step 0: eval loss: 0.8258063072148314
Eval: 2022-05-09 14:23:35.301429: total loss: 1.0666486593207005, mse:4.587390031660868, ic :0.19260559449123982, sharpe5:18.16186070680618, irr5:598.3734130859375, ndcg5:0.8386267398219206, pnl5:8.355916023254395 
train 17, step: 0, loss: 1.2826895878232758, grad_norm: 0.25400234999495985, ic: -0.11316177592323
train 17, step: 500, loss: 1.731660606156843, grad_norm: 0.5319599111553676, ic: 0.24711489689816948
train 17, step: 1000, loss: 1.283612830767829, grad_norm: 0.08285592892314049, ic: 0.15209354888224663
train 17, step: 1500, loss: 4.5232832303912565, grad_norm: 1.320890234615276, ic: 0.22629721545928633
train 17, step: 2000, loss: 1.276523926946599, grad_norm: 0.7522600484915689, ic: 0.06401690858567224
Epoch 17: 2022-05-09 14:29:43.583306: train loss: 1.6205214683094233
Eval step 0: eval loss: 0.8351055824593321
Eval: 2022-05-09 14:30:04.842366: total loss: 1.0672629687690702, mse:4.583026279841693, ic :0.1965712235858595, sharpe5:18.150449059009553, irr5:614.6743774414062, ndcg5:0.8519874328349656, pnl5:14.747727394104004 
train 18, step: 0, loss: 1.436961200272915, grad_norm: 0.6205657080460225, ic: 0.13652878882542846
train 18, step: 500, loss: 1.4592535405316787, grad_norm: 1.3346879519498687, ic: -0.04532106594244828
train 18, step: 1000, loss: 0.6518444322559931, grad_norm: 0.014629778706326951, ic: 0.5746255988677766
train 18, step: 1500, loss: 1.4213015394656021, grad_norm: 0.05522793748368156, ic: 0.22303537437800974
train 18, step: 2000, loss: 0.9105385944342158, grad_norm: 0.005680071619165139, ic: -0.0060937193155329444
Epoch 18: 2022-05-09 14:36:06.213524: train loss: 1.6154165155848625
Eval step 0: eval loss: 0.8214320995785036
Eval: 2022-05-09 14:36:27.350939: total loss: 1.0668549360592636, mse:4.625745329575453, ic :0.18846291171997537, sharpe5:18.513319405317304, irr5:630.2703247070312, ndcg5:0.8513928654033438, pnl5:10.413586616516113 
train 19, step: 0, loss: 1.4828836108010912, grad_norm: 0.8305004365459728, ic: 0.04479657231869655
train 19, step: 500, loss: 0.8587461400915075, grad_norm: 0.03380036388172229, ic: 0.228899296893467
train 19, step: 1000, loss: 0.9572048549319002, grad_norm: 0.015546948804342686, ic: 0.21025069457161344
train 19, step: 1500, loss: 3.9533717141382643, grad_norm: 1.22511939526325, ic: 0.14250126005397307
train 19, step: 2000, loss: 1.0023934232271634, grad_norm: 0.15070813073597564, ic: 0.25667961485589663
Epoch 19: 2022-05-09 14:42:32.132291: train loss: 1.6208362559165352
Eval step 0: eval loss: 0.8277239945213711
Eval: 2022-05-09 14:42:53.282827: total loss: 1.0672902433273557, mse:4.608305978640724, ic :0.1900347650934626, sharpe5:18.47707153201103, irr5:627.9638671875, ndcg5:0.852922235618931, pnl5:10.048839569091797 
train 20, step: 0, loss: 2.3308541640933793, grad_norm: 1.1642688950858797, ic: 0.047169736486039344
train 20, step: 500, loss: 3.216764914772727, grad_norm: 0.5710279162207132, ic: 0.12261021883046068
train 20, step: 1000, loss: 0.9684738159179688, grad_norm: 0.14324537184315111, ic: 0.17748512631937316
train 20, step: 1500, loss: 1.8074921261844372, grad_norm: 0.9246881413674037, ic: 0.2654986615661996
train 20, step: 2000, loss: 1.0391545007901712, grad_norm: 0.07521445815772863, ic: 0.0005357928967498395
Epoch 20: 2022-05-09 14:48:53.285545: train loss: 1.6187899172281337
Eval step 0: eval loss: 0.8269944025166293
Eval: 2022-05-09 14:49:14.333000: total loss: 1.0661757571599269, mse:4.5898503501128, ic :0.19523997833374904, sharpe5:18.6649096763134, irr5:630.0921020507812, ndcg5:0.8496009170073637, pnl5:10.2569580078125 
train 21, step: 0, loss: 1.0191933452219204, grad_norm: 0.47372390816423504, ic: 0.052669526272948096
train 21, step: 500, loss: 0.7638928573743432, grad_norm: 0.009139136215820971, ic: 0.1998738422145529
train 21, step: 1000, loss: 0.9348621033785636, grad_norm: 0.782792573117518, ic: 0.16660398844093105
train 21, step: 1500, loss: 0.9870928880377069, grad_norm: 0.30003178702148114, ic: 0.3188947187103932
train 21, step: 2000, loss: 0.9455632645954457, grad_norm: 0.07628152448735204, ic: 0.07250286133047569
Epoch 21: 2022-05-09 14:55:20.781314: train loss: 1.6169272084789708
Eval step 0: eval loss: 0.824811221927687
Eval: 2022-05-09 14:55:41.899959: total loss: 1.0649960611222726, mse:4.590743094567087, ic :0.1927920054340132, sharpe5:17.980316752195357, irr5:620.6415405273438, ndcg5:0.8515456867605595, pnl5:7.546708106994629 
train 22, step: 0, loss: 1.0371067887645657, grad_norm: 0.01542095859817365, ic: 0.2232989389584836
train 22, step: 500, loss: 3.2517248634400406, grad_norm: 0.8574468248885364, ic: -0.20641163763764891
train 22, step: 1000, loss: 1.1893959706918353, grad_norm: 0.0219536989076961, ic: 0.46594202842242843
train 22, step: 1500, loss: 0.9733087987075617, grad_norm: 0.0894886445737813, ic: 0.10373445510122604
train 22, step: 2000, loss: 1.7485470587974774, grad_norm: 1.7389358076515582, ic: 0.17570456102361365
Epoch 22: 2022-05-09 15:01:48.159656: train loss: 1.6145821998794383
Eval step 0: eval loss: 0.8233171790618413
Eval: 2022-05-09 15:02:09.174793: total loss: 1.0655458831118607, mse:4.59553189054213, ic :0.19222215134338796, sharpe5:18.64590413689613, irr5:631.4104614257812, ndcg5:0.8554203605941, pnl5:13.266800880432129 
train 23, step: 0, loss: 0.9783686481222983, grad_norm: 0.047148370097133616, ic: 0.1638613140608969
train 23, step: 500, loss: 1.4197898397640307, grad_norm: 0.17779073085043362, ic: 0.03954680734359906
train 23, step: 1000, loss: 1.6565730794270834, grad_norm: 0.07772668812386482, ic: 0.259801796989496
train 23, step: 1500, loss: 1.1102994741701842, grad_norm: 0.2544721193942697, ic: 0.11843786175298485
train 23, step: 2000, loss: 1.934192167051578, grad_norm: 1.339404033487992, ic: 0.43464146158636985
Epoch 23: 2022-05-09 15:08:05.939973: train loss: 1.6161257694707623
Eval step 0: eval loss: 0.832837570077878
Eval: 2022-05-09 15:08:25.800747: total loss: 1.065550963696371, mse:4.569597111514622, ic :0.19765651283274155, sharpe5:18.514057227373122, irr5:622.965576171875, ndcg5:0.8610983964078397, pnl5:9.681525230407715 
train 24, step: 0, loss: 2.1873530397886993, grad_norm: 0.281505570741493, ic: 0.17419620878175765
train 24, step: 500, loss: 1.214792831955253, grad_norm: 0.1872526200107565, ic: 0.17382666718699846
train 24, step: 1000, loss: 0.9086007395762998, grad_norm: 0.09640350538543192, ic: 0.5226399177256703
train 24, step: 1500, loss: 2.6206175449556883, grad_norm: 1.4859765987812967, ic: 0.025910354621784194
train 24, step: 2000, loss: 0.9329444990958639, grad_norm: 0.036210294007676516, ic: 0.12143346270503746
Epoch 24: 2022-05-09 15:14:10.602658: train loss: 1.6127924689027069
Eval step 0: eval loss: 0.8218040988869862
Eval: 2022-05-09 15:14:30.483256: total loss: 1.0696792831997828, mse:4.728918595739934, ic :0.18058270961949766, sharpe5:18.884407954216, irr5:637.1492309570312, ndcg5:0.8456682229583264, pnl5:6.315690040588379 
train 25, step: 0, loss: 0.8316981959987332, grad_norm: 0.09761933647394905, ic: 0.613096178932137
train 25, step: 500, loss: 0.8633231079470355, grad_norm: 0.008033600999440733, ic: 0.25868751599216355
train 25, step: 1000, loss: 2.114314152644231, grad_norm: 0.11484140506594064, ic: 0.23482760509726014
train 25, step: 1500, loss: 1.1345330471921515, grad_norm: 0.34007175514348376, ic: 0.536934812272443
train 25, step: 2000, loss: 0.995187413932382, grad_norm: 0.7833117795134711, ic: 0.5964122623439112
Epoch 25: 2022-05-09 15:20:14.055249: train loss: 1.6156235019296867
Eval step 0: eval loss: 0.8285966236046166
Eval: 2022-05-09 15:20:33.795579: total loss: 1.073074197796268, mse:4.84683342689318, ic :0.17219987295539516, sharpe5:18.255244385004044, irr5:613.6339111328125, ndcg5:0.8545268204283226, pnl5:8.251333236694336 
train 26, step: 0, loss: 6.732984256439696, grad_norm: 1.970525723870758, ic: 0.12041382803956525
train 26, step: 500, loss: 3.903394757256646, grad_norm: 2.3873573081054373, ic: 0.35537140956879965
train 26, step: 1000, loss: 1.2725864572090857, grad_norm: 0.9804923970616002, ic: 0.025119215770113203
train 26, step: 1500, loss: 0.8347028510258496, grad_norm: 0.16185483579531573, ic: 0.2968339439741237
train 26, step: 2000, loss: 0.9600519860100315, grad_norm: 0.30740811127785594, ic: 0.12431182591053205
Epoch 26: 2022-05-09 15:26:15.570519: train loss: 1.6119538036212846
Eval step 0: eval loss: 0.8252604638311709
Eval: 2022-05-09 15:26:35.372062: total loss: 1.0636688754137422, mse:4.579265695433481, ic :0.19719804164572408, sharpe5:19.381034088134765, irr5:656.9593505859375, ndcg5:0.8523521540022133, pnl5:5.7326531410217285 
train 27, step: 0, loss: 0.827119332107843, grad_norm: 0.008875702759506984, ic: 0.1028670279267313
train 27, step: 500, loss: 0.9302923216700473, grad_norm: 1.682874044824916, ic: 0.2963946093442048
train 27, step: 1000, loss: 0.754228749758314, grad_norm: 0.23269777973737493, ic: 0.18894524961037695
train 27, step: 1500, loss: 0.6370870862095348, grad_norm: 0.15171227397290193, ic: 0.5252409032634469
train 27, step: 2000, loss: 1.384819013382688, grad_norm: 0.03378734924994354, ic: 0.021890551742427006
Epoch 27: 2022-05-09 15:32:22.651595: train loss: 1.6123830672913417
Eval step 0: eval loss: 0.8281632032484851
Eval: 2022-05-09 15:32:42.650528: total loss: 1.0682852152904199, mse:4.595646023098556, ic :0.1881729192693991, sharpe5:18.091107355356215, irr5:623.9474487304688, ndcg5:0.8479363440364709, pnl5:6.832039833068848 
train 28, step: 0, loss: 1.539282886402027, grad_norm: 0.4187524841042233, ic: 0.20378029635708522
train 28, step: 500, loss: 1.3529621122135271, grad_norm: 1.189292180456257, ic: 0.14522097200585082
train 28, step: 1000, loss: 0.9249396595528455, grad_norm: 0.30217291830133086, ic: 0.5641479499950267
train 28, step: 1500, loss: 1.0302709714330809, grad_norm: 0.11775864878731866, ic: 0.0908137362820457
train 28, step: 2000, loss: 1.0467682820887654, grad_norm: 0.19311347923222094, ic: 0.07774804401728924
Epoch 28: 2022-05-09 15:38:28.568573: train loss: 1.6087443107134785
Eval step 0: eval loss: 0.8502979338859984
Eval: 2022-05-09 15:38:48.495686: total loss: 1.110529325564422, mse:4.846610891081662, ic :0.15271695256994844, sharpe5:17.656791239976883, irr5:599.5419921875, ndcg5:0.8509589751696234, pnl5:5.278279781341553 
train 29, step: 0, loss: 0.9075678781273692, grad_norm: 0.06021174719759148, ic: 0.09293457617994973
train 29, step: 500, loss: 1.0998645542792131, grad_norm: 0.09077217946485956, ic: 0.6158218086784725
train 29, step: 1000, loss: 1.0556743677451075, grad_norm: 0.3122729976743298, ic: 0.05654877883733114
train 29, step: 1500, loss: 2.3567315726239264, grad_norm: 0.2959863869747923, ic: -0.007054517989379928
train 29, step: 2000, loss: 3.7369829342689043, grad_norm: 14.766743448085016, ic: 0.2176990997446865
Epoch 29: 2022-05-09 15:44:33.572365: train loss: 1.6108331569183458
Eval step 0: eval loss: 0.8281108506487092
Eval: 2022-05-09 15:44:53.432889: total loss: 1.0640732159541437, mse:4.576307206077069, ic :0.19910941459156106, sharpe5:19.481466804742812, irr5:647.795654296875, ndcg5:0.8486868176510264, pnl5:8.734145164489746 
train 30, step: 0, loss: 1.001274573451232, grad_norm: 0.030755320450683736, ic: 0.5180616592237828
train 30, step: 500, loss: 1.4136310981080804, grad_norm: 1.5686852676214875, ic: 0.009255079001650374
train 30, step: 1000, loss: 0.9789929939038826, grad_norm: 0.06868426322498576, ic: -0.03940039463091506
train 30, step: 1500, loss: 1.4963257293619443, grad_norm: 2.6686826234575713, ic: 0.2087564076625798
train 30, step: 2000, loss: 1.8368471900105203, grad_norm: 0.20079758610863035, ic: 0.0971781235557236
Epoch 30: 2022-05-09 15:50:38.582900: train loss: 1.6110485974868207
Eval step 0: eval loss: 0.8258227075992821
Eval: 2022-05-09 15:50:58.386122: total loss: 1.0643770861487771, mse:4.600897484276465, ic :0.20151736732661857, sharpe5:18.8810404586792, irr5:655.3525390625, ndcg5:0.8427565931648631, pnl5:6.575326919555664 
train 31, step: 0, loss: 1.0460992601759007, grad_norm: 0.13927622267770834, ic: 0.3472208499709323
train 31, step: 500, loss: 1.5168848861882716, grad_norm: 1.059288906633335, ic: 0.011199270101335496
train 31, step: 1000, loss: 4.441612845189305, grad_norm: 3.254501716064588, ic: 0.4739429103737779
train 31, step: 1500, loss: 0.7694171443283955, grad_norm: 0.0517917779491329, ic: 0.7092040385368186
train 31, step: 2000, loss: 1.277061578350589, grad_norm: 3.8178883018585195, ic: 0.2071967377324024
Epoch 31: 2022-05-09 15:56:43.302315: train loss: 1.6076452147739
Eval step 0: eval loss: 0.8322908262809535
Eval: 2022-05-09 15:57:03.175467: total loss: 1.0649680923406557, mse:4.584857813038391, ic :0.19299315812924156, sharpe5:17.691930488348007, irr5:613.9815673828125, ndcg5:0.8438123944937247, pnl5:5.345663547515869 
train 32, step: 0, loss: 1.1171845505224571, grad_norm: 0.019152441701320456, ic: 0.19995532119924606
train 32, step: 500, loss: 1.5214247816190944, grad_norm: 1.377951909512995, ic: 0.056571294555512526
train 32, step: 1000, loss: 1.043873186901267, grad_norm: 0.14976608062807092, ic: 0.5072878720929199
train 32, step: 1500, loss: 0.9466673780451469, grad_norm: 0.8111986768271932, ic: 0.06131738448790208
train 32, step: 2000, loss: 0.9385924577565932, grad_norm: 0.08934478435412334, ic: 0.5653115243910896
Epoch 32: 2022-05-09 16:02:45.795878: train loss: 1.6067372036969836
Eval step 0: eval loss: 0.819805888908555
Eval: 2022-05-09 16:03:05.588868: total loss: 1.0655836022588436, mse:4.665927831579164, ic :0.1915888246536352, sharpe5:19.011277402639386, irr5:644.0305786132812, ndcg5:0.8569228542658769, pnl5:8.063440322875977 
train 33, step: 0, loss: 1.2628048154585132, grad_norm: 0.231915713891655, ic: 0.1984594674163384
train 33, step: 500, loss: 0.9896882758785333, grad_norm: 0.04455503981648084, ic: 0.13892229548213023
train 33, step: 1000, loss: 1.0522691863653872, grad_norm: 0.9147802132147744, ic: 0.21110923871583606
train 33, step: 1500, loss: 0.9057675833379671, grad_norm: 0.08217802628901266, ic: 0.552431531072127
train 33, step: 2000, loss: 0.8174658048360737, grad_norm: 0.9161247263633869, ic: 0.2362076280186797
Epoch 33: 2022-05-09 16:08:48.990407: train loss: 1.6103866482528812
Eval step 0: eval loss: 0.8215503109770153
Eval: 2022-05-09 16:09:08.963353: total loss: 1.065816451074651, mse:4.590380727480544, ic :0.20212723936634022, sharpe5:20.02227712869644, irr5:658.2235107421875, ndcg5:0.8401693969051565, pnl5:5.5549750328063965 
train 34, step: 0, loss: 1.0047269420107563, grad_norm: 0.9068761101661128, ic: 0.6064723824423535
train 34, step: 500, loss: 0.8173109515362195, grad_norm: 0.5828427118092476, ic: 0.22825745864960495
train 34, step: 1000, loss: 3.1339115078365016, grad_norm: 0.3906608221813497, ic: 0.3216046445641922
train 34, step: 1500, loss: 0.8009951276631406, grad_norm: 0.5758556708563048, ic: 0.6866756248208556
train 34, step: 2000, loss: 4.0878006087819525, grad_norm: 138.21666540085874, ic: 0.40959547301258453
Epoch 34: 2022-05-09 16:14:53.748497: train loss: 1.603833320016408
Eval step 0: eval loss: 0.8217610076807823
Eval: 2022-05-09 16:15:13.644341: total loss: 1.0644861943518014, mse:4.5862592590407205, ic :0.20237389226334243, sharpe5:18.790494557619095, irr5:652.7040405273438, ndcg5:0.8578175445496519, pnl5:10.183764457702637 
train 35, step: 0, loss: 1.1903091969209558, grad_norm: 0.9726460127052314, ic: 0.5538603037982641
train 35, step: 500, loss: 1.1683961458120402, grad_norm: 0.9881363412672315, ic: 0.13623390118750214
train 35, step: 1000, loss: 1.6989915851827893, grad_norm: 2.6742669435487416, ic: 0.05973640271593038
train 35, step: 1500, loss: 1.63988046287594, grad_norm: 1.435850992974052, ic: 0.06420444511309746
train 35, step: 2000, loss: 0.7883855646306818, grad_norm: 0.04492670343096175, ic: 0.5565549563510042
Epoch 35: 2022-05-09 16:20:58.166470: train loss: 1.604693650522263
Eval step 0: eval loss: 0.8266233036214107
Eval: 2022-05-09 16:21:18.100813: total loss: 1.0656159527815223, mse:4.576959357978135, ic :0.1965213109223614, sharpe5:18.053052763938904, irr5:631.8950805664062, ndcg5:0.8484502949835384, pnl5:5.2786784172058105 
train 36, step: 0, loss: 1.8461251011822999, grad_norm: 1.7223703427095112, ic: 0.09100674550478774
train 36, step: 500, loss: 0.8249428991824271, grad_norm: 0.07256691549072222, ic: 0.26869997659505596
train 36, step: 1000, loss: 1.6035983664772726, grad_norm: 0.33634759064189196, ic: 0.26826631919242305
train 36, step: 1500, loss: 0.7736886206038823, grad_norm: 0.06395648538301923, ic: 0.35148101773915774
train 36, step: 2000, loss: 1.1357810677186755, grad_norm: 1.516320291952926, ic: 0.7772828115996842
Epoch 36: 2022-05-09 16:27:03.054955: train loss: 1.6116641652341486
Eval step 0: eval loss: 0.8243982538156941
Eval: 2022-05-09 16:27:23.201529: total loss: 1.0632718644215373, mse:4.581697948055143, ic :0.19979211201346478, sharpe5:19.08845358967781, irr5:655.7109985351562, ndcg5:0.8316530950022912, pnl5:3.9804751873016357 
train 37, step: 0, loss: 2.0214163296803216, grad_norm: 1.3827637524203356, ic: 0.22744494893035766
train 37, step: 500, loss: 2.35745532983279, grad_norm: 1.4552368211969524, ic: -0.04566176726520901
train 37, step: 1000, loss: 1.0667883468000856, grad_norm: 0.171523842150213, ic: 0.06342802008754575
train 37, step: 1500, loss: 2.0254130460778064, grad_norm: 1.486768785068421, ic: 0.6098868089557432
train 37, step: 2000, loss: 1.3100348691211587, grad_norm: 0.12700449675978037, ic: 0.22687191171122853
Epoch 37: 2022-05-09 16:33:05.679520: train loss: 1.6041947046708904
Eval step 0: eval loss: 0.8223995293153977
Eval: 2022-05-09 16:33:25.716183: total loss: 1.0660832552836998, mse:4.600748121563346, ic :0.1945566159651273, sharpe5:19.171619378328323, irr5:653.8250732421875, ndcg5:0.8510433183903157, pnl5:4.57579231262207 
train 38, step: 0, loss: 1.3361612645591179, grad_norm: 0.4280062564644382, ic: -0.07996082287447998
train 38, step: 500, loss: 0.9027532130111883, grad_norm: 0.08267065609527025, ic: 0.2705537269099658
train 38, step: 1000, loss: 0.9084780485733696, grad_norm: 0.16331057765647217, ic: 0.10909458700916713
train 38, step: 1500, loss: 0.9494546413783694, grad_norm: 0.008747051493034308, ic: 0.2100890153956364
train 38, step: 2000, loss: 2.3289665844505287, grad_norm: 2.5355220600074713, ic: 0.041483432327937914
Epoch 38: 2022-05-09 16:39:09.959119: train loss: 1.6023517049246818
Eval step 0: eval loss: 0.8216354643456928
Eval: 2022-05-09 16:39:29.922242: total loss: 1.0653322642696401, mse:4.636955940887646, ic :0.1964949743620495, sharpe5:18.991971058845518, irr5:648.8982543945312, ndcg5:0.8500506057809857, pnl5:6.944239139556885 
train 39, step: 0, loss: 0.9666578193687883, grad_norm: 0.009998533429453036, ic: 0.08617459854607934
train 39, step: 500, loss: 0.8781858381173137, grad_norm: 0.1056576458365399, ic: 0.2226379915500315
train 39, step: 1000, loss: 0.9399271817581398, grad_norm: 0.11470540856763686, ic: 0.21697262050326355
train 39, step: 1500, loss: 2.0589417761365887, grad_norm: 0.19131044669702799, ic: 0.2225484021476941
train 39, step: 2000, loss: 0.6179491377554799, grad_norm: 0.04432788963181999, ic: 0.12356527478362184
Epoch 39: 2022-05-09 16:45:14.460572: train loss: 1.6103836573985728
Eval step 0: eval loss: 0.8264236048225104
Eval: 2022-05-09 16:45:34.191331: total loss: 1.0641353176376116, mse:4.586645337970944, ic :0.19694984604139484, sharpe5:18.50603199362755, irr5:645.071533203125, ndcg5:0.857351533234566, pnl5:4.967275619506836 
