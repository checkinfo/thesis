Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNAdjTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
6189
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795769399921814, grad_norm: 4.817410748300853, ic: 0.022639284186324875
train 0, step: 500, loss: 0.8631121729582706, grad_norm: 0.026539013055886733, ic: 0.0452333632140176
train 0, step: 1000, loss: 1.9500708311425567, grad_norm: 0.5128282975973397, ic: 0.015577196346035584
train 0, step: 1500, loss: 0.956321698215168, grad_norm: 0.04890907998762369, ic: 0.016722293915968127
train 0, step: 2000, loss: 1.0022300462056786, grad_norm: 0.1581953874340771, ic: 0.025557711268895906
Epoch 0: 2022-05-08 17:46:04.210585: train loss: 1.6484905538827823
Eval step 0: eval loss: 0.8363101424608139
Eval: 2022-05-08 17:46:34.569693: total loss: 1.0793432954228601, mse:4.822845386623999, ic :0.00851812742216696, sharpe5:7.9187035918235775, irr5:224.28302001953125, ndcg5:0.8418168462347985, pnl5:2.6084134578704834 
train 1, step: 0, loss: 2.774759797127016, grad_norm: 0.8787270114373624, ic: 0.057297651374835265
train 1, step: 500, loss: 1.755490782577486, grad_norm: 0.7706146748821112, ic: 0.09686715963979954
train 1, step: 1000, loss: 0.8777525317890882, grad_norm: 0.17539568385758636, ic: 0.08011237480568102
train 1, step: 1500, loss: 1.7132346668462644, grad_norm: 0.2066517021251713, ic: -0.03192894655525072
train 1, step: 2000, loss: 2.1772373046875, grad_norm: 0.8873953941713526, ic: -0.04657413193758908
Epoch 1: 2022-05-08 17:56:23.530160: train loss: 1.646735874820108
Eval step 0: eval loss: 0.8346084900223919
Eval: 2022-05-08 17:57:13.522648: total loss: 1.0789703893339302, mse:4.823386826274969, ic :0.008603910106959106, sharpe5:7.6850306171178815, irr5:215.89015197753906, ndcg5:0.8530428556509938, pnl5:2.562556505203247 
train 2, step: 0, loss: 2.1417748579545455, grad_norm: 0.009291029061663403, ic: 0.1310227808314155
train 2, step: 500, loss: 3.300052266725352, grad_norm: 0.28507041619007545, ic: 0.049045368954716485
train 2, step: 1000, loss: 2.072319504310345, grad_norm: 0.00020582744865719766, ic: 0.19092372605715585
train 2, step: 1500, loss: 1.4855448249642176, grad_norm: 0.0597577504744988, ic: -0.038316748786716086
train 2, step: 2000, loss: 3.2350803786057694, grad_norm: 0.7875915204227429, ic: 0.20750618165220552
Epoch 2: 2022-05-08 18:08:58.323587: train loss: 1.6464934735212584
Eval step 0: eval loss: 0.8358936370109984
Eval: 2022-05-08 18:09:38.977404: total loss: 1.0793799163278401, mse:4.822492326745625, ic :0.013127184750139109, sharpe5:7.519789219200611, irr5:214.69659423828125, ndcg5:0.8482273139315265, pnl5:2.918808937072754 
train 3, step: 0, loss: 1.52277633542937, grad_norm: 0.5248466760781189, ic: 0.0005633205589092825
train 3, step: 500, loss: 1.501499830229542, grad_norm: 0.3424543738326394, ic: 0.09713761393373962
train 3, step: 1000, loss: 3.6792939514968337, grad_norm: 0.7058692569892743, ic: -0.04934564316246916
train 3, step: 1500, loss: 1.9813554080145255, grad_norm: 1.2639241381701816, ic: -0.06506727890710831
train 3, step: 2000, loss: 0.8989726298564189, grad_norm: 0.001223075558368539, ic: 0.005484485811401679
Epoch 3: 2022-05-08 18:22:10.048962: train loss: 1.645916554053143
Eval step 0: eval loss: 0.8351621798644955
Eval: 2022-05-08 18:22:59.814614: total loss: 1.0787274920541383, mse:4.8215124256024815, ic :0.02464782790166813, sharpe5:7.928988642096519, irr5:228.99623107910156, ndcg5:0.8459232278625575, pnl5:2.9435818195343018 
train 4, step: 0, loss: 1.432467115752551, grad_norm: 0.0434588395958073, ic: 0.11531994525995662
train 4, step: 500, loss: 1.648337921382874, grad_norm: 0.5607798222327496, ic: 0.05355283167510144
train 4, step: 1000, loss: 2.9581269054878048, grad_norm: 0.7871994431177824, ic: 0.07542957696760806
train 4, step: 1500, loss: 2.149598661656118, grad_norm: 0.478084357998387, ic: 0.01638343461772249
train 4, step: 2000, loss: 1.0799375448034116, grad_norm: 0.39720722152089166, ic: 0.23790929399436633
Epoch 4: 2022-05-08 18:35:26.015029: train loss: 1.6451635972043164
Eval step 0: eval loss: 0.8540505347940266
Eval: 2022-05-08 18:36:07.900045: total loss: 1.085634224445412, mse:4.825278420129581, ic :0.05457731916338355, sharpe5:10.517424491643904, irr5:314.09130859375, ndcg5:0.8372827393993099, pnl5:2.4855432510375977 
train 5, step: 0, loss: 1.364683256213297, grad_norm: 0.19623714021422833, ic: 0.011160800300691196
train 5, step: 500, loss: 0.8897105859065226, grad_norm: 0.008571831705713642, ic: 0.034763504548431065
train 5, step: 1000, loss: 0.9821219842552682, grad_norm: 0.1528827041329659, ic: 0.00010446417136324507
train 5, step: 1500, loss: 1.5300323813469086, grad_norm: 0.15462222802913567, ic: 0.03226104762305877
train 5, step: 2000, loss: 1.118064083798579, grad_norm: 0.04199616273002492, ic: 0.11184477891554678
Epoch 5: 2022-05-08 18:48:24.912611: train loss: 1.6456148721305706
Eval step 0: eval loss: 0.8380308965234786
Eval: 2022-05-08 18:49:17.499783: total loss: 1.0787272078770234, mse:4.814142757757416, ic :0.04862238675178379, sharpe5:10.267392677664755, irr5:316.58929443359375, ndcg5:0.8337247920427568, pnl5:3.365081310272217 
train 6, step: 0, loss: 1.3239676978979764, grad_norm: 0.40284715462896037, ic: 0.10795304705570592
train 6, step: 500, loss: 1.0064189729792783, grad_norm: 0.041842204910187974, ic: 0.03456306040297575
train 6, step: 1000, loss: 1.1167986387535647, grad_norm: 0.08088060349220183, ic: 0.18773202346609477
train 6, step: 1500, loss: 1.5673758514656508, grad_norm: 0.7020525032569922, ic: 0.1414806134055362
train 6, step: 2000, loss: 0.8158071378978182, grad_norm: 0.07405075202384943, ic: 0.03676886863095568
Epoch 6: 2022-05-08 19:01:55.940659: train loss: 1.642767307230335
Eval step 0: eval loss: 0.8308753766300052
Eval: 2022-05-08 19:02:35.475443: total loss: 1.074448401096216, mse:4.734963384239689, ic :0.13471327727324772, sharpe5:11.604670559763909, irr5:382.1978759765625, ndcg5:0.849355477089755, pnl5:3.4912285804748535 
train 7, step: 0, loss: 0.9895549774169923, grad_norm: 0.05337766949373285, ic: 0.08922247350243076
train 7, step: 500, loss: 0.6506996966422872, grad_norm: 0.002458896044225103, ic: 0.04281816157944868
train 7, step: 1000, loss: 1.011395770361291, grad_norm: 0.23850545486000915, ic: 0.08075603180829066
train 7, step: 1500, loss: 2.252607306739014, grad_norm: 0.6973190896486061, ic: 0.43240468042219
train 7, step: 2000, loss: 0.9164786639236673, grad_norm: 0.044187692901497114, ic: -0.03154677816941578
Epoch 7: 2022-05-08 19:15:16.977634: train loss: 1.639361607856861
Eval step 0: eval loss: 0.834797126601192
Eval: 2022-05-08 19:16:07.703672: total loss: 1.0737902794319785, mse:4.709518558887972, ic :0.1466366089719924, sharpe5:11.565196133852005, irr5:388.7100830078125, ndcg5:0.8488548602314273, pnl5:3.7838873863220215 
train 8, step: 0, loss: 3.605743319746377, grad_norm: 1.0724770398486145, ic: 0.004302985895625264
train 8, step: 500, loss: 2.7444376380247912, grad_norm: 0.8817038270448181, ic: 0.02283302946350065
train 8, step: 1000, loss: 3.057823822463768, grad_norm: 0.8830999222570928, ic: 0.11691947225651722
train 8, step: 1500, loss: 0.7211276617649892, grad_norm: 0.04228864986843487, ic: 0.4315477082086502
train 8, step: 2000, loss: 1.0818997139374613, grad_norm: 0.3482079961756301, ic: 0.5261948028740716
Epoch 8: 2022-05-08 19:25:55.203091: train loss: 1.6312928396602944
Eval step 0: eval loss: 0.8245408406875658
Eval: 2022-05-08 19:26:27.325542: total loss: 1.0701186025059217, mse:4.686486422593145, ic :0.16460451428126488, sharpe5:14.932135466337204, irr5:491.849365234375, ndcg5:0.8659931721553514, pnl5:3.77760648727417 
train 9, step: 0, loss: 5.444476113935407, grad_norm: 0.9029056678566431, ic: 0.13932223918793685
train 9, step: 500, loss: 1.3438464547376967, grad_norm: 1.2903690831874837, ic: 0.3284754158860742
train 9, step: 1000, loss: 0.9291950094288792, grad_norm: 0.009834341814680837, ic: 0.0011690043333075373
train 9, step: 1500, loss: 1.0883569731452503, grad_norm: 0.026910322801045564, ic: 0.41858327042086585
train 9, step: 2000, loss: 1.0614985117161089, grad_norm: 0.2086904760723064, ic: 0.2795926118120868
Epoch 9: 2022-05-08 19:34:53.437323: train loss: 1.629537261301483
Eval step 0: eval loss: 0.8246770603513567
Eval: 2022-05-08 19:35:26.971715: total loss: 1.070331335846466, mse:4.697576351481443, ic :0.16366445380845035, sharpe5:17.37355270385742, irr5:543.1051025390625, ndcg5:0.8401328092632955, pnl5:6.626577854156494 
train 10, step: 0, loss: 7.06295269223761, grad_norm: 2.2425529014271763, ic: 0.24279965184365696
train 10, step: 500, loss: 1.1274134278110284, grad_norm: 0.1218695890608004, ic: 0.03491719419898474
train 10, step: 1000, loss: 2.3927880035587616, grad_norm: 0.8333770059416433, ic: 0.035648982013210866
train 10, step: 1500, loss: 1.1168850985440342, grad_norm: 0.32725425302654426, ic: 0.017505184095104445
train 10, step: 2000, loss: 2.759692716743446, grad_norm: 0.38722710627658474, ic: 0.402609284053235
Epoch 10: 2022-05-08 19:43:54.303697: train loss: 1.6285969079930054
Eval step 0: eval loss: 0.8259911491948762
Eval: 2022-05-08 19:44:28.000933: total loss: 1.0696871571934692, mse:4.687464093666672, ic :0.16867317440787244, sharpe5:17.794570995569227, irr5:581.2412719726562, ndcg5:0.852721138403581, pnl5:6.639246463775635 
train 11, step: 0, loss: 1.2529746259909127, grad_norm: 0.07200243450328792, ic: 0.21318679339404856
train 11, step: 500, loss: 0.6616373584967037, grad_norm: 0.010589798619008467, ic: 0.516830504783322
train 11, step: 1000, loss: 0.9349878473116756, grad_norm: 0.134162971883551, ic: 0.04836006990328032
train 11, step: 1500, loss: 1.052442048725329, grad_norm: 0.05188572749314098, ic: 0.17247058882091298
train 11, step: 2000, loss: 0.7876637535637588, grad_norm: 0.005063908398693966, ic: 0.13986709626416718
Epoch 11: 2022-05-08 19:52:48.737360: train loss: 1.6275156597119294
Eval step 0: eval loss: 0.8283729352237552
Eval: 2022-05-08 19:53:21.565105: total loss: 1.0705026227919203, mse:4.6852652266006745, ic :0.1694527030240299, sharpe5:17.354496084451675, irr5:566.8904418945312, ndcg5:0.8561785064600855, pnl5:4.672821998596191 
train 12, step: 0, loss: 0.960455576578776, grad_norm: 0.2713292039305656, ic: 0.4051844253801593
train 12, step: 500, loss: 0.9303712337159603, grad_norm: 0.07121598958771984, ic: 0.1708176035662499
train 12, step: 1000, loss: 2.9618289242884157, grad_norm: 0.34583289293549857, ic: 0.4043058284343847
train 12, step: 1500, loss: 0.9444531733446783, grad_norm: 0.11514194687413569, ic: -0.11963972118339111
train 12, step: 2000, loss: 0.8723937435092523, grad_norm: 0.003760446029279829, ic: 0.24602628750819286
Epoch 12: 2022-05-08 20:01:39.816027: train loss: 1.6264021977841172
Eval step 0: eval loss: 0.8301019216362618
Eval: 2022-05-08 20:02:12.074077: total loss: 1.069278680815703, mse:4.7116645195962965, ic :0.15557306714551095, sharpe5:17.263376952409743, irr5:542.1470336914062, ndcg5:0.8414543315762263, pnl5:5.064504146575928 
train 13, step: 0, loss: 2.075202273214757, grad_norm: 0.6462524991503527, ic: 0.19999399684718538
train 13, step: 500, loss: 0.8287193641818724, grad_norm: 0.18436259667009686, ic: 0.5006367803654747
train 13, step: 1000, loss: 0.9634793479970637, grad_norm: 0.33473157455272545, ic: 0.45350264944231133
train 13, step: 1500, loss: 2.422822212260929, grad_norm: 0.8243635576389544, ic: -0.023047994878086002
train 13, step: 2000, loss: 1.4838939546345746, grad_norm: 0.12074222738001693, ic: 0.18986892896695157
Epoch 13: 2022-05-08 20:10:51.699069: train loss: 1.6260493495742803
Eval step 0: eval loss: 0.8209970713415436
Eval: 2022-05-08 20:11:24.440122: total loss: 1.0686948993455159, mse:4.678070746179162, ic :0.17588006647364252, sharpe5:17.959723949432373, irr5:594.06494140625, ndcg5:0.8465734796467917, pnl5:4.805578231811523 
train 14, step: 0, loss: 4.511921375292512, grad_norm: 1.128274113581227, ic: 0.19908019043639788
train 14, step: 500, loss: 0.8289376821722095, grad_norm: 0.004403299258650719, ic: 0.09094422938183494
train 14, step: 1000, loss: 1.845855689718584, grad_norm: 0.34454058864160403, ic: 0.3771910410253968
train 14, step: 1500, loss: 1.1263050217082027, grad_norm: 0.06691584704992568, ic: -0.08437378620197203
train 14, step: 2000, loss: 1.136333846160818, grad_norm: 0.201125630689828, ic: 0.12388746379837463
Epoch 14: 2022-05-08 20:19:51.211429: train loss: 1.6261454449526982
Eval step 0: eval loss: 0.8290360895926633
Eval: 2022-05-08 20:20:25.159377: total loss: 1.0721395463233916, mse:4.7502296082716855, ic :0.12701298135221809, sharpe5:17.16758873105049, irr5:545.685302734375, ndcg5:0.8429915503366952, pnl5:4.399257183074951 
train 15, step: 0, loss: 3.3410916220817124, grad_norm: 0.6242564268271137, ic: 0.08202677252727583
train 15, step: 500, loss: 1.2534936665497713, grad_norm: 0.10762979183633149, ic: 0.0025575624753507165
train 15, step: 1000, loss: 1.3196982779153963, grad_norm: 0.12149145300760357, ic: 0.05425571597043156
train 15, step: 1500, loss: 0.8561261995570866, grad_norm: 0.1912951868900865, ic: 0.0786026564189542
train 15, step: 2000, loss: 1.4670355116495573, grad_norm: 0.5334395655505523, ic: 0.03869612319182649
Epoch 15: 2022-05-08 20:28:47.373245: train loss: 1.6256207744598015
Eval step 0: eval loss: 0.8400619715860445
Eval: 2022-05-08 20:29:19.361310: total loss: 1.0725077582531044, mse:4.645153482109405, ic :0.1859570374566763, sharpe5:17.95494513273239, irr5:600.3350219726562, ndcg5:0.8493458511035678, pnl5:6.769623756408691 
train 16, step: 0, loss: 0.6882232883480552, grad_norm: 0.24520377665490106, ic: -0.018286829105698885
train 16, step: 500, loss: 1.611050122626059, grad_norm: 0.6533550018668086, ic: 0.17390513229330148
train 16, step: 1000, loss: 0.8803470495975378, grad_norm: 0.004568782178165552, ic: -0.1366152778065433
train 16, step: 1500, loss: 0.8529652023616037, grad_norm: 0.22186455615615558, ic: 0.1433344602930819
train 16, step: 2000, loss: 3.3373007360910405, grad_norm: 1.0802304200617814, ic: 0.06373594762589144
Epoch 16: 2022-05-08 20:37:43.143667: train loss: 1.622913193224972
Eval step 0: eval loss: 0.8307622461349117
Eval: 2022-05-08 20:38:16.050317: total loss: 1.069858286437809, mse:4.645082610645042, ic :0.17346438312768414, sharpe5:15.964306901693343, irr5:510.770751953125, ndcg5:0.8476245047971335, pnl5:5.424483299255371 
train 17, step: 0, loss: 1.2792634594661803, grad_norm: 0.24529136551203248, ic: -0.11183134917252785
train 17, step: 500, loss: 1.7269663575542005, grad_norm: 0.695636753500619, ic: 0.23264975870092222
train 17, step: 1000, loss: 1.280593435180276, grad_norm: 0.08302912603541213, ic: 0.141644066548593
train 17, step: 1500, loss: 4.5208497170049595, grad_norm: 1.2419340075628107, ic: 0.2345172794445908
train 17, step: 2000, loss: 1.2818480182851035, grad_norm: 0.7071565121114665, ic: 0.07549702472709992
Epoch 17: 2022-05-08 20:46:46.819259: train loss: 1.6223141120009505
Eval step 0: eval loss: 0.8374913560326659
Eval: 2022-05-08 20:47:17.330976: total loss: 1.0683086305405487, mse:4.581038424474076, ic :0.19511458034055496, sharpe5:18.037380666732787, irr5:604.3148193359375, ndcg5:0.8467526769529693, pnl5:6.225561618804932 
train 18, step: 0, loss: 1.4294072473077302, grad_norm: 0.6341551155347678, ic: 0.16792597668251424
train 18, step: 500, loss: 1.490677138506356, grad_norm: 0.737544202820251, ic: -0.0071711862494517765
train 18, step: 1000, loss: 0.6550343803510273, grad_norm: 0.006565962253029352, ic: 0.5706785212102924
train 18, step: 1500, loss: 1.4261871512848077, grad_norm: 0.03761337221637984, ic: 0.21846719930873115
train 18, step: 2000, loss: 0.9097531069615844, grad_norm: 0.010416627410102398, ic: -0.03019923893102292
Epoch 18: 2022-05-08 20:55:49.310920: train loss: 1.6204045748976248
Eval step 0: eval loss: 0.8222151375419849
Eval: 2022-05-08 20:56:21.928641: total loss: 1.0653063878717741, mse:4.5848339682770725, ic :0.1962575086242927, sharpe5:18.33265326976776, irr5:618.2158813476562, ndcg5:0.8479687177034988, pnl5:6.111644268035889 
train 19, step: 0, loss: 1.4785560244605656, grad_norm: 0.8377260979948229, ic: 0.0564028918415448
train 19, step: 500, loss: 0.858052995469835, grad_norm: 0.009665071562973637, ic: 0.2333428758826323
train 19, step: 1000, loss: 0.9603987037004081, grad_norm: 0.1270450953866174, ic: 0.21101839387682172
train 19, step: 1500, loss: 3.980124585492797, grad_norm: 0.9746868967162468, ic: 0.08416398510116947
train 19, step: 2000, loss: 1.0113787372295673, grad_norm: 0.19315786192196616, ic: 0.2382074841830325
Epoch 19: 2022-05-08 21:04:56.312723: train loss: 1.6211929183193508
Eval step 0: eval loss: 0.8285833103513567
Eval: 2022-05-08 21:05:27.841811: total loss: 1.0664385310913627, mse:4.579632724371922, ic :0.19623605401780556, sharpe5:18.207957558631897, irr5:619.5118408203125, ndcg5:0.8484808112731088, pnl5:8.196338653564453 
train 20, step: 0, loss: 2.3218964226161067, grad_norm: 1.1011826248611563, ic: 0.04513837906604042
train 20, step: 500, loss: 3.2109996448863636, grad_norm: 0.6800291247208101, ic: 0.11134901499198355
train 20, step: 1000, loss: 0.9722280502319336, grad_norm: 0.10739878022156638, ic: 0.15763651491035002
train 20, step: 1500, loss: 1.731428488107772, grad_norm: 0.8609644570021049, ic: 0.2597506815241093
train 20, step: 2000, loss: 1.0371689166369291, grad_norm: 0.05248302627230593, ic: 0.008842556111648588
Epoch 20: 2022-05-08 21:13:45.316460: train loss: 1.6180832055315229
Eval step 0: eval loss: 0.8328084995924986
Eval: 2022-05-08 21:14:18.820741: total loss: 1.0690917466797156, mse:4.608982283451529, ic :0.18961160708600341, sharpe5:17.975354425907135, irr5:607.930419921875, ndcg5:0.8578984517500757, pnl5:4.801997184753418 
train 21, step: 0, loss: 1.0118430550092965, grad_norm: 0.35526874204791364, ic: 0.08771520016793341
train 21, step: 500, loss: 0.7688703283799433, grad_norm: 0.00949981019698935, ic: 0.19332114268544182
train 21, step: 1000, loss: 0.9249396742435924, grad_norm: 0.5712205401879766, ic: 0.1723925428185441
train 21, step: 1500, loss: 0.992251918674631, grad_norm: 0.20765731339079133, ic: 0.321599235788949
train 21, step: 2000, loss: 0.9428605494705149, grad_norm: 0.09956434197917964, ic: 0.0641319786153071
Epoch 21: 2022-05-08 21:22:42.258373: train loss: 1.6198117864628487
Eval step 0: eval loss: 0.8270219937516464
Eval: 2022-05-08 21:23:15.924708: total loss: 1.0659362512398511, mse:4.588261269602506, ic :0.19143832203343586, sharpe5:18.236603972911833, irr5:618.5831298828125, ndcg5:0.851050015787225, pnl5:6.09390926361084 
train 22, step: 0, loss: 1.0394138831876765, grad_norm: 0.021203828428672404, ic: 0.226305124620719
train 22, step: 500, loss: 3.2514571027057926, grad_norm: 0.7173923280685437, ic: -0.2516819904832569
train 22, step: 1000, loss: 1.1940186958092487, grad_norm: 0.046841078602788716, ic: 0.46014310713517403
train 22, step: 1500, loss: 0.9724029666602366, grad_norm: 0.15015630618270398, ic: 0.1157982174330381
train 22, step: 2000, loss: 1.7381731582607, grad_norm: 0.7111180911216446, ic: 0.21031047164274036
Epoch 22: 2022-05-08 21:31:50.352713: train loss: 1.6177756867565825
Eval step 0: eval loss: 0.8317506426378095
Eval: 2022-05-08 21:32:22.956179: total loss: 1.0663651671092855, mse:4.592048646214498, ic :0.19034343364112224, sharpe5:17.645324728488923, irr5:597.9957885742188, ndcg5:0.8485693081149197, pnl5:5.478112697601318 
train 23, step: 0, loss: 0.9819973376711095, grad_norm: 0.05540277137868088, ic: 0.1734765159503717
train 23, step: 500, loss: 1.4207702349269036, grad_norm: 0.1244518164930814, ic: 0.02356712075924528
train 23, step: 1000, loss: 1.6386417643229167, grad_norm: 0.07429133153637335, ic: 0.25751289009398387
train 23, step: 1500, loss: 1.1286842188099577, grad_norm: 0.6574653775288092, ic: 0.09128953478212387
train 23, step: 2000, loss: 1.973271582106428, grad_norm: 0.8167763665538568, ic: 0.45210552612147914
Epoch 23: 2022-05-08 21:41:02.384482: train loss: 1.615116436450473
Eval step 0: eval loss: 0.8361821551468651
Eval: 2022-05-08 21:41:34.992570: total loss: 1.0667501229807277, mse:4.582199017894594, ic :0.19115149959731306, sharpe5:17.631622048616407, irr5:592.9544067382812, ndcg5:0.863076347759021, pnl5:4.96373987197876 
train 24, step: 0, loss: 2.207524289613006, grad_norm: 0.07614399634425624, ic: 0.12493665446366573
train 24, step: 500, loss: 1.222987122294504, grad_norm: 0.20521274643535792, ic: 0.13190175005111496
train 24, step: 1000, loss: 0.9110329143104322, grad_norm: 0.20935751547531337, ic: 0.524412372932286
train 24, step: 1500, loss: 2.607008466225268, grad_norm: 1.842426795373593, ic: 0.05926008149212944
train 24, step: 2000, loss: 0.9412038938340465, grad_norm: 0.0666463939635025, ic: 0.08974500667036107
Epoch 24: 2022-05-08 21:49:58.544722: train loss: 1.6137079600256485
Eval step 0: eval loss: 0.8297295364363804
Eval: 2022-05-08 21:50:31.038867: total loss: 1.0674161908105002, mse:4.604694520955566, ic :0.189090599112344, sharpe5:17.79441397190094, irr5:600.99951171875, ndcg5:0.8522158490822163, pnl5:8.097967147827148 
train 25, step: 0, loss: 0.8324963543866132, grad_norm: 0.0511392403765781, ic: 0.616303620443986
train 25, step: 500, loss: 0.866057519826284, grad_norm: 0.009686900435408864, ic: 0.276889696747772
train 25, step: 1000, loss: 2.095616848343488, grad_norm: 0.13659010123680415, ic: 0.2598346515779921
train 25, step: 1500, loss: 1.1334591092534254, grad_norm: 0.5580919500567112, ic: 0.5352342960086806
train 25, step: 2000, loss: 1.0224104490154406, grad_norm: 0.41062580268862414, ic: 0.5857284658593812
Epoch 25: 2022-05-08 21:59:12.746164: train loss: 1.6154227398330714
Eval step 0: eval loss: 0.8278995107926106
Eval: 2022-05-08 21:59:46.019478: total loss: 1.0668282884365157, mse:4.62415837057169, ic :0.19744553340871096, sharpe5:18.23752530455589, irr5:607.6219482421875, ndcg5:0.8609032995342414, pnl5:6.62994909286499 
