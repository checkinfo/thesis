Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=False, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
6316
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.8978182723319, grad_norm: 4.894494425242579, ic: 0.026634357070640297
train 0, step: 500, loss: 0.8650486478252166, grad_norm: 0.024844444541939802, ic: 0.01966038230403106
train 0, step: 1000, loss: 1.943431884944221, grad_norm: 0.5522128403805042, ic: -0.05217185717458491
train 0, step: 1500, loss: 0.9573660989995059, grad_norm: 0.05978749419700259, ic: 0.04695453591544486
train 0, step: 2000, loss: 0.9967589409253509, grad_norm: 0.1565950461111082, ic: 0.06521145156722653
Epoch 0: 2022-05-08 17:45:53.423873: train loss: 1.6490035280231035
Eval step 0: eval loss: 0.8354306959628556
Eval: 2022-05-08 17:46:23.665135: total loss: 1.0790783953199434, mse:4.823194619847872, ic :0.007841469257975053, sharpe5:7.541868071258068, irr5:212.51763916015625, ndcg5:0.8527671333080954, pnl5:2.6570351123809814 
train 1, step: 0, loss: 2.76587150327621, grad_norm: 0.9145845971762533, ic: 0.043330040439479495
train 1, step: 500, loss: 1.7663428256765263, grad_norm: 0.8186035718173066, ic: 0.10466155339617746
train 1, step: 1000, loss: 0.8751238084440033, grad_norm: 0.1851360839690793, ic: 0.08028132748098793
train 1, step: 1500, loss: 1.7128702799479167, grad_norm: 0.22651447813097841, ic: -0.036394569777390126
train 1, step: 2000, loss: 2.18890234375, grad_norm: 0.9214494078784529, ic: -0.018772096201926547
Epoch 1: 2022-05-08 17:55:17.927614: train loss: 1.646809587820779
Eval step 0: eval loss: 0.8336905830150158
Eval: 2022-05-08 17:55:56.508563: total loss: 1.078844102091978, mse:4.824513623730875, ic :0.007957348267789497, sharpe5:7.443165925741195, irr5:210.261962890625, ndcg5:0.86854655869461, pnl5:2.6958577632904053 
train 2, step: 0, loss: 2.1430078125, grad_norm: 0.010734560974089667, ic: 0.13731132636355248
train 2, step: 500, loss: 3.306670044136346, grad_norm: 0.2921455143947511, ic: 0.04663935710631541
train 2, step: 1000, loss: 2.072025038912835, grad_norm: 0.00030728683082777673, ic: 0.18142937334444653
train 2, step: 1500, loss: 1.4863435002683683, grad_norm: 0.06790104131903386, ic: -0.02965644104623831
train 2, step: 2000, loss: 3.24181640625, grad_norm: 0.8220633655074288, ic: 0.23558838702520188
Epoch 2: 2022-05-08 18:06:46.185540: train loss: 1.646566037201226
Eval step 0: eval loss: 0.8356191395959562
Eval: 2022-05-08 18:07:31.306132: total loss: 1.0792471936968098, mse:4.822401907252218, ic :0.015897024921602602, sharpe5:7.460902978777885, irr5:210.89027404785156, ndcg5:0.8375054762536992, pnl5:3.0685055255889893 
train 3, step: 0, loss: 1.5193408997078253, grad_norm: 0.5648929909174861, ic: -0.003416355542229409
train 3, step: 500, loss: 1.4975050625244715, grad_norm: 0.37436565234803104, ic: 0.08193982011711076
train 3, step: 1000, loss: 3.6700354418537713, grad_norm: 0.749692780765538, ic: 0.014746796574886522
train 3, step: 1500, loss: 1.973773662860577, grad_norm: 0.9682946088653669, ic: -0.030383996639655943
train 3, step: 2000, loss: 0.8984746489653717, grad_norm: 0.0006601010341061405, ic: 0.009420072421519007
Epoch 3: 2022-05-08 18:19:34.141855: train loss: 1.645686880368471
Eval step 0: eval loss: 0.8330541194678609
Eval: 2022-05-08 18:20:17.693344: total loss: 1.0774677154650425, mse:4.819612917314216, ic :0.0363189589543214, sharpe5:7.526377591788768, irr5:198.55633544921875, ndcg5:0.8434311265501752, pnl5:3.812492609024048 
train 4, step: 0, loss: 1.4392506377551022, grad_norm: 0.047934633063063725, ic: 0.07959286430273427
train 4, step: 500, loss: 1.646632781742126, grad_norm: 0.6275556620165582, ic: 0.0963503900837891
train 4, step: 1000, loss: 2.948418221822599, grad_norm: 0.7049300980447712, ic: 0.0763625199020908
train 4, step: 1500, loss: 2.144332023008966, grad_norm: 0.47798916976607786, ic: 0.0033725431674802298
train 4, step: 2000, loss: 1.0778140339473172, grad_norm: 0.46827492975299956, ic: 0.29665077328118955
Epoch 4: 2022-05-08 18:32:09.038474: train loss: 1.6448107252558741
Eval step 0: eval loss: 0.8633586855407007
Eval: 2022-05-08 18:32:58.566434: total loss: 1.0924711969182894, mse:4.792265771148799, ic :0.12071890562535381, sharpe5:11.776755257248878, irr5:396.8821716308594, ndcg5:0.8447058146943864, pnl5:2.8655922412872314 
train 5, step: 0, loss: 1.3521323799286913, grad_norm: 0.1614042674874639, ic: 0.29408972961290936
train 5, step: 500, loss: 0.8822432902954005, grad_norm: 0.011802179384738331, ic: 0.8476638457136515
train 5, step: 1000, loss: 0.9855309731202108, grad_norm: 0.16426543729310736, ic: -0.01087340689105596
train 5, step: 1500, loss: 1.5277105217505744, grad_norm: 0.28310127377007277, ic: 0.020558321244647362
train 5, step: 2000, loss: 1.2173545465668503, grad_norm: 1.5248174157889307, ic: 0.1272849160816871
Epoch 5: 2022-05-08 18:44:39.375799: train loss: 1.6424410141402497
Eval step 0: eval loss: 0.8364311837295837
Eval: 2022-05-08 18:45:30.722424: total loss: 1.0754395058220827, mse:4.715647516666217, ic :0.1464415517290366, sharpe5:11.467374172210693, irr5:388.78240966796875, ndcg5:0.8483688044464517, pnl5:3.1097819805145264 
train 6, step: 0, loss: 1.33531371486244, grad_norm: 0.6345376933522737, ic: 0.13155090077412462
train 6, step: 500, loss: 1.0078990521511295, grad_norm: 0.05643712962878954, ic: 0.022452119464281974
train 6, step: 1000, loss: 1.1192048627614068, grad_norm: 0.10183855274590245, ic: 0.5308308391252009
train 6, step: 1500, loss: 1.5761012558109504, grad_norm: 0.8057293293942822, ic: 0.15997117912452186
train 6, step: 2000, loss: 0.7983024603276371, grad_norm: 0.043451114271796425, ic: 0.35985570273506845
Epoch 6: 2022-05-08 18:57:09.804479: train loss: 1.6356272873150186
Eval step 0: eval loss: 0.8299884695650026
Eval: 2022-05-08 18:57:54.344579: total loss: 1.0713007616495311, mse:4.704032314772358, ic :0.1532644262246083, sharpe5:14.056670814156531, irr5:456.66326904296875, ndcg5:0.8468690531791309, pnl5:6.79503870010376 
train 7, step: 0, loss: 0.989934539794922, grad_norm: 0.04740710909340774, ic: 0.07552243475647577
train 7, step: 500, loss: 0.6489988750237462, grad_norm: 0.004799638090915237, ic: 0.06908354607275202
train 7, step: 1000, loss: 1.0265876206385662, grad_norm: 0.20061343087736394, ic: 0.11734811572942917
train 7, step: 1500, loss: 2.2185333350080385, grad_norm: 0.7317560686500632, ic: 0.442344599515345
train 7, step: 2000, loss: 0.9204985043921649, grad_norm: 0.1063732858884151, ic: -0.07235445583615593
Epoch 7: 2022-05-08 19:09:44.340738: train loss: 1.629228424654138
Eval step 0: eval loss: 0.8278523034114857
Eval: 2022-05-08 19:10:24.666725: total loss: 1.0703068348459062, mse:4.678851922622342, ic :0.16844608251171195, sharpe5:15.854746001958846, irr5:510.4576110839844, ndcg5:0.8467327319746544, pnl5:5.129122257232666 
train 8, step: 0, loss: 3.5944254557291666, grad_norm: 2.0304966098857884, ic: 0.18489737449155147
train 8, step: 500, loss: 2.760808417137633, grad_norm: 0.8993671621469501, ic: 0.019610934852796264
train 8, step: 1000, loss: 3.0418665789175723, grad_norm: 0.9076433594990849, ic: 0.11545010405059274
train 8, step: 1500, loss: 0.7103418992274199, grad_norm: 0.27544459237258856, ic: 0.467530813797017
train 8, step: 2000, loss: 1.0983262736507444, grad_norm: 0.36263749064500117, ic: 0.5133364736387178
Epoch 8: 2022-05-08 19:21:50.430662: train loss: 1.6283900863747989
Eval step 0: eval loss: 0.8223460190414251
Eval: 2022-05-08 19:22:22.696719: total loss: 1.06931747331445, mse:4.678412145734909, ic :0.1725524918958158, sharpe5:16.11250319123268, irr5:535.4434814453125, ndcg5:0.8567191009394249, pnl5:5.843842506408691 
train 9, step: 0, loss: 5.434328118769936, grad_norm: 0.9274423774585795, ic: 0.03965459644886997
train 9, step: 500, loss: 1.3334949256130384, grad_norm: 1.4793964529010353, ic: 0.3277355015300134
train 9, step: 1000, loss: 0.9340593810934934, grad_norm: 0.2573199625908859, ic: -0.021844958287513677
train 9, step: 1500, loss: 1.0893423448430597, grad_norm: 0.019272651715505122, ic: 0.4041298368395018
train 9, step: 2000, loss: 1.064066362671146, grad_norm: 0.4943576449197625, ic: 0.26244694057125373
Epoch 9: 2022-05-08 19:30:18.056890: train loss: 1.6271761927554822
Eval step 0: eval loss: 0.8235702595042478
Eval: 2022-05-08 19:30:50.635977: total loss: 1.0704961187020403, mse:4.684575059909311, ic :0.16589445406354755, sharpe5:16.307570099830627, irr5:530.7667846679688, ndcg5:0.8406860773761552, pnl5:9.854927062988281 
train 10, step: 0, loss: 7.080962867848032, grad_norm: 1.7542246084673019, ic: 0.25568282880140847
train 10, step: 500, loss: 1.1480768464236657, grad_norm: 0.4890596904410995, ic: 0.03855566006101377
train 10, step: 1000, loss: 2.3847212528158552, grad_norm: 0.7283428938195473, ic: 0.15772844873755518
train 10, step: 1500, loss: 1.1217540394176138, grad_norm: 0.29356361966373623, ic: 0.0038233997321024636
train 10, step: 2000, loss: 2.7178363278281723, grad_norm: 0.9401561222643723, ic: 0.4796239690468636
Epoch 10: 2022-05-08 19:38:53.717111: train loss: 1.6264903425220834
Eval step 0: eval loss: 0.8253809905780756
Eval: 2022-05-08 19:39:25.270793: total loss: 1.0688250581524112, mse:4.653881670918775, ic :0.17772798293112038, sharpe5:16.317375565767286, irr5:531.9038696289062, ndcg5:0.8459793144367487, pnl5:6.414143085479736 
train 11, step: 0, loss: 1.2493429161832947, grad_norm: 0.059401363494884625, ic: 0.2132184021696029
train 11, step: 500, loss: 0.6500953605487725, grad_norm: 0.10175056899599838, ic: 0.6010190150078099
train 11, step: 1000, loss: 0.9324569765045342, grad_norm: 0.11277281931028706, ic: 0.05878245396248441
train 11, step: 1500, loss: 1.0588113215931674, grad_norm: 0.4125164471713467, ic: 0.16958494595243376
train 11, step: 2000, loss: 0.7879528750740521, grad_norm: 0.10144296852221221, ic: 0.11487704462081966
Epoch 11: 2022-05-08 19:47:22.276739: train loss: 1.624920076749622
Eval step 0: eval loss: 0.8299344447691649
Eval: 2022-05-08 19:47:56.060763: total loss: 1.0684938049544936, mse:4.595102401116705, ic :0.1834048167551168, sharpe5:16.318344193696976, irr5:544.451904296875, ndcg5:0.8566669150907597, pnl5:4.980492115020752 
train 12, step: 0, loss: 0.9575220743815104, grad_norm: 0.08830052512188208, ic: 0.39499014056316395
train 12, step: 500, loss: 0.9355433758509931, grad_norm: 0.08108743514742767, ic: 0.1759102327184555
train 12, step: 1000, loss: 2.935059760026871, grad_norm: 0.40375589323047234, ic: 0.42985809283226245
train 12, step: 1500, loss: 0.9268281911950209, grad_norm: 0.14981373396541772, ic: -0.12460412837571427
train 12, step: 2000, loss: 0.8742394576980268, grad_norm: 0.0064055833197147255, ic: 0.1978544007895738
Epoch 12: 2022-05-08 19:55:54.167568: train loss: 1.621391744584568
Eval step 0: eval loss: 0.826987392156217
Eval: 2022-05-08 19:56:26.575571: total loss: 1.0658044867753222, mse:4.588314297161571, ic :0.1883892139332182, sharpe5:16.135385134220122, irr5:527.779541015625, ndcg5:0.845982567100937, pnl5:7.71227502822876 
train 13, step: 0, loss: 2.042501350894271, grad_norm: 0.6520953670457305, ic: 0.4507692863808008
train 13, step: 500, loss: 0.8129839513344134, grad_norm: 0.050770726335939216, ic: 0.5942382301433913
train 13, step: 1000, loss: 0.9472100789114932, grad_norm: 0.38789826380684145, ic: 0.5822786165582132
train 13, step: 1500, loss: 2.396273777505367, grad_norm: 0.2855406745802863, ic: -0.10543474671215056
train 13, step: 2000, loss: 1.4640388753067262, grad_norm: 0.062432426294617746, ic: 0.19604139435030032
Epoch 13: 2022-05-08 20:04:24.255578: train loss: 1.6211730241869302
Eval step 0: eval loss: 0.8221088244616043
Eval: 2022-05-08 20:04:56.572728: total loss: 1.0677449571093274, mse:4.612807102245625, ic :0.18527551670130674, sharpe5:16.742726197242735, irr5:565.1339721679688, ndcg5:0.8492926416235478, pnl5:7.703783988952637 
train 14, step: 0, loss: 4.559246569081513, grad_norm: 1.291075471385808, ic: 0.19971159260963542
train 14, step: 500, loss: 0.829345889776854, grad_norm: 0.0024332733940980685, ic: 0.03566231500268793
train 14, step: 1000, loss: 1.8172524929408695, grad_norm: 0.9490142998879931, ic: 0.4615817143891906
train 14, step: 1500, loss: 1.1280716876594388, grad_norm: 0.06745291350122118, ic: -0.07905754177256713
train 14, step: 2000, loss: 1.1634066198928725, grad_norm: 0.22547914164178556, ic: 0.06841205322325988
Epoch 14: 2022-05-08 20:13:02.048517: train loss: 1.6202951328719255
Eval step 0: eval loss: 0.8314412220511722
Eval: 2022-05-08 20:13:34.389561: total loss: 1.0664831191249449, mse:4.583646444470994, ic :0.191853158431458, sharpe5:17.088736363649367, irr5:577.0477294921875, ndcg5:0.855294614084142, pnl5:5.1884050369262695 
train 15, step: 0, loss: 3.439965725316148, grad_norm: 1.1004273801863382, ic: 0.13737882813923818
train 15, step: 500, loss: 1.2560641446513923, grad_norm: 0.0230509384956854, ic: 0.027976178572279076
train 15, step: 1000, loss: 1.3136927162728658, grad_norm: 0.1215579254914104, ic: 0.06468880270796201
train 15, step: 1500, loss: 0.8576018085629921, grad_norm: 0.24240942082742784, ic: 0.06717168509705246
train 15, step: 2000, loss: 1.4523561142876913, grad_norm: 0.5472827406554763, ic: 0.054311315818012865
Epoch 15: 2022-05-08 20:21:27.729456: train loss: 1.6210714558355315
Eval step 0: eval loss: 0.8344875773840884
Eval: 2022-05-08 20:21:59.524468: total loss: 1.0681331723841354, mse:4.5791718170758, ic :0.19524953999267633, sharpe5:17.167006040811536, irr5:583.914306640625, ndcg5:0.8454240339265057, pnl5:6.902237892150879 
train 16, step: 0, loss: 0.701710414810966, grad_norm: 0.22951428118344655, ic: -0.06813305502308539
train 16, step: 500, loss: 1.6014071495334834, grad_norm: 0.5172073394758914, ic: 0.1828159793346884
train 16, step: 1000, loss: 0.8792427756569602, grad_norm: 0.005098133551160372, ic: -0.10112280334684282
train 16, step: 1500, loss: 0.8579609467346336, grad_norm: 0.198345483526764, ic: 0.1622652066566453
train 16, step: 2000, loss: 3.3436911320706026, grad_norm: 0.875685387308082, ic: 0.02766092052623868
Epoch 16: 2022-05-08 20:29:53.968135: train loss: 1.6186614128485532
Eval step 0: eval loss: 0.8291367429325276
Eval: 2022-05-08 20:30:26.476212: total loss: 1.0676875012924554, mse:4.597760921351541, ic :0.18301411078196111, sharpe5:16.304471247196197, irr5:524.3923950195312, ndcg5:0.8516516955897797, pnl5:10.579339027404785 
train 17, step: 0, loss: 1.2889742985328247, grad_norm: 0.23821790504763846, ic: -0.1038354811117026
train 17, step: 500, loss: 1.7376251799627371, grad_norm: 0.46767549736431946, ic: 0.1824253006071014
train 17, step: 1000, loss: 1.2829694697358829, grad_norm: 0.08912870491675128, ic: 0.16043397627711437
train 17, step: 1500, loss: 4.525168189750183, grad_norm: 1.0398255575170678, ic: 0.2276636858260885
train 17, step: 2000, loss: 1.2725328006911298, grad_norm: 0.5728708881405178, ic: 0.10957046546071704
Epoch 17: 2022-05-08 20:38:28.389730: train loss: 1.6198579476761223
Eval step 0: eval loss: 0.8303724315068493
Eval: 2022-05-08 20:39:01.026992: total loss: 1.065423972985053, mse:4.572333080449317, ic :0.2005552900039013, sharpe5:18.690831824541092, irr5:631.0934448242188, ndcg5:0.8468359276090487, pnl5:6.759912014007568 
train 18, step: 0, loss: 1.4098728278975707, grad_norm: 1.442680321553198, ic: 0.23437085793484233
train 18, step: 500, loss: 1.5201612076901734, grad_norm: 0.6666773077418401, ic: -0.032385742127879105
train 18, step: 1000, loss: 0.6571250936429794, grad_norm: 0.029141047868257566, ic: 0.5756756294436537
train 18, step: 1500, loss: 1.4240986592060811, grad_norm: 0.03721335863495791, ic: 0.22411934221624794
train 18, step: 2000, loss: 0.9123832556852111, grad_norm: 0.0074131836230447124, ic: -0.022389324985131624
Epoch 18: 2022-05-08 20:47:09.639874: train loss: 1.617251677787007
Eval step 0: eval loss: 0.8226577549764554
Eval: 2022-05-08 20:47:39.363895: total loss: 1.0649077581127524, mse:4.59402004711509, ic :0.19530460811162853, sharpe5:17.605595848560334, irr5:604.0100708007812, ndcg5:0.8640309178997563, pnl5:6.045523643493652 
train 19, step: 0, loss: 1.4816377185639882, grad_norm: 0.6931590764984739, ic: -0.017449202254341475
train 19, step: 500, loss: 0.8599640175148292, grad_norm: 0.026411465000570872, ic: 0.22471078932884864
train 19, step: 1000, loss: 0.9537020765114845, grad_norm: 0.0035893933632611835, ic: 0.21185035635499694
train 19, step: 1500, loss: 3.960828039902495, grad_norm: 0.7696216650615214, ic: 0.13905789588463283
train 19, step: 2000, loss: 1.012650428185096, grad_norm: 0.08529512997520107, ic: 0.21412466768330704
Epoch 19: 2022-05-08 20:55:49.634137: train loss: 1.6193719045300117
Eval step 0: eval loss: 0.8281479605382309
Eval: 2022-05-08 20:56:22.466025: total loss: 1.0657234778056883, mse:4.5770708912339355, ic :0.19725317376892956, sharpe5:17.880065653324127, irr5:615.1795043945312, ndcg5:0.8439093755909217, pnl5:7.7441887855529785 
train 20, step: 0, loss: 2.30182844923419, grad_norm: 0.6792386618752073, ic: 0.04147702911725638
train 20, step: 500, loss: 3.236438920454545, grad_norm: 0.5466243992077717, ic: 0.05129459492352299
train 20, step: 1000, loss: 0.9675446510314942, grad_norm: 0.07603933361145421, ic: 0.2217702422650784
train 20, step: 1500, loss: 1.6635702930584801, grad_norm: 0.7547081641883006, ic: 0.2624828231464987
train 20, step: 2000, loss: 1.0382186615836562, grad_norm: 0.07668375881155773, ic: -0.005065144730233978
Epoch 20: 2022-05-08 21:04:30.809762: train loss: 1.6175468542232756
Eval step 0: eval loss: 0.8280429980777463
Eval: 2022-05-08 21:05:00.718566: total loss: 1.0646461812242674, mse:4.5783035960426846, ic :0.1984323992065987, sharpe5:18.034610996246336, irr5:609.2794799804688, ndcg5:0.8458369075751916, pnl5:5.766488075256348 
train 21, step: 0, loss: 1.0127658916738178, grad_norm: 0.3499174024321837, ic: 0.07087075444028443
train 21, step: 500, loss: 0.7703519838046183, grad_norm: 0.009281582808697486, ic: 0.19442133318978932
train 21, step: 1000, loss: 0.9447899533991228, grad_norm: 0.48703256835597697, ic: 0.14389116237569974
train 21, step: 1500, loss: 0.9888669941560925, grad_norm: 0.21295807430706157, ic: 0.32170163609751473
train 21, step: 2000, loss: 0.9378282244947397, grad_norm: 0.05420883703042356, ic: 0.06659516690294448
Epoch 21: 2022-05-08 21:12:56.677032: train loss: 1.6171938279443394
Eval step 0: eval loss: 0.8239268874720099
Eval: 2022-05-08 21:13:28.734113: total loss: 1.065725773065555, mse:4.590855188659875, ic :0.19289079970441775, sharpe5:17.921096127033234, irr5:596.59716796875, ndcg5:0.8427290192893698, pnl5:6.930291652679443 
train 22, step: 0, loss: 1.0441320386983581, grad_norm: 0.14220398304347381, ic: 0.2179063443253504
train 22, step: 500, loss: 3.255141641260163, grad_norm: 0.6185777221486086, ic: -0.2225989118580609
train 22, step: 1000, loss: 1.1852939848265898, grad_norm: 0.013620238259791693, ic: 0.4690110058702547
train 22, step: 1500, loss: 0.966162913130144, grad_norm: 0.04871917574534115, ic: 0.13617730328320193
train 22, step: 2000, loss: 1.7744887994260206, grad_norm: 0.8752310629146105, ic: 0.12101665775603646
Epoch 22: 2022-05-08 21:21:23.427032: train loss: 1.6160495844597322
Eval step 0: eval loss: 0.8233603345832784
Eval: 2022-05-08 21:21:56.015601: total loss: 1.065013070905977, mse:4.592618666725049, ic :0.19381205458982606, sharpe5:18.24221142053604, irr5:611.0411987304688, ndcg5:0.8392651504897292, pnl5:6.17042350769043 
train 23, step: 0, loss: 0.988106130110771, grad_norm: 0.03057628500656847, ic: 0.1184142850672022
train 23, step: 500, loss: 1.4230934035272764, grad_norm: 0.16829934749306752, ic: 0.040497914987465765
train 23, step: 1000, loss: 1.6596959431966147, grad_norm: 0.07948052860874467, ic: 0.2593316854506175
train 23, step: 1500, loss: 1.1132772215860034, grad_norm: 0.18502112499352835, ic: 0.09022050403021828
train 23, step: 2000, loss: 1.9248977203016742, grad_norm: 1.1057433606160407, ic: 0.44044628897557003
Epoch 23: 2022-05-08 21:29:55.377055: train loss: 1.6147398673017042
Eval step 0: eval loss: 0.8275012708690068
Eval: 2022-05-08 21:30:28.382601: total loss: 1.064775761270369, mse:4.576496770561515, ic :0.20005000667301606, sharpe5:17.711072241067885, irr5:608.8978271484375, ndcg5:0.8538474151420786, pnl5:6.0814642906188965 
train 24, step: 0, loss: 2.1826612938647862, grad_norm: 0.06967006091895026, ic: 0.1702295309036581
train 24, step: 500, loss: 1.2174301206833658, grad_norm: 0.07706988940721768, ic: 0.17662294798545283
train 24, step: 1000, loss: 0.9128175641458475, grad_norm: 0.04453917998753662, ic: 0.5189189005383881
train 24, step: 1500, loss: 2.622604483331616, grad_norm: 0.8737801056678786, ic: 0.05726790290672521
train 24, step: 2000, loss: 0.9283577593342501, grad_norm: 0.02942530866317302, ic: 0.178138008927559
Epoch 24: 2022-05-08 21:38:30.410287: train loss: 1.6130470960393015
Eval step 0: eval loss: 0.8197543724068097
Eval: 2022-05-08 21:39:01.705213: total loss: 1.065417821716429, mse:4.604265391158292, ic :0.19654998206903868, sharpe5:18.486432412862776, irr5:621.7109985351562, ndcg5:0.841120831917702, pnl5:8.855491638183594 
train 25, step: 0, loss: 0.8380587191195101, grad_norm: 0.1809620995598629, ic: 0.6092466204849272
train 25, step: 500, loss: 0.8677251994429759, grad_norm: 0.0076681548008339625, ic: 0.2233385246887891
train 25, step: 1000, loss: 2.108119339953827, grad_norm: 0.5429890091180607, ic: 0.24040737227492795
train 25, step: 1500, loss: 1.13486556077681, grad_norm: 0.4022721511167582, ic: 0.5377409711046318
train 25, step: 2000, loss: 1.0230309660446304, grad_norm: 0.603181171062521, ic: 0.5944084715173898
Epoch 25: 2022-05-08 21:47:00.342665: train loss: 1.6160574669342769
Eval step 0: eval loss: 0.8222855627222734
Eval: 2022-05-08 21:47:32.454758: total loss: 1.0648073426069171, mse:4.591463098238098, ic :0.1998638072389369, sharpe5:18.5738529753685, irr5:630.1817626953125, ndcg5:0.8484426458207484, pnl5:6.243985652923584 
train 26, step: 0, loss: 6.82767805885583, grad_norm: 2.2883094606924104, ic: 0.08766656565280927
train 26, step: 500, loss: 3.878136605746677, grad_norm: 1.3893566629079979, ic: 0.38217530041518233
train 26, step: 1000, loss: 1.266659339892808, grad_norm: 0.8365045695124609, ic: -0.0011881040757402567
train 26, step: 1500, loss: 0.8257029237561009, grad_norm: 0.1254581754764562, ic: 0.30921341880351966
train 26, step: 2000, loss: 0.949214908290716, grad_norm: 0.11435931319613996, ic: 0.19286351336395763
Epoch 26: 2022-05-08 21:55:25.747614: train loss: 1.614419581351039
Eval step 0: eval loss: 0.8265716584891991
Eval: 2022-05-08 21:55:57.925198: total loss: 1.0654473412422327, mse:4.590004730833854, ic :0.193905124812939, sharpe5:18.348376446962355, irr5:630.8956298828125, ndcg5:0.8401287402233835, pnl5:9.399325370788574 
