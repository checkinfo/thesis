Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='GNNModel', dataset_type='AdjTimeDataset', seed=10086, num_days=1, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=3, num_heads=1, gnn_layers=2, print_inteval=500, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=False)
469698
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
    (1): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.8461935388036522, grad_norm: 0.00011524124802556255, ic: -0.04387934991937202
train 0, step: 500, loss: 0.9488727250732862, grad_norm: 0.1426263177196994, ic: 0.0007482292116875455
train 0, step: 1000, loss: 1.0168796393878299, grad_norm: 0.024280028691221502, ic: -0.12283369982883878
train 0, step: 1500, loss: 0.9328410501297791, grad_norm: 0.11124448344870537, ic: 0.12156844205536738
train 0, step: 2000, loss: 1.0551925890643232, grad_norm: 0.33735468996403367, ic: 0.09002974842680121
Epoch 0: train loss: 1.6294103097256922
Eval step 0: eval loss: 1.0092608877205107
Eval: total loss: 1.0858607940943374, ic :0.018744902551813762 
train 1, step: 0, loss: 0.945767765791815, grad_norm: 0.07843794842386327, ic: 0.08136242547791077
train 1, step: 500, loss: 1.6765145959500378, grad_norm: 0.10599878799836844, ic: -0.043308062296549175
train 1, step: 1000, loss: 1.0924213863485595, grad_norm: 0.35888624244156536, ic: -0.008162970603599069
train 1, step: 1500, loss: 0.8832263456659072, grad_norm: 0.1262454195758968, ic: -0.07390746701096285
train 1, step: 2000, loss: 4.717456973084835, grad_norm: 1.4097531460035104, ic: 0.06360192280068071
Epoch 1: train loss: 1.6277817821236866
Eval step 0: eval loss: 1.0014204896080172
Eval: total loss: 1.0840357399733227, ic :0.051139292069189324 
train 2, step: 0, loss: 0.9486903665636881, grad_norm: 0.10482356046576297, ic: -0.007357076633587747
train 2, step: 500, loss: 1.286452794366274, grad_norm: 0.0919290674369505, ic: 0.1786603380170685
train 2, step: 1000, loss: 1.0473382949829102, grad_norm: 0.009448881121157066, ic: 0.010601610344701591
train 2, step: 1500, loss: 0.9409108706790906, grad_norm: 0.2758491539927152, ic: -0.004580077936787147
train 2, step: 2000, loss: 1.700987576362782, grad_norm: 0.07442499380103827, ic: 0.09859489807260237
Epoch 2: train loss: 1.6266177590305428
Eval step 0: eval loss: 0.9921790791411598
Eval: total loss: 1.0849473804979763, ic :0.044879253077122754 
train 3, step: 0, loss: 0.8343600643382353, grad_norm: 0.065451864453434, ic: 0.2220932736226455
train 3, step: 500, loss: 1.1850591646166213, grad_norm: 0.0037249424676476595, ic: 0.09594268336313778
train 3, step: 1000, loss: 0.9112522446106527, grad_norm: 0.033828794778568774, ic: 0.20772654692489664
train 3, step: 1500, loss: 4.379898864559442, grad_norm: 0.6960657349405865, ic: 0.09414759207643704
train 3, step: 2000, loss: 1.6449327867514068, grad_norm: 0.38148136620975165, ic: -0.03044046799629283
Epoch 3: train loss: 1.6263245134536473
Eval step 0: eval loss: 0.9991433865274486
Eval: total loss: 1.0849788029694352, ic :0.051847605926105 
train 4, step: 0, loss: 1.301102497443398, grad_norm: 0.19627604891821787, ic: 0.012043378247585989
train 4, step: 500, loss: 0.8272807821236756, grad_norm: 0.03546512449756413, ic: -0.03660809593613056
train 4, step: 1000, loss: 1.8660491117206728, grad_norm: 0.18350620987427468, ic: -0.05325242728116823
train 4, step: 1500, loss: 1.1740132934872218, grad_norm: 0.011166603211807284, ic: 0.16114740992349005
train 4, step: 2000, loss: 6.637623355263158, grad_norm: 1.0087327099781205, ic: -0.04319220639398597
Epoch 4: train loss: 1.6264234885121078
Eval step 0: eval loss: 0.9907035004031727
Eval: total loss: 1.0852776318652173, ic :0.050721336933255444 
train 5, step: 0, loss: 3.7213537337932183, grad_norm: 0.5358166891645678, ic: 0.14207659517548435
train 5, step: 500, loss: 3.436458133166795, grad_norm: 0.718030731855896, ic: -0.05346835250303402
train 5, step: 1000, loss: 1.0423555854560222, grad_norm: 0.04741231036732611, ic: 0.06451826498727374
train 5, step: 1500, loss: 1.4925317415849122, grad_norm: 0.9604547792972521, ic: -0.07334852877145856
train 5, step: 2000, loss: 1.771083275924406, grad_norm: 0.46662320602666457, ic: 0.02758737700414163
Epoch 5: train loss: 1.6267322561623192
Eval step 0: eval loss: 0.9973625355861637
Eval: total loss: 1.0857623885548042, ic :0.05281327310251826 
train 6, step: 0, loss: 1.1042148003725891, grad_norm: 0.02443201229684175, ic: -0.037564807940024966
train 6, step: 500, loss: 1.8401781839037699, grad_norm: 0.1032116793097028, ic: 0.19252658010990661
train 6, step: 1000, loss: 0.8850901533917683, grad_norm: 0.1785430290204923, ic: 0.06612429579408388
train 6, step: 1500, loss: 0.8450018874737394, grad_norm: 0.0029239934597309098, ic: 0.08736927186799924
train 6, step: 2000, loss: 1.9342097063472763, grad_norm: 0.20164289818807815, ic: 0.05635827116344867
Epoch 6: train loss: 1.6256138755034588
Eval step 0: eval loss: 0.9911553984004738
Eval: total loss: 1.0817314962314697, ic :0.05648275722939166 
train 7, step: 0, loss: 0.8147938869494875, grad_norm: 0.05571612995725911, ic: -0.028403821205086218
train 7, step: 500, loss: 1.1236232069672132, grad_norm: 0.011478364863802321, ic: 0.021960408684070613
train 7, step: 1000, loss: 1.1003815020447238, grad_norm: 0.02518284627484211, ic: -0.06992928091198017
train 7, step: 1500, loss: 1.0783528550596109, grad_norm: 0.07073486244957321, ic: 0.17642462403581516
train 7, step: 2000, loss: 0.7096545400382805, grad_norm: 0.014840744487535407, ic: -0.0372128811203398
Epoch 7: train loss: 1.6257525950180918
Eval step 0: eval loss: 0.9974326022742231
Eval: total loss: 1.0931810059551594, ic :0.03805252859999972 
train 8, step: 0, loss: 2.012340517277165, grad_norm: 0.35077285941026604, ic: -0.08018053728129616
train 8, step: 500, loss: 0.9607026198814654, grad_norm: 0.03217000576366824, ic: 0.0991939877225532
train 8, step: 1000, loss: 0.7475681980232101, grad_norm: 0.03450391672518012, ic: 0.06356065443937337
train 8, step: 1500, loss: 0.8406994628906249, grad_norm: 0.03527017982682247, ic: 0.18594775068183217
train 8, step: 2000, loss: 1.4630834482230393, grad_norm: 0.7428522820963005, ic: -0.003400479055408042
Epoch 8: train loss: 1.6260849580852457
Eval step 0: eval loss: 0.997749895092812
Eval: total loss: 1.0850673483283644, ic :0.052547233659970154 
train 9, step: 0, loss: 0.784311727865477, grad_norm: 0.0059813924084169585, ic: 0.06675523397558596
train 9, step: 500, loss: 1.7453492618667628, grad_norm: 0.047220881340598125, ic: -0.14040202912597904
train 9, step: 1000, loss: 0.780721704561185, grad_norm: 0.01118215303656071, ic: 0.12262966244224109
train 9, step: 1500, loss: 0.9827336609127965, grad_norm: 0.037043084821223066, ic: 0.04102700142129449
train 9, step: 2000, loss: 4.467117800245098, grad_norm: 2.4003065688259753, ic: 0.23323666659248515
Epoch 9: train loss: 1.6255263325667433
Eval step 0: eval loss: 0.9987727401757504
Eval: total loss: 1.0968335165779586, ic :0.03943965194290566 
train 10, step: 0, loss: 0.7809635323126197, grad_norm: 0.0017712197020220571, ic: 0.09905608490467843
train 10, step: 500, loss: 0.784341643128214, grad_norm: 0.07719463366675268, ic: -0.039257702963111867
train 10, step: 1000, loss: 0.9994708137333114, grad_norm: 0.3751585599501545, ic: -0.030910225773312993
train 10, step: 1500, loss: 1.3038800048828125, grad_norm: 0.13158913900878427, ic: -0.09525298594782838
train 10, step: 2000, loss: 1.270912182735511, grad_norm: 0.4333337008630228, ic: -0.17738080492962324
Epoch 10: train loss: 1.6254621517099002
Eval step 0: eval loss: 0.9885424251867758
Eval: total loss: 1.0816560421790087, ic :0.056721323251202986 
train 11, step: 0, loss: 1.5341573749667905, grad_norm: 0.024278219938415502, ic: 0.22718044879326177
train 11, step: 500, loss: 1.342385984174519, grad_norm: 0.19596494679654858, ic: 0.15868530247023852
train 11, step: 1000, loss: 2.2423051428918392, grad_norm: 0.7017532475542964, ic: 0.2888841995430092
train 11, step: 1500, loss: 1.3812215788255604, grad_norm: 0.728857059894497, ic: 0.016754698103512013
train 11, step: 2000, loss: 3.7198586945956453, grad_norm: 2.2838541954496696, ic: 0.20504285087943958
Epoch 11: train loss: 1.623839573847323
Eval step 0: eval loss: 1.004803553576718
Eval: total loss: 1.086285008325753, ic :0.07483863198527238 
train 12, step: 0, loss: 0.9906392958325158, grad_norm: 0.05146039719827443, ic: 0.07234262028215109
train 12, step: 500, loss: 1.2056199495982403, grad_norm: 0.09562149730362587, ic: 0.09535856756682325
train 12, step: 1000, loss: 1.2107115647513742, grad_norm: 0.40693789810634107, ic: 0.21863940348902242
train 12, step: 1500, loss: 0.7527436127533784, grad_norm: 0.02496129931325093, ic: 0.372501269231912
train 12, step: 2000, loss: 2.693295606926306, grad_norm: 0.549948995940989, ic: -0.06889009210730582
Epoch 12: train loss: 1.619893303876052
Eval step 0: eval loss: 0.9953036034590573
Eval: total loss: 1.0830731969630902, ic :0.055376470785032904 
train 13, step: 0, loss: 1.352317676005061, grad_norm: 0.544751700555082, ic: 0.011052620853946874
train 13, step: 500, loss: 1.4369018183088373, grad_norm: 0.12575233648534853, ic: -0.09517984966529427
train 13, step: 1000, loss: 2.16763258561383, grad_norm: 0.6519099228166546, ic: 0.13746837134061238
train 13, step: 1500, loss: 1.5668935723315986, grad_norm: 0.8562693885516597, ic: -0.12146497875254425
train 13, step: 2000, loss: 0.7158519784577662, grad_norm: 0.02343611833636618, ic: -0.04160210650037076
Epoch 13: train loss: 1.6180116793537116
Eval step 0: eval loss: 1.0052533302889677
Eval: total loss: 1.0819740359394017, ic :0.0962481078888503 
train 14, step: 0, loss: 1.9776505539986813, grad_norm: 0.6880422489560507, ic: 0.15150823761831747
train 14, step: 500, loss: 2.766288336069306, grad_norm: 1.2238771494476355, ic: -0.14651898753965897
train 14, step: 1000, loss: 1.0528084856719369, grad_norm: 0.09055636860048603, ic: 0.18678539415916642
train 14, step: 1500, loss: 2.9417210679109926, grad_norm: 0.49247900344666296, ic: 0.15412592185244053
train 14, step: 2000, loss: 0.7939142603412985, grad_norm: 0.07030442156079741, ic: -0.054110067624877245
Epoch 14: train loss: 1.616095887465358
Eval step 0: eval loss: 0.9996029982885728
Eval: total loss: 1.0808664663611165, ic :0.10567241868729563 
train 15, step: 0, loss: 1.2385034695477553, grad_norm: 0.2554718314472853, ic: 0.34984343283937613
train 15, step: 500, loss: 1.8047183920269363, grad_norm: 0.9590938994776445, ic: -0.046596235683218024
train 15, step: 1000, loss: 1.730006167763158, grad_norm: 0.4852809182295148, ic: 0.18446409255363108
train 15, step: 1500, loss: 1.4003510560170178, grad_norm: 0.4507540018964553, ic: -0.09520343030940163
train 15, step: 2000, loss: 3.1093152958284658, grad_norm: 0.13892114944917397, ic: 0.2014876197533489
Epoch 15: train loss: 1.614084011842633
Eval step 0: eval loss: 1.0026456924285807
Eval: total loss: 1.0811790213378345, ic :0.10868342925314851 
train 16, step: 0, loss: 1.8069378281673443, grad_norm: 0.5542268874443093, ic: 0.15731156640787455
train 16, step: 500, loss: 1.3424023930330453, grad_norm: 0.5656041753600201, ic: 0.35021898493972786
train 16, step: 1000, loss: 0.905881672792307, grad_norm: 0.014864701000970648, ic: -0.01655985162702017
train 16, step: 1500, loss: 1.3289696279198233, grad_norm: 0.7947400853243878, ic: 0.23522527223374573
train 16, step: 2000, loss: 1.2372574515964674, grad_norm: 0.12118590512650436, ic: 0.19722383090683693
Epoch 16: train loss: 1.614710529544197
Eval step 0: eval loss: 0.9970430572011584
Eval: total loss: 1.0815114198023965, ic :0.10323598644270696 
train 17, step: 0, loss: 1.5566408105171923, grad_norm: 0.6083936639504944, ic: 0.10677884127903935
train 17, step: 500, loss: 2.2263191025732496, grad_norm: 0.8166447024013621, ic: 0.12054795598479531
train 17, step: 1000, loss: 0.9610379823151125, grad_norm: 0.0023908715931598174, ic: 0.17724359331094736
train 17, step: 1500, loss: 0.8321130773523352, grad_norm: 0.23010989633569795, ic: 0.28973765098161114
train 17, step: 2000, loss: 3.1345748002381186, grad_norm: 0.777316105562817, ic: 0.01671623988665073
Epoch 17: train loss: 1.6162451863899698
Eval step 0: eval loss: 0.9988449924302264
Eval: total loss: 1.0836241409562821, ic :0.09604535283704115 
train 18, step: 0, loss: 1.0694500940159035, grad_norm: 0.20064968341346162, ic: 0.02043657167305556
train 18, step: 500, loss: 0.7939893988114369, grad_norm: 0.12514923936769334, ic: -0.03381434467255075
train 18, step: 1000, loss: 1.1724124048553328, grad_norm: 0.8520939731868931, ic: 0.04500975821202975
train 18, step: 1500, loss: 0.906126763410629, grad_norm: 0.0103757004684112, ic: 0.14573184440325648
train 18, step: 2000, loss: 1.7243074915715768, grad_norm: 1.0150451705508778, ic: 0.06949388441730431
Epoch 18: train loss: 1.6141224136015428
Eval step 0: eval loss: 0.9936649428975776
Eval: total loss: 1.0801090271001739, ic :0.08928963781063967 
train 19, step: 0, loss: 1.0869281034514378, grad_norm: 0.7064306867540746, ic: 0.06530733992423468
train 19, step: 500, loss: 2.2170888389029155, grad_norm: 0.8678316809393669, ic: 0.21547595329833685
train 19, step: 1000, loss: 1.3095145089285714, grad_norm: 0.08228129757051084, ic: 0.19117030419571118
train 19, step: 1500, loss: 1.5034816576086956, grad_norm: 0.06289486574659386, ic: 0.14215308187545467
train 19, step: 2000, loss: 1.1532109151359733, grad_norm: 0.32295654744120494, ic: 0.19127134119003214
Epoch 19: train loss: 1.614819100765746
Eval step 0: eval loss: 1.0031629002929172
Eval: total loss: 1.0804118872274853, ic :0.10325539847463121 
