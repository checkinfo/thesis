Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjSeqTimeDataset', dout=0.3, epochs=20, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=1, mask_adj=True, mask_type='soft', model_type='BiGLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
76808
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8208717040345277, grad_norm: 0.13168714152038605, ic: -0.0230562899935301
train 0, step: 500, loss: 1.1523019546055748, grad_norm: 0.014445694490993328, ic: 0.07162848141269788
train 0, step: 1000, loss: 0.7122004291798809, grad_norm: 9.751501581066981e-05, ic: 0.09316232057168285
train 0, step: 1500, loss: 0.8560661089302289, grad_norm: 0.16727046685228453, ic: 0.06041363378005464
train 0, step: 2000, loss: 2.386690299526584, grad_norm: 0.7219719419081527, ic: 0.10038341700283286
Epoch 0: train loss: 1.6485118844868019
Eval step 0: eval loss: 0.8352970489083904
Eval: total loss: 1.0790262734386897, mse:4.823660061867534, ic :0.009858865836591285, sharpe5:7.612329604625701, irr5:213.43630981445312, ndcg5:0.8526229361870383 
train 1, step: 0, loss: 2.3076760326272425, grad_norm: 0.04311813963814819, ic: 0.05314961345685805
train 1, step: 500, loss: 0.6242349635427192, grad_norm: 0.03810193394467851, ic: -0.08982611367365126
train 1, step: 1000, loss: 1.1952715523635284, grad_norm: 0.09888166179899505, ic: 0.05103444532609944
train 1, step: 1500, loss: 2.2699105363348324, grad_norm: 0.8629738742842019, ic: 0.07218536025514831
train 1, step: 2000, loss: 1.166280175162193, grad_norm: 0.012056135155307377, ic: -0.00990128642862615
Epoch 1: train loss: 1.6468736576927832
Eval step 0: eval loss: 0.8363394702071258
Eval: total loss: 1.0793169093415245, mse:4.822869941503163, ic :0.01911057065138894, sharpe5:9.465941982865333, irr5:260.0500183105469, ndcg5:0.8559197991077864 
train 2, step: 0, loss: 2.0054411711516202, grad_norm: 0.2390515268225528, ic: 0.08904838520988953
train 2, step: 500, loss: 1.148375245766062, grad_norm: 0.4247554005214264, ic: 0.5137498546805976
train 2, step: 1000, loss: 1.128490197966777, grad_norm: 0.05632257653194871, ic: 0.1912816332402782
