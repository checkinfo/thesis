Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_60_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
77951
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795814448543784, grad_norm: 4.816621748204249, ic: 0.022429326638039926
train 0, step: 500, loss: 0.8631394019051432, grad_norm: 0.026600887370210005, ic: 0.04508553880625055
train 0, step: 1000, loss: 1.950101371045172, grad_norm: 0.5126675494219992, ic: 0.015558687182952786
train 0, step: 1500, loss: 0.9563130133708004, grad_norm: 0.048864651436590835, ic: 0.014594422297543055
train 0, step: 2000, loss: 1.0022359344064975, grad_norm: 0.1582161327989396, ic: 0.025620365549984134
Epoch 0: 2022-05-07 05:24:22.466348: train loss: 1.648491751175531
Eval step 0: eval loss: 0.8363123291787408
Eval: 2022-05-07 05:24:54.844421: total loss: 1.079345484508216, mse:4.822855858191031, ic :0.00817956088962848, sharpe5:7.944834789633751, irr5:224.6867218017578, ndcg5:0.8552702988812534, pnl5:2.5358266830444336 
train 1, step: 0, loss: 2.774787361391129, grad_norm: 0.8786389890251776, ic: 0.057401197118142525
train 1, step: 500, loss: 1.755407353779893, grad_norm: 0.7705686220825717, ic: 0.09699995042371429
train 1, step: 1000, loss: 0.8777574119275396, grad_norm: 0.17542778268804235, ic: 0.08002845465392212
train 1, step: 1500, loss: 1.7132315800107758, grad_norm: 0.20669172228044547, ic: -0.03181129492046889
train 1, step: 2000, loss: 2.1772998046875003, grad_norm: 0.8877119260623971, ic: -0.046881331964649484
Epoch 1: 2022-05-07 05:32:35.081268: train loss: 1.646735820330142
Eval step 0: eval loss: 0.8345949838234324
Eval: 2022-05-07 05:33:07.829789: total loss: 1.078981568288052, mse:4.823505594884003, ic :0.007776322073643108, sharpe5:7.617468819022179, irr5:214.40362548828125, ndcg5:0.8559364816113356, pnl5:2.5892131328582764 
train 2, step: 0, loss: 2.1417832031249997, grad_norm: 0.009298489294828852, ic: 0.13182245485044758
train 2, step: 500, loss: 3.3000220834555947, grad_norm: 0.28515957174402307, ic: 0.04749034941644814
train 2, step: 1000, loss: 2.072376751077586, grad_norm: 0.00019856318801825148, ic: 0.19027168610894707
train 2, step: 1500, loss: 1.4855094152552482, grad_norm: 0.05971568853848265, ic: -0.03798589436692595
train 2, step: 2000, loss: 3.2348197115384614, grad_norm: 0.787179036421027, ic: 0.20627849781202945
Epoch 2: 2022-05-07 05:40:36.612491: train loss: 1.6464975937121926
Eval step 0: eval loss: 0.8358758216914185
Eval: 2022-05-07 05:41:09.376899: total loss: 1.0794621625857266, mse:4.823039227572543, ic :0.010409739519593172, sharpe5:7.6071114054322235, irr5:215.15647888183594, ndcg5:0.8498534078382091, pnl5:2.670518398284912 
train 3, step: 0, loss: 1.5229711517085873, grad_norm: 0.525173045313232, ic: -0.002504961720160001
train 3, step: 500, loss: 1.5013449720414056, grad_norm: 0.342285375992984, ic: 0.09227055620629987
train 3, step: 1000, loss: 3.6798595369170988, grad_norm: 0.7054658914546649, ic: -0.04810665651745033
train 3, step: 1500, loss: 1.981620925928294, grad_norm: 1.2464851665061887, ic: -0.06833618510929157
train 3, step: 2000, loss: 0.8989815376900337, grad_norm: 0.0011330506897498983, ic: 0.007962629590896332
Epoch 3: 2022-05-07 05:48:47.075050: train loss: 1.6459151980951365
Eval step 0: eval loss: 0.83496646861005
Eval: 2022-05-07 05:49:19.897012: total loss: 1.0792937669323188, mse:4.824392759912826, ic :0.013931332391875382, sharpe5:8.120211308598519, irr5:225.05520629882812, ndcg5:0.8527845271729791, pnl5:2.945019006729126 
train 4, step: 0, loss: 1.431441127232143, grad_norm: 0.04422250479151907, ic: 0.12401603199059688
train 4, step: 500, loss: 1.6503060408464567, grad_norm: 0.5620535809869599, ic: 0.02837690473173234
train 4, step: 1000, loss: 2.975390364484089, grad_norm: 0.770610765226305, ic: 0.07749023559360663
train 4, step: 1500, loss: 2.1465329970991562, grad_norm: 0.48937169190851476, ic: 0.0157092847142214
train 4, step: 2000, loss: 1.0866065136566874, grad_norm: 0.39040855051115797, ic: 0.2336414504440246
Epoch 4: 2022-05-07 05:56:53.675874: train loss: 1.6455749677450058
Eval step 0: eval loss: 0.8490386416211143
Eval: 2022-05-07 05:57:25.487580: total loss: 1.0844134923764133, mse:4.824319706696427, ic :0.04702312996513416, sharpe5:10.666027529239654, irr5:317.69818115234375, ndcg5:0.8552902836346977, pnl5:3.550793409347534 
train 5, step: 0, loss: 1.356050755033557, grad_norm: 0.16356029654596238, ic: 0.040035423779874464
train 5, step: 500, loss: 0.8907387451365484, grad_norm: 0.01034002765276594, ic: 0.07191800948356988
train 5, step: 1000, loss: 0.9821666965996169, grad_norm: 0.1485920015129395, ic: 0.006442038604028005
train 5, step: 1500, loss: 1.5329268310920752, grad_norm: 0.156440060312373, ic: 0.02347976609151462
train 5, step: 2000, loss: 1.1102463441215922, grad_norm: 0.02682700867899825, ic: 0.10568375042702621
Epoch 5: 2022-05-07 06:04:57.231112: train loss: 1.6454690682449875
Eval step 0: eval loss: 0.8380067783110511
Eval: 2022-05-07 06:05:29.772919: total loss: 1.0798400830547514, mse:4.81787035956978, ic :0.03947593214206753, sharpe5:10.73010359108448, irr5:318.5983581542969, ndcg5:0.8501488023787294, pnl5:3.0803844928741455 
train 6, step: 0, loss: 1.3351352819415372, grad_norm: 0.4201596220249594, ic: 0.10880641106737424
train 6, step: 500, loss: 1.008967961870693, grad_norm: 0.04484765937601731, ic: 0.07886854700083486
train 6, step: 1000, loss: 1.1242983973978136, grad_norm: 0.10003608061469635, ic: 0.49677366314785515
train 6, step: 1500, loss: 1.5716372151020144, grad_norm: 0.7330601557809499, ic: 0.1437329898855208
train 6, step: 2000, loss: 0.8112698166345576, grad_norm: 0.0470274003739133, ic: -0.033031220108828224
Epoch 6: 2022-05-07 06:13:05.628312: train loss: 1.642975101031185
Eval step 0: eval loss: 0.8318852544207718
Eval: 2022-05-07 06:13:36.485389: total loss: 1.076086081560601, mse:4.7252459959208535, ic :0.13420379513104372, sharpe5:11.766158051490784, irr5:390.4624328613281, ndcg5:0.8447355764687009, pnl5:2.943155527114868 
train 7, step: 0, loss: 0.9884065628051758, grad_norm: 0.04740078808202374, ic: 0.11942449294922147
train 7, step: 500, loss: 0.6512712751116071, grad_norm: 0.004010352224099154, ic: -0.01091711200875574
train 7, step: 1000, loss: 1.0268134460681646, grad_norm: 0.24511351269181603, ic: 0.16532970829273674
train 7, step: 1500, loss: 2.2572734542470525, grad_norm: 0.7669465151127465, ic: 0.4399301314499559
train 7, step: 2000, loss: 0.9181762103695477, grad_norm: 0.076642189741342, ic: -0.037349864718617995
Epoch 7: 2022-05-07 06:21:10.466104: train loss: 1.633025804854395
Eval step 0: eval loss: 0.8395274476834167
Eval: 2022-05-07 06:21:42.537812: total loss: 1.0832308023213342, mse:4.786053099489945, ic :0.11515312200548593, sharpe5:14.096628608107567, irr5:460.48992919921875, ndcg5:0.8334268547054343, pnl5:2.8016109466552734 
train 8, step: 0, loss: 3.5852907042572464, grad_norm: 1.103865914613348, ic: 0.2125408984283939
train 8, step: 500, loss: 2.75084540184508, grad_norm: 0.9649397155266984, ic: 0.03179725209004387
train 8, step: 1000, loss: 3.0710845504981883, grad_norm: 0.9973049471385699, ic: 0.10691382304305422
train 8, step: 1500, loss: 0.7299990577556643, grad_norm: 1.2820522397288172, ic: 0.4068995760167168
train 8, step: 2000, loss: 1.0790077379856158, grad_norm: 0.38817024965548313, ic: 0.5107559956687848
Epoch 8: 2022-05-07 06:29:12.391144: train loss: 1.6307496833903934
Eval step 0: eval loss: 0.8298955983683483
Eval: 2022-05-07 06:29:45.123463: total loss: 1.0727333899159468, mse:4.6941940173559935, ic :0.15491624931401507, sharpe5:15.207806596755981, irr5:492.67486572265625, ndcg5:0.8542264846181036, pnl5:5.365050315856934 
train 9, step: 0, loss: 5.470128401614833, grad_norm: 1.1846235675790409, ic: -0.01733978727942678
train 9, step: 500, loss: 1.365000246977529, grad_norm: 1.8356400370938475, ic: 0.29740653745342926
train 9, step: 1000, loss: 0.9317490651298235, grad_norm: 0.061987622175616375, ic: 0.03689134525585677
train 9, step: 1500, loss: 1.0907138830081, grad_norm: 0.017348470350593826, ic: 0.41138966905926205
train 9, step: 2000, loss: 1.0702132920786849, grad_norm: 0.34288428520365355, ic: 0.2759590466319151
Epoch 9: 2022-05-07 06:37:04.876024: train loss: 1.6287462309923246
Eval step 0: eval loss: 0.8286285882754872
Eval: 2022-05-07 06:37:37.304198: total loss: 1.0721725417071597, mse:4.701580464345099, ic :0.15627758530629193, sharpe5:16.21811390519142, irr5:515.1158447265625, ndcg5:0.871575139800831, pnl5:7.217560291290283 
train 10, step: 0, loss: 7.096459747403426, grad_norm: 2.0363332049676264, ic: 0.261865531222202
train 10, step: 500, loss: 1.1317644583374216, grad_norm: 0.17575290867456067, ic: 0.03610208982662238
train 10, step: 1000, loss: 2.3910219157400308, grad_norm: 0.7699202357497279, ic: 0.15189302775798522
train 10, step: 1500, loss: 1.1137736927379263, grad_norm: 0.3370322041006197, ic: -0.008520913606768243
train 10, step: 2000, loss: 2.747135058000095, grad_norm: 1.389463126037985, ic: 0.43122737056857
Epoch 10: 2022-05-07 06:45:21.463626: train loss: 1.6283965798188496
Eval step 0: eval loss: 0.8301504796372826
Eval: 2022-05-07 06:45:53.215023: total loss: 1.0713174620664296, mse:4.689605144428726, ic :0.16302919428156984, sharpe5:16.110130809545517, irr5:524.659423828125, ndcg5:0.8621969205887319, pnl5:4.377639293670654 
train 11, step: 0, loss: 1.2479020522162607, grad_norm: 0.07205299005351766, ic: 0.21582572274931966
train 11, step: 500, loss: 0.6698474553126084, grad_norm: 0.1637680569901905, ic: 0.5342076074217498
train 11, step: 1000, loss: 0.9288362281340169, grad_norm: 0.13810318515444378, ic: 0.07636800270623292
train 11, step: 1500, loss: 1.0549668429190653, grad_norm: 0.06793318215456293, ic: 0.18470673253667888
train 11, step: 2000, loss: 0.7897401541518563, grad_norm: 0.0028616126288364156, ic: 0.12326728149076885
Epoch 11: 2022-05-07 06:53:27.843064: train loss: 1.6281304547669166
Eval step 0: eval loss: 0.8332867476661288
Eval: 2022-05-07 06:53:59.886578: total loss: 1.0718797678327654, mse:4.683858362411717, ic :0.1663350807669087, sharpe5:17.1389082634449, irr5:550.4370727539062, ndcg5:0.8528563573418663, pnl5:6.683228015899658 
train 12, step: 0, loss: 0.9666671752929688, grad_norm: 0.08779056618148827, ic: 0.39993630412590264
train 12, step: 500, loss: 0.9313596933225841, grad_norm: 0.1278772970459847, ic: 0.1867727208769886
train 12, step: 1000, loss: 2.9365772806155457, grad_norm: 0.48518656905930546, ic: 0.1674739042836589
train 12, step: 1500, loss: 0.9365095848902799, grad_norm: 0.15705186396900386, ic: -0.12817309084861694
train 12, step: 2000, loss: 0.8731749381903795, grad_norm: 0.003218446302888959, ic: 0.25191660919577863
Epoch 12: 2022-05-07 07:01:34.236459: train loss: 1.6269755322346382
Eval step 0: eval loss: 0.83274810758858
Eval: 2022-05-07 07:02:06.344933: total loss: 1.0711141634463377, mse:4.67901650704821, ic :0.16965194578888054, sharpe5:17.0044392478466, irr5:554.6226196289062, ndcg5:0.8527675682274122, pnl5:6.366924285888672 
train 13, step: 0, loss: 2.0873665089592133, grad_norm: 0.7062085479774614, ic: 0.364554371199084
train 13, step: 500, loss: 0.837374199298687, grad_norm: 0.06776247458770834, ic: 0.4953703123199418
train 13, step: 1000, loss: 0.945556640625, grad_norm: 0.5483675375164743, ic: 0.44078129704714075
train 13, step: 1500, loss: 2.3765033421155346, grad_norm: 0.3142418660324883, ic: 0.027310628636508245
train 13, step: 2000, loss: 1.4886661614703618, grad_norm: 0.27592597892692605, ic: 0.17316384221061085
Epoch 13: 2022-05-07 07:09:41.435322: train loss: 1.627571284523891
Eval step 0: eval loss: 0.8274075635743215
Eval: 2022-05-07 07:10:13.650465: total loss: 1.0705178864961797, mse:4.689685297246547, ic :0.16584499611948667, sharpe5:17.394016860723493, irr5:560.5776977539062, ndcg5:0.8616180624752919, pnl5:7.054330825805664 
train 14, step: 0, loss: 4.586824937841264, grad_norm: 2.259906984265392, ic: 0.1501339401855726
train 14, step: 500, loss: 0.8280619116733563, grad_norm: 0.002818016958938324, ic: 0.11252407285799203
train 14, step: 1000, loss: 1.868339799259681, grad_norm: 0.6362097353269403, ic: 0.3741750822898788
train 14, step: 1500, loss: 1.129162655146193, grad_norm: 0.06725451006396481, ic: -0.02466117421051259
train 14, step: 2000, loss: 1.1459778256869924, grad_norm: 0.19973006115975142, ic: 0.10751203776681477
Epoch 14: 2022-05-07 07:17:56.830379: train loss: 1.626725059748968
Eval step 0: eval loss: 0.8385324910267387
Eval: 2022-05-07 07:18:28.302991: total loss: 1.0729924484298357, mse:4.680210049571224, ic :0.1660964554955626, sharpe5:15.685772454142569, irr5:513.5493774414062, ndcg5:0.8422618631239659, pnl5:4.7648773193359375 
train 15, step: 0, loss: 3.3753739056420238, grad_norm: 0.5914473339038366, ic: 0.07679523872889031
train 15, step: 500, loss: 1.2605782094243558, grad_norm: 0.03109066558361978, ic: -0.0434551753475475
train 15, step: 1000, loss: 1.3326340590066057, grad_norm: 0.15837363052711406, ic: 0.08959868654611493
train 15, step: 1500, loss: 0.8495601624015748, grad_norm: 0.24780284108557443, ic: 0.07882384674308564
train 15, step: 2000, loss: 1.4721803526947463, grad_norm: 0.7400567458984881, ic: 0.05217084572075373
Epoch 15: 2022-05-07 07:26:15.190604: train loss: 1.6266634946881005
Eval step 0: eval loss: 0.8417369332027134
Eval: 2022-05-07 07:26:47.473515: total loss: 1.0754214823291375, mse:4.667785509046285, ic :0.17340559398880742, sharpe5:17.22951848745346, irr5:552.9793090820312, ndcg5:0.8503263987136604, pnl5:5.955832481384277 
train 16, step: 0, loss: 0.6941693110401821, grad_norm: 0.3050017863471621, ic: 0.05274059419028884
train 16, step: 500, loss: 1.583308432454341, grad_norm: 0.35765674729169805, ic: 0.1570083054075266
train 16, step: 1000, loss: 0.8768015543619792, grad_norm: 0.020427906138291645, ic: -0.0076665482290072424
train 16, step: 1500, loss: 0.8454706238610619, grad_norm: 0.31778559038847687, ic: 0.17262566273820912
train 16, step: 2000, loss: 3.403878751419281, grad_norm: 0.950743019741963, ic: -0.015466512805585666
Epoch 16: 2022-05-07 07:34:19.213386: train loss: 1.6261793658093995
Eval step 0: eval loss: 0.8309507540832455
Eval: 2022-05-07 07:34:51.187368: total loss: 1.0710468876180002, mse:4.66303797853646, ic :0.17543099298118597, sharpe5:17.035492097139358, irr5:579.8040161132812, ndcg5:0.8528666684222214, pnl5:5.438905239105225 
train 17, step: 0, loss: 1.2747704948607426, grad_norm: 0.2791691086252413, ic: -0.08978021557562567
train 17, step: 500, loss: 1.7567524400829946, grad_norm: 0.8545633556606574, ic: 0.18713014141019702
train 17, step: 1000, loss: 1.2773965651511137, grad_norm: 0.11829531643721528, ic: 0.1519324799979483
train 17, step: 1500, loss: 4.540065611223365, grad_norm: 1.4967598897116134, ic: 0.2119657382469197
train 17, step: 2000, loss: 1.2580288664727526, grad_norm: 0.880408891523083, ic: 0.01737274379695431
Epoch 17: 2022-05-07 07:42:38.468942: train loss: 1.6255643162806692
Eval step 0: eval loss: 0.8377151087287604
Eval: 2022-05-07 07:43:08.861412: total loss: 1.0718611154446906, mse:4.63201333268338, ic :0.1880837902075489, sharpe5:18.660151669979093, irr5:612.8560180664062, ndcg5:0.8494670272425143, pnl5:7.567510604858398 
train 18, step: 0, loss: 1.4109403428648701, grad_norm: 1.2099902019710669, ic: 0.2082063986419461
train 18, step: 500, loss: 1.494583092936592, grad_norm: 0.8814346234233945, ic: -0.050902387312558854
train 18, step: 1000, loss: 0.6667762735445205, grad_norm: 0.01736668946904413, ic: 0.5581728832674602
train 18, step: 1500, loss: 1.429624215390817, grad_norm: 0.05786283316831323, ic: 0.16262387537286616
train 18, step: 2000, loss: 0.9134900524358082, grad_norm: 0.013494453158553271, ic: -0.046625263215700964
Epoch 18: 2022-05-07 07:50:51.219773: train loss: 1.6253217468458623
Eval step 0: eval loss: 0.8271335163659115
Eval: 2022-05-07 07:51:24.398065: total loss: 1.0681278478805083, mse:4.6257639391216845, ic :0.187728238240147, sharpe5:17.894046435356138, irr5:598.4557495117188, ndcg5:0.8279372362065528, pnl5:6.122512340545654 
train 19, step: 0, loss: 1.4602508060515873, grad_norm: 0.9275911627661475, ic: 0.022655067797683802
train 19, step: 500, loss: 0.8613204956054688, grad_norm: 0.05433662963016052, ic: 0.24221689950457337
train 19, step: 1000, loss: 0.9631816658361808, grad_norm: 0.021571927204876544, ic: 0.21812642823002187
train 19, step: 1500, loss: 3.969463034741743, grad_norm: 1.425994631499846, ic: 0.15195281032087837
train 19, step: 2000, loss: 1.0143201622596154, grad_norm: 0.26844326664043466, ic: 0.24866446628608257
Epoch 19: 2022-05-07 07:58:55.933115: train loss: 1.622751957914838
Eval step 0: eval loss: 0.8297704409246575
Eval: 2022-05-07 07:59:28.495713: total loss: 1.068578195334599, mse:4.590036709034785, ic :0.18720485911946788, sharpe5:17.8960877430439, irr5:591.451904296875, ndcg5:0.8510957834755587, pnl5:6.131752967834473 
train 20, step: 0, loss: 2.331533318922925, grad_norm: 1.4386148386066933, ic: 0.0455502274914112
train 20, step: 500, loss: 3.215560369318182, grad_norm: 0.65997952135065, ic: 0.09871399741008462
train 20, step: 1000, loss: 0.9583229064941406, grad_norm: 0.11945798788318035, ic: 0.20304713816481726
train 20, step: 1500, loss: 1.7752470568529863, grad_norm: 1.820776318903705, ic: 0.2578409543958994
train 20, step: 2000, loss: 1.0305064265140702, grad_norm: 0.07455470160812758, ic: 0.004435970320274669
Epoch 20: 2022-05-07 08:07:12.327268: train loss: 1.6200356679574235
Eval step 0: eval loss: 0.8359670206920112
Eval: 2022-05-07 08:07:44.464447: total loss: 1.0687733034779567, mse:4.591381603398407, ic :0.18770673383752431, sharpe5:17.80848934650421, irr5:590.2083129882812, ndcg5:0.8589260615983787, pnl5:7.514039516448975 
train 21, step: 0, loss: 1.0066465469465105, grad_norm: 0.5105676806816537, ic: 0.05594367762634977
train 21, step: 500, loss: 0.7689479051438053, grad_norm: 0.01637059518448067, ic: 0.1951507978571972
train 21, step: 1000, loss: 0.9414211072419818, grad_norm: 0.9280447660313722, ic: 0.17302357781376812
train 21, step: 1500, loss: 0.9893448172583693, grad_norm: 0.31576932079838377, ic: 0.3187208812175083
train 21, step: 2000, loss: 0.9401070055111433, grad_norm: 0.11491757203402841, ic: 0.07598491382996518
Epoch 21: 2022-05-07 08:15:30.050141: train loss: 1.620320885352131
Eval step 0: eval loss: 0.8298419594639094
Eval: 2022-05-07 08:16:02.372248: total loss: 1.0690226134779006, mse:4.596904718050072, ic :0.18117885122774594, sharpe5:16.500135980844497, irr5:564.4984741210938, ndcg5:0.8525513758201259, pnl5:3.988050937652588 
train 22, step: 0, loss: 1.0360224664548023, grad_norm: 0.03775431254339203, ic: 0.2335036865834432
train 22, step: 500, loss: 3.2598541507876018, grad_norm: 1.3114395806877845, ic: -0.21796141425025936
train 22, step: 1000, loss: 1.1985930161669076, grad_norm: 0.024241148386004753, ic: 0.4557504084623784
train 22, step: 1500, loss: 0.9739020704732511, grad_norm: 0.1041902075768231, ic: 0.1021620357654531
train 22, step: 2000, loss: 1.7086290977979735, grad_norm: 1.9858772131967835, ic: 0.19783029595343576
Epoch 22: 2022-05-07 08:23:24.936547: train loss: 1.6198736506097098
Eval step 0: eval loss: 0.8245561477130532
Eval: 2022-05-07 08:23:51.159739: total loss: 1.0672036259590825, mse:4.600838241429494, ic :0.18388676079651584, sharpe5:17.440420192480087, irr5:592.7021484375, ndcg5:0.8562555514652425, pnl5:6.938337326049805 
train 23, step: 0, loss: 0.9725569051693084, grad_norm: 0.04314507794555017, ic: 0.20515718964806645
train 23, step: 500, loss: 1.4230615924254317, grad_norm: 0.20207202778890143, ic: 0.032148917768506896
train 23, step: 1000, loss: 1.6472802734375, grad_norm: 0.08966069056896925, ic: 0.2564628746121866
train 23, step: 1500, loss: 1.1410428308470837, grad_norm: 1.4448341946512215, ic: 0.0545394461827914
train 23, step: 2000, loss: 1.8887372798787527, grad_norm: 0.94962029516289, ic: 0.4404924838771816
Epoch 23: 2022-05-07 08:31:31.129180: train loss: 1.6180336747496322
Eval step 0: eval loss: 0.8325841037440727
Eval: 2022-05-07 08:32:03.122811: total loss: 1.0673020234926793, mse:4.589979397185258, ic :0.18563739741564936, sharpe5:16.902388998270034, irr5:578.3389892578125, ndcg5:0.8369150495315886, pnl5:5.390995979309082 
train 24, step: 0, loss: 2.204559646446308, grad_norm: 0.19538682666576102, ic: 0.13650813044654725
train 24, step: 500, loss: 1.222903620500973, grad_norm: 0.16493863237013423, ic: 0.10736276496123215
train 24, step: 1000, loss: 0.908341103192522, grad_norm: 0.05347502950443169, ic: 0.5188214406186148
train 24, step: 1500, loss: 2.610271457002267, grad_norm: 2.1560087743489564, ic: 0.0016569469871866964
train 24, step: 2000, loss: 0.9318508406237265, grad_norm: 0.09415568094538768, ic: 0.12724492062913254
Epoch 24: 2022-05-07 08:39:43.037763: train loss: 1.6136559439467137
Eval step 0: eval loss: 0.8227342257886591
Eval: 2022-05-07 08:40:15.061224: total loss: 1.0673079854298537, mse:4.608757002168979, ic :0.1905515636543592, sharpe5:18.20563247323036, irr5:603.6744995117188, ndcg5:0.8398531909521129, pnl5:6.752045631408691 
train 25, step: 0, loss: 0.8373923636771538, grad_norm: 0.07757102004834066, ic: 0.6138212870391941
train 25, step: 500, loss: 0.8688360024074774, grad_norm: 0.043612595192472944, ic: 0.21804401752977864
train 25, step: 1000, loss: 2.087686461705065, grad_norm: 0.11379077068089123, ic: 0.25949874725190053
train 25, step: 1500, loss: 1.1329888329615614, grad_norm: 0.4310356542279753, ic: 0.5362089276481651
train 25, step: 2000, loss: 1.0232289385084143, grad_norm: 0.7131554394670585, ic: 0.5940994480251498
Epoch 25: 2022-05-07 08:48:08.812466: train loss: 1.6163813001616194
Eval step 0: eval loss: 0.8252041236869402
Eval: 2022-05-07 08:48:40.896118: total loss: 1.0656398054687497, mse:4.585642407653316, ic :0.19081648311710314, sharpe5:18.81609698295593, irr5:611.3477783203125, ndcg5:0.8412903097967288, pnl5:8.757405281066895 
train 26, step: 0, loss: 6.685634235223642, grad_norm: 0.5065433531103757, ic: 0.12160529019218749
train 26, step: 500, loss: 3.9477527609460514, grad_norm: 2.4639102973848646, ic: 0.364636818543308
train 26, step: 1000, loss: 1.2484598444245487, grad_norm: 1.5590378615329832, ic: -0.02540044042615898
train 26, step: 1500, loss: 0.8353579528764461, grad_norm: 0.23533170401291403, ic: 0.3029201461373886
train 26, step: 2000, loss: 0.9514299418211546, grad_norm: 0.32043483959102603, ic: 0.1318788105553235
Epoch 26: 2022-05-07 08:56:23.860099: train loss: 1.6190466165802024
Eval step 0: eval loss: 0.828108985506948
Eval: 2022-05-07 08:56:55.898058: total loss: 1.0664772552108912, mse:4.585189140452271, ic :0.18899791751261855, sharpe5:17.77679894328117, irr5:591.0419921875, ndcg5:0.846820156539046, pnl5:9.459827423095703 
train 27, step: 0, loss: 0.8261113664215686, grad_norm: 0.027919981707406536, ic: 0.13219631590505762
train 27, step: 500, loss: 0.8896688238199374, grad_norm: 1.9663922569196195, ic: 0.31816641913428423
train 27, step: 1000, loss: 0.7501531779443398, grad_norm: 0.4044624887918804, ic: 0.18840126257302503
train 27, step: 1500, loss: 0.6424034542046382, grad_norm: 0.16034078568989124, ic: 0.5185461909457151
train 27, step: 2000, loss: 1.388144479344628, grad_norm: 0.12862937101111432, ic: 0.03053372562930377
Epoch 27: 2022-05-07 09:04:40.920714: train loss: 1.6165323784615349
Eval step 0: eval loss: 0.830176398676238
Eval: 2022-05-07 09:05:13.491467: total loss: 1.0670716020869349, mse:4.596249176244563, ic :0.1840458354938514, sharpe5:16.92865546345711, irr5:566.0095825195312, ndcg5:0.8521606453878909, pnl5:5.706267833709717 
train 28, step: 0, loss: 1.5549248537041507, grad_norm: 1.204926275766969, ic: 0.2349955818386645
train 28, step: 500, loss: 1.388446703378284, grad_norm: 2.12978057071176, ic: 0.18591490170823316
train 28, step: 1000, loss: 0.9119360285399729, grad_norm: 0.3364500950400403, ic: 0.5796654744527283
train 28, step: 1500, loss: 1.0342966359812062, grad_norm: 0.034145257891426986, ic: 0.023459191042256505
train 28, step: 2000, loss: 1.0429691244488113, grad_norm: 0.32289037348307836, ic: 0.1600789985712631
Epoch 28: 2022-05-07 09:13:01.864924: train loss: 1.6139303156861922
Eval step 0: eval loss: 0.8237861657418993
Eval: 2022-05-07 09:13:33.300402: total loss: 1.0676460931737, mse:4.613688571822913, ic :0.1872529811162761, sharpe5:17.52810183048248, irr5:581.7926025390625, ndcg5:0.8448460834202433, pnl5:10.910552978515625 
train 29, step: 0, loss: 0.9093360929800037, grad_norm: 0.08138015037265942, ic: 0.1281934076313287
train 29, step: 500, loss: 1.112329201913902, grad_norm: 0.2394024708985073, ic: 0.6087875893589392
train 29, step: 1000, loss: 1.0834471906777627, grad_norm: 0.9588899768825563, ic: 0.1138127141558061
train 29, step: 1500, loss: 2.386661574209602, grad_norm: 0.686890409201907, ic: -0.029701386520385887
train 29, step: 2000, loss: 4.3974993670428235, grad_norm: 8.63443784681251, ic: 0.22189192659418347
Epoch 29: 2022-05-07 09:21:19.431469: train loss: 1.6151250673187059
Eval step 0: eval loss: 0.8332082830816978
Eval: 2022-05-07 09:21:50.933037: total loss: 1.067935397270275, mse:4.590251865037055, ic :0.18470687661802784, sharpe5:16.44360367655754, irr5:555.52783203125, ndcg5:0.8508290376110516, pnl5:4.980863094329834 
train 30, step: 0, loss: 1.0059248163351382, grad_norm: 0.17718943647140806, ic: 0.5246559305768133
train 30, step: 500, loss: 1.4365131231093622, grad_norm: 1.5621601572068995, ic: 0.01704198290841251
train 30, step: 1000, loss: 0.9893042362097538, grad_norm: 0.23072420365039198, ic: -0.04187504911845702
train 30, step: 1500, loss: 1.4805214488065748, grad_norm: 2.7400915859822312, ic: 0.13917820869285363
train 30, step: 2000, loss: 1.8419577052051452, grad_norm: 0.6710998104920859, ic: 0.1215732952385975
Epoch 30: 2022-05-07 09:29:40.479280: train loss: 1.6144240062910238
Eval step 0: eval loss: 0.8299055029142518
Eval: 2022-05-07 09:30:12.551337: total loss: 1.0720033659957695, mse:4.668665836610736, ic :0.18822558680639098, sharpe5:17.737889235019683, irr5:592.9462890625, ndcg5:0.8548454958802204, pnl5:7.0212554931640625 
train 31, step: 0, loss: 1.0464404927557782, grad_norm: 1.7105221644098254, ic: 0.36102877817076096
train 31, step: 500, loss: 1.477930290316358, grad_norm: 2.3178700678037343, ic: -0.011138436355955383
train 31, step: 1000, loss: 4.493042087480484, grad_norm: 5.240517112712017, ic: 0.4631148306462882
train 31, step: 1500, loss: 0.770114944397054, grad_norm: 0.057707747604752904, ic: 0.7081499702777565
train 31, step: 2000, loss: 1.2224928093536285, grad_norm: 4.1926885278088495, ic: 0.21049501766565978
Epoch 31: 2022-05-07 09:38:00.796608: train loss: 1.613254614751054
Eval step 0: eval loss: 0.8321168535753095
Eval: 2022-05-07 09:38:32.721069: total loss: 1.0672675049310523, mse:4.604263804189953, ic :0.18034901839256842, sharpe5:16.7271127474308, irr5:557.2606811523438, ndcg5:0.8507768105541897, pnl5:5.4301347732543945 
train 32, step: 0, loss: 1.1321282212100545, grad_norm: 0.06649223331994455, ic: 0.16931251834879948
train 32, step: 500, loss: 1.4992382620263287, grad_norm: 1.5977962048187335, ic: 0.09628849289876562
train 32, step: 1000, loss: 1.049928417606613, grad_norm: 0.2361993830310703, ic: 0.5060873139369286
train 32, step: 1500, loss: 0.9572569543764302, grad_norm: 2.3189446214612426, ic: 0.08466788497904776
train 32, step: 2000, loss: 0.9458141103340917, grad_norm: 0.039507254845889385, ic: 0.5483543489489449
Epoch 32: 2022-05-07 09:46:16.805941: train loss: 1.6129090428691604
Eval step 0: eval loss: 0.8238073254536025
Eval: 2022-05-07 09:46:48.496833: total loss: 1.064956340529364, mse:4.606177562535559, ic :0.1897372511403713, sharpe5:17.842077276706696, irr5:590.6240234375, ndcg5:0.8505635607999565, pnl5:6.938747882843018 
train 33, step: 0, loss: 1.2741929473767097, grad_norm: 0.7639966784189318, ic: 0.2272720383603197
train 33, step: 500, loss: 0.985729710030319, grad_norm: 0.14566806341886593, ic: 0.18701807482015453
train 33, step: 1000, loss: 1.0594008140114215, grad_norm: 3.770950068050092, ic: 0.22208061333519635
train 33, step: 1500, loss: 0.9104471625240955, grad_norm: 0.3436584604714292, ic: 0.5503680907962288
train 33, step: 2000, loss: 0.801389763736876, grad_norm: 0.048691514633893304, ic: 0.26994160353230634
Epoch 33: 2022-05-07 09:54:39.493543: train loss: 1.6133859542707563
Eval step 0: eval loss: 0.8266455566920772
Eval: 2022-05-07 09:55:12.003795: total loss: 1.066004830382003, mse:4.580856465014852, ic :0.1937027378567299, sharpe5:17.384705168008804, irr5:592.1808471679688, ndcg5:0.8449739710708343, pnl5:4.901142597198486 
train 34, step: 0, loss: 1.0053660957288775, grad_norm: 0.40815559388657396, ic: 0.5944033112101632
train 34, step: 500, loss: 0.7872913605576262, grad_norm: 0.4980012409473037, ic: 0.30151629180866074
train 34, step: 1000, loss: 3.1800475230414746, grad_norm: 1.3862171769873495, ic: 0.3261353820749453
train 34, step: 1500, loss: 0.8273728635512887, grad_norm: 0.3817485205853489, ic: 0.6822917465646525
train 34, step: 2000, loss: 6.35002012127227, grad_norm: 14.857225565900624, ic: 0.43889044391926874
Epoch 34: 2022-05-07 10:03:11.955117: train loss: 1.6128431558331158
Eval step 0: eval loss: 0.824090248164186
Eval: 2022-05-07 10:03:44.486666: total loss: 1.065524270380249, mse:4.595245725133006, ic :0.1941790790489665, sharpe5:18.69017346024513, irr5:612.1790161132812, ndcg5:0.8551244190870257, pnl5:5.649067401885986 
train 35, step: 0, loss: 1.169270881204044, grad_norm: 1.018196477106724, ic: 0.5594609464916679
train 35, step: 500, loss: 1.1735374718928864, grad_norm: 1.3804653097832456, ic: 0.13514117541866583
train 35, step: 1000, loss: 1.822210680400743, grad_norm: 7.866942963930333, ic: 0.04625677945336021
train 35, step: 1500, loss: 1.5816989984727445, grad_norm: 1.4813862750995765, ic: 0.023879180105802002
train 35, step: 2000, loss: 0.7882723884786513, grad_norm: 0.10035457193268649, ic: 0.5595136733714992
Epoch 35: 2022-05-07 10:11:39.131783: train loss: 1.6109198400008111
Eval step 0: eval loss: 0.8309081130836735
Eval: 2022-05-07 10:12:10.988842: total loss: 1.06582794851161, mse:4.5872693862369855, ic :0.19177657666988038, sharpe5:17.133962963819503, irr5:583.2643432617188, ndcg5:0.8525736522859241, pnl5:6.108782768249512 
train 36, step: 0, loss: 1.8632969639177788, grad_norm: 4.237784328752021, ic: 0.08284189408658853
train 36, step: 500, loss: 0.828636175445725, grad_norm: 0.06562202283664675, ic: 0.21861999780574176
train 36, step: 1000, loss: 1.669379971590909, grad_norm: 4.410526402084584, ic: 0.2704864603245898
train 36, step: 1500, loss: 0.7621032833820662, grad_norm: 0.0671721184364241, ic: 0.3924813242694455
train 36, step: 2000, loss: 1.1157418539639594, grad_norm: 1.8234728777501705, ic: 0.7780300062103258
Epoch 36: 2022-05-07 10:19:51.445443: train loss: 1.6090431160681942
Eval step 0: eval loss: 0.8296024495356954
Eval: 2022-05-07 10:20:23.840047: total loss: 1.0658193623846905, mse:4.588113450632166, ic :0.1909372182220536, sharpe5:17.64791467308998, irr5:593.1776733398438, ndcg5:0.8510270028176973, pnl5:5.500713348388672 
train 37, step: 0, loss: 2.022564127105666, grad_norm: 2.9201243487988595, ic: 0.18757574560417958
train 37, step: 500, loss: 2.3441154143938623, grad_norm: 3.6834112569911337, ic: -0.03601300797568229
train 37, step: 1000, loss: 1.075464721865487, grad_norm: 0.4872107475346819, ic: 0.008873961861113819
train 37, step: 1500, loss: 2.023520668784341, grad_norm: 2.132957176825914, ic: 0.600083014587444
train 37, step: 2000, loss: 1.3088103132786546, grad_norm: 0.3558459797982131, ic: 0.20587438997095775
Epoch 37: 2022-05-07 10:28:14.429720: train loss: 1.6064734612229499
Eval step 0: eval loss: 0.8246454172566516
Eval: 2022-05-07 10:28:46.491764: total loss: 1.0691951985459327, mse:4.618026551457253, ic :0.1866714368139867, sharpe5:17.82884566783905, irr5:600.24755859375, ndcg5:0.8548170404302295, pnl5:4.480992317199707 
train 38, step: 0, loss: 1.3480453491210938, grad_norm: 0.46104040642811833, ic: -0.08261901459106252
train 38, step: 500, loss: 0.9063494646990741, grad_norm: 0.1384071900572289, ic: 0.2668524023096539
train 38, step: 1000, loss: 0.9045465160264328, grad_norm: 0.3987786852584945, ic: 0.12008475310418351
train 38, step: 1500, loss: 0.9567993442126994, grad_norm: 0.38729646675831275, ic: 0.19483650913003625
train 38, step: 2000, loss: 2.3083241626935425, grad_norm: 7.5877667837222145, ic: -0.051714372004909054
Epoch 38: 2022-05-07 10:36:29.679249: train loss: 1.6048884462198365
Eval step 0: eval loss: 0.8273334724257442
Eval: 2022-05-07 10:37:01.343851: total loss: 1.0644392492007555, mse:4.584348003464715, ic :0.19629752660390903, sharpe5:18.8470003759861, irr5:613.296142578125, ndcg5:0.8505412226948161, pnl5:4.366948127746582 
train 39, step: 0, loss: 0.9676638728617164, grad_norm: 0.008134898143057023, ic: 0.07591873712979495
train 39, step: 500, loss: 0.8909772305558225, grad_norm: 0.1334915822280715, ic: 0.20464499315981668
train 39, step: 1000, loss: 0.9396407395396992, grad_norm: 0.28428277202336144, ic: 0.22864174726288317
train 39, step: 1500, loss: 2.1066606282810025, grad_norm: 0.6143380181091584, ic: 0.22945003557667581
train 39, step: 2000, loss: 0.6118432597903091, grad_norm: 0.05282873474095187, ic: 0.05533980537432148
Epoch 39: 2022-05-07 10:44:54.647223: train loss: 1.6101143940538485
Eval step 0: eval loss: 0.8285626651615187
Eval: 2022-05-07 10:45:25.934993: total loss: 1.0662652421829057, mse:4.605500450730949, ic :0.19167710481901046, sharpe5:17.269901947975157, irr5:591.728271484375, ndcg5:0.8605309790343855, pnl5:4.352445125579834 
