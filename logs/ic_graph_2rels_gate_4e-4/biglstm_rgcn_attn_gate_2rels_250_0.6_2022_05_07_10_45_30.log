Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
4672
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795784288873143, grad_norm: 4.816807742217264, ic: 0.022723851365593503
train 0, step: 500, loss: 0.8631325946684251, grad_norm: 0.02655641331889846, ic: 0.045017281820121685
train 0, step: 1000, loss: 1.9500690451833396, grad_norm: 0.5127044214996189, ic: 0.015259223310297611
train 0, step: 1500, loss: 0.956348428236166, grad_norm: 0.048943194801787215, ic: 0.015586481034837167
train 0, step: 2000, loss: 1.00223496768696, grad_norm: 0.15821312651250358, ic: 0.025514352727951
Epoch 0: 2022-05-07 10:55:09.510886: train loss: 1.648490775139149
Eval step 0: eval loss: 0.8363082130038197
Eval: 2022-05-07 10:55:40.750843: total loss: 1.079345017100057, mse:4.822858655167973, ic :0.008115577261632577, sharpe5:7.948754705786705, irr5:224.34423828125, ndcg5:0.8561967645972611, pnl5:2.6805551052093506 
train 1, step: 0, loss: 2.7747684601814515, grad_norm: 0.8786158708074759, ic: 0.05693144267333372
train 1, step: 500, loss: 1.7554737280719006, grad_norm: 0.7706139174804727, ic: 0.0970319011337056
train 1, step: 1000, loss: 0.8777621583635679, grad_norm: 0.1754064002306826, ic: 0.07978703069677227
train 1, step: 1500, loss: 1.7132736732219827, grad_norm: 0.20670107817448655, ic: -0.03187902091881806
train 1, step: 2000, loss: 2.177287890625, grad_norm: 0.8877323096395215, ic: -0.045576643510469174
Epoch 1: 2022-05-07 11:03:41.189855: train loss: 1.646735774347132
Eval step 0: eval loss: 0.8345841145490318
Eval: 2022-05-07 11:04:12.735264: total loss: 1.0789890762872132, mse:4.8235399102899335, ic :0.0076124823374506215, sharpe5:7.692875652015209, irr5:215.70553588867188, ndcg5:0.8406719653717635, pnl5:2.7930374145507812 
train 2, step: 0, loss: 2.141781427556818, grad_norm: 0.009298064905604787, ic: 0.1312739149412046
train 2, step: 500, loss: 3.300011385587833, grad_norm: 0.2848145314401993, ic: 0.048717977741724885
train 2, step: 1000, loss: 2.0723206267959773, grad_norm: 0.00020643144969963498, ic: 0.18987442290954804
train 2, step: 1500, loss: 1.4855378362058682, grad_norm: 0.059738520307497454, ic: -0.038277497583263306
train 2, step: 2000, loss: 3.2352527794471153, grad_norm: 0.7878513787290613, ic: 0.2074984794012808
Epoch 2: 2022-05-07 11:12:12.080622: train loss: 1.6464907581250543
Eval step 0: eval loss: 0.835836139192571
Eval: 2022-05-07 11:12:43.947298: total loss: 1.079498434615493, mse:4.8233169412314805, ic :0.00993108187205893, sharpe5:7.68193838596344, irr5:216.85601806640625, ndcg5:0.8698072839537849, pnl5:3.002235174179077 
train 3, step: 0, loss: 1.5228930465574186, grad_norm: 0.5250404215891062, ic: -0.001546696949800256
train 3, step: 500, loss: 1.501439512009348, grad_norm: 0.34258914546564245, ic: 0.09619083778721416
train 3, step: 1000, loss: 3.6792219883419692, grad_norm: 0.7058020769555956, ic: -0.04950754505189113
train 3, step: 1500, loss: 1.978695633758695, grad_norm: 1.24477547793926, ic: -0.06603688236988087
train 3, step: 2000, loss: 0.8988218565244933, grad_norm: 0.001357787418855339, ic: 0.01173492214526776
Epoch 3: 2022-05-07 11:20:37.649363: train loss: 1.6458497795732487
Eval step 0: eval loss: 0.834842082949157
Eval: 2022-05-07 11:21:09.393936: total loss: 1.0793579138693734, mse:4.825508858248907, ic :0.01332344081913934, sharpe5:7.546909382343292, irr5:214.04818725585938, ndcg5:0.850229095584165, pnl5:2.5926694869995117 
train 4, step: 0, loss: 1.4310245934311225, grad_norm: 0.044533645466634034, ic: 0.13456364063066778
train 4, step: 500, loss: 1.648656265378937, grad_norm: 0.5593348973658527, ic: 0.04316144726360088
train 4, step: 1000, loss: 2.9626036853325077, grad_norm: 0.799340000967145, ic: 0.06197495563746891
train 4, step: 1500, loss: 2.1501862473628695, grad_norm: 0.4792396413403609, ic: 0.013025332441433637
train 4, step: 2000, loss: 1.0821846447317263, grad_norm: 0.393270807475831, ic: 0.24004141493060865
Epoch 4: 2022-05-07 11:29:10.552133: train loss: 1.6452392696885754
Eval step 0: eval loss: 0.8525996474496179
Eval: 2022-05-07 11:29:41.693363: total loss: 1.0855203006345227, mse:4.8263120912551045, ic :0.04777053519146197, sharpe5:10.504048156142234, irr5:313.2135314941406, ndcg5:0.8530014936683475, pnl5:3.11576509475708 
train 5, step: 0, loss: 1.362716203727978, grad_norm: 0.18298987666210162, ic: 0.0182987373155596
train 5, step: 500, loss: 0.8919126917500991, grad_norm: 0.011884732337285292, ic: 0.03575210833106526
train 5, step: 1000, loss: 0.9837907462284483, grad_norm: 0.15836383258732795, ic: 0.02047377452814674
train 5, step: 1500, loss: 1.5288818733250384, grad_norm: 0.15282338399458933, ic: 0.032629632618445335
train 5, step: 2000, loss: 1.1063179317040674, grad_norm: 0.0280505448671214, ic: 0.13318755998976117
Epoch 5: 2022-05-07 11:37:50.959477: train loss: 1.6456482840506828
Eval step 0: eval loss: 0.8396345968618282
Eval: 2022-05-07 11:38:22.889132: total loss: 1.080275237100904, mse:4.794097418371674, ic :0.08325599197872487, sharpe5:11.10711268901825, irr5:362.4681091308594, ndcg5:0.8532312174209089, pnl5:3.096632242202759 
train 6, step: 0, loss: 1.3387895063920454, grad_norm: 0.43666595339286357, ic: 0.11368181744934237
train 6, step: 500, loss: 1.005113960955446, grad_norm: 0.04222183187470536, ic: 0.058190095562081996
train 6, step: 1000, loss: 1.1125752844284695, grad_norm: 0.08068949898072655, ic: 0.7309409263500621
train 6, step: 1500, loss: 1.5665620157541322, grad_norm: 0.7111598006485869, ic: 0.1079586018069636
train 6, step: 2000, loss: 0.80225606266263, grad_norm: 0.0474592960906612, ic: 0.35619340851641723
Epoch 6: 2022-05-07 11:45:55.357133: train loss: 1.6395131963252612
Eval step 0: eval loss: 0.8302890789646996
Eval: 2022-05-07 11:46:23.302083: total loss: 1.0737837716771128, mse:4.720895253669462, ic :0.1416598427910439, sharpe5:11.634140875339508, irr5:384.692138671875, ndcg5:0.8501295860828469, pnl5:2.8155925273895264 
train 7, step: 0, loss: 0.992409896850586, grad_norm: 0.04972464399776351, ic: 0.08163746930419129
train 7, step: 500, loss: 0.6513539694000522, grad_norm: 0.019836758210453255, ic: 0.020269318657895767
train 7, step: 1000, loss: 1.0327313925052362, grad_norm: 0.2142803044654355, ic: 0.07953687969815974
train 7, step: 1500, loss: 2.245180250619641, grad_norm: 0.7308226631992925, ic: 0.4413615590290604
train 7, step: 2000, loss: 0.9168467483150747, grad_norm: 0.045608863199913634, ic: -0.038632179950186044
Epoch 7: 2022-05-07 11:54:12.576337: train loss: 1.6346984440131174
Eval step 0: eval loss: 0.8339543397861235
Eval: 2022-05-07 11:54:44.995433: total loss: 1.0749912973321685, mse:4.714413586466061, ic :0.1400440253548463, sharpe5:11.284792537093162, irr5:383.8711242675781, ndcg5:0.8525175807412654, pnl5:3.0768885612487793 
train 8, step: 0, loss: 3.6068323992300724, grad_norm: 1.1085971034164455, ic: -0.018149262071099143
train 8, step: 500, loss: 2.7692555714523177, grad_norm: 0.8643179620107058, ic: 0.037975150759098804
train 8, step: 1000, loss: 3.048171775928442, grad_norm: 0.8772056258793517, ic: 0.11681268771094999
train 8, step: 1500, loss: 0.7153809496859587, grad_norm: 0.08095730226707766, ic: 0.4622287175849479
train 8, step: 2000, loss: 1.0810334842199132, grad_norm: 0.3633344338212364, ic: 0.5393246942157269
Epoch 8: 2022-05-07 12:02:20.149847: train loss: 1.6296570172037825
Eval step 0: eval loss: 0.8264120923957784
Eval: 2022-05-07 12:02:51.775504: total loss: 1.0735666986100314, mse:4.717923899595132, ic :0.15085589281639813, sharpe5:16.27082466959953, irr5:521.1603393554688, ndcg5:0.8340741061305553, pnl5:5.252556800842285 
train 9, step: 0, loss: 5.447101307067385, grad_norm: 1.2943308845993506, ic: 0.013511871870608615
train 9, step: 500, loss: 1.3465495236671225, grad_norm: 1.316049374452115, ic: 0.3236366634794819
train 9, step: 1000, loss: 0.9296397943606322, grad_norm: 0.0687169203812399, ic: 0.06912526598784623
train 9, step: 1500, loss: 1.0856832917203607, grad_norm: 0.035454938357236836, ic: 0.4323503988999024
train 9, step: 2000, loss: 1.0677717207572683, grad_norm: 0.23029823362996116, ic: 0.2715196477833415
Epoch 9: 2022-05-07 12:10:33.364935: train loss: 1.6282140936317302
Eval step 0: eval loss: 0.8239277878852739
Eval: 2022-05-07 12:11:03.791582: total loss: 1.0717972140079812, mse:4.701398937583827, ic :0.1582692879516073, sharpe5:16.794842920303346, irr5:527.0732421875, ndcg5:0.8506493376337699, pnl5:5.8298516273498535 
train 10, step: 0, loss: 7.062277924562682, grad_norm: 2.3510904682701064, ic: 0.23928959905282243
train 10, step: 500, loss: 1.140192963077659, grad_norm: 1.0438763697590536, ic: 0.02111605179464629
train 10, step: 1000, loss: 2.387472141008436, grad_norm: 0.7255016709594251, ic: 0.14439588811043785
train 10, step: 1500, loss: 1.1192799357624799, grad_norm: 0.33675467812627713, ic: 0.004856359767763589
train 10, step: 2000, loss: 2.7521343752968277, grad_norm: 3.760414284721484, ic: 0.4432875869125355
Epoch 10: 2022-05-07 12:18:40.301424: train loss: 1.6273483335207048
Eval step 0: eval loss: 0.8289553096598392
Eval: 2022-05-07 12:19:12.117708: total loss: 1.0720435071748209, mse:4.697147144224015, ic :0.1599880779844756, sharpe5:16.481049091815947, irr5:534.0868530273438, ndcg5:0.8522596569394881, pnl5:5.054105758666992 
train 11, step: 0, loss: 1.275110797926334, grad_norm: 0.20013957362651508, ic: 0.12735624393042577
train 11, step: 500, loss: 0.6631170268856262, grad_norm: 0.036905943675039174, ic: 0.5327989984389015
train 11, step: 1000, loss: 0.9431343550340067, grad_norm: 0.19021350252735095, ic: 0.05263627341828587
train 11, step: 1500, loss: 1.0603682534736498, grad_norm: 0.07536094989164388, ic: 0.17880516327654947
train 11, step: 2000, loss: 0.787507935534533, grad_norm: 0.002922602604231464, ic: 0.12921041692456364
Epoch 11: 2022-05-07 12:26:54.200643: train loss: 1.6296105980361215
Eval step 0: eval loss: 0.8349365620266398
Eval: 2022-05-07 12:27:25.723645: total loss: 1.0741261491684306, mse:4.703727109448588, ic :0.15377322918703312, sharpe5:14.788526916503905, irr5:493.4155578613281, ndcg5:0.8448633942816463, pnl5:4.103042125701904 
train 12, step: 0, loss: 0.970344066619873, grad_norm: 0.2437835332072483, ic: 0.3968767445470681
train 12, step: 500, loss: 0.9374732065161753, grad_norm: 0.08449565685796545, ic: 0.18597980017661198
train 12, step: 1000, loss: 2.9522382408190686, grad_norm: 0.3496568290178198, ic: 0.1844964659467239
train 12, step: 1500, loss: 0.9485579154369764, grad_norm: 0.14025894431522223, ic: -0.09910250761218892
train 12, step: 2000, loss: 0.8732899092475453, grad_norm: 0.003973852315568098, ic: 0.21978980420510946
Epoch 12: 2022-05-07 12:35:13.009614: train loss: 1.6261353879559493
Eval step 0: eval loss: 0.826265517979452
Eval: 2022-05-07 12:35:45.286016: total loss: 1.0692894250491347, mse:4.668434667968724, ic :0.17170140221235364, sharpe5:16.54491988778114, irr5:524.9903564453125, ndcg5:0.8449392814205497, pnl5:7.236983299255371 
train 13, step: 0, loss: 2.0597133938588965, grad_norm: 0.6964316306770957, ic: 0.39064129465384867
train 13, step: 500, loss: 0.829751882514069, grad_norm: 0.10819363822763448, ic: 0.5233087233785078
train 13, step: 1000, loss: 0.950687198510906, grad_norm: 0.4117264018426549, ic: 0.5025584454042771
train 13, step: 1500, loss: 2.390774419398907, grad_norm: 0.3963806197206072, ic: -0.02893511752819014
train 13, step: 2000, loss: 1.4710480041738836, grad_norm: 0.04303833963140933, ic: 0.16940878212112756
Epoch 13: 2022-05-07 12:43:31.943230: train loss: 1.625476908183371
Eval step 0: eval loss: 0.8234437514406612
Eval: 2022-05-07 12:44:04.316162: total loss: 1.0706971076571157, mse:4.672073653270121, ic :0.17217647318552112, sharpe5:16.634393109083174, irr5:533.0099487304688, ndcg5:0.8430668495451997, pnl5:7.261073589324951 
train 14, step: 0, loss: 4.554717589094189, grad_norm: 1.8561335511104533, ic: 0.16386740092799928
train 14, step: 500, loss: 0.8278284102040329, grad_norm: 0.0031277731741555857, ic: 0.18471949243641303
train 14, step: 1000, loss: 1.8236248042425969, grad_norm: 0.26405651410586467, ic: 0.3959920529043073
train 14, step: 1500, loss: 1.1236765815100078, grad_norm: 0.06326396855074401, ic: -0.04874842878954519
train 14, step: 2000, loss: 1.1587964122090704, grad_norm: 0.23182316441874115, ic: 0.08567831056166504
Epoch 14: 2022-05-07 12:51:41.514440: train loss: 1.6248642493443042
Eval step 0: eval loss: 0.8297259990985576
Eval: 2022-05-07 12:52:13.141843: total loss: 1.069807355072463, mse:4.637530058204612, ic :0.17898803986080827, sharpe5:16.775027668476103, irr5:534.4336547851562, ndcg5:0.8436616491195551, pnl5:6.425381660461426 
train 15, step: 0, loss: 3.433636308365759, grad_norm: 1.0461222824212841, ic: 0.05978065513471437
train 15, step: 500, loss: 1.2589377183668433, grad_norm: 0.01890047096095452, ic: -0.050584033688053094
train 15, step: 1000, loss: 1.3234700520833333, grad_norm: 0.14360058438000495, ic: 0.06684880844001032
train 15, step: 1500, loss: 0.8503031572957677, grad_norm: 0.22849968228035217, ic: 0.0683956145706195
train 15, step: 2000, loss: 1.462128127830616, grad_norm: 0.70892268465039, ic: 0.05200407403309606
Epoch 15: 2022-05-07 12:59:56.411707: train loss: 1.6238994250218188
Eval step 0: eval loss: 0.8396107359103332
Eval: 2022-05-07 13:00:28.139446: total loss: 1.0716431918654585, mse:4.592950652149376, ic :0.18702421681841477, sharpe5:17.518604736328125, irr5:579.9095458984375, ndcg5:0.8378478500506377, pnl5:4.829988956451416 
train 16, step: 0, loss: 0.7000030735043052, grad_norm: 0.37749641518919297, ic: -0.021275976318378145
train 16, step: 500, loss: 1.6131840590259396, grad_norm: 0.5656896990634518, ic: 0.1799574436496103
train 16, step: 1000, loss: 0.8801944617069128, grad_norm: 0.014343122572979709, ic: -0.10525366832280031
train 16, step: 1500, loss: 0.8445039693533295, grad_norm: 0.2323070039265035, ic: 0.1374254888111081
train 16, step: 2000, loss: 3.329780559713047, grad_norm: 1.071999519257472, ic: 0.0029873252466539203
Epoch 16: 2022-05-07 13:08:13.225080: train loss: 1.6214920249674951
Eval step 0: eval loss: 0.8266286417857612
Eval: 2022-05-07 13:08:45.461386: total loss: 1.0680234563843463, mse:4.595088557484398, ic :0.1853519529108249, sharpe5:17.572040079832075, irr5:564.2540893554688, ndcg5:0.8409815880129502, pnl5:7.901464462280273 
train 17, step: 0, loss: 1.2819491358587531, grad_norm: 0.25017179239116616, ic: -0.12416886339844027
train 17, step: 500, loss: 1.7490559895833333, grad_norm: 0.46896537209943007, ic: 0.16002198974159093
train 17, step: 1000, loss: 1.2834343207350214, grad_norm: 0.0855713182380858, ic: 0.15384347206729554
train 17, step: 1500, loss: 4.531374850753123, grad_norm: 1.0042100790059032, ic: 0.1965795542773204
train 17, step: 2000, loss: 1.2668697870052705, grad_norm: 0.6326937813310777, ic: 0.07818032719514093
Epoch 17: 2022-05-07 13:16:27.268966: train loss: 1.6206145267935876
Eval step 0: eval loss: 0.8315849022820073
Eval: 2022-05-07 13:16:59.307390: total loss: 1.067768424622676, mse:4.585311830142706, ic :0.1903941871000216, sharpe5:17.69878088235855, irr5:588.4674072265625, ndcg5:0.840696472577843, pnl5:7.1619391441345215 
train 18, step: 0, loss: 1.4092396181648308, grad_norm: 0.6830357046249644, ic: 0.26177060746369374
train 18, step: 500, loss: 1.516894767705811, grad_norm: 0.7265252831125077, ic: 0.03916325226714003
train 18, step: 1000, loss: 0.6553803911601027, grad_norm: 0.02345630956238773, ic: 0.5738939870109213
train 18, step: 1500, loss: 1.4243236044993859, grad_norm: 0.03947134525453461, ic: 0.2247807051003426
train 18, step: 2000, loss: 0.9108008123507166, grad_norm: 0.00989231545572547, ic: -0.024530115775402945
Epoch 18: 2022-05-07 13:24:49.354626: train loss: 1.6201793761191006
Eval step 0: eval loss: 0.8211797909188948
Eval: 2022-05-07 13:25:21.110110: total loss: 1.0661821234577056, mse:4.592063094579708, ic :0.1904050786515688, sharpe5:17.623623300790786, irr5:586.6900024414062, ndcg5:0.8429268081917327, pnl5:7.625025272369385 
train 19, step: 0, loss: 1.4706056625124009, grad_norm: 0.8798356578356958, ic: 0.07377342790739633
train 19, step: 500, loss: 0.859448892098886, grad_norm: 0.026955360863637232, ic: 0.231360070794466
train 19, step: 1000, loss: 0.9623022405324602, grad_norm: 0.009727107554053788, ic: 0.17393813633859417
train 19, step: 1500, loss: 3.952024908863317, grad_norm: 0.9804437802240356, ic: 0.15801503100195968
train 19, step: 2000, loss: 1.0173809344951923, grad_norm: 0.129373732609594, ic: 0.22808687528751714
Epoch 19: 2022-05-07 13:33:11.790078: train loss: 1.619768724314109
Eval step 0: eval loss: 0.8269368403829689
Eval: 2022-05-07 13:33:43.829585: total loss: 1.0666532464696077, mse:4.584945025417473, ic :0.1912488957021108, sharpe5:17.906614004373548, irr5:598.262939453125, ndcg5:0.854499219919428, pnl5:6.552277565002441 
train 20, step: 0, loss: 2.3163776865118577, grad_norm: 0.8707304844278827, ic: 0.047927885251581474
train 20, step: 500, loss: 3.2521725852272727, grad_norm: 0.6193525017206205, ic: 0.04856508031569873
train 20, step: 1000, loss: 0.9672475814819337, grad_norm: 0.15747171908471796, ic: 0.19735778651657548
train 20, step: 1500, loss: 1.7040510893233156, grad_norm: 1.1783099176251155, ic: 0.28713874648032395
train 20, step: 2000, loss: 1.0351962763178018, grad_norm: 0.043587099519480435, ic: 0.00018326580088872182
Epoch 20: 2022-05-07 13:41:30.726850: train loss: 1.6184496848436585
Eval step 0: eval loss: 0.8284059289383561
Eval: 2022-05-07 13:42:03.035637: total loss: 1.0670485777203533, mse:4.58389092962372, ic :0.192779345666942, sharpe5:17.445622783899307, irr5:592.7230834960938, ndcg5:0.8528908097379003, pnl5:7.35382604598999 
train 21, step: 0, loss: 1.020388442446129, grad_norm: 0.3301692692852457, ic: 0.07118203939263766
train 21, step: 500, loss: 0.7663350063087666, grad_norm: 0.02511055960912856, ic: 0.18353834245725828
train 21, step: 1000, loss: 0.9622887059261924, grad_norm: 0.7357808876193563, ic: 0.17001909903014234
train 21, step: 1500, loss: 0.9954776860038697, grad_norm: 0.20134608372555576, ic: 0.31748026635552407
train 21, step: 2000, loss: 0.9396022319265642, grad_norm: 0.06994608889656773, ic: 0.05796108709902012
Epoch 21: 2022-05-07 13:49:40.816037: train loss: 1.618633061696048
Eval step 0: eval loss: 0.8237871304703964
Eval: 2022-05-07 13:50:12.252946: total loss: 1.066573466261742, mse:4.596930333500063, ic :0.1852149025709953, sharpe5:17.04004767537117, irr5:561.9104614257812, ndcg5:0.8553347991378103, pnl5:5.359338283538818 
train 22, step: 0, loss: 1.0312844831391244, grad_norm: 0.0403654898121477, ic: 0.2509488011942418
train 22, step: 500, loss: 3.2436910489710367, grad_norm: 0.7351731041410332, ic: -0.23779548481982157
train 22, step: 1000, loss: 1.1910609250812862, grad_norm: 0.06427010157081316, ic: 0.46334412299249
train 22, step: 1500, loss: 0.9720531322337963, grad_norm: 0.1285080202414964, ic: 0.11535919510779892
train 22, step: 2000, loss: 1.7055024646577381, grad_norm: 1.0450986435281473, ic: 0.18858026580234144
Epoch 22: 2022-05-07 13:57:54.480004: train loss: 1.617628234671302
Eval step 0: eval loss: 0.8254425402561907
Eval: 2022-05-07 13:58:26.818260: total loss: 1.0675390779551979, mse:4.596118211605638, ic :0.18444995454420168, sharpe5:17.4181663441658, irr5:575.3704223632812, ndcg5:0.8521269631230174, pnl5:4.680960178375244 
train 23, step: 0, loss: 0.978942695199928, grad_norm: 0.039478117161110424, ic: 0.1920630072971149
train 23, step: 500, loss: 1.4204185881083202, grad_norm: 0.11873734308569905, ic: 0.056315726173173375
train 23, step: 1000, loss: 1.6426609293619794, grad_norm: 0.08232191682671627, ic: 0.25975606072063967
train 23, step: 1500, loss: 1.1121392414740023, grad_norm: 0.7948653901603784, ic: 0.12418107212328991
train 23, step: 2000, loss: 1.9040835573638375, grad_norm: 0.970456354124606, ic: 0.4375381501004699
Epoch 23: 2022-05-07 14:06:15.001918: train loss: 1.617617685476788
Eval step 0: eval loss: 0.8318928436182823
Eval: 2022-05-07 14:06:47.331209: total loss: 1.0673391214720367, mse:4.585425577178906, ic :0.18613421231980837, sharpe5:17.433844116926192, irr5:570.146484375, ndcg5:0.8650135146276468, pnl5:4.749053955078125 
train 24, step: 0, loss: 2.192747376311844, grad_norm: 0.09234518773442839, ic: 0.15912824348168936
train 24, step: 500, loss: 1.2272364041220818, grad_norm: 0.11749761444650637, ic: 0.09857626029470916
train 24, step: 1000, loss: 0.9155762213295493, grad_norm: 0.030870860940613504, ic: 0.5132175366754774
train 24, step: 1500, loss: 2.6066697286557092, grad_norm: 1.5853345775514547, ic: 0.022268413118246302
train 24, step: 2000, loss: 0.9353280040240424, grad_norm: 0.05584165120296125, ic: 0.08211211911467817
Epoch 24: 2022-05-07 14:14:24.877289: train loss: 1.6136830659826975
Eval step 0: eval loss: 0.8201001311001712
Eval: 2022-05-07 14:14:56.834179: total loss: 1.066172366277967, mse:4.6109560112019405, ic :0.18720198894791268, sharpe5:17.333645989894865, irr5:587.5811157226562, ndcg5:0.8546093024266259, pnl5:5.167866230010986 
train 25, step: 0, loss: 0.8393321269267314, grad_norm: 0.07065247587422692, ic: 0.6175086303996403
train 25, step: 500, loss: 0.8677888161466201, grad_norm: 0.006565309046753255, ic: 0.21195159813599068
train 25, step: 1000, loss: 2.1048491943545318, grad_norm: 0.09366532012455467, ic: 0.2499958795545862
train 25, step: 1500, loss: 1.1248738001099459, grad_norm: 0.4239344213721567, ic: 0.5382926489432539
train 25, step: 2000, loss: 1.0214956417255812, grad_norm: 0.4150711095191428, ic: 0.5995859713300913
Epoch 25: 2022-05-07 14:22:34.343269: train loss: 1.615443481632214
Eval step 0: eval loss: 0.8221263182050184
Eval: 2022-05-07 14:23:05.829067: total loss: 1.065022605258007, mse:4.582826217913081, ic :0.1944516465104566, sharpe5:17.711671957969664, irr5:607.3853149414062, ndcg5:0.8540255117536295, pnl5:5.603054523468018 
train 26, step: 0, loss: 6.6874266798123, grad_norm: 0.5973120856249887, ic: 0.12084242672888039
train 26, step: 500, loss: 3.902801108043393, grad_norm: 1.7681309764013724, ic: 0.37469585405270633
train 26, step: 1000, loss: 1.2651896094976454, grad_norm: 1.1008219887500994, ic: 0.0003027778186239713
train 26, step: 1500, loss: 0.8408214599432845, grad_norm: 0.2009502663356557, ic: 0.3097880490748579
train 26, step: 2000, loss: 0.9591827992845201, grad_norm: 0.2724742140070275, ic: 0.13582867775870255
Epoch 26: 2022-05-07 14:30:52.069397: train loss: 1.6140594188572113
Eval step 0: eval loss: 0.8244531147095626
Eval: 2022-05-07 14:31:22.812422: total loss: 1.0654717758310526, mse:4.591104912250326, ic :0.1890350019020795, sharpe5:17.9378617143631, irr5:585.1038818359375, ndcg5:0.8460101690548462, pnl5:8.392664909362793 
train 27, step: 0, loss: 0.8299793198529412, grad_norm: 0.013989410119522795, ic: 0.13465979927774924
train 27, step: 500, loss: 0.9222565625792343, grad_norm: 1.1751249774847947, ic: 0.2662943431329367
train 27, step: 1000, loss: 0.7506806859652939, grad_norm: 0.3987141352153998, ic: 0.19069560294527957
train 27, step: 1500, loss: 0.6406880940135338, grad_norm: 0.15107665167264203, ic: 0.523867815027356
train 27, step: 2000, loss: 1.3852546477553151, grad_norm: 0.06098207342218491, ic: 0.011781370450990269
Epoch 27: 2022-05-07 14:39:11.719836: train loss: 1.6145471316966415
Eval step 0: eval loss: 0.8275967146749867
Eval: 2022-05-07 14:39:43.752168: total loss: 1.0657203030974143, mse:4.592476434708036, ic :0.18820937372701244, sharpe5:17.101897974014282, irr5:585.4176025390625, ndcg5:0.8499472268732641, pnl5:6.957659721374512 
train 28, step: 0, loss: 1.5477520209942084, grad_norm: 0.8450379356359956, ic: 0.19774614933974655
train 28, step: 500, loss: 1.4004250148476802, grad_norm: 3.676514240947696, ic: 0.18918138864368275
train 28, step: 1000, loss: 0.8957545996358401, grad_norm: 0.19363039400479287, ic: 0.577266346823775
train 28, step: 1500, loss: 1.03220588547737, grad_norm: 0.0242040487763889, ic: 0.05567115653688087
train 28, step: 2000, loss: 1.0389792787516776, grad_norm: 0.24957359288227615, ic: 0.1559222852551305
Epoch 28: 2022-05-07 14:47:36.413870: train loss: 1.6120469906975252
Eval step 0: eval loss: 0.8205390182511195
Eval: 2022-05-07 14:48:08.343952: total loss: 1.0746141876458613, mse:4.668932367539634, ic :0.1789492778868942, sharpe5:17.218492777347564, irr5:582.2698974609375, ndcg5:0.8426628083154956, pnl5:10.143808364868164 
train 29, step: 0, loss: 0.8996065615523123, grad_norm: 0.06103694031656348, ic: 0.12815705211789089
train 29, step: 500, loss: 1.0935558917168233, grad_norm: 0.08874882606576709, ic: 0.6184121878501443
train 29, step: 1000, loss: 1.0594025243728415, grad_norm: 0.7727960997901371, ic: 0.10050454272769895
train 29, step: 1500, loss: 2.373569842896175, grad_norm: 0.3822468196960044, ic: -0.020344602106423127
train 29, step: 2000, loss: 4.141824604552469, grad_norm: 7.173229010413202, ic: 0.22644354127374117
Epoch 29: 2022-05-07 14:55:49.092691: train loss: 1.6135462338653292
Eval step 0: eval loss: 0.8280209379527792
Eval: 2022-05-07 14:56:19.911421: total loss: 1.065689924722956, mse:4.585515254545274, ic :0.1909380325525829, sharpe5:17.24257604598999, irr5:583.2955322265625, ndcg5:0.85224463135294, pnl5:5.603187084197998 
train 30, step: 0, loss: 0.9982911688833962, grad_norm: 0.0516973361482385, ic: 0.5298767107368466
train 30, step: 500, loss: 1.4307395678450574, grad_norm: 0.9878972891528223, ic: 0.06873448332338686
train 30, step: 1000, loss: 0.979454179243608, grad_norm: 0.07810963751290768, ic: -0.032017825905018206
train 30, step: 1500, loss: 1.4669993011850562, grad_norm: 2.6927797861681295, ic: 0.15718099749041586
train 30, step: 2000, loss: 1.83611140583158, grad_norm: 0.47361902618947016, ic: 0.12260258158993022
Epoch 30: 2022-05-07 15:03:59.721920: train loss: 1.6166982350364487
Eval step 0: eval loss: 0.8313762636657007
Eval: 2022-05-07 15:04:32.083754: total loss: 1.0718090545587284, mse:4.657409863866358, ic :0.19031764044561655, sharpe5:16.663684644699096, irr5:574.41552734375, ndcg5:0.8475561951015296, pnl5:3.7397923469543457 
train 31, step: 0, loss: 1.0472554842847128, grad_norm: 0.4070849974372354, ic: 0.3655112456377181
train 31, step: 500, loss: 1.4944331918724278, grad_norm: 1.7918532406068715, ic: 0.013741776925765743
train 31, step: 1000, loss: 4.403228831235363, grad_norm: 2.9928104187108846, ic: 0.4702680903583326
train 31, step: 1500, loss: 0.7676325771367344, grad_norm: 0.0886808552526924, ic: 0.7116432447982939
train 31, step: 2000, loss: 1.230232772131459, grad_norm: 3.3060536254295076, ic: 0.21425718407375313
Epoch 31: 2022-05-07 15:11:57.231670: train loss: 1.609074010778287
Eval step 0: eval loss: 0.8310171274038461
Eval: 2022-05-07 15:12:27.032362: total loss: 1.065651004084922, mse:4.5847746592069125, ic :0.18898844696747236, sharpe5:17.73250691771507, irr5:579.9389038085938, ndcg5:0.8501163231287576, pnl5:6.272085666656494 
train 32, step: 0, loss: 1.1249824934236166, grad_norm: 0.05645582946090964, ic: 0.19207162453889748
train 32, step: 500, loss: 1.4719780388779526, grad_norm: 0.7934074165457325, ic: 0.1103853325801264
train 32, step: 1000, loss: 1.044305487822543, grad_norm: 0.2132790387536087, ic: 0.507376696423075
train 32, step: 1500, loss: 0.9815499707999619, grad_norm: 1.5558911023027293, ic: 0.06343741857437656
train 32, step: 2000, loss: 0.9415729765094849, grad_norm: 0.11937569057715064, ic: 0.5593188545509229
Epoch 32: 2022-05-07 15:19:53.894460: train loss: 1.6136565906550104
Eval step 0: eval loss: 0.8199500193460221
Eval: 2022-05-07 15:20:25.437078: total loss: 1.0646037623819067, mse:4.598815196030462, ic :0.19716004710726598, sharpe5:17.978366253376006, irr5:613.133056640625, ndcg5:0.8528602793816401, pnl5:6.422930717468262 
train 33, step: 0, loss: 1.273401819260709, grad_norm: 0.4708911091451351, ic: 0.22311000644064827
train 33, step: 500, loss: 0.9886529629607524, grad_norm: 0.0388531381930807, ic: 0.1854229270708651
train 33, step: 1000, loss: 1.0699193367748525, grad_norm: 2.170592637056287, ic: 0.2281924770530968
train 33, step: 1500, loss: 0.9033983622015866, grad_norm: 0.12711960069941647, ic: 0.5510438528272265
train 33, step: 2000, loss: 0.8053168101847267, grad_norm: 0.07750276966013481, ic: 0.27565197204967606
Epoch 33: 2022-05-07 15:27:56.690509: train loss: 1.609609420060413
Eval step 0: eval loss: 0.8248123796018835
Eval: 2022-05-07 15:28:27.933327: total loss: 1.0648621057478898, mse:4.577188208586602, ic :0.19833831825957354, sharpe5:18.1108885538578, irr5:591.6763305664062, ndcg5:0.8471335916776822, pnl5:7.162081718444824 
train 34, step: 0, loss: 1.0137328678001387, grad_norm: 0.4016587789373678, ic: 0.5926384776755718
train 34, step: 500, loss: 0.7935092937691132, grad_norm: 0.5368301687315442, ic: 0.3051998054274774
train 34, step: 1000, loss: 3.143802203341014, grad_norm: 0.6629169563605202, ic: 0.3301155132632373
train 34, step: 1500, loss: 0.8234031049798868, grad_norm: 0.40348552912946445, ic: 0.6817134511772118
train 34, step: 2000, loss: 6.166531516266461, grad_norm: 19.659403924070403, ic: 0.4236971761539968
Epoch 34: 2022-05-07 15:35:56.837538: train loss: 1.6062919701323153
Eval step 0: eval loss: 0.8220129304489923
Eval: 2022-05-07 15:36:27.563935: total loss: 1.0665823281816147, mse:4.604610405501857, ic :0.19526853317892065, sharpe5:17.42997338891029, irr5:594.2303466796875, ndcg5:0.8500216347643936, pnl5:8.8317232131958 
train 35, step: 0, loss: 1.1698220645680146, grad_norm: 0.9551279860983052, ic: 0.554895243961535
train 35, step: 500, loss: 1.1815671632256746, grad_norm: 0.798676200390414, ic: 0.13859476448813512
train 35, step: 1000, loss: 1.5907802703274283, grad_norm: 2.250941088288334, ic: 0.06250317872868726
train 35, step: 1500, loss: 1.6084746167175752, grad_norm: 2.5525006697009864, ic: 0.04852820473702166
train 35, step: 2000, loss: 0.783102433311748, grad_norm: 0.14041730377384679, ic: 0.562607006335788
Epoch 35: 2022-05-07 15:44:07.851140: train loss: 1.6072024121392783
Eval step 0: eval loss: 0.8316565494517254
Eval: 2022-05-07 15:44:39.473437: total loss: 1.0666023872702857, mse:4.600631816423826, ic :0.19133236983013366, sharpe5:17.948134467601776, irr5:606.78955078125, ndcg5:0.8462067507560471, pnl5:6.720512866973877 
train 36, step: 0, loss: 1.8549298776000787, grad_norm: 2.983396586312921, ic: 0.12483239726204998
train 36, step: 500, loss: 0.8358465003816983, grad_norm: 0.08376512350738329, ic: 0.184147808768914
train 36, step: 1000, loss: 1.7484016335227273, grad_norm: 3.135417168939773, ic: 0.26688599660998086
train 36, step: 1500, loss: 0.7641211318226121, grad_norm: 0.06101828969855111, ic: 0.37516800052892696
train 36, step: 2000, loss: 1.1262734483593129, grad_norm: 1.1516335591420068, ic: 0.7729629334453296
Epoch 36: 2022-05-07 15:52:15.075942: train loss: 1.603316895125817
Eval step 0: eval loss: 0.8261552816698498
Eval: 2022-05-07 15:52:46.155764: total loss: 1.0643620822696962, mse:4.581385555536169, ic :0.1959915338603855, sharpe5:18.743701504468916, irr5:638.61279296875, ndcg5:0.8548678619894639, pnl5:6.211714744567871 
train 37, step: 0, loss: 2.0028788404479325, grad_norm: 5.440388603618087, ic: 0.2046915029539279
train 37, step: 500, loss: 2.335536042006525, grad_norm: 2.2338951285254076, ic: -0.028583849079157272
train 37, step: 1000, loss: 1.0751865799205669, grad_norm: 0.184245297031636, ic: 0.04609176432760998
train 37, step: 1500, loss: 2.0376135618009226, grad_norm: 2.6478474952622477, ic: 0.6024032823200207
train 37, step: 2000, loss: 1.3071544558502908, grad_norm: 0.2414404978520599, ic: 0.2423182224654471
Epoch 37: 2022-05-07 16:00:22.495393: train loss: 1.6079618177864485
Eval step 0: eval loss: 0.8236874418590292
Eval: 2022-05-07 16:00:53.358421: total loss: 1.0650671690647098, mse:4.593270825848422, ic :0.19509888419854998, sharpe5:18.370422191619873, irr5:624.8380126953125, ndcg5:0.8388883072435377, pnl5:7.268714427947998 
train 38, step: 0, loss: 1.3051679657726754, grad_norm: 1.004578931380659, ic: -0.052714905915446864
train 38, step: 500, loss: 0.8961724928867669, grad_norm: 0.10841392185910197, ic: 0.26302548205709864
train 38, step: 1000, loss: 0.9017084053853754, grad_norm: 0.2901269794724528, ic: 0.12084032710658038
train 38, step: 1500, loss: 0.9541473504532081, grad_norm: 0.035934850254702805, ic: 0.19732691112175516
train 38, step: 2000, loss: 2.2899184673574395, grad_norm: 6.312948607437158, ic: 0.009903623888079301
Epoch 38: 2022-05-07 16:08:33.601846: train loss: 1.604125825712489
Eval step 0: eval loss: 0.8233508802440068
Eval: 2022-05-07 16:09:04.964663: total loss: 1.0631527447781068, mse:4.5772612393252725, ic :0.20044623323025829, sharpe5:19.033029910326004, irr5:642.9259033203125, ndcg5:0.8381695847298513, pnl5:7.981775760650635 
train 39, step: 0, loss: 0.9696686070993406, grad_norm: 0.015202350555433916, ic: 0.08233692331997064
train 39, step: 500, loss: 0.9063394181458974, grad_norm: 0.7029666444836116, ic: 0.21296479136263674
train 39, step: 1000, loss: 0.9472966771587015, grad_norm: 0.501104027698813, ic: 0.21270635401598864
train 39, step: 1500, loss: 2.1304668541377776, grad_norm: 0.4451307280983401, ic: 0.23352650413416667
train 39, step: 2000, loss: 0.6076913746235684, grad_norm: 0.06299495013108265, ic: 0.15137026823378724
Epoch 39: 2022-05-07 16:16:34.768924: train loss: 1.6050236263797624
Eval step 0: eval loss: 0.8287823016826923
Eval: 2022-05-07 16:17:06.120330: total loss: 1.0657790176649122, mse:4.596368621573988, ic :0.18858841417227318, sharpe5:17.151630964279175, irr5:584.7504272460938, ndcg5:0.8450951543628514, pnl5:6.312036037445068 
