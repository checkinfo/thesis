Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_20_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
56179
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795805286112197, grad_norm: 4.816275878622871, ic: 0.02289694486093561
train 0, step: 500, loss: 0.8631359062970987, grad_norm: 0.02655787221708271, ic: 0.04517694636279546
train 0, step: 1000, loss: 1.9500822612815472, grad_norm: 0.5126443160129678, ic: 0.015282218393546782
train 0, step: 1500, loss: 0.9563165838068182, grad_norm: 0.0488456076545713, ic: 0.015065024821900478
train 0, step: 2000, loss: 1.0022355828721201, grad_norm: 0.15819568051577695, ic: 0.025702659551112967
Epoch 0: 2022-05-06 14:25:12.714516: train loss: 1.648491029900274
Eval step 0: eval loss: 0.8363050615573959
Eval: 2022-05-06 14:25:38.368365: total loss: 1.0793448969197608, mse:4.822856890166927, ic :0.00815228031629659, sharpe5:7.835258755087852, irr5:222.46929931640625, ndcg5:0.8446954469147636, pnl5:2.5436177253723145 
train 1, step: 0, loss: 2.7747747605846773, grad_norm: 0.8785553615613352, ic: 0.05712670682295819
train 1, step: 500, loss: 1.755452525173065, grad_norm: 0.7704962927434649, ic: 0.09701108809118868
train 1, step: 1000, loss: 0.8777548047302848, grad_norm: 0.17539047246373257, ic: 0.08018677470940638
train 1, step: 1500, loss: 1.7132360699533045, grad_norm: 0.2066586525075221, ic: -0.03148714967689946
train 1, step: 2000, loss: 2.1772324218750003, grad_norm: 0.8875293385101765, ic: -0.04567717932399962
Epoch 1: 2022-05-06 14:31:49.290027: train loss: 1.6467352124399328
Eval step 0: eval loss: 0.8345544652265542
Eval: 2022-05-06 14:32:14.819620: total loss: 1.078984387677997, mse:4.823523009950308, ic :0.007775401592498404, sharpe5:7.321032254397869, irr5:207.767578125, ndcg5:0.8551881169236, pnl5:2.615943431854248 
train 2, step: 0, loss: 2.1417761008522724, grad_norm: 0.00930193682743644, ic: 0.13147112691580615
train 2, step: 500, loss: 3.300122949065923, grad_norm: 0.2852022416678196, ic: 0.04867221510037809
train 2, step: 1000, loss: 2.0723161368534484, grad_norm: 0.0002073960264107333, ic: 0.19151614214441198
train 2, step: 1500, loss: 1.485515099445372, grad_norm: 0.05971017673216172, ic: -0.03778207068720837
train 2, step: 2000, loss: 3.2350721153846154, grad_norm: 0.7875006649599812, ic: 0.20809419080943756
Epoch 2: 2022-05-06 14:38:23.779077: train loss: 1.6464946228162187
Eval step 0: eval loss: 0.8356983759631849
Eval: 2022-05-06 14:38:49.076602: total loss: 1.0794920227054396, mse:4.823234889185682, ic :0.010290939989156346, sharpe5:7.482871630489826, irr5:211.25559997558594, ndcg5:0.8445137945253675, pnl5:2.9086384773254395 
train 3, step: 0, loss: 1.5230841908028456, grad_norm: 0.5251503610898174, ic: -0.001332863118487404
train 3, step: 500, loss: 1.5014789912881754, grad_norm: 0.3423609316615221, ic: 0.09501412726091157
train 3, step: 1000, loss: 3.67953738935665, grad_norm: 0.7056409055153429, ic: -0.048963305753118816
train 3, step: 1500, loss: 1.9814886664407734, grad_norm: 1.2599229438631703, ic: -0.07101176815188096
train 3, step: 2000, loss: 0.8989047983530406, grad_norm: 0.0016302114727550256, ic: 0.01107136821123817
Epoch 3: 2022-05-06 14:44:56.441425: train loss: 1.645887016987243
Eval step 0: eval loss: 0.8340948685705347
Eval: 2022-05-06 14:45:21.930711: total loss: 1.0794763811610626, mse:4.825950259870204, ic :0.013698417703618227, sharpe5:7.355647459924221, irr5:208.3988494873047, ndcg5:0.8472255079483602, pnl5:3.029688835144043 
train 4, step: 0, loss: 1.4306898716517857, grad_norm: 0.04468213108649459, ic: 0.13140183737656014
train 4, step: 500, loss: 1.6488350455216536, grad_norm: 0.5595204033073851, ic: 0.03918413759119739
train 4, step: 1000, loss: 2.964119143602325, grad_norm: 0.7858765927317857, ic: 0.07597450731646713
train 4, step: 1500, loss: 2.150607364187764, grad_norm: 0.4767405741836562, ic: 0.010949079551800044
train 4, step: 2000, loss: 1.0827777204024105, grad_norm: 0.3937739321018501, ic: 0.2380077512599828
Epoch 4: 2022-05-06 14:51:32.790495: train loss: 1.6452335733426542
Eval step 0: eval loss: 0.8557392598705874
Eval: 2022-05-06 14:51:58.070158: total loss: 1.0876098410360964, mse:4.828468282819816, ic :0.061381010491479523, sharpe5:10.389034940600395, irr5:193.38912963867188, ndcg5:0.8478504710009261, pnl5:2.2312679290771484 
train 5, step: 0, loss: 1.3665670714922398, grad_norm: 0.2042175846670835, ic: 0.09532673427625747
train 5, step: 500, loss: 0.8893463110564532, grad_norm: 0.009140827579314262, ic: 0.03393398758903658
train 5, step: 1000, loss: 0.9782240593570403, grad_norm: 0.15138028620811944, ic: 0.007161218362879113
train 5, step: 1500, loss: 1.5363264799483156, grad_norm: 0.16981823773484458, ic: 0.03292719768223515
train 5, step: 2000, loss: 1.1017875228846432, grad_norm: 0.030217006908742385, ic: 0.16699553141267193
Epoch 5: 2022-05-06 14:58:10.801617: train loss: 1.6436933067242463
Eval step 0: eval loss: 0.8365235404043729
Eval: 2022-05-06 14:58:36.349379: total loss: 1.0797370291274604, mse:4.7458731732178405, ic :0.12688321003161926, sharpe5:11.40116977751255, irr5:394.62646484375, ndcg5:0.8507395491282651, pnl5:2.6405081748962402 
train 6, step: 0, loss: 1.3421091569477672, grad_norm: 0.44506537645856875, ic: 0.10126986797827023
train 6, step: 500, loss: 1.0071699016857294, grad_norm: 0.04408065437112672, ic: 0.045895236173479695
train 6, step: 1000, loss: 1.1106721943322242, grad_norm: 0.0800464602789301, ic: 0.7470724020928601
train 6, step: 1500, loss: 1.5688210227272728, grad_norm: 0.7221943193808935, ic: 0.13872081043757337
train 6, step: 2000, loss: 0.8119803459955464, grad_norm: 0.06740714060119374, ic: 0.3328617952719324
Epoch 6: 2022-05-06 15:04:54.249642: train loss: 1.6363214767942433
Eval step 0: eval loss: 0.8264090695798209
Eval: 2022-05-06 15:05:19.980529: total loss: 1.072935503212669, mse:4.712595957021413, ic :0.14688754382307664, sharpe5:14.501638053059578, irr5:480.4196472167969, ndcg5:0.8567142163073942, pnl5:4.478456974029541 
train 7, step: 0, loss: 0.9913189888000489, grad_norm: 0.05277984503890064, ic: 0.06329219843449405
train 7, step: 500, loss: 0.6505209972068532, grad_norm: 0.002165859568603025, ic: 0.033305987872334006
train 7, step: 1000, loss: 1.0261217452875095, grad_norm: 0.24070362804543058, ic: 0.058979471633828724
train 7, step: 1500, loss: 2.244673390608253, grad_norm: 0.7226042938360722, ic: 0.43903885280984223
train 7, step: 2000, loss: 0.9134106366430231, grad_norm: 0.06661650801902384, ic: -0.0579258453067033
Epoch 7: 2022-05-06 15:11:34.136239: train loss: 1.6299627179307818
Eval step 0: eval loss: 0.8265114594309799
Eval: 2022-05-06 15:11:59.482452: total loss: 1.0719748739963046, mse:4.697217268139413, ic :0.1594805849914839, sharpe5:15.520759497284889, irr5:510.7486267089844, ndcg5:0.8461256172392031, pnl5:4.014823913574219 
train 8, step: 0, loss: 3.6087066208106884, grad_norm: 1.1253479163313767, ic: 0.1823057145323029
train 8, step: 500, loss: 2.7497248780038945, grad_norm: 0.8732944647523092, ic: 0.03801955670212402
train 8, step: 1000, loss: 3.0531943500905796, grad_norm: 0.8858054056298306, ic: 0.1203025514625597
train 8, step: 1500, loss: 0.7108973868825139, grad_norm: 0.03350429768040371, ic: 0.46144125573700945
train 8, step: 2000, loss: 1.083104514602396, grad_norm: 0.33638795853765435, ic: 0.5280963463065466
Epoch 8: 2022-05-06 15:18:12.796128: train loss: 1.6276185262519236
Eval step 0: eval loss: 0.8235151413494467
Eval: 2022-05-06 15:18:38.410023: total loss: 1.0707609864744485, mse:4.68903362387872, ic :0.16367719375602977, sharpe5:16.415292119979856, irr5:532.8873901367188, ndcg5:0.8400906181200897, pnl5:6.586935520172119 
train 9, step: 0, loss: 5.442054566013756, grad_norm: 0.9318073977797392, ic: -0.003395184553312766
train 9, step: 500, loss: 1.339056592564508, grad_norm: 1.2486831656100437, ic: 0.3265797105448422
train 9, step: 1000, loss: 0.9316472396474753, grad_norm: 0.07279028281168945, ic: 0.02669700118624377
train 9, step: 1500, loss: 1.0902432201537187, grad_norm: 0.06918758094461232, ic: 0.42052526174780697
train 9, step: 2000, loss: 1.0656003713446585, grad_norm: 0.25962558909181754, ic: 0.287987990855853
Epoch 9: 2022-05-06 15:24:51.826055: train loss: 1.6270928823596509
Eval step 0: eval loss: 0.8239892089329227
Eval: 2022-05-06 15:25:17.348052: total loss: 1.071680167520739, mse:4.699087887306261, ic :0.15746413604982865, sharpe5:15.84182844042778, irr5:516.70703125, ndcg5:0.8480505791598413, pnl5:4.845642566680908 
train 10, step: 0, loss: 7.102987484056122, grad_norm: 1.9960821773334927, ic: 0.2559569550149525
train 10, step: 500, loss: 1.1260849310427297, grad_norm: 0.07529027022432284, ic: 0.04038497075862123
train 10, step: 1000, loss: 2.4014989934815953, grad_norm: 0.6690733652943288, ic: 0.1624100443895744
train 10, step: 1500, loss: 1.117119429947494, grad_norm: 0.3367220143402382, ic: -0.015962051822440777
train 10, step: 2000, loss: 2.7465189554046354, grad_norm: 0.5367893411422385, ic: 0.43588552201683345
Epoch 10: 2022-05-06 15:31:28.998429: train loss: 1.6268673586221896
Eval step 0: eval loss: 0.8255866706936578
Eval: 2022-05-06 15:31:54.916025: total loss: 1.0708742557846331, mse:4.684737298211267, ic :0.1651452035624772, sharpe5:17.03891067266464, irr5:554.6314697265625, ndcg5:0.8362215414689197, pnl5:6.667294025421143 
train 11, step: 0, loss: 1.2550377370033354, grad_norm: 0.039766223969520495, ic: 0.2026398756175654
train 11, step: 500, loss: 0.667461704992735, grad_norm: 0.027723720721636435, ic: 0.520914966735347
train 11, step: 1000, loss: 0.9327009158594394, grad_norm: 0.123224741724627, ic: 0.06987789779258738
train 11, step: 1500, loss: 1.0587312129505893, grad_norm: 0.07005793846257757, ic: 0.17625179810179065
train 11, step: 2000, loss: 0.7878416040309044, grad_norm: 0.003822445110041824, ic: 0.1343755881771054
Epoch 11: 2022-05-06 15:38:03.729354: train loss: 1.626036233969863
Eval step 0: eval loss: 0.8289947992129872
Eval: 2022-05-06 15:38:29.020517: total loss: 1.0711186760475506, mse:4.6775325199802404, ic :0.1697580380414578, sharpe5:17.472825715541838, irr5:560.9403686523438, ndcg5:0.8263335819482837, pnl5:6.3779191970825195 
train 12, step: 0, loss: 0.9616846243540446, grad_norm: 0.19664395606207372, ic: 0.4025679567492322
train 12, step: 500, loss: 0.9308361206554566, grad_norm: 0.07041972726906132, ic: 0.18752043395422274
train 12, step: 1000, loss: 2.9563048538888337, grad_norm: 0.35619843180264105, ic: 0.17066439327911165
train 12, step: 1500, loss: 0.9403488031345202, grad_norm: 0.11596331649533731, ic: -0.1351150197991926
train 12, step: 2000, loss: 0.8730442934526057, grad_norm: 0.0035832927029924035, ic: 0.2326597618672067
Epoch 12: 2022-05-06 15:44:36.002485: train loss: 1.6249113541028917
Eval step 0: eval loss: 0.8273117338769428
Eval: 2022-05-06 15:45:01.541543: total loss: 1.0703082005945481, mse:4.675333945847345, ic :0.1696315995220146, sharpe5:17.198323749303817, irr5:549.8492431640625, ndcg5:0.8411849621297958, pnl5:7.155819892883301 
train 13, step: 0, loss: 2.0736657598831836, grad_norm: 0.6745413373349654, ic: 0.3965408872821461
train 13, step: 500, loss: 0.8282505675520123, grad_norm: 0.07696334366412558, ic: 0.5134520795645887
train 13, step: 1000, loss: 0.9570414088716442, grad_norm: 0.3478219882286944, ic: 0.5352675705811135
train 13, step: 1500, loss: 2.377308758416276, grad_norm: 0.2513091692532389, ic: -0.05260894841257721
train 13, step: 2000, loss: 1.4632847457780023, grad_norm: 0.16733712678693652, ic: 0.1873243514508727
Epoch 13: 2022-05-06 15:51:09.423740: train loss: 1.6255939474312948
Eval step 0: eval loss: 0.8246292098179003
Eval: 2022-05-06 15:51:35.209158: total loss: 1.0708495049990858, mse:4.6675896019720025, ic :0.16818011717170625, sharpe5:17.503914510011672, irr5:560.0732421875, ndcg5:0.8564622679633228, pnl5:5.9980645179748535 
train 14, step: 0, loss: 4.570941323981084, grad_norm: 1.4980769669724006, ic: 0.1815471406168564
train 14, step: 500, loss: 0.8285953626720184, grad_norm: 0.0031369007762691217, ic: 0.08654742935031291
train 14, step: 1000, loss: 1.8187434747532272, grad_norm: 0.1589652925494266, ic: 0.4221094019171925
train 14, step: 1500, loss: 1.1257251398155417, grad_norm: 0.08243658366840814, ic: -0.05201723687897047
train 14, step: 2000, loss: 1.1499048219756018, grad_norm: 0.2490340728992425, ic: 0.08965930236185599
Epoch 14: 2022-05-06 15:57:51.666221: train loss: 1.6232916858132742
Eval step 0: eval loss: 0.8317225368809272
Eval: 2022-05-06 15:58:18.277361: total loss: 1.0706665948524847, mse:4.622239582099523, ic :0.17937972961617618, sharpe5:17.37685209274292, irr5:564.4130249023438, ndcg5:0.8481501183299636, pnl5:5.62275505065918 
train 15, step: 0, loss: 3.419604207198444, grad_norm: 1.0677615737963333, ic: 0.05800847429324182
train 15, step: 500, loss: 1.2570972268222673, grad_norm: 0.013640850759427094, ic: -0.009388561837827478
train 15, step: 1000, loss: 1.3243228571201728, grad_norm: 0.13347827196260936, ic: 0.012987420664951434
train 15, step: 1500, loss: 0.8484632597194882, grad_norm: 0.19023396664612813, ic: 0.0760921791814033
train 15, step: 2000, loss: 1.4662677070752819, grad_norm: 0.570753481036516, ic: 0.046327149634801626
Epoch 15: 2022-05-06 16:04:57.455794: train loss: 1.6208253226747573
Eval step 0: eval loss: 0.8422616168746706
Eval: 2022-05-06 16:05:24.748363: total loss: 1.0727658796173773, mse:4.59891452739537, ic :0.18207981782829796, sharpe5:16.347894912958143, irr5:529.4264526367188, ndcg5:0.8530356041426927, pnl5:6.277264595031738 
train 16, step: 0, loss: 0.6868038706050821, grad_norm: 0.4232229854203804, ic: 0.007818263026461842
train 16, step: 500, loss: 1.647163419385257, grad_norm: 1.0904902018859735, ic: 0.17212702907278016
train 16, step: 1000, loss: 0.8831272934422348, grad_norm: 0.017633680287212357, ic: -0.1018729980481477
train 16, step: 1500, loss: 0.8477262793045706, grad_norm: 0.2391010596321609, ic: 0.15442002172871563
train 16, step: 2000, loss: 3.3232962169694464, grad_norm: 1.1433280204000622, ic: 0.052789968380896085
Epoch 16: 2022-05-06 16:11:52.189170: train loss: 1.6185012066666145
Eval step 0: eval loss: 0.8286125737824354
Eval: 2022-05-06 16:12:18.969109: total loss: 1.0687463760018947, mse:4.601229518958406, ic :0.18150962265088955, sharpe5:17.097425637245177, irr5:567.813720703125, ndcg5:0.8419573375656063, pnl5:8.090723037719727 
train 17, step: 0, loss: 1.28187025965683, grad_norm: 0.2621440298401453, ic: -0.12285064835726461
train 17, step: 500, loss: 1.7468458883807587, grad_norm: 0.533007315642162, ic: 0.1944297585283316
train 17, step: 1000, loss: 1.2734610596997615, grad_norm: 0.09417972994940191, ic: 0.15507987518696906
train 17, step: 1500, loss: 4.527525644631704, grad_norm: 1.1318748422692138, ic: 0.21209094111964444
train 17, step: 2000, loss: 1.313175513996619, grad_norm: 0.9829235802420593, ic: 0.08221884215550303
Epoch 17: 2022-05-06 16:18:40.382355: train loss: 1.622828737054134
Eval step 0: eval loss: 0.8401565792939936
Eval: 2022-05-06 16:19:07.139906: total loss: 1.0765211599682651, mse:4.703851565405238, ic :0.16964341643757078, sharpe5:17.61598968029022, irr5:595.9453735351562, ndcg5:0.8447324213345271, pnl5:6.726998805999756 
train 18, step: 0, loss: 1.4357278195072778, grad_norm: 0.9131937418009024, ic: 0.22326535390768693
train 18, step: 500, loss: 1.4783206474790658, grad_norm: 0.9695076940133059, ic: -0.0017169890634033246
train 18, step: 1000, loss: 0.6535314105308219, grad_norm: 0.02176270869840181, ic: 0.5750280653447358
train 18, step: 1500, loss: 1.426908775785729, grad_norm: 0.03768854759865273, ic: 0.18317592828392282
train 18, step: 2000, loss: 0.9095908000970343, grad_norm: 0.005631350098867395, ic: -0.025087689336367992
Epoch 18: 2022-05-06 16:25:32.899842: train loss: 1.6192440623400097
Eval step 0: eval loss: 0.8211690502749605
Eval: 2022-05-06 16:25:59.822839: total loss: 1.0660046959973712, mse:4.5893822145344485, ic :0.1906282801420438, sharpe5:17.78944218635559, irr5:605.2699584960938, ndcg5:0.8478185888270384, pnl5:8.298493385314941 
train 19, step: 0, loss: 1.4777361188616072, grad_norm: 0.8162657862997402, ic: 0.04964439435301253
train 19, step: 500, loss: 0.8604243243182147, grad_norm: 0.014011942532724916, ic: 0.22779556089455302
train 19, step: 1000, loss: 0.9586624690050779, grad_norm: 0.056403727579884586, ic: 0.20897656393680952
train 19, step: 1500, loss: 3.942348842124912, grad_norm: 1.079241556969253, ic: 0.15529985924899448
train 19, step: 2000, loss: 1.0082207782451924, grad_norm: 0.21281355439770572, ic: 0.24368988497440258
Epoch 19: 2022-05-06 16:32:34.505497: train loss: 1.6216594334412724
Eval step 0: eval loss: 0.8262954888780953
Eval: 2022-05-06 16:33:02.093642: total loss: 1.0670881694495948, mse:4.586986017962915, ic :0.18903756593979584, sharpe5:17.493441220521927, irr5:602.3985595703125, ndcg5:0.8403824532000254, pnl5:8.843132972717285 
train 20, step: 0, loss: 2.3140798696887352, grad_norm: 1.566360000002276, ic: 0.04348207919849845
train 20, step: 500, loss: 3.2145124289772724, grad_norm: 0.6464221723947947, ic: 0.07172149812434617
train 20, step: 1000, loss: 0.9724145889282227, grad_norm: 0.21745606961782488, ic: 0.10734249499191356
train 20, step: 1500, loss: 1.7088972620357963, grad_norm: 1.2687649440286468, ic: 0.24980310884805365
train 20, step: 2000, loss: 1.037505555891874, grad_norm: 0.049930066422009325, ic: -0.010820405314316685
Epoch 20: 2022-05-06 16:39:43.770253: train loss: 1.6192326815396894
Eval step 0: eval loss: 0.8333729300785365
Eval: 2022-05-06 16:40:09.919917: total loss: 1.068566738549671, mse:4.585146583470988, ic :0.1915603741521828, sharpe5:17.940542467832564, irr5:586.2852172851562, ndcg5:0.8483615419539514, pnl5:5.540178298950195 
train 21, step: 0, loss: 1.0005228259379768, grad_norm: 0.4241327993000641, ic: 0.08948857155232867
train 21, step: 500, loss: 0.7676781848468611, grad_norm: 0.07176227169221847, ic: 0.1574295580467796
train 21, step: 1000, loss: 0.9443537394205729, grad_norm: 0.8717968202290471, ic: 0.1629956782410038
train 21, step: 1500, loss: 0.988270000899928, grad_norm: 0.34757453860052556, ic: 0.3192867037923911
train 21, step: 2000, loss: 0.9407808931166943, grad_norm: 0.08169540999401384, ic: 0.06680829109354423
Epoch 21: 2022-05-06 16:46:30.115841: train loss: 1.6188072808997767
Eval step 0: eval loss: 0.825041599092795
Eval: 2022-05-06 16:46:55.876766: total loss: 1.0667644344927234, mse:4.597620904353712, ic :0.1844783545850865, sharpe5:17.425126465559003, irr5:589.5294189453125, ndcg5:0.8570698626519911, pnl5:6.762298107147217 
train 22, step: 0, loss: 1.0415397687146892, grad_norm: 0.06487367660196777, ic: 0.2158208652816092
train 22, step: 500, loss: 3.255220242632114, grad_norm: 1.0574834608866395, ic: -0.20700212463452627
train 22, step: 1000, loss: 1.199277738890896, grad_norm: 0.033095903939692806, ic: 0.4550953719430664
train 22, step: 1500, loss: 0.9740255473572531, grad_norm: 0.17836954854523507, ic: 0.08443723376546709
train 22, step: 2000, loss: 1.7237583428553713, grad_norm: 1.898830713148127, ic: 0.15280796030715288
Epoch 22: 2022-05-06 16:53:07.248060: train loss: 1.6177710270996986
Eval step 0: eval loss: 0.8304294791186446
Eval: 2022-05-06 16:53:33.125174: total loss: 1.0676439584509543, mse:4.592769705822952, ic :0.18077924438903667, sharpe5:17.45880331277847, irr5:574.665283203125, ndcg5:0.8377508231894059, pnl5:5.171730041503906 
train 23, step: 0, loss: 0.9781977496847983, grad_norm: 0.09716782837642368, ic: 0.19375190756837157
train 23, step: 500, loss: 1.417791106075844, grad_norm: 0.2593232213714826, ic: 0.07322445461839995
train 23, step: 1000, loss: 1.6486069742838543, grad_norm: 0.06970946606388953, ic: 0.25552238755821965
train 23, step: 1500, loss: 1.1240786174033481, grad_norm: 1.0832605052130155, ic: 0.11558534683665347
train 23, step: 2000, loss: 1.9101632039669938, grad_norm: 1.566309123679352, ic: 0.437942061906731
Epoch 23: 2022-05-06 16:59:49.704444: train loss: 1.6173454797079951
Eval step 0: eval loss: 0.8267832556062302
Eval: 2022-05-06 17:00:15.735154: total loss: 1.0652479005758206, mse:4.5785165014845255, ic :0.19180101329156668, sharpe5:17.07247022509575, irr5:588.203857421875, ndcg5:0.8389080668482007, pnl5:4.197875022888184 
train 24, step: 0, loss: 2.19132554084286, grad_norm: 0.04422366030103231, ic: 0.15441650959026687
train 24, step: 500, loss: 1.2238285049854087, grad_norm: 0.15210624941828021, ic: 0.07671368966874913
train 24, step: 1000, loss: 0.913899629684335, grad_norm: 0.09242428380377796, ic: 0.5193683876502919
train 24, step: 1500, loss: 2.595492596480833, grad_norm: 2.6592907461047606, ic: 0.03875505169913025
train 24, step: 2000, loss: 0.9320833412922779, grad_norm: 0.07868816807250963, ic: 0.10044848642957807
Epoch 24: 2022-05-06 17:06:32.255875: train loss: 1.6123378981375016
Eval step 0: eval loss: 0.8214214232498024
Eval: 2022-05-06 17:06:57.896788: total loss: 1.0665451806337372, mse:4.611189397359288, ic :0.18786920364806253, sharpe5:17.444941717386246, irr5:589.8567504882812, ndcg5:0.8494950710782896, pnl5:5.287123203277588 
train 25, step: 0, loss: 0.8369981920396959, grad_norm: 0.06990616239615059, ic: 0.6140518915476612
train 25, step: 500, loss: 0.8698007635479608, grad_norm: 0.016873843114376036, ic: 0.2130000457454513
train 25, step: 1000, loss: 2.08813439374286, grad_norm: 0.10061441933229884, ic: 0.2626118437161973
train 25, step: 1500, loss: 1.1382896413015902, grad_norm: 0.5350597029983785, ic: 0.5225617635461304
train 25, step: 2000, loss: 0.9998722539685981, grad_norm: 0.5029453171830192, ic: 0.5941904406908057
Epoch 25: 2022-05-06 17:13:14.091346: train loss: 1.6160703614342946
Eval step 0: eval loss: 0.8240490864149762
Eval: 2022-05-06 17:13:40.385407: total loss: 1.0657844624289803, mse:4.608031143764869, ic :0.19172630094163984, sharpe5:17.16375016450882, irr5:601.8966064453125, ndcg5:0.8551787963232628, pnl5:3.734422206878662 
train 26, step: 0, loss: 6.715514551717252, grad_norm: 16.706194567823044, ic: 0.13355293514220906
train 26, step: 500, loss: 3.89082275581509, grad_norm: 3.239462343781445, ic: 0.3536784070859958
train 26, step: 1000, loss: 1.282795138037186, grad_norm: 1.3574929820924697, ic: -0.01785394922474541
train 26, step: 1500, loss: 0.842897626169333, grad_norm: 0.17687040267027349, ic: 0.2858613159591347
train 26, step: 2000, loss: 0.9589672793936861, grad_norm: 0.16183455319494636, ic: 0.14808235357672195
Epoch 26: 2022-05-06 17:19:55.128053: train loss: 1.614606279482151
Eval step 0: eval loss: 0.8251863726825934
Eval: 2022-05-06 17:20:20.998835: total loss: 1.0652659204172992, mse:4.585678460251193, ic :0.19185599378829277, sharpe5:18.448086476325987, irr5:610.1995239257812, ndcg5:0.8460436451443684, pnl5:7.654341697692871 
train 27, step: 0, loss: 0.8274802772671568, grad_norm: 0.019172571634585456, ic: 0.11301277269305327
train 27, step: 500, loss: 0.9304280103744084, grad_norm: 1.5085404369328737, ic: 0.26028291684428745
train 27, step: 1000, loss: 0.7388621460810615, grad_norm: 0.49465252025287354, ic: 0.19606792476941698
train 27, step: 1500, loss: 0.6384529719804135, grad_norm: 0.07005253302874548, ic: 0.5134014836138499
train 27, step: 2000, loss: 1.3869057947157366, grad_norm: 0.07174661193246393, ic: 0.043303578663358044
Epoch 27: 2022-05-06 17:26:34.563788: train loss: 1.6135074669757752
Eval step 0: eval loss: 0.8269172885520942
Eval: 2022-05-06 17:27:00.598818: total loss: 1.0680277196889392, mse:4.603746846281419, ic :0.1839984639761434, sharpe5:16.67336335659027, irr5:570.30078125, ndcg5:0.8608808134191601, pnl5:5.475947380065918 
train 28, step: 0, loss: 1.5644169280888032, grad_norm: 3.3276926550701584, ic: 0.21600246922609506
train 28, step: 500, loss: 1.3950866733335663, grad_norm: 2.6609142743833827, ic: 0.13838675132899497
train 28, step: 1000, loss: 0.9283986492208672, grad_norm: 0.3319068151180724, ic: 0.5666819573207165
train 28, step: 1500, loss: 1.043241724638209, grad_norm: 0.08093419706221816, ic: 0.011494983691980222
train 28, step: 2000, loss: 1.0366061157975461, grad_norm: 0.2782227215209701, ic: 0.1537058798424547
Epoch 28: 2022-05-06 17:33:16.209452: train loss: 1.6110211049280474
Eval step 0: eval loss: 0.8214901119187961
Eval: 2022-05-06 17:33:41.924954: total loss: 1.0692961340687162, mse:4.624273987186245, ic :0.19124699830388203, sharpe5:18.359055948257446, irr5:613.1879272460938, ndcg5:0.827968382363568, pnl5:10.15786075592041 
train 29, step: 0, loss: 0.912136769095906, grad_norm: 0.2310739282149302, ic: 0.09513527894904183
train 29, step: 500, loss: 1.1015834304352612, grad_norm: 0.28133993587897566, ic: 0.6173127360633124
train 29, step: 1000, loss: 1.067310488416155, grad_norm: 1.914346334658737, ic: 0.07551335272716907
train 29, step: 1500, loss: 2.4091657747243365, grad_norm: 0.8864516199217014, ic: 0.050482411698188276
train 29, step: 2000, loss: 4.128885151427469, grad_norm: 7.427788218304929, ic: 0.23895952838043646
Epoch 29: 2022-05-06 17:39:52.934989: train loss: 1.61288845610314
Eval step 0: eval loss: 0.8281465456031019
Eval: 2022-05-06 17:40:19.001908: total loss: 1.0653493709913953, mse:4.576163597153952, ic :0.19248590030303184, sharpe5:17.95437000989914, irr5:605.9166870117188, ndcg5:0.8450993127465475, pnl5:9.331811904907227 
train 30, step: 0, loss: 1.0068598458097928, grad_norm: 0.07422454033502994, ic: 0.5185342934230032
train 30, step: 500, loss: 1.4574381176617617, grad_norm: 2.595426548842669, ic: 0.013647066808864211
train 30, step: 1000, loss: 0.9842209324692235, grad_norm: 0.7726862805225454, ic: -0.050624123016304576
train 30, step: 1500, loss: 1.475862115861007, grad_norm: 1.7550847272474206, ic: 0.17116216499632786
train 30, step: 2000, loss: 1.8322841699741774, grad_norm: 1.0399560736948368, ic: 0.12568477158254965
Epoch 30: 2022-05-06 17:46:36.055622: train loss: 1.609802640090346
Eval step 0: eval loss: 0.8275432044010141
Eval: 2022-05-06 17:47:02.025457: total loss: 1.0657963257504237, mse:4.594794131399314, ic :0.19345661155605187, sharpe5:17.517378816604612, irr5:600.4354248046875, ndcg5:0.8420309023083641, pnl5:9.174005508422852 
train 31, step: 0, loss: 1.0527154545164854, grad_norm: 0.7287179645339213, ic: 0.33609334964194537
train 31, step: 500, loss: 1.4797208558384773, grad_norm: 1.8112101132700447, ic: 0.03174776428541567
train 31, step: 1000, loss: 4.475508178425058, grad_norm: 7.702292022495817, ic: 0.4627940485221573
train 31, step: 1500, loss: 0.7629742374750333, grad_norm: 0.2480575585472173, ic: 0.7139428614416743
train 31, step: 2000, loss: 1.2938992114777261, grad_norm: 9.361656597229654, ic: 0.1727295779443635
Epoch 31: 2022-05-06 17:53:13.054250: train loss: 1.6086913568014114
Eval step 0: eval loss: 0.8300918241446588
Eval: 2022-05-06 17:53:39.158583: total loss: 1.0658960236781911, mse:4.581145814080688, ic :0.19069762323591344, sharpe5:17.45124347448349, irr5:587.5128173828125, ndcg5:0.8507374014679072, pnl5:9.86299991607666 
train 32, step: 0, loss: 1.1301749059211808, grad_norm: 0.3113169667669624, ic: 0.20262208936833484
train 32, step: 500, loss: 1.4693663109005906, grad_norm: 1.4582047888815648, ic: 0.039831674586195624
train 32, step: 1000, loss: 1.048585795059487, grad_norm: 0.3431709303811401, ic: 0.5122948641563562
train 32, step: 1500, loss: 0.9686701095895309, grad_norm: 3.2994259514445994, ic: 0.03268221368079817
train 32, step: 2000, loss: 0.9476535239387338, grad_norm: 0.031492740493860825, ic: 0.545565101267643
Epoch 32: 2022-05-06 17:59:51.440596: train loss: 1.6133287585827811
Eval step 0: eval loss: 0.8214142199436907
Eval: 2022-05-06 18:00:17.631901: total loss: 1.064185841316891, mse:4.59553983965348, ic :0.19585912840617997, sharpe5:18.459324073791503, irr5:616.1297607421875, ndcg5:0.8412434053780262, pnl5:8.918787956237793 
train 33, step: 0, loss: 1.2791842961156856, grad_norm: 0.7527338048769765, ic: 0.22183058538874909
train 33, step: 500, loss: 0.9887610450785428, grad_norm: 0.126935835319394, ic: 0.1728619870110026
train 33, step: 1000, loss: 1.0537291891705185, grad_norm: 3.5632233116278105, ic: 0.22638218479821237
train 33, step: 1500, loss: 0.9069885036699289, grad_norm: 0.12331254027016555, ic: 0.5496278015809002
train 33, step: 2000, loss: 0.8099732799499803, grad_norm: 0.08979866145104545, ic: 0.24288807794575468
Epoch 33: 2022-05-06 18:06:29.502904: train loss: 1.6076190823760768
Eval step 0: eval loss: 0.8239081717391662
Eval: 2022-05-06 18:06:55.816836: total loss: 1.064453236567381, mse:4.581243904514562, ic :0.19613578911180624, sharpe5:18.063389840126035, irr5:603.6078491210938, ndcg5:0.8639882108669873, pnl5:8.846382141113281 
train 34, step: 0, loss: 1.0086614011808206, grad_norm: 0.92475744498552, ic: 0.5975199543211452
train 34, step: 500, loss: 0.8058990571841552, grad_norm: 1.1253726905247272, ic: 0.25984179750641434
train 34, step: 1000, loss: 3.1640621249759984, grad_norm: 1.278273902512831, ic: 0.32870100901680543
train 34, step: 1500, loss: 0.8107240879208879, grad_norm: 0.36607826826588497, ic: 0.6830311413716617
train 34, step: 2000, loss: 5.295208187088497, grad_norm: 70.38197679733122, ic: 0.4572889046097027
Epoch 34: 2022-05-06 18:13:11.945452: train loss: 1.6078254329839516
Eval step 0: eval loss: 0.821777793956632
Eval: 2022-05-06 18:13:38.294527: total loss: 1.0664161501012952, mse:4.5916217718813614, ic :0.19727528075483022, sharpe5:18.37368185162544, irr5:623.594970703125, ndcg5:0.8614549712439076, pnl5:8.5533447265625 
train 35, step: 0, loss: 1.196605296415441, grad_norm: 0.8923129050214266, ic: 0.5559361113531235
train 35, step: 500, loss: 1.1838183274350982, grad_norm: 1.1695901061629934, ic: 0.0740863244063104
train 35, step: 1000, loss: 1.816340290563628, grad_norm: 4.61226287813926, ic: 0.08908895681627763
train 35, step: 1500, loss: 1.5980342090578008, grad_norm: 1.2559786003909492, ic: 0.06521395833090275
train 35, step: 2000, loss: 0.7826115409320687, grad_norm: 0.0811830321839473, ic: 0.5656414702451446
Epoch 35: 2022-05-06 18:19:49.792517: train loss: 1.6078157256473533
Eval step 0: eval loss: 0.8279886517057429
Eval: 2022-05-06 18:20:15.646964: total loss: 1.0656225733018418, mse:4.58153026864597, ic :0.19209419232149919, sharpe5:18.69024156689644, irr5:605.5636596679688, ndcg5:0.8576009220775442, pnl5:8.45104694366455 
train 36, step: 0, loss: 1.825030507996468, grad_norm: 1.8289921135246103, ic: 0.1371184206035811
train 36, step: 500, loss: 0.8447362781040683, grad_norm: 0.021219903300960213, ic: 0.060553436990675884
train 36, step: 1000, loss: 1.6704085582386363, grad_norm: 3.246210479279675, ic: 0.27653628602367614
train 36, step: 1500, loss: 0.770830874477136, grad_norm: 0.2197012187857846, ic: 0.389890930047216
train 36, step: 2000, loss: 1.1002283558379666, grad_norm: 1.4181143940555803, ic: 0.7783191340373067
Epoch 36: 2022-05-06 18:26:34.196773: train loss: 1.6184996367752837
Eval step 0: eval loss: 0.8251891382376185
Eval: 2022-05-06 18:27:00.186567: total loss: 1.064866783949896, mse:4.586821973329524, ic :0.19206661537740524, sharpe5:18.711460572481155, irr5:614.7764892578125, ndcg5:0.8338706786633132, pnl5:6.566585540771484 
train 37, step: 0, loss: 1.9947702311447169, grad_norm: 6.888057750399238, ic: 0.22907102839816634
train 37, step: 500, loss: 2.319320404899062, grad_norm: 1.6664391836434076, ic: -0.013953786723877513
train 37, step: 1000, loss: 1.077494024686073, grad_norm: 0.1790950387964812, ic: 0.08161674976163849
train 37, step: 1500, loss: 2.003416052418564, grad_norm: 4.4202934730405365, ic: 0.6110565215514726
train 37, step: 2000, loss: 1.321172061552637, grad_norm: 0.16989216112876224, ic: 0.11697713439118258
Epoch 37: 2022-05-06 18:33:12.104588: train loss: 1.6068063319977424
Eval step 0: eval loss: 0.8232508700564739
Eval: 2022-05-06 18:33:38.004939: total loss: 1.064666001837723, mse:4.590789196685694, ic :0.19428799272524264, sharpe5:18.5741159427166, irr5:624.1666870117188, ndcg5:0.8401465334185525, pnl5:11.101824760437012 
train 38, step: 0, loss: 1.321350748946027, grad_norm: 1.6243913012087772, ic: -0.07987055670969391
train 38, step: 500, loss: 0.9083247432002315, grad_norm: 0.0780158952646553, ic: 0.26179765332499644
train 38, step: 1000, loss: 0.9178952183176877, grad_norm: 0.23645306388204596, ic: 0.10023992389184289
train 38, step: 1500, loss: 0.9587498739440965, grad_norm: 0.20401604515114066, ic: 0.2201187802182841
train 38, step: 2000, loss: 2.3415848193211857, grad_norm: 6.113798584659626, ic: -0.011084458980661911
Epoch 38: 2022-05-06 18:39:52.933039: train loss: 1.6077337898431183
Eval step 0: eval loss: 0.822545139003227
Eval: 2022-05-06 18:40:19.079155: total loss: 1.0634293620924304, mse:4.5831333260968545, ic :0.19571334901727003, sharpe5:18.838061378002166, irr5:626.3391723632812, ndcg5:0.8544932473912263, pnl5:9.809457778930664 
train 39, step: 0, loss: 0.9694262396669534, grad_norm: 0.02739197223983452, ic: 0.08667690651077943
train 39, step: 500, loss: 0.8878598869259223, grad_norm: 0.16452266778062474, ic: 0.2124106424555381
train 39, step: 1000, loss: 0.9484503485279417, grad_norm: 0.33480132259374396, ic: 0.21578398941453295
train 39, step: 1500, loss: 2.0823477429179875, grad_norm: 0.8233947130564996, ic: 0.24063676735450518
train 39, step: 2000, loss: 0.6099894977105549, grad_norm: 0.07974068918004078, ic: 0.14877695982265957
Epoch 39: 2022-05-06 18:46:37.267457: train loss: 1.606221661745345
Eval step 0: eval loss: 0.8249157341815397
Eval: 2022-05-06 18:47:03.430013: total loss: 1.0652497948282034, mse:4.582367079603286, ic :0.19517541256706894, sharpe5:18.54522169589996, irr5:623.36865234375, ndcg5:0.8368784222653525, pnl5:23.86741065979004 
