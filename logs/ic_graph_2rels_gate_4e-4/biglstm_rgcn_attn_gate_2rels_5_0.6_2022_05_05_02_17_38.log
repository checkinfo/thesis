Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_5_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
14035
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795801468432369, grad_norm: 4.816428940788592, ic: 0.022851361137673748
train 0, step: 500, loss: 0.8631190721846741, grad_norm: 0.026543275285884252, ic: 0.045234040373648456
train 0, step: 1000, loss: 1.9500715455262436, grad_norm: 0.5126918032970958, ic: 0.015072885676600114
train 0, step: 1500, loss: 0.9563932034337944, grad_norm: 0.049028309358158474, ic: 0.014663636688914168
train 0, step: 2000, loss: 1.0022182698040407, grad_norm: 0.1581604279863682, ic: 0.02611479928791557
Epoch 0: 2022-05-05 14:25:01.700269: train loss: 1.6484907653764216
Eval step 0: eval loss: 0.83630924204755
Eval: 2022-05-05 14:25:27.256065: total loss: 1.0793450029326666, mse:4.822852775610153, ic :0.008261893982036915, sharpe5:7.9437252187728875, irr5:224.6282501220703, ndcg5:0.8554191508217873, pnl5:2.611677885055542 
train 1, step: 0, loss: 2.7747639317666333, grad_norm: 0.8785401503736902, ic: 0.05696702070392069
train 1, step: 500, loss: 1.7554400799933134, grad_norm: 0.7704850013356419, ic: 0.09712452855582959
train 1, step: 1000, loss: 0.8777651666680928, grad_norm: 0.17543178169443904, ic: 0.07997214287284642
train 1, step: 1500, loss: 1.713270025143678, grad_norm: 0.2066996097544678, ic: -0.031590904773253464
train 1, step: 2000, loss: 2.177273046875, grad_norm: 0.8876962612230421, ic: -0.04555572463688185
Epoch 1: 2022-05-05 14:31:41.256040: train loss: 1.6467352828258677
Eval step 0: eval loss: 0.8345758178839567
Eval: 2022-05-05 14:32:06.660061: total loss: 1.0789854876376441, mse:4.823499914556116, ic :0.008012566094864258, sharpe5:7.497621923089027, irr5:211.4997100830078, ndcg5:0.867945276440503, pnl5:2.5449347496032715 
train 2, step: 0, loss: 2.1417546164772725, grad_norm: 0.00931645775368341, ic: 0.13341618909353273
train 2, step: 500, loss: 3.3000664031934663, grad_norm: 0.28491277465195985, ic: 0.050772895765922754
train 2, step: 1000, loss: 2.0723105244252875, grad_norm: 0.00020839854544479417, ic: 0.1878860432461829
train 2, step: 1500, loss: 1.4855481795682253, grad_norm: 0.059731014397250176, ic: -0.038066995360669265
train 2, step: 2000, loss: 3.235245643028846, grad_norm: 0.7878729739354902, ic: 0.20807847291841877
Epoch 2: 2022-05-05 14:38:11.954910: train loss: 1.6464877378003782
Eval step 0: eval loss: 0.8358805167034378
Eval: 2022-05-05 14:38:36.835667: total loss: 1.0794960855062425, mse:4.8230542735416, ic :0.011281719881677038, sharpe5:7.707067375481128, irr5:216.47695922851562, ndcg5:0.8542327826373889, pnl5:2.388455629348755 
train 3, step: 0, loss: 1.5231404622395834, grad_norm: 0.5248390134605462, ic: -0.0007283984517235473
train 3, step: 500, loss: 1.5014929476434025, grad_norm: 0.3423068090784428, ic: 0.09561868402027848
train 3, step: 1000, loss: 3.6791210712615143, grad_norm: 0.705852680130965, ic: -0.04944549511397422
train 3, step: 1500, loss: 1.9800917505242432, grad_norm: 1.302438948907092, ic: -0.06810935743397341
train 3, step: 2000, loss: 0.8989218881967905, grad_norm: 0.0008188033262451619, ic: 0.009509591425620453
Epoch 3: 2022-05-05 14:44:47.064371: train loss: 1.6458149320221953
Eval step 0: eval loss: 0.8346316435063224
Eval: 2022-05-05 14:45:12.985583: total loss: 1.0792033542542143, mse:4.824400616514678, ic :0.01734654461580136, sharpe5:7.651692884266376, irr5:215.17379760742188, ndcg5:0.8557641630006104, pnl5:2.6718523502349854 
train 4, step: 0, loss: 1.4318578603316328, grad_norm: 0.04405414174450733, ic: 0.12382179158279769
train 4, step: 500, loss: 1.6511107437253936, grad_norm: 0.5637308044542657, ic: 0.056465300979124
train 4, step: 1000, loss: 2.959104026236185, grad_norm: 0.7998138036184903, ic: 0.07560046510087147
train 4, step: 1500, loss: 2.1491190334915613, grad_norm: 0.4810884390019907, ic: 0.011623959736898053
train 4, step: 2000, loss: 1.081627924371841, grad_norm: 0.3962834203566456, ic: 0.23940141989328823
Epoch 4: 2022-05-05 14:51:18.155713: train loss: 1.6451970334949402
Eval step 0: eval loss: 0.8564393311833179
Eval: 2022-05-05 14:51:43.630296: total loss: 1.0878475964957022, mse:4.831775130292567, ic :0.04961749777395773, sharpe5:9.41599994301796, irr5:282.0520935058594, ndcg5:0.8453024251471524, pnl5:3.1518514156341553 
train 5, step: 0, loss: 1.3713736105285235, grad_norm: 0.2313738622452843, ic: 0.02606358242842927
train 5, step: 500, loss: 0.8880421625631939, grad_norm: 0.009648618067635204, ic: 0.047497094376387675
train 5, step: 1000, loss: 0.9887697183309387, grad_norm: 0.3690235346667301, ic: -0.00679725941659831
train 5, step: 1500, loss: 1.527582095556566, grad_norm: 0.27298913280615633, ic: 0.02160629347256436
train 5, step: 2000, loss: 1.111962975592735, grad_norm: 0.04260343782032561, ic: 0.10685836199031656
Epoch 5: 2022-05-05 14:57:52.714523: train loss: 1.647276479141499
Eval step 0: eval loss: 0.8446392224133956
Eval: 2022-05-05 14:58:18.416696: total loss: 1.0825863512959768, mse:4.824610350565999, ic :0.030340887022901646, sharpe5:-1.7515290212631225, irr5:-20.021480560302734, ndcg5:0.8523129647235634, pnl5:0.9500928521156311 
train 6, step: 0, loss: 1.3538974110969897, grad_norm: 1.91273284595198, ic: 0.09783188664127525
train 6, step: 500, loss: 1.0050037611863516, grad_norm: 0.05007710108369924, ic: 0.03803465566264796
train 6, step: 1000, loss: 1.1139076610028515, grad_norm: 0.09742004428042499, ic: 0.1122980785474785
train 6, step: 1500, loss: 1.5726356695506198, grad_norm: 1.1376558682618878, ic: 0.13940308489121503
train 6, step: 2000, loss: 0.8116967206577762, grad_norm: 0.1624091669194821, ic: 0.025767336111678876
Epoch 6: 2022-05-05 15:04:31.343420: train loss: 1.6463448933802312
Eval step 0: eval loss: 0.8341969368455281
Eval: 2022-05-05 15:04:57.549103: total loss: 1.0778715691138563, mse:4.820021511269225, ic :0.034555153429851465, sharpe5:0.16838227034546435, irr5:1.9934433698654175, ndcg5:0.8584871489918007, pnl5:1.043797254562378 
train 7, step: 0, loss: 0.9936510086059571, grad_norm: 0.0900809018065314, ic: 0.0843661099473118
train 7, step: 500, loss: 0.6514433423074183, grad_norm: 0.007277292457668593, ic: 0.040214948676896733
train 7, step: 1000, loss: 1.041261996917841, grad_norm: 1.2855881731776233, ic: 0.05368766270876524
train 7, step: 1500, loss: 2.282792298867899, grad_norm: 1.0348451236712852, ic: 0.1418217380990629
train 7, step: 2000, loss: 0.9069035396052101, grad_norm: 0.10036632723888622, ic: -0.06779014451655976
Epoch 7: 2022-05-05 15:11:10.591287: train loss: 1.6460507171712293
Eval step 0: eval loss: 0.8381282697864528
Eval: 2022-05-05 15:11:36.554498: total loss: 1.0798952226199197, mse:4.8224391742576, ic :0.0207793495750294, sharpe5:3.5468385587632656, irr5:88.83150482177734, ndcg5:0.844117109875218, pnl5:1.5115985870361328 
train 8, step: 0, loss: 3.6117587324501814, grad_norm: 1.4073232736880876, ic: -0.07527963304410364
train 8, step: 500, loss: 2.7559842280703837, grad_norm: 1.2317805959475996, ic: 0.03837202221878872
train 8, step: 1000, loss: 3.0530213286911234, grad_norm: 0.963921069500514, ic: 0.01821994334822159
train 8, step: 1500, loss: 0.7536828056363671, grad_norm: 0.0009211715823708371, ic: 0.10551935694476937
train 8, step: 2000, loss: 1.1163202858444479, grad_norm: 0.31158896261286073, ic: 0.16347759971231612
Epoch 8: 2022-05-05 15:17:55.215433: train loss: 1.645387832670419
Eval step 0: eval loss: 0.8347344192488804
Eval: 2022-05-05 15:18:20.281049: total loss: 1.077461423494521, mse:4.816788738299837, ic :0.04408158609583767, sharpe5:10.718822481036186, irr5:326.0196838378906, ndcg5:0.8405686674084039, pnl5:3.026743173599243 
train 9, step: 0, loss: 5.440453828997209, grad_norm: 2.0892524599727995, ic: 0.11760425123215518
train 9, step: 500, loss: 1.3644634046052633, grad_norm: 1.8431392422949116, ic: 0.09637820400268794
train 9, step: 1000, loss: 0.9301322849317528, grad_norm: 0.04058368298552891, ic: 0.05441738235583582
train 9, step: 1500, loss: 1.108203196911819, grad_norm: 0.0221096692477795, ic: 0.1710226744419243
train 9, step: 2000, loss: 1.098357143234449, grad_norm: 0.1741099304672969, ic: 0.01592745564303568
Epoch 9: 2022-05-05 15:24:38.723135: train loss: 1.6438702473786628
Eval step 0: eval loss: 0.8331137396889817
Eval: 2022-05-05 15:25:04.468024: total loss: 1.0775061130165702, mse:4.77462311095849, ic :0.10530864449707668, sharpe5:10.909598670601845, irr5:365.82061767578125, ndcg5:0.8352329612887137, pnl5:2.9040751457214355 
train 10, step: 0, loss: 7.101160344159985, grad_norm: 2.0427875551263117, ic: 0.007297936670520344
train 10, step: 500, loss: 1.1345849149455456, grad_norm: 0.06077458566266157, ic: -0.024911623235934707
train 10, step: 1000, loss: 2.37887854078796, grad_norm: 0.7150510573840645, ic: -0.0037540660637104886
train 10, step: 1500, loss: 1.1043960769455154, grad_norm: 0.262058015076276, ic: -0.0317831194455435
train 10, step: 2000, loss: 2.8192350161474162, grad_norm: 0.4251141737977546, ic: 0.444361632461764
Epoch 10: 2022-05-05 15:31:19.211122: train loss: 1.6381071703162662
Eval step 0: eval loss: 0.8303917903920245
Eval: 2022-05-05 15:31:44.646822: total loss: 1.0740776106497854, mse:4.728116119984991, ic :0.13798120979981304, sharpe5:11.773497489094733, irr5:406.75665283203125, ndcg5:0.8399804506126244, pnl5:3.0908772945404053 
train 11, step: 0, loss: 1.2774456168733082, grad_norm: 0.0912485997202694, ic: 0.1312578079880754
train 11, step: 500, loss: 0.7000841277080803, grad_norm: 0.013849442849773, ic: 0.49942316907461515
train 11, step: 1000, loss: 0.9310494947315541, grad_norm: 0.1149433680125255, ic: 0.059255672992880634
train 11, step: 1500, loss: 1.0585543983861019, grad_norm: 0.04914402655764005, ic: 0.17321931742969723
train 11, step: 2000, loss: 0.7869492179786236, grad_norm: 0.004297534862775648, ic: 0.1485987762428063
Epoch 11: 2022-05-05 15:38:02.729673: train loss: 1.6357467127818994
Eval step 0: eval loss: 0.8343601045920047
Eval: 2022-05-05 15:38:28.832015: total loss: 1.0736122874531713, mse:4.70650870711043, ic :0.14465277366599266, sharpe5:11.840111469626427, irr5:390.198486328125, ndcg5:0.8455948679589834, pnl5:4.064830780029297 
train 12, step: 0, loss: 0.9932919343312581, grad_norm: 0.06458722932769527, ic: 0.2831121087397219
train 12, step: 500, loss: 0.940283523047195, grad_norm: 0.061559965032201854, ic: 0.03768748470019203
train 12, step: 1000, loss: 2.9674791469695463, grad_norm: 0.2976556738477131, ic: 0.4076867873577046
train 12, step: 1500, loss: 0.9529871246965442, grad_norm: 0.11740559724532319, ic: -0.13171385561726617
train 12, step: 2000, loss: 0.8752730908350642, grad_norm: 0.002951807773230574, ic: 0.1329442887515274
Epoch 12: 2022-05-05 15:44:42.678328: train loss: 1.6343064828890856
Eval step 0: eval loss: 0.8310563596960616
Eval: 2022-05-05 15:45:08.698073: total loss: 1.074188431562267, mse:4.722909622018394, ic :0.1445655188412102, sharpe5:11.612757278680801, irr5:403.86669921875, ndcg5:0.8461967546115775, pnl5:2.7885913848876953 
train 13, step: 0, loss: 2.0683890224887804, grad_norm: 0.9480875053970204, ic: 0.29160956397656
train 13, step: 500, loss: 0.8469579678440484, grad_norm: 0.1851592438927725, ic: 0.48653208053246516
train 13, step: 1000, loss: 0.9730812840813758, grad_norm: 0.3177877812730705, ic: 0.4348182605722768
train 13, step: 1500, loss: 2.3618937841530054, grad_norm: 0.2952271508652655, ic: 0.020993709599341156
train 13, step: 2000, loss: 1.468723311802348, grad_norm: 0.12363209074166502, ic: 0.16564675374244708
Epoch 13: 2022-05-05 15:51:20.299356: train loss: 1.6339879224187135
Eval step 0: eval loss: 0.8284808561849644
Eval: 2022-05-05 15:51:46.008455: total loss: 1.07301485633616, mse:4.72374307586975, ic :0.13955516572937868, sharpe5:13.1568797904253, irr5:433.42816162109375, ndcg5:0.8342864428305583, pnl5:3.7453339099884033 
train 14, step: 0, loss: 4.561259110520671, grad_norm: 1.678618307975741, ic: 0.1433634309942285
train 14, step: 500, loss: 0.827232524160216, grad_norm: 0.002940216812033071, ic: 0.12651698898478542
train 14, step: 1000, loss: 1.879292203516515, grad_norm: 1.0620871545276835, ic: 0.3476920141104033
train 14, step: 1500, loss: 1.1260727623381084, grad_norm: 0.06446981123998732, ic: -0.05016280233254712
train 14, step: 2000, loss: 1.1520765264736803, grad_norm: 0.21000565440065533, ic: 0.10300844248934543
Epoch 14: 2022-05-05 15:58:10.593301: train loss: 1.6302685082532598
Eval step 0: eval loss: 0.8315463131421232
Eval: 2022-05-05 15:58:37.828652: total loss: 1.071167655143145, mse:4.685844797893552, ic :0.16503584234055999, sharpe5:15.586479578018187, irr5:503.92254638671875, ndcg5:0.8487507172848676, pnl5:3.5710530281066895 
train 15, step: 0, loss: 3.39161220209144, grad_norm: 0.8950955197271672, ic: 0.05545042582082648
train 15, step: 500, loss: 1.2623270721308708, grad_norm: 0.06571976218654169, ic: -0.052931171793201816
train 15, step: 1000, loss: 1.3175600625635162, grad_norm: 0.13014611291793154, ic: 0.04570575106957289
train 15, step: 1500, loss: 0.8537361205093503, grad_norm: 0.23002235142027805, ic: 0.04643219036319468
train 15, step: 2000, loss: 1.46341487866093, grad_norm: 0.5266196092806853, ic: 0.04953255237059632
Epoch 15: 2022-05-05 16:05:24.659627: train loss: 1.6274958765461667
Eval step 0: eval loss: 0.8356174673998946
Eval: 2022-05-05 16:05:52.216707: total loss: 1.0729307979221852, mse:4.6955932293491545, ic :0.16387997372317983, sharpe5:16.092040926218033, irr5:526.669677734375, ndcg5:0.8357009008253468, pnl5:4.268214225769043 
train 16, step: 0, loss: 0.7051846828143556, grad_norm: 0.2319154933817598, ic: -0.10026205874292721
train 16, step: 500, loss: 1.5832298525178667, grad_norm: 0.5150849182322853, ic: 0.20311019552294848
train 16, step: 1000, loss: 0.8772237141927083, grad_norm: 0.019972775789011413, ic: -0.07372761843581946
train 16, step: 1500, loss: 0.859735727873818, grad_norm: 0.26207268674698125, ic: 0.1690807058034294
train 16, step: 2000, loss: 3.370020619903489, grad_norm: 0.962150417926089, ic: -0.017986214069940845
Epoch 16: 2022-05-05 16:12:29.795332: train loss: 1.6272053779545106
Eval step 0: eval loss: 0.8271969955010208
Eval: 2022-05-05 16:12:57.859422: total loss: 1.0688714999973021, mse:4.681640269459654, ic :0.17110865411258178, sharpe5:16.70429701924324, irr5:546.47705078125, ndcg5:0.8496076201299819, pnl5:6.250079154968262 
train 17, step: 0, loss: 1.2818069255636604, grad_norm: 0.27281786514873263, ic: -0.09549869572293042
train 17, step: 500, loss: 1.7556059715870596, grad_norm: 0.6786452987913125, ic: 0.1733063267551647
train 17, step: 1000, loss: 1.29755874908868, grad_norm: 0.2176098115158893, ic: 0.10318933440245001
train 17, step: 1500, loss: 4.524874001193974, grad_norm: 1.411265481633183, ic: 0.21039281014685024
train 17, step: 2000, loss: 1.272492304811804, grad_norm: 0.6542105473785582, ic: 0.05754537170439231
Epoch 17: 2022-05-05 16:19:43.747634: train loss: 1.6262892007822183
Eval step 0: eval loss: 0.8347884440447181
Eval: 2022-05-05 16:20:11.552061: total loss: 1.0725599719389762, mse:4.689191989347774, ic :0.16657715538924953, sharpe5:17.234429733753203, irr5:547.4697265625, ndcg5:0.8418561724108756, pnl5:4.56317138671875 
train 18, step: 0, loss: 1.4206651497037275, grad_norm: 0.7831360548571463, ic: 0.16550340324804594
train 18, step: 500, loss: 1.515717414812853, grad_norm: 0.6435831670955078, ic: -0.004058958433231796
train 18, step: 1000, loss: 0.6691431667380137, grad_norm: 0.007736562066954592, ic: 0.47207341242317735
train 18, step: 1500, loss: 1.4207404759162572, grad_norm: 0.0414924786716174, ic: 0.20919928438121166
train 18, step: 2000, loss: 0.9118453104784535, grad_norm: 0.006691744752311477, ic: -0.00977211816214281
Epoch 18: 2022-05-05 16:26:56.518070: train loss: 1.6260428829272147
Eval step 0: eval loss: 0.823847522474315
Eval: 2022-05-05 16:27:24.233398: total loss: 1.0679813226667116, mse:4.683303143213491, ic :0.1738940094858633, sharpe5:17.497391406297684, irr5:576.8645629882812, ndcg5:0.8324203400650866, pnl5:6.355162143707275 
train 19, step: 0, loss: 1.478727019779266, grad_norm: 0.8532588562330076, ic: 0.05103925236460801
train 19, step: 500, loss: 0.8607108504683882, grad_norm: 0.13199522309325581, ic: 0.25164566396208987
train 19, step: 1000, loss: 0.9582373083950266, grad_norm: 0.02295900537851492, ic: 0.19937923096192806
train 19, step: 1500, loss: 3.959323049345573, grad_norm: 1.00414668268051, ic: 0.15052204603850539
train 19, step: 2000, loss: 1.0162145057091345, grad_norm: 0.08670370562270736, ic: 0.22275555103378192
Epoch 19: 2022-05-05 16:34:16.099754: train loss: 1.6256824431640569
Eval step 0: eval loss: 0.8306687961011591
Eval: 2022-05-05 16:34:44.261485: total loss: 1.0706766516507749, mse:4.682377369645828, ic :0.16903997240360302, sharpe5:17.47614402294159, irr5:576.01025390625, ndcg5:0.8448445514122426, pnl5:5.171409606933594 
train 20, step: 0, loss: 2.2855501559412055, grad_norm: 0.7081503825129986, ic: 0.06649233465246972
train 20, step: 500, loss: 3.203822443181818, grad_norm: 0.42926842304817525, ic: 0.09397267438267863
train 20, step: 1000, loss: 0.9664878845214844, grad_norm: 0.09903237263461662, ic: 0.21315075720586257
train 20, step: 1500, loss: 1.9129539594898546, grad_norm: 0.6510711289350359, ic: 0.2722393740171283
train 20, step: 2000, loss: 1.0146785070223288, grad_norm: 0.061508513441057136, ic: 0.10018355853338934
Epoch 20: 2022-05-05 16:41:23.917477: train loss: 1.6255939393512835
Eval step 0: eval loss: 0.8301096394642387
Eval: 2022-05-05 16:41:49.819379: total loss: 1.0696838101789181, mse:4.675089392254242, ic :0.17475838201798882, sharpe5:17.4419696187973, irr5:567.4819946289062, ndcg5:0.8566642486410284, pnl5:8.026802062988281 
train 21, step: 0, loss: 1.0182924830758964, grad_norm: 0.3021854855621697, ic: 0.06662951904171399
train 21, step: 500, loss: 0.7695812124066648, grad_norm: 0.026415407491665502, ic: 0.18442800451526636
train 21, step: 1000, loss: 0.927210623757881, grad_norm: 0.4606452460899484, ic: 0.1749631914765903
train 21, step: 1500, loss: 0.99408437949964, grad_norm: 0.19654769158332072, ic: 0.3195909442064908
train 21, step: 2000, loss: 0.9377834788984634, grad_norm: 0.04632738609484717, ic: 0.09583619325146624
Epoch 21: 2022-05-05 16:48:22.276741: train loss: 1.6259199312905956
Eval step 0: eval loss: 0.8310355858757573
Eval: 2022-05-05 16:48:49.005540: total loss: 1.0718259963224823, mse:4.696831855162812, ic :0.17222032894868616, sharpe5:16.8663094997406, irr5:559.583251953125, ndcg5:0.8487080833399028, pnl5:7.757783889770508 
train 22, step: 0, loss: 1.050959355413577, grad_norm: 0.04141009920492908, ic: 0.18110685546854155
train 22, step: 500, loss: 3.243215867949695, grad_norm: 0.619863021494883, ic: -0.17971568278783495
train 22, step: 1000, loss: 1.2029065435332371, grad_norm: 0.09621855876544641, ic: 0.4415706061924561
train 22, step: 1500, loss: 0.9704419045781892, grad_norm: 0.050580073291800205, ic: 0.09873993620400758
train 22, step: 2000, loss: 1.7817283163265307, grad_norm: 0.7495786673347171, ic: 0.1704263657509131
Epoch 22: 2022-05-05 16:55:32.338629: train loss: 1.6258025514469174
Eval step 0: eval loss: 0.830607246423044
Eval: 2022-05-05 16:55:59.036014: total loss: 1.0707229073243008, mse:4.6833922065356175, ic :0.16825609006347564, sharpe5:17.024016126394272, irr5:558.3561401367188, ndcg5:0.8357002033174897, pnl5:5.9927897453308105 
train 23, step: 0, loss: 0.9885237020443084, grad_norm: 0.05734951670265882, ic: 0.19615459298943572
train 23, step: 500, loss: 1.4249315486410912, grad_norm: 0.09224513471875578, ic: 0.00931581260837965
train 23, step: 1000, loss: 1.6545225016276042, grad_norm: 0.06683107122063121, ic: 0.2519098838495741
train 23, step: 1500, loss: 1.1073186351808326, grad_norm: 0.16481135865644078, ic: 0.09443146237785341
train 23, step: 2000, loss: 1.911438411157621, grad_norm: 1.1084974890529666, ic: 0.33307485347084376
Epoch 23: 2022-05-05 17:02:34.120349: train loss: 1.6240805847212751
Eval step 0: eval loss: 0.8343902041211143
Eval: 2022-05-05 17:02:59.538098: total loss: 1.0704406948856573, mse:4.659256156191005, ic :0.17694452186763768, sharpe5:16.754043252468108, irr5:545.5883178710938, ndcg5:0.8497086106780503, pnl5:6.512710094451904 
train 24, step: 0, loss: 2.1958456196706333, grad_norm: 0.17113984314345, ic: 0.15157911052488737
train 24, step: 500, loss: 1.2254772616731517, grad_norm: 0.23445374832949267, ic: 0.1500221135925526
train 24, step: 1000, loss: 0.923028115240969, grad_norm: 0.03502485508920439, ic: 0.3915764096891513
train 24, step: 1500, loss: 2.5960549451257213, grad_norm: 0.8581290128671468, ic: 0.04209994939419251
train 24, step: 2000, loss: 0.9353363609158516, grad_norm: 0.023838035258148097, ic: 0.0835072573276604
Epoch 24: 2022-05-05 17:09:12.970726: train loss: 1.6195978889587674
Eval step 0: eval loss: 0.8279669131569415
Eval: 2022-05-05 17:09:39.237821: total loss: 1.0705764472620705, mse:4.658454256504382, ic :0.17543992466041686, sharpe5:17.491691257953644, irr5:562.0538940429688, ndcg5:0.8555342813174482, pnl5:10.415749549865723 
train 25, step: 0, loss: 0.853360315271326, grad_norm: 0.12043362733401992, ic: 0.5709143840567229
train 25, step: 500, loss: 0.8663986528748112, grad_norm: 0.004515093196963107, ic: 0.22752367557824918
train 25, step: 1000, loss: 2.09271951458968, grad_norm: 0.26679358745811277, ic: 0.2510235704372492
train 25, step: 1500, loss: 1.1497403817183272, grad_norm: 0.3318856442992837, ic: 0.46482125781745237
train 25, step: 2000, loss: 1.04621729430517, grad_norm: 0.3410854435237625, ic: 0.541722768688169
Epoch 25: 2022-05-05 17:15:59.573063: train loss: 1.6203591598381908
Eval step 0: eval loss: 0.8293127737256322
Eval: 2022-05-05 17:16:26.172364: total loss: 1.06799712205333, mse:4.615193618789579, ic :0.19074098883734522, sharpe5:17.33968477964401, irr5:575.4486083984375, ndcg5:0.8594596634326489, pnl5:4.330198287963867 
train 26, step: 0, loss: 6.731265912040734, grad_norm: 1.3553593065422929, ic: 0.1304840031614149
train 26, step: 500, loss: 3.918784206411259, grad_norm: 1.3739516904228144, ic: 0.3694350248790966
train 26, step: 1000, loss: 1.2575879059556516, grad_norm: 0.8173194852584771, ic: -0.01492667736117936
train 26, step: 1500, loss: 0.8386889807314263, grad_norm: 0.2363702930179021, ic: 0.29786932150767487
train 26, step: 2000, loss: 0.9569038973372345, grad_norm: 0.2731530177614141, ic: 0.13676849394068114
Epoch 26: 2022-05-05 17:22:56.976248: train loss: 1.6193555668707327
Eval step 0: eval loss: 0.8287869323794783
Eval: 2022-05-05 17:23:23.618958: total loss: 1.0678907057160187, mse:4.603780514110786, ic :0.18401842003065005, sharpe5:17.365803680419923, irr5:568.0816650390625, ndcg5:0.8521666085934555, pnl5:6.418360233306885 
train 27, step: 0, loss: 0.8302883731617646, grad_norm: 0.03017832564629052, ic: 0.07129578283446339
train 27, step: 500, loss: 0.9513209344890974, grad_norm: 0.6784631284734346, ic: 0.2567763594751115
train 27, step: 1000, loss: 0.7568208321249034, grad_norm: 0.44848279930622625, ic: 0.19871628018063864
train 27, step: 1500, loss: 0.6365060899670157, grad_norm: 0.023454141002415455, ic: 0.5098671030050639
train 27, step: 2000, loss: 1.3797743673476652, grad_norm: 0.008620500727673716, ic: 0.029428676356603522
Epoch 27: 2022-05-05 17:29:49.852789: train loss: 1.6184944790246927
Eval step 0: eval loss: 0.8348651078026211
Eval: 2022-05-05 17:30:17.309738: total loss: 1.0699561207044, mse:4.603669703023244, ic :0.18393799986913792, sharpe5:17.11104507565498, irr5:555.3351440429688, ndcg5:0.8501573778437169, pnl5:4.931079864501953 
train 28, step: 0, loss: 1.5329086480453669, grad_norm: 0.1358365624993507, ic: 0.23537079215658196
train 28, step: 500, loss: 1.3693990947282002, grad_norm: 0.2674660095986263, ic: 0.17724682770338734
train 28, step: 1000, loss: 0.9121460126344427, grad_norm: 0.17899667842370975, ic: 0.5648214599717348
train 28, step: 1500, loss: 1.030901146379662, grad_norm: 0.01694120174799418, ic: 0.056443364469259245
train 28, step: 2000, loss: 1.036914848842504, grad_norm: 0.06445507399778656, ic: 0.1582018158236314
Epoch 28: 2022-05-05 17:36:35.929296: train loss: 1.6153117349576613
Eval step 0: eval loss: 0.8243893139982876
Eval: 2022-05-05 17:37:02.617944: total loss: 1.0723666680763515, mse:4.6465390956567845, ic :0.177896197919518, sharpe5:16.88389047503471, irr5:548.8306884765625, ndcg5:0.8350319476063687, pnl5:5.765924453735352 
train 29, step: 0, loss: 0.9047647032256918, grad_norm: 0.015651711372473995, ic: 0.09183862646970507
train 29, step: 500, loss: 1.1043369295452876, grad_norm: 0.2334680084304746, ic: 0.6168863655609426
train 29, step: 1000, loss: 1.0410772691025518, grad_norm: 0.26105591540088136, ic: 0.08552585095008194
train 29, step: 1500, loss: 2.3152899879854605, grad_norm: 0.1577985127311897, ic: -0.04528758053956175
train 29, step: 2000, loss: 4.203248954113619, grad_norm: 6.641680490355425, ic: 0.21934062141695368
Epoch 29: 2022-05-05 17:43:33.018834: train loss: 1.6149458907396896
Eval step 0: eval loss: 0.833773292404834
Eval: 2022-05-05 17:43:59.799372: total loss: 1.0684112224349391, mse:4.602582843851789, ic :0.18842039008836492, sharpe5:17.628335902690885, irr5:591.0686645507812, ndcg5:0.8520173436014754, pnl5:5.692540168762207 
train 30, step: 0, loss: 1.021722538547552, grad_norm: 0.040872780494412495, ic: 0.5005857718497271
train 30, step: 500, loss: 1.4163484894124283, grad_norm: 0.9756321255039295, ic: -0.009838347350074045
train 30, step: 1000, loss: 0.9705386999881629, grad_norm: 0.025314827013248855, ic: -0.040448593074858805
train 30, step: 1500, loss: 1.5214038229780396, grad_norm: 0.9859986656237636, ic: 0.13916056782910668
train 30, step: 2000, loss: 1.8267584102429226, grad_norm: 0.4241726179842181, ic: 0.11174051517143496
Epoch 30: 2022-05-05 17:50:30.314999: train loss: 1.6123749269874226
Eval step 0: eval loss: 0.8321919094523841
Eval: 2022-05-05 17:50:56.842132: total loss: 1.068474509678147, mse:4.599558060892502, ic :0.18940157641419347, sharpe5:18.053291137218473, irr5:612.8522338867188, ndcg5:0.856282953715444, pnl5:7.518837928771973 
train 31, step: 0, loss: 1.0296460740992521, grad_norm: 0.09188464283712565, ic: 0.3759136034632297
train 31, step: 500, loss: 1.511572667502572, grad_norm: 0.4747220161753567, ic: 0.04414567372678705
train 31, step: 1000, loss: 4.419673350897736, grad_norm: 3.389937270956545, ic: 0.45969060093976244
train 31, step: 1500, loss: 0.7705202470924185, grad_norm: 0.1330388735789405, ic: 0.7137518108699097
train 31, step: 2000, loss: 1.2268886392239742, grad_norm: 0.4615209878391458, ic: 0.18508943416761575
Epoch 31: 2022-05-05 17:57:18.316033: train loss: 1.6107203794318292
Eval step 0: eval loss: 0.8366304323218519
Eval: 2022-05-05 17:57:45.472905: total loss: 1.0719740933401616, mse:4.618973284716735, ic :0.1629276967964186, sharpe5:14.610817744731902, irr5:482.32745361328125, ndcg5:0.8516483890283262, pnl5:3.7206339836120605 
train 32, step: 0, loss: 1.1272155333568785, grad_norm: 0.035703827979503625, ic: 0.17468924743275108
train 32, step: 500, loss: 1.464120363250492, grad_norm: 0.42800489107081097, ic: 0.10819157830426837
train 32, step: 1000, loss: 1.0362744231159224, grad_norm: 0.06954395533518692, ic: 0.5147456833748988
train 32, step: 1500, loss: 0.9891701006507437, grad_norm: 0.8466309789730448, ic: 0.09627213397631001
train 32, step: 2000, loss: 0.9426695007663094, grad_norm: 0.09532113996766613, ic: 0.5511995288335595
Epoch 32: 2022-05-05 18:04:12.177638: train loss: 1.608797354506542
Eval step 0: eval loss: 0.8230883454623287
Eval: 2022-05-05 18:04:39.791914: total loss: 1.065006721364014, mse:4.596791085812234, ic :0.18893348654231212, sharpe5:17.503717757463455, irr5:591.5398559570312, ndcg5:0.8516655312754939, pnl5:8.880752563476562 
train 33, step: 0, loss: 1.2558504108733801, grad_norm: 0.2226184851264471, ic: 0.21331258806611336
train 33, step: 500, loss: 0.996410069709702, grad_norm: 0.07692925679248076, ic: 0.16799758855560887
train 33, step: 1000, loss: 1.072919675902666, grad_norm: 0.318099639870004, ic: 0.19905064966151698
train 33, step: 1500, loss: 0.8826748631885751, grad_norm: 0.04873824118653565, ic: 0.5648288725654846
train 33, step: 2000, loss: 0.8118673043470187, grad_norm: 0.07815491572118657, ic: 0.2866957060702834
Epoch 33: 2022-05-05 18:11:16.395658: train loss: 1.6113068740810215
Eval step 0: eval loss: 0.8288755587707455
Eval: 2022-05-05 18:11:44.100086: total loss: 1.0672807246008431, mse:4.588303570879491, ic :0.1886489168995059, sharpe5:17.772793893814086, irr5:585.785888671875, ndcg5:0.8525332741604918, pnl5:4.8232903480529785 
train 34, step: 0, loss: 1.0130439032952376, grad_norm: 0.39795922188911553, ic: 0.6031233566090737
train 34, step: 500, loss: 0.8004511561962443, grad_norm: 0.08892402384095456, ic: 0.271001060669777
train 34, step: 1000, loss: 3.1035175001200077, grad_norm: 1.0884310060032127, ic: 0.3404721880416753
train 34, step: 1500, loss: 0.812128987727205, grad_norm: 0.2387361356764874, ic: 0.6907695386113737
train 34, step: 2000, loss: 5.601691851036019, grad_norm: 45.473080995913264, ic: 0.425107013807075
Epoch 34: 2022-05-05 18:18:11.467213: train loss: 1.6094736511410832
Eval step 0: eval loss: 0.823958144675316
Eval: 2022-05-05 18:18:38.584115: total loss: 1.0690579752261995, mse:4.615551117141458, ic :0.18726684490490628, sharpe5:17.806686412096024, irr5:593.13134765625, ndcg5:0.8430631246191943, pnl5:5.684762477874756 
train 35, step: 0, loss: 1.1811664895450367, grad_norm: 0.7028546860470984, ic: 0.5624012335249399
train 35, step: 500, loss: 1.1809076042518398, grad_norm: 0.42313957299778054, ic: 0.1140098505666684
train 35, step: 1000, loss: 1.6862710033505839, grad_norm: 1.3396644853643842, ic: 0.047157843766566815
train 35, step: 1500, loss: 1.6439060297227444, grad_norm: 0.8771559658291965, ic: 0.01928001670188221
train 35, step: 2000, loss: 0.7878834117542614, grad_norm: 0.24688200941166052, ic: 0.5610705103616758
Epoch 35: 2022-05-05 18:25:12.979069: train loss: 1.6107615169207974
Eval step 0: eval loss: 0.8293280164358864
Eval: 2022-05-05 18:25:40.041171: total loss: 1.0672774991165916, mse:4.587567039603472, ic :0.18865289380540884, sharpe5:17.689219465255736, irr5:579.1690063476562, ndcg5:0.8549271019497708, pnl5:4.535382270812988 
train 36, step: 0, loss: 1.8202858629930339, grad_norm: 0.6089969154550658, ic: 0.12894056329832376
train 36, step: 500, loss: 0.844368912837372, grad_norm: 0.12163487093548829, ic: 0.10140318537415328
train 36, step: 1000, loss: 1.7302499999999998, grad_norm: 1.5490927792372466, ic: 0.22165423250399574
train 36, step: 1500, loss: 0.7651784399874106, grad_norm: 0.041491055835588514, ic: 0.39880893663003114
train 36, step: 2000, loss: 1.1134590712495036, grad_norm: 0.32859617793764934, ic: 0.779286576399791
Epoch 36: 2022-05-05 18:32:20.681913: train loss: 1.6094983585697717
Eval step 0: eval loss: 0.8282390952235905
Eval: 2022-05-05 18:32:47.387441: total loss: 1.0669895413453436, mse:4.589466328534004, ic :0.19057966597170298, sharpe5:17.83250450849533, irr5:598.5140380859375, ndcg5:0.8459910815873265, pnl5:6.558335304260254 
train 37, step: 0, loss: 2.0279935454153906, grad_norm: 0.9318749648566997, ic: 0.19659540839375614
train 37, step: 500, loss: 2.3458024938188213, grad_norm: 0.5743155637724052, ic: -0.018056977905698525
train 37, step: 1000, loss: 1.0729350608233448, grad_norm: 0.13309715349791856, ic: 0.13172358398393505
train 37, step: 1500, loss: 2.023753886320153, grad_norm: 0.6358683368340388, ic: 0.595900298995947
train 37, step: 2000, loss: 1.317473249974045, grad_norm: 0.06384965151122121, ic: 0.1584645567390273
Epoch 37: 2022-05-05 18:39:23.415964: train loss: 1.610090920499755
Eval step 0: eval loss: 0.8252593061569744
Eval: 2022-05-05 18:39:49.866870: total loss: 1.0685815192877202, mse:4.6054712561878945, ic :0.18746800008042594, sharpe5:17.78342420697212, irr5:592.2267456054688, ndcg5:0.8527788823423742, pnl5:4.906790256500244 
train 38, step: 0, loss: 1.3152002474156823, grad_norm: 0.29517645024287387, ic: -0.0565305185603659
train 38, step: 500, loss: 0.9122825339988426, grad_norm: 0.14302468382588476, ic: 0.2630634167142395
train 38, step: 1000, loss: 0.9104093649641798, grad_norm: 0.20055304307138605, ic: 0.03358023886472472
train 38, step: 1500, loss: 0.9576715398395976, grad_norm: 0.21070279242309511, ic: 0.20388110923012548
train 38, step: 2000, loss: 2.324017020510763, grad_norm: 1.1816041033091116, ic: 0.0006995530861886971
Epoch 38: 2022-05-05 18:46:20.145444: train loss: 1.6105007191074971
Eval step 0: eval loss: 0.8249352216971811
Eval: 2022-05-05 18:46:46.957841: total loss: 1.0649265957349525, mse:4.582125689799276, ic :0.1958294969071188, sharpe5:18.535853247642518, irr5:628.177734375, ndcg5:0.8643006584931148, pnl5:6.724369525909424 
train 39, step: 0, loss: 0.9668999068233946, grad_norm: 0.3730663532163902, ic: 0.12124928310430426
train 39, step: 500, loss: 0.8842411261169293, grad_norm: 0.1764079934824182, ic: 0.22390405257695947
train 39, step: 1000, loss: 0.9530742380878713, grad_norm: 0.02203434169418247, ic: 0.17350930546325222
train 39, step: 1500, loss: 2.0560643215753767, grad_norm: 0.1389362217659949, ic: 0.21903967616545794
train 39, step: 2000, loss: 0.6206826063882306, grad_norm: 0.03623716916101507, ic: 0.11828637624863239
Epoch 39: 2022-05-05 18:53:10.032343: train loss: 1.6135825999735325
Eval step 0: eval loss: 0.8321474033110511
Eval: 2022-05-05 18:53:36.858014: total loss: 1.080798721745548, mse:4.6894178994603575, ic :0.18463675561325496, sharpe5:18.331414107084274, irr5:601.8008422851562, ndcg5:0.8575027337281533, pnl5:5.3451995849609375 
