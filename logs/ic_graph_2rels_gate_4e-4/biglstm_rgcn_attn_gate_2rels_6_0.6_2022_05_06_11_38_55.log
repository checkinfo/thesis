Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_60_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
52596
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795796887216575, grad_norm: 4.816844966158497, ic: 0.02272751325747671
train 0, step: 500, loss: 0.8631232117205162, grad_norm: 0.026555770123779885, ic: 0.045078222437103604
train 0, step: 1000, loss: 1.9500365407255853, grad_norm: 0.5127221206738346, ic: 0.015381835862266347
train 0, step: 1500, loss: 0.956382878118824, grad_norm: 0.04903256114455948, ic: 0.015260634862826259
train 0, step: 2000, loss: 1.002225124724397, grad_norm: 0.15817025686694167, ic: 0.025730364839411306
Epoch 0: 2022-05-06 23:48:27.161360: train loss: 1.648489922441594
Eval step 0: eval loss: 0.8363103997217465
Eval: 2022-05-06 23:49:02.913150: total loss: 1.0793453602456662, mse:4.822857459775748, ic :0.00812621110636699, sharpe5:7.918176238238811, irr5:224.24533081054688, ndcg5:0.8488503790933507, pnl5:2.6108968257904053 
train 1, step: 0, loss: 2.774781060987903, grad_norm: 0.8787030133752571, ic: 0.057225233862207944
train 1, step: 500, loss: 1.7554755718022341, grad_norm: 0.7706763243651429, ic: 0.09691706815666487
train 1, step: 1000, loss: 0.8777566097129997, grad_norm: 0.1754059140921697, ic: 0.0799903194888797
train 1, step: 1500, loss: 1.7132767600574712, grad_norm: 0.20671466750697226, ic: -0.031915432767572846
train 1, step: 2000, loss: 2.177276171875, grad_norm: 0.8876133026817774, ic: -0.045832477649571005
Epoch 1: 2022-05-06 23:56:43.453473: train loss: 1.6467366978751168
Eval step 0: eval loss: 0.8345945979320336
Eval: 2022-05-06 23:57:18.688803: total loss: 1.0789899875706208, mse:4.8235324659113195, ic :0.0076362975460589275, sharpe5:7.522730575203895, irr5:212.41702270507812, ndcg5:0.8564565559077469, pnl5:2.5319998264312744 
train 2, step: 0, loss: 2.1417819602272727, grad_norm: 0.009307379017950422, ic: 0.13195845444207382
train 2, step: 500, loss: 3.300098114730047, grad_norm: 0.28490298560827765, ic: 0.04889414270634479
train 2, step: 1000, loss: 2.0723239942528737, grad_norm: 0.00020669965001924044, ic: 0.18944545072876903
train 2, step: 1500, loss: 1.4855516273556775, grad_norm: 0.059742311223735634, ic: -0.03838098689917162
train 2, step: 2000, loss: 3.2352471454326923, grad_norm: 0.7878169009744455, ic: 0.2082580456256617
Epoch 2: 2022-05-07 00:04:58.110354: train loss: 1.6464908291154494
Eval step 0: eval loss: 0.8358901639884088
Eval: 2022-05-07 00:05:32.569446: total loss: 1.0795028276019425, mse:4.823286352122432, ic :0.009883924746504793, sharpe5:7.462779222428798, irr5:211.67379760742188, ndcg5:0.856035086885774, pnl5:2.8721470832824707 
train 3, step: 0, loss: 1.522950806656504, grad_norm: 0.52488508646768, ic: -0.001829716822635592
train 3, step: 500, loss: 1.5013962090715545, grad_norm: 0.34266370546193525, ic: 0.09354722350908817
train 3, step: 1000, loss: 3.6792672464198333, grad_norm: 0.7058087818071598, ic: -0.04961946934539847
train 3, step: 1500, loss: 1.9813142517389526, grad_norm: 1.2727747762139465, ic: -0.06512563394127845
train 3, step: 2000, loss: 0.8987171399915541, grad_norm: 0.0010633597080967486, ic: 0.011518998904369485
Epoch 3: 2022-05-07 00:13:16.987927: train loss: 1.6458464939623487
Eval step 0: eval loss: 0.8351230118875131
Eval: 2022-05-07 00:13:51.990969: total loss: 1.079417366596161, mse:4.8258624386232025, ic :0.012845150991537107, sharpe5:7.509661665558815, irr5:211.3848114013672, ndcg5:0.8402844411478912, pnl5:3.1801233291625977 
train 4, step: 0, loss: 1.4310184151785714, grad_norm: 0.044210717251684885, ic: 0.13057387982420776
train 4, step: 500, loss: 1.6481064683809055, grad_norm: 0.5604393438652273, ic: 0.05257419082981003
train 4, step: 1000, loss: 2.9618653088081173, grad_norm: 0.7876725368704258, ic: 0.0638865069727192
train 4, step: 1500, loss: 2.1499534381592826, grad_norm: 0.478306952480955, ic: 0.014979289745532795
train 4, step: 2000, loss: 1.081124550447123, grad_norm: 0.3951401347712222, ic: 0.23268310849967455
Epoch 4: 2022-05-07 00:21:25.936979: train loss: 1.6451961883382136
Eval step 0: eval loss: 0.8549596306144626
Eval: 2022-05-07 00:22:00.062405: total loss: 1.0870155205875898, mse:4.830000666415211, ic :0.047245760598046596, sharpe5:10.148375358581543, irr5:306.288330078125, ndcg5:0.8424035621997635, pnl5:3.2387778759002686 
train 5, step: 0, loss: 1.36701447147651, grad_norm: 0.20173129935654652, ic: 0.029828657560650736
train 5, step: 500, loss: 0.8882854319488501, grad_norm: 0.007686151521951479, ic: 0.04278317131585861
train 5, step: 1000, loss: 0.982586038523707, grad_norm: 0.1500424096605665, ic: -0.0009339213751022798
train 5, step: 1500, loss: 1.532329191292592, grad_norm: 0.1602816657224636, ic: 0.034849491443943545
train 5, step: 2000, loss: 1.1029459163373694, grad_norm: 0.030318500724561485, ic: 0.16054362677237288
Epoch 5: 2022-05-07 00:29:39.516590: train loss: 1.6433717343191832
Eval step 0: eval loss: 0.8447746702943888
Eval: 2022-05-07 00:30:14.705526: total loss: 1.0842914142034963, mse:4.792889412246161, ic :0.10666873390869927, sharpe5:11.427628265619278, irr5:374.1166687011719, ndcg5:0.8506650397983891, pnl5:2.9019320011138916 
train 6, step: 0, loss: 1.345530240729665, grad_norm: 0.45300548316565237, ic: 0.09807714633219183
train 6, step: 500, loss: 1.0061789449834897, grad_norm: 0.043081279863221536, ic: 0.05465327057032914
train 6, step: 1000, loss: 1.128651434024477, grad_norm: 0.1666286076100002, ic: 0.08540754361681033
train 6, step: 1500, loss: 1.5673420551394628, grad_norm: 0.7282615260331973, ic: 0.15555557361542463
train 6, step: 2000, loss: 0.8050080689551641, grad_norm: 0.05630829114641707, ic: 0.3540259110518304
Epoch 6: 2022-05-07 00:38:00.472327: train loss: 1.63430605117819
Eval step 0: eval loss: 0.824593707809207
Eval: 2022-05-07 00:38:33.783338: total loss: 1.0720289313069244, mse:4.7091600051010065, ic :0.15548984753574194, sharpe5:16.1154658305645, irr5:518.9098510742188, ndcg5:0.8603982923316662, pnl5:4.390408992767334 
train 7, step: 0, loss: 0.9932153701782227, grad_norm: 0.05577395093052939, ic: 0.03329263547304311
train 7, step: 500, loss: 0.6482327353990549, grad_norm: 0.00456123861345718, ic: 0.05571434547333564
train 7, step: 1000, loss: 1.0154938185750666, grad_norm: 0.25240551912776077, ic: 0.08744932338517226
train 7, step: 1500, loss: 2.2376459086950695, grad_norm: 0.7335873040998446, ic: 0.44978804553108176
train 7, step: 2000, loss: 0.918331608441034, grad_norm: 0.054533103888505116, ic: -0.041925411216859484
Epoch 7: 2022-05-07 00:46:26.606335: train loss: 1.6299343498955177
Eval step 0: eval loss: 0.8288889363392387
Eval: 2022-05-07 00:47:02.101816: total loss: 1.0723615448327264, mse:4.699399519566583, ic :0.15679259043456026, sharpe5:15.415783499479293, irr5:505.3514709472656, ndcg5:0.854509341191835, pnl5:3.9474904537200928 
train 8, step: 0, loss: 3.6096297554347827, grad_norm: 1.07284347234627, ic: 0.1627723946272927
train 8, step: 500, loss: 2.7570025319386398, grad_norm: 0.8833550849191103, ic: 0.04569822316490711
train 8, step: 1000, loss: 3.06201171875, grad_norm: 0.8946745523192712, ic: 0.1146788201573821
train 8, step: 1500, loss: 0.715170525152204, grad_norm: 0.05705039680383882, ic: 0.4393719124871879
train 8, step: 2000, loss: 1.089573711082894, grad_norm: 0.34808607856561186, ic: 0.5286599958273162
Epoch 8: 2022-05-07 00:54:44.185681: train loss: 1.628502059132752
Eval step 0: eval loss: 0.8240372524120785
Eval: 2022-05-07 00:55:19.947060: total loss: 1.070598791572473, mse:4.688663466530373, ic :0.16034578515840073, sharpe5:16.106555210351942, irr5:516.8078002929688, ndcg5:0.8552752987230381, pnl5:3.4673397541046143 
train 9, step: 0, loss: 5.426436185456539, grad_norm: 0.8608922910385304, ic: 0.1809181138086139
train 9, step: 500, loss: 1.3445615214349795, grad_norm: 1.3229208688342409, ic: 0.3261916998343332
train 9, step: 1000, loss: 0.9347021043398501, grad_norm: 0.04264858235380384, ic: 0.008139212616782944
train 9, step: 1500, loss: 1.0954041516131259, grad_norm: 0.018948721466849915, ic: 0.4242509856234954
train 9, step: 2000, loss: 1.0608636470482589, grad_norm: 0.2648027230910171, ic: 0.2758985108144083
Epoch 9: 2022-05-07 01:03:01.145918: train loss: 1.6281135546509653
Eval step 0: eval loss: 0.8238899705281875
Eval: 2022-05-07 01:03:34.090701: total loss: 1.0714874924198454, mse:4.693987645811611, ic :0.16339953290397438, sharpe5:17.383689243793487, irr5:561.7756958007812, ndcg5:0.8527985812929686, pnl5:6.977056980133057 
train 10, step: 0, loss: 7.080512310951166, grad_norm: 2.5496598018803307, ic: 0.252224139157674
train 10, step: 500, loss: 1.1232812423346743, grad_norm: 0.05125071376039089, ic: 0.06865545588284559
train 10, step: 1000, loss: 2.3907341518285086, grad_norm: 0.6811307603485232, ic: 0.1469496257774864
train 10, step: 1500, loss: 1.1009313409978694, grad_norm: 0.28079471843859527, ic: -0.017478545066439477
train 10, step: 2000, loss: 2.7383604658410903, grad_norm: 0.6108765035705319, ic: 0.4426373103784707
Epoch 10: 2022-05-07 01:11:28.773589: train loss: 1.6274765937212463
Eval step 0: eval loss: 0.8254813866570073
Eval: 2022-05-07 01:12:04.295084: total loss: 1.070188858834068, mse:4.675503748726932, ic :0.17154755611474107, sharpe5:17.903221914768217, irr5:585.9810180664062, ndcg5:0.8477857530832079, pnl5:7.927728176116943 
train 11, step: 0, loss: 1.2540007530029487, grad_norm: 0.033699572496217675, ic: 0.20714390645948402
train 11, step: 500, loss: 0.6587880205396642, grad_norm: 0.018205781105159455, ic: 0.5545888897740587
train 11, step: 1000, loss: 0.9363353505964036, grad_norm: 0.13109469692946085, ic: 0.04321180793913188
train 11, step: 1500, loss: 1.0528023703056468, grad_norm: 0.054072977510228215, ic: 0.18122702088518783
train 11, step: 2000, loss: 0.7870696491163112, grad_norm: 0.007253069621617997, ic: 0.14693171620653397
Epoch 11: 2022-05-07 01:19:43.235427: train loss: 1.6265886878664797
Eval step 0: eval loss: 0.830613677946358
Eval: 2022-05-07 01:20:18.982180: total loss: 1.0726496233977898, mse:4.678388367878748, ic :0.16717372608477368, sharpe5:17.191217955350876, irr5:560.5260620117188, ndcg5:0.842800080555581, pnl5:4.788243293762207 
train 12, step: 0, loss: 0.9641444683074951, grad_norm: 0.18359560509828465, ic: 0.40022141595033633
train 12, step: 500, loss: 0.9311963330518018, grad_norm: 0.06452525602551555, ic: 0.18187554598059733
train 12, step: 1000, loss: 2.9512068633061306, grad_norm: 0.29840601877046935, ic: 0.2143705376670172
train 12, step: 1500, loss: 0.9296204682442403, grad_norm: 0.12743235029773428, ic: -0.1331184052775442
train 12, step: 2000, loss: 0.8731220164687972, grad_norm: 0.004728393830369247, ic: 0.24512862184322484
Epoch 12: 2022-05-07 01:27:57.918009: train loss: 1.6247447025002102
Eval step 0: eval loss: 0.8289967286699815
Eval: 2022-05-07 01:28:31.351504: total loss: 1.069624459642652, mse:4.641312371937631, ic :0.17625431313069628, sharpe5:16.044264110326765, irr5:523.985595703125, ndcg5:0.8478957683831374, pnl5:3.804095506668091 
train 13, step: 0, loss: 2.0685174090054117, grad_norm: 0.6862872239555895, ic: 0.43042353733263145
train 13, step: 500, loss: 0.8303142719346862, grad_norm: 0.059416282067843204, ic: 0.5624584824818983
train 13, step: 1000, loss: 0.9535449546455537, grad_norm: 0.5407893904027273, ic: 0.5417807245540012
train 13, step: 1500, loss: 2.4220190829918034, grad_norm: 0.4660296509421547, ic: -0.05229927320613109
train 13, step: 2000, loss: 1.4467964201729695, grad_norm: 0.027333267266616293, ic: 0.202426600323184
Epoch 13: 2022-05-07 01:36:18.617266: train loss: 1.6239700036857039
Eval step 0: eval loss: 0.8244815420426106
Eval: 2022-05-07 01:36:53.443178: total loss: 1.0682879193122867, mse:4.60500781766424, ic :0.17930214459791297, sharpe5:16.274148652553556, irr5:550.791748046875, ndcg5:0.835488064771217, pnl5:6.95660400390625 
train 14, step: 0, loss: 4.4504284077613105, grad_norm: 1.4703520412038893, ic: 0.18293913200654457
train 14, step: 500, loss: 0.8287575631331231, grad_norm: 0.0039062023037472514, ic: 0.12705390277749437
train 14, step: 1000, loss: 1.8525646444333714, grad_norm: 0.7177797581791676, ic: 0.41212531577599776
train 14, step: 1500, loss: 1.126907133045526, grad_norm: 0.08080051831046599, ic: -0.04370443656924157
train 14, step: 2000, loss: 1.1572423523374058, grad_norm: 0.21319444717423122, ic: 0.07602582188068563
Epoch 14: 2022-05-07 01:44:34.762080: train loss: 1.6208812321433546
Eval step 0: eval loss: 0.8303638132656085
Eval: 2022-05-07 01:45:10.911668: total loss: 1.0706836539267401, mse:4.643117593286178, ic :0.1757902658363583, sharpe5:16.996860491037367, irr5:556.6158447265625, ndcg5:0.8487415037659165, pnl5:5.066361904144287 
train 15, step: 0, loss: 3.4370234982976657, grad_norm: 0.9586439792201062, ic: 0.1155942923022013
train 15, step: 500, loss: 1.2538194917978491, grad_norm: 0.01040686226342021, ic: 0.06415444558346543
train 15, step: 1000, loss: 1.3156696598704267, grad_norm: 0.12551596795428943, ic: 0.06147679979452743
train 15, step: 1500, loss: 0.8543339766855315, grad_norm: 0.21542136772538759, ic: 0.06569843922885263
train 15, step: 2000, loss: 1.4706201722272545, grad_norm: 0.8241743261501295, ic: 0.0489665458121378
Epoch 15: 2022-05-07 01:52:53.636872: train loss: 1.6201524171863224
Eval step 0: eval loss: 0.8381487863458245
Eval: 2022-05-07 01:53:28.545463: total loss: 1.0712334379646875, mse:4.5873809447794995, ic :0.18880492213495315, sharpe5:17.05601490139961, irr5:566.5106201171875, ndcg5:0.8350476148926264, pnl5:9.442150115966797 
train 16, step: 0, loss: 0.6942867421009006, grad_norm: 0.46544383658974026, ic: 0.025854214313859057
train 16, step: 500, loss: 1.6004203768114744, grad_norm: 0.4207810431349936, ic: 0.16620716529077625
train 16, step: 1000, loss: 0.8792548902107008, grad_norm: 0.017705664383000128, ic: -0.1111052967313973
train 16, step: 1500, loss: 0.84205323227566, grad_norm: 0.2352507096252764, ic: 0.2080191852736325
train 16, step: 2000, loss: 3.2810746058267957, grad_norm: 1.45202210212431, ic: 0.035186830954163154
Epoch 16: 2022-05-07 02:01:13.755319: train loss: 1.6190178628617846
Eval step 0: eval loss: 0.8246519130951988
Eval: 2022-05-07 02:01:47.758665: total loss: 1.0680278079365266, mse:4.592708406888256, ic :0.18749695632700641, sharpe5:17.559818719625472, irr5:558.7804565429688, ndcg5:0.82688950084477, pnl5:13.968194961547852 
train 17, step: 0, loss: 1.278868948938992, grad_norm: 0.30342598413147, ic: -0.13257330420023838
train 17, step: 500, loss: 1.7641678311314364, grad_norm: 0.8180912947669572, ic: 0.2045064116197764
train 17, step: 1000, loss: 1.2889414652786984, grad_norm: 0.09546382606132539, ic: 0.15180261069336298
train 17, step: 1500, loss: 4.486661777874724, grad_norm: 1.354054969489461, ic: 0.229747346507103
train 17, step: 2000, loss: 1.2769462688631166, grad_norm: 1.075265261939691, ic: 0.07342096916327827
Epoch 17: 2022-05-07 02:09:45.708410: train loss: 1.6190751500374072
Eval step 0: eval loss: 0.8336235665420837
Eval: 2022-05-07 02:10:20.375579: total loss: 1.0692638666993737, mse:4.59105824818734, ic :0.1891464159070463, sharpe5:17.614027830362318, irr5:581.241455078125, ndcg5:0.858285974296698, pnl5:10.824599266052246 
train 18, step: 0, loss: 1.417773206997443, grad_norm: 0.9199189485448114, ic: 0.25189867746735006
train 18, step: 500, loss: 1.5112415033545197, grad_norm: 1.8524738420351692, ic: 0.04967349454947461
train 18, step: 1000, loss: 0.6537524079623287, grad_norm: 0.01464212795086563, ic: 0.5730279981992469
train 18, step: 1500, loss: 1.4172661208794022, grad_norm: 0.08883260156743708, ic: 0.22877011528486085
train 18, step: 2000, loss: 0.9093067144892019, grad_norm: 0.011003061478702045, ic: -0.017792223942026952
Epoch 18: 2022-05-07 02:18:03.862499: train loss: 1.6189811354681436
Eval step 0: eval loss: 0.820193645449157
Eval: 2022-05-07 02:18:38.690729: total loss: 1.068822167060704, mse:4.704260471660728, ic :0.1770149236502413, sharpe5:18.093510006666182, irr5:600.2597045898438, ndcg5:0.8433105369855437, pnl5:9.205811500549316 
train 19, step: 0, loss: 1.4837613544766866, grad_norm: 0.9915534329331317, ic: 0.06880617061789092
train 19, step: 500, loss: 0.8595915193910951, grad_norm: 0.022569568101673945, ic: 0.22461776455508786
train 19, step: 1000, loss: 0.9564460169300494, grad_norm: 0.014787156824416406, ic: 0.206842972777862
train 19, step: 1500, loss: 3.9476574852863666, grad_norm: 1.3672475575757181, ic: 0.15018377677741368
train 19, step: 2000, loss: 1.0146755746694711, grad_norm: 0.46007612088282407, ic: 0.2294779926095903
Epoch 19: 2022-05-07 02:26:26.362531: train loss: 1.6191617104340124
Eval step 0: eval loss: 0.8341125552596482
Eval: 2022-05-07 02:27:01.766122: total loss: 1.0703662343010423, mse:4.643693715037476, ic :0.17764352724648977, sharpe5:17.923759853839872, irr5:595.115478515625, ndcg5:0.8433411881845542, pnl5:8.355977058410645 
train 20, step: 0, loss: 2.3329329298418973, grad_norm: 3.3306427112461794, ic: 0.05071979586495392
train 20, step: 500, loss: 3.2236874999999996, grad_norm: 1.4055315727010749, ic: 0.11360270267029048
train 20, step: 1000, loss: 0.9623396873474122, grad_norm: 0.09332887706491239, ic: 0.22365333082452493
train 20, step: 1500, loss: 1.6789896615979136, grad_norm: 1.424227961629996, ic: 0.26222415142382344
train 20, step: 2000, loss: 1.0425392138432912, grad_norm: 0.06724793511189092, ic: -0.012445907016617054
Epoch 20: 2022-05-07 02:35:08.027439: train loss: 1.6164669219735883
Eval step 0: eval loss: 0.8312339983699947
Eval: 2022-05-07 02:35:43.033033: total loss: 1.0667215880193994, mse:4.581120814159782, ic :0.19257507600956086, sharpe5:18.11324390888214, irr5:592.9833984375, ndcg5:0.8568312058413416, pnl5:7.1064677238464355 
train 21, step: 0, loss: 1.002019234780225, grad_norm: 0.6571134789480182, ic: 0.04065726452779078
train 21, step: 500, loss: 0.7668600166793418, grad_norm: 0.011097482753769145, ic: 0.2132908026137646
train 21, step: 1000, loss: 0.9397217599969161, grad_norm: 1.9197071709737508, ic: 0.16647822938824466
train 21, step: 1500, loss: 0.9861680010911626, grad_norm: 0.25754967183070304, ic: 0.3254858137377038
train 21, step: 2000, loss: 0.9412100993649639, grad_norm: 0.16084523839289186, ic: 0.06879227563219663
Epoch 21: 2022-05-07 02:43:37.384693: train loss: 1.6190134908591307
Eval step 0: eval loss: 0.8246773176122892
Eval: 2022-05-07 02:44:13.108700: total loss: 1.0665776532806863, mse:4.5913431910240545, ic :0.1889632330377558, sharpe5:17.409391937255858, irr5:579.4560546875, ndcg5:0.8353996439631994, pnl5:5.771239757537842 
train 22, step: 0, loss: 1.0363335043697033, grad_norm: 0.025262554626075004, ic: 0.22678867397913463
train 22, step: 500, loss: 3.2472469670985773, grad_norm: 1.0032920837169845, ic: -0.152730385403961
train 22, step: 1000, loss: 1.1942377167630058, grad_norm: 0.01905585359388082, ic: 0.4611841048672955
train 22, step: 1500, loss: 0.9760448816872428, grad_norm: 0.23368878435087825, ic: 0.09638050969358578
train 22, step: 2000, loss: 1.760192040683461, grad_norm: 8.906930792171535, ic: 0.1636641005875895
Epoch 22: 2022-05-07 02:51:56.477784: train loss: 1.6167451142588498
Eval step 0: eval loss: 0.8274776671784444
Eval: 2022-05-07 02:52:31.253766: total loss: 1.0661930497369696, mse:4.589791397997673, ic :0.18909899988627332, sharpe5:17.50797442317009, irr5:571.9673461914062, ndcg5:0.8347916853291589, pnl5:10.624345779418945 
train 23, step: 0, loss: 0.9792699984239914, grad_norm: 0.06694010438347013, ic: 0.16237898330609452
train 23, step: 500, loss: 1.4140849210778061, grad_norm: 0.2281377932849584, ic: 0.08448698085746684
train 23, step: 1000, loss: 1.647933553059896, grad_norm: 0.08693080931015032, ic: 0.2581274371924821
train 23, step: 1500, loss: 1.1288402964313125, grad_norm: 1.705690585985479, ic: 0.07147089230540106
train 23, step: 2000, loss: 1.847518674220554, grad_norm: 2.3076250571633405, ic: 0.42775769839752054
Epoch 23: 2022-05-07 03:00:44.708501: train loss: 1.6166280058402633
Eval step 0: eval loss: 0.8303121038181638
Eval: 2022-05-07 03:01:16.900814: total loss: 1.0672135503616076, mse:4.5863793803773865, ic :0.18970288736473107, sharpe5:17.436545680761338, irr5:577.746826171875, ndcg5:0.8547640172941636, pnl5:5.817056179046631 
train 24, step: 0, loss: 2.18806899040714, grad_norm: 0.08081086805077943, ic: 0.15964337207820722
train 24, step: 500, loss: 1.2159548273346303, grad_norm: 0.04608390475343176, ic: 0.15560463379616835
train 24, step: 1000, loss: 0.9100303057478056, grad_norm: 0.11944140597478399, ic: 0.5177611407788638
train 24, step: 1500, loss: 2.609477245208162, grad_norm: 2.0890734205283206, ic: 0.029802419388819362
train 24, step: 2000, loss: 0.9346529860368276, grad_norm: 0.050406703153491564, ic: 0.05772451614530144
Epoch 24: 2022-05-07 03:09:01.431072: train loss: 1.612045020851329
Eval step 0: eval loss: 0.8239099725656941
Eval: 2022-05-07 03:09:36.614109: total loss: 1.0673351124639283, mse:4.605547451335382, ic :0.18821338239437885, sharpe5:17.375342395305633, irr5:587.611328125, ndcg5:0.854240420169974, pnl5:7.225128650665283 
train 25, step: 0, loss: 0.8332545924831082, grad_norm: 0.086778928936547, ic: 0.6150466642544337
train 25, step: 500, loss: 0.8691094620350265, grad_norm: 0.03065445824478096, ic: 0.23135973431590368
train 25, step: 1000, loss: 2.1098177256878334, grad_norm: 0.11730970496955813, ic: 0.24084487694355355
train 25, step: 1500, loss: 1.1351034244439278, grad_norm: 0.4137771061593335, ic: 0.5258072729863909
train 25, step: 2000, loss: 1.0125398655718685, grad_norm: 0.42047371172541914, ic: 0.5841331813198636
Epoch 25: 2022-05-07 03:17:33.051801: train loss: 1.614689120010047
Eval step 0: eval loss: 0.8231938224446785
Eval: 2022-05-07 03:18:07.462217: total loss: 1.0646901774292217, mse:4.582068249905119, ic :0.19497716500069082, sharpe5:18.515814757347105, irr5:621.6174926757812, ndcg5:0.841250571221582, pnl5:13.635632514953613 
train 26, step: 0, loss: 6.606362632288338, grad_norm: 1.0984917020449614, ic: 0.18356690619504612
train 26, step: 500, loss: 3.8855490129007038, grad_norm: 2.472794703431335, ic: 0.31882176348807706
train 26, step: 1000, loss: 1.2557076973668073, grad_norm: 1.4110083906544881, ic: 0.029355076907935533
train 26, step: 1500, loss: 0.8364284291960412, grad_norm: 0.18195550654381415, ic: 0.3045078592928227
train 26, step: 2000, loss: 0.956788069802321, grad_norm: 0.4824691620814652, ic: 0.13788582537119534
Epoch 26: 2022-05-07 03:26:11.307826: train loss: 1.6148793395332814
Eval step 0: eval loss: 0.8258865083105571
Eval: 2022-05-07 03:26:44.282791: total loss: 1.065421584536611, mse:4.585235221086504, ic :0.19235654984867515, sharpe5:17.954559195041657, irr5:601.251953125, ndcg5:0.8599483468030721, pnl5:6.6058759689331055 
train 27, step: 0, loss: 0.8239813112745098, grad_norm: 0.01335190835757195, ic: 0.15050560767537008
train 27, step: 500, loss: 0.9385972297054598, grad_norm: 1.6178799090295448, ic: 0.2699830575234961
train 27, step: 1000, loss: 0.750776416279969, grad_norm: 0.7562328596977487, ic: 0.18724719763410785
train 27, step: 1500, loss: 0.6403354316852558, grad_norm: 0.16067946869010932, ic: 0.5178524241275753
train 27, step: 2000, loss: 1.3856554313781322, grad_norm: 0.23430122377911805, ic: 0.039770014594547834
Epoch 27: 2022-05-07 03:34:45.134033: train loss: 1.6127217693307958
Eval step 0: eval loss: 0.8341223311750856
Eval: 2022-05-07 03:35:18.867646: total loss: 1.0674795823665353, mse:4.593161756973455, ic :0.18785917822748535, sharpe5:17.141467938423155, irr5:579.949951171875, ndcg5:0.8417831474352404, pnl5:7.621365547180176 
train 28, step: 0, loss: 1.5649572423986486, grad_norm: 4.215792932033576, ic: 0.2052674510436765
train 28, step: 500, loss: 1.3796781110257128, grad_norm: 6.522862249444114, ic: 0.18617055994907342
train 28, step: 1000, loss: 0.9136918230754573, grad_norm: 0.3051468629536279, ic: 0.5804434323757758
train 28, step: 1500, loss: 1.029256564290744, grad_norm: 0.09360944777685071, ic: 0.08728506132522731
train 28, step: 2000, loss: 1.0368846121009874, grad_norm: 0.28649033664130685, ic: 0.1476711518443275
Epoch 28: 2022-05-07 03:43:05.286002: train loss: 1.6096122764058818
Eval step 0: eval loss: 0.8256693157682429
Eval: 2022-05-07 03:43:40.480714: total loss: 1.0720966547636155, mse:4.675657098755989, ic :0.17667876575665337, sharpe5:17.558791444301605, irr5:574.16162109375, ndcg5:0.850088195899437, pnl5:4.402769088745117 
train 29, step: 0, loss: 0.9027826108202236, grad_norm: 0.14827706143528596, ic: 0.13639261487430776
train 29, step: 500, loss: 1.10178599690803, grad_norm: 0.1003054525650723, ic: 0.6130827486841353
train 29, step: 1000, loss: 1.0740628597467383, grad_norm: 0.9586864295461746, ic: 0.04971394856216664
train 29, step: 1500, loss: 2.363525009453064, grad_norm: 0.28627467178622895, ic: 0.0198844422571089
train 29, step: 2000, loss: 3.790296012972608, grad_norm: 10.276064958425424, ic: 0.20649223558938926
Epoch 29: 2022-05-07 03:51:23.467797: train loss: 1.6118256779850137
Eval step 0: eval loss: 0.8319432024458311
Eval: 2022-05-07 03:51:56.756336: total loss: 1.065650143468317, mse:4.579514159077638, ic :0.19496492229171947, sharpe5:18.04040006160736, irr5:613.165283203125, ndcg5:0.8591476352600407, pnl5:5.933465480804443 
train 30, step: 0, loss: 1.0078722707744823, grad_norm: 0.08196862116236739, ic: 0.5090991047603007
train 30, step: 500, loss: 1.4257937674643664, grad_norm: 1.7325088767095822, ic: 0.046323265063234934
train 30, step: 1000, loss: 0.9783038515033144, grad_norm: 0.14379768810071533, ic: -0.02361182488956766
train 30, step: 1500, loss: 1.491844369309052, grad_norm: 4.8469025873980796, ic: 0.1752572110016741
train 30, step: 2000, loss: 1.8487353627701797, grad_norm: 0.6469561790602827, ic: 0.08671466343383671
Epoch 30: 2022-05-07 03:59:49.822928: train loss: 1.6106903197243154
Eval step 0: eval loss: 0.8335004671858535
Eval: 2022-05-07 04:00:24.886429: total loss: 1.067602406818109, mse:4.6001504907571995, ic :0.19406851495305158, sharpe5:18.443061718940733, irr5:613.8812255859375, ndcg5:0.8602684113225985, pnl5:7.320585250854492 
train 31, step: 0, loss: 1.028314254625892, grad_norm: 0.1324749323334609, ic: 0.3780709024420539
train 31, step: 500, loss: 1.4990141943158435, grad_norm: 2.6706079402674403, ic: 0.005414027206386413
train 31, step: 1000, loss: 4.4420004970482045, grad_norm: 6.003327337317551, ic: 0.4594612565128921
train 31, step: 1500, loss: 0.7615491356576648, grad_norm: 0.08553408516516967, ic: 0.7160833045816266
train 31, step: 2000, loss: 1.2381616841695953, grad_norm: 3.078539350679592, ic: 0.1410227356203558
Epoch 31: 2022-05-07 04:08:23.042891: train loss: 1.610194990731023
Eval step 0: eval loss: 0.8283105494476093
Eval: 2022-05-07 04:08:58.391913: total loss: 1.0654926394140127, mse:4.582921633343185, ic :0.1905842209249288, sharpe5:17.553848036527633, irr5:583.4930419921875, ndcg5:0.8667304064945713, pnl5:6.160975456237793 
train 32, step: 0, loss: 1.1207635036596357, grad_norm: 0.084088483455836, ic: 0.23767361099779333
train 32, step: 500, loss: 1.4954741710752952, grad_norm: 1.9099824557950675, ic: 0.09906944491569816
train 32, step: 1000, loss: 1.0540763939856304, grad_norm: 0.25630274927098884, ic: 0.5106378829944007
train 32, step: 1500, loss: 0.9509073427667335, grad_norm: 2.898249051772086, ic: 0.08979819617438446
train 32, step: 2000, loss: 0.9409330298619679, grad_norm: 0.08843110686496361, ic: 0.55821550611399
Epoch 32: 2022-05-07 04:16:39.993166: train loss: 1.60795744964144
Eval step 0: eval loss: 0.8214832945040832
Eval: 2022-05-07 04:17:14.633175: total loss: 1.0641706922946403, mse:4.592185770075466, ic :0.19598057662558777, sharpe5:18.309385389089584, irr5:611.8878173828125, ndcg5:0.8545616796646907, pnl5:13.437899589538574 
train 33, step: 0, loss: 1.2649757160052195, grad_norm: 0.35321746864140535, ic: 0.2194771527911305
train 33, step: 500, loss: 0.9914551713796314, grad_norm: 0.0773627242828863, ic: 0.1716151427173249
train 33, step: 1000, loss: 1.0179724003341037, grad_norm: 8.296844422338, ic: 0.2128599972159466
train 33, step: 1500, loss: 0.8795607669780546, grad_norm: 0.08833828560671476, ic: 0.5533649917713868
train 33, step: 2000, loss: 0.8077606346263372, grad_norm: 0.2067754628736052, ic: 0.2661700714182297
Epoch 33: 2022-05-07 04:25:03.876432: train loss: 1.6059113632025777
Eval step 0: eval loss: 0.8301165855094178
Eval: 2022-05-07 04:25:39.618855: total loss: 1.0647921687929178, mse:4.575180388477571, ic :0.19489381975841066, sharpe5:17.38702836155891, irr5:596.2139282226562, ndcg5:0.8459023367333007, pnl5:6.9852070808410645 
train 34, step: 0, loss: 1.001769807750694, grad_norm: 0.5208930942248633, ic: 0.6073508890069886
train 34, step: 500, loss: 0.7906366284105026, grad_norm: 0.40589492039268965, ic: 0.31912194701836805
train 34, step: 1000, loss: 3.140465802311348, grad_norm: 1.2752051892421805, ic: 0.34497544511736156
train 34, step: 1500, loss: 0.8113377858685935, grad_norm: 0.4221317766634621, ic: 0.6910880411968098
train 34, step: 2000, loss: 5.013901075958559, grad_norm: 77.88021878585761, ic: 0.42676867204672286
Epoch 34: 2022-05-07 04:33:40.256348: train loss: 1.6069468557389226
Eval step 0: eval loss: 0.8228735325836406
Eval: 2022-05-07 04:34:14.871868: total loss: 1.0645593903758432, mse:4.592557760556818, ic :0.1942216093115505, sharpe5:17.486500017642975, irr5:590.0307006835938, ndcg5:0.8509204135586581, pnl5:6.81585693359375 
train 35, step: 0, loss: 1.179015323414522, grad_norm: 0.7446232150289903, ic: 0.5531677456683023
train 35, step: 500, loss: 1.1831079640548345, grad_norm: 2.112986984751231, ic: 0.09265709042045508
train 35, step: 1000, loss: 1.6682682239832138, grad_norm: 4.645095124849034, ic: 0.061354749500846906
train 35, step: 1500, loss: 1.6467244772086467, grad_norm: 5.691216084031708, ic: 0.07658403057211224
train 35, step: 2000, loss: 0.8443217558019301, grad_norm: 1.0460871052313527, ic: 0.5267131015469609
Epoch 35: 2022-05-07 04:42:15.290529: train loss: 1.6053378120809432
Eval step 0: eval loss: 0.8309953888550448
Eval: 2022-05-07 04:42:50.599170: total loss: 1.0650899877074023, mse:4.58060965110662, ic :0.19430876431706373, sharpe5:18.368165212869645, irr5:616.9098510742188, ndcg5:0.8408663243738077, pnl5:10.794031143188477 
train 36, step: 0, loss: 1.840777003409537, grad_norm: 6.1217279937837485, ic: 0.11374626329646811
train 36, step: 500, loss: 0.8411558375197006, grad_norm: 0.05349384835999832, ic: 0.10086703326238458
train 36, step: 1000, loss: 1.6324392755681818, grad_norm: 0.6516581079188771, ic: 0.29295845065528436
train 36, step: 1500, loss: 0.7745531861699967, grad_norm: 0.1031378557131784, ic: 0.37815186032976456
train 36, step: 2000, loss: 1.1312598121649127, grad_norm: 2.3772573022297836, ic: 0.7676279099047744
Epoch 36: 2022-05-07 04:50:37.620680: train loss: 1.5994172874359922
Eval step 0: eval loss: 0.8333082289539976
Eval: 2022-05-07 04:51:12.301077: total loss: 1.067586113480055, mse:4.59143673736234, ic :0.1910580363228045, sharpe5:17.429918525218962, irr5:596.7296752929688, ndcg5:0.8544325211154603, pnl5:9.554755210876465 
train 37, step: 0, loss: 1.9201065694391273, grad_norm: 24.54177460865229, ic: 0.23400189990699644
train 37, step: 500, loss: 2.3491097430031607, grad_norm: 5.927563316207679, ic: -0.02045542346703845
train 37, step: 1000, loss: 1.06329577952578, grad_norm: 0.6984541540506886, ic: 0.05792440944330489
train 37, step: 1500, loss: 2.010383066988815, grad_norm: 7.00436433162002, ic: 0.6089889560231214
train 37, step: 2000, loss: 1.3094485463494083, grad_norm: 0.3719615027950138, ic: 0.2475041142609983
Epoch 37: 2022-05-07 04:59:10.213178: train loss: 1.6014282246329716
Eval step 0: eval loss: 0.8256812140863737
Eval: 2022-05-07 04:59:45.672713: total loss: 1.0688082648562152, mse:4.609484794548916, ic :0.18939853309354532, sharpe5:17.86410410284996, irr5:601.7296752929688, ndcg5:0.8494996892214463, pnl5:8.087979316711426 
train 38, step: 0, loss: 1.3315670664717512, grad_norm: 2.7615392528394036, ic: -0.08035761718262843
train 38, step: 500, loss: 0.9018835750626929, grad_norm: 0.09176735666230204, ic: 0.2514962431978324
train 38, step: 1000, loss: 0.904836300333498, grad_norm: 0.1935430695211745, ic: 0.09929505277431089
train 38, step: 1500, loss: 0.9527291288498957, grad_norm: 0.02628981542480314, ic: 0.20597843510818165
train 38, step: 2000, loss: 2.318146581736216, grad_norm: 4.642933214906282, ic: 0.05066462769025183
Epoch 38: 2022-05-07 05:07:08.454398: train loss: 1.6006435260067873
Eval step 0: eval loss: 0.8291238798858995
Eval: 2022-05-07 05:07:44.184112: total loss: 1.0648744281832863, mse:4.583622024442272, ic :0.1988831468784943, sharpe5:18.50997650384903, irr5:621.424560546875, ndcg5:0.8524818472324325, pnl5:6.399813175201416 
train 39, step: 0, loss: 0.97196162512543, grad_norm: 0.17492544217646744, ic: 0.07225965618077837
train 39, step: 500, loss: 0.9029001504251537, grad_norm: 1.3945538863899454, ic: 0.21631386972267572
train 39, step: 1000, loss: 0.9389110138102151, grad_norm: 0.36604368911724483, ic: 0.23783333636263163
train 39, step: 1500, loss: 2.0805497690917196, grad_norm: 1.0333240885050123, ic: 0.20368511543067744
train 39, step: 2000, loss: 0.6128879444670715, grad_norm: 0.08519241611478467, ic: 0.12228747361705053
Epoch 39: 2022-05-07 05:15:17.131702: train loss: 1.607603221587078
Eval step 0: eval loss: 0.8262725926550974
Eval: 2022-05-07 05:15:51.570380: total loss: 1.0650073271745617, mse:4.582445272573271, ic :0.19410723703588487, sharpe5:17.35013725876808, irr5:589.6815185546875, ndcg5:0.8456834845034678, pnl5:6.301978588104248 
