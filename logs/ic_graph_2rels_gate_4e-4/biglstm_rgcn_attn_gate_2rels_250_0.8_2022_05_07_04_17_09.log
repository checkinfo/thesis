Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
32211
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795775508209538, grad_norm: 4.816644294952649, ic: 0.02281513316069529
train 0, step: 500, loss: 0.8631263393698192, grad_norm: 0.026568909780320157, ic: 0.045192262726378746
train 0, step: 1000, loss: 1.950039934048098, grad_norm: 0.5127160560180851, ic: 0.014822595818915327
train 0, step: 1500, loss: 0.9562556933979743, grad_norm: 0.048764243603463264, ic: 0.016182913381642995
train 0, step: 2000, loss: 1.0022268823962832, grad_norm: 0.15819130231005835, ic: 0.025844782377556566
Epoch 0: 2022-05-07 16:25:59.078305: train loss: 1.648490031357445
Eval step 0: eval loss: 0.8363085988952186
Eval: 2022-05-07 16:26:28.546904: total loss: 1.0793448102870373, mse:4.822856660461895, ic :0.008174046848974043, sharpe5:8.119796047210693, irr5:230.11077880859375, ndcg5:0.859430347676418, pnl5:2.773507833480835 
train 1, step: 0, loss: 2.774773185483871, grad_norm: 0.8787262274037762, ic: 0.056613614137974604
train 1, step: 500, loss: 1.7555341102403241, grad_norm: 0.7706146625464976, ic: 0.09704959771255053
train 1, step: 1000, loss: 0.8777645650071878, grad_norm: 0.17540319980366495, ic: 0.07986076544335914
train 1, step: 1500, loss: 1.7132725507363507, grad_norm: 0.20669351865795563, ic: -0.03175333688896189
train 1, step: 2000, loss: 2.1772984375, grad_norm: 0.8874737106918646, ic: -0.04613686729151448
Epoch 1: 2022-05-07 16:33:57.753032: train loss: 1.6467382008736666
Eval step 0: eval loss: 0.8345837929728661
Eval: 2022-05-07 16:34:27.430383: total loss: 1.078984397239269, mse:4.8235149047899855, ic :0.007717885214732359, sharpe5:7.798194547891616, irr5:219.67225646972656, ndcg5:0.8472510766924229, pnl5:2.7925877571105957 
train 2, step: 0, loss: 2.1417682883522726, grad_norm: 0.009300683886308163, ic: 0.1321850443223104
train 2, step: 500, loss: 3.300002215986894, grad_norm: 0.2846867325891437, ic: 0.04817524460316887
train 2, step: 1000, loss: 2.0723479406130267, grad_norm: 0.00020236590724313146, ic: 0.19034383472989824
train 2, step: 1500, loss: 1.4855060606512405, grad_norm: 0.05970919208404055, ic: -0.03759709956502563
train 2, step: 2000, loss: 3.235213341346154, grad_norm: 0.7874548652890232, ic: 0.20207538456457502
Epoch 2: 2022-05-07 16:41:48.663948: train loss: 1.6464963558274275
Eval step 0: eval loss: 0.8358377470733995
Eval: 2022-05-07 16:42:17.992454: total loss: 1.0794611045111782, mse:4.823078194652516, ic :0.010284549008983809, sharpe5:7.619298239350319, irr5:216.12205505371094, ndcg5:0.8412181625633416, pnl5:2.2627432346343994 
train 3, step: 0, loss: 1.5228050169905996, grad_norm: 0.5247546237799791, ic: -0.0034685761628942286
train 3, step: 500, loss: 1.5013441117181383, grad_norm: 0.3423024669021301, ic: 0.09346781553823912
train 3, step: 1000, loss: 3.6799343111327003, grad_norm: 0.7052781680867395, ic: -0.04868219784570335
train 3, step: 1500, loss: 1.9804507691029052, grad_norm: 1.2340747537391374, ic: -0.07188255342131836
train 3, step: 2000, loss: 0.8989209644214528, grad_norm: 0.0009032853466489349, ic: 0.008401769506753658
Epoch 3: 2022-05-07 16:49:41.164794: train loss: 1.6459263879850632
Eval step 0: eval loss: 0.8348450414498814
Eval: 2022-05-07 16:50:11.253104: total loss: 1.0791991859861911, mse:4.824466607523041, ic :0.014710141508385349, sharpe5:7.694675748646259, irr5:217.5531005859375, ndcg5:0.8531323929264377, pnl5:2.5547425746917725 
train 4, step: 0, loss: 1.4315132732780613, grad_norm: 0.04430096491651773, ic: 0.12682329564804098
train 4, step: 500, loss: 1.6488450418307086, grad_norm: 0.5600196628073302, ic: 0.034979018221125764
train 4, step: 1000, loss: 2.9633982588605186, grad_norm: 0.7826383209736366, ic: 0.06022106260800661
train 4, step: 1500, loss: 2.1483318087420886, grad_norm: 0.4800781598011116, ic: 0.006456283758949289
train 4, step: 2000, loss: 1.0823666111306862, grad_norm: 0.3944953700041512, ic: 0.24010489390591955
Epoch 4: 2022-05-07 16:57:33.314912: train loss: 1.645326195284959
Eval step 0: eval loss: 0.8605415496986959
Eval: 2022-05-07 16:58:02.958559: total loss: 1.0906608272927947, mse:4.841111145879242, ic :0.04138693021618341, sharpe5:9.579836168289184, irr5:269.3374328613281, ndcg5:0.8367049340988081, pnl5:3.418015956878662 
train 5, step: 0, loss: 1.375953623112416, grad_norm: 0.26667368562104454, ic: 0.02747909207353441
train 5, step: 500, loss: 0.8875502995514473, grad_norm: 0.00786703790683259, ic: 0.026753308238823464
train 5, step: 1000, loss: 0.9822126249700671, grad_norm: 0.14923981065045802, ic: -0.005742827093547802
train 5, step: 1500, loss: 1.5300694884786563, grad_norm: 0.15562411691665037, ic: 0.03987721935198982
train 5, step: 2000, loss: 1.106631108613689, grad_norm: 0.02701928266552381, ic: 0.12329369209008476
Epoch 5: 2022-05-07 17:05:26.709581: train loss: 1.6448571822293112
Eval step 0: eval loss: 0.8427695785860115
Eval: 2022-05-07 17:05:55.574157: total loss: 1.081295321680443, mse:4.790077971731122, ic :0.08773508022563319, sharpe5:10.490995327234268, irr5:340.91448974609375, ndcg5:0.8393120000701719, pnl5:2.748964786529541 
train 6, step: 0, loss: 1.3482467432341507, grad_norm: 0.4782492411810423, ic: 0.11182374051495761
train 6, step: 500, loss: 1.0055419547999618, grad_norm: 0.04264775761776528, ic: 0.06139660282061281
train 6, step: 1000, loss: 1.1133916238414923, grad_norm: 0.08691926621104107, ic: 0.7168414120644581
train 6, step: 1500, loss: 1.571793283509814, grad_norm: 0.7673092214421737, ic: 0.1561257227166088
train 6, step: 2000, loss: 0.8029162099210618, grad_norm: 0.05275170062847541, ic: 0.3587747129411869
Epoch 6: 2022-05-07 17:13:24.797230: train loss: 1.6371038304089545
Eval step 0: eval loss: 0.8290281145037539
Eval: 2022-05-07 17:13:53.414212: total loss: 1.0746293656119565, mse:4.7367586272750355, ic :0.13067648745839533, sharpe5:12.331213397383689, irr5:410.1630554199219, ndcg5:0.8474747624648743, pnl5:3.0254769325256348 
train 7, step: 0, loss: 0.9901214599609376, grad_norm: 0.04999777873586843, ic: 0.10038329246729949
train 7, step: 500, loss: 0.6497151569030205, grad_norm: 0.004128857333787155, ic: 0.044973083615438086
train 7, step: 1000, loss: 0.9575197171910701, grad_norm: 0.4042895482780599, ic: 0.12961443255985267
train 7, step: 1500, loss: 2.254627419949089, grad_norm: 0.719328792601181, ic: 0.44503612171633294
train 7, step: 2000, loss: 0.9173556375580574, grad_norm: 0.051061804833774135, ic: -0.036833963619511544
Epoch 7: 2022-05-07 17:21:20.596618: train loss: 1.6303165498209282
Eval step 0: eval loss: 0.8333980130194613
Eval: 2022-05-07 17:21:49.494182: total loss: 1.07407698161778, mse:4.7057349980653145, ic :0.14756806894300384, sharpe5:13.331527946591377, irr5:445.6229248046875, ndcg5:0.8572010873835875, pnl5:3.107351303100586 
train 8, step: 0, loss: 3.600659533514493, grad_norm: 1.109704726177058, ic: 0.15593801311872976
train 8, step: 500, loss: 2.7602168028115504, grad_norm: 0.8615598247828358, ic: 0.039479657366132584
train 8, step: 1000, loss: 3.037003580729167, grad_norm: 0.87553005374732, ic: 0.11679510819668945
train 8, step: 1500, loss: 0.7159666162530827, grad_norm: 0.025949588412202712, ic: 0.47188147979267175
train 8, step: 2000, loss: 1.0878694217199132, grad_norm: 0.3385552400905768, ic: 0.5356435114820557
Epoch 8: 2022-05-07 17:29:12.711891: train loss: 1.628083911276099
Eval step 0: eval loss: 0.8289540233551764
Eval: 2022-05-07 17:29:42.434549: total loss: 1.0715780418610399, mse:4.691892803717342, ic :0.16112504624637058, sharpe5:16.486338708400726, irr5:525.6255493164062, ndcg5:0.8658119209133247, pnl5:5.935194969177246 
train 9, step: 0, loss: 5.446128638357257, grad_norm: 0.9302980858781233, ic: 0.020254876547043013
train 9, step: 500, loss: 1.3390566760028624, grad_norm: 1.1200515879401587, ic: 0.32467494109600403
train 9, step: 1000, loss: 0.9237612769717262, grad_norm: 0.020219392665236396, ic: 0.08389664073852518
train 9, step: 1500, loss: 1.0869775246801363, grad_norm: 0.030308730771785655, ic: 0.42543201161160904
train 9, step: 2000, loss: 1.0590680827153058, grad_norm: 0.42205352006727637, ic: 0.30883574195269925
Epoch 9: 2022-05-07 17:37:12.659486: train loss: 1.6271348780311365
Eval step 0: eval loss: 0.8308898475574618
Eval: 2022-05-07 17:37:41.007073: total loss: 1.0715754410534786, mse:4.688613435058474, ic :0.1659419906341593, sharpe5:17.586735981702805, irr5:565.8443603515625, ndcg5:0.8476705942320006, pnl5:6.277092456817627 
train 10, step: 0, loss: 7.138991464331268, grad_norm: 1.4145648221360732, ic: 0.2492724413425215
train 10, step: 500, loss: 1.128591396744996, grad_norm: 0.07193440428899511, ic: 0.03548626864925259
train 10, step: 1000, loss: 2.392873939560966, grad_norm: 0.6830173996727517, ic: 0.13875725805497705
train 10, step: 1500, loss: 1.1073650013316763, grad_norm: 0.2914102250000017, ic: 0.0008957396390987605
train 10, step: 2000, loss: 2.7515843168218086, grad_norm: 0.5439252067541086, ic: 0.4120103433572858
Epoch 10: 2022-05-07 17:45:01.475983: train loss: 1.6277713542271655
Eval step 0: eval loss: 0.8292991388962064
Eval: 2022-05-07 17:45:30.760180: total loss: 1.0702751017204044, mse:4.683150399959203, ic :0.16891911143049992, sharpe5:18.330562773942948, irr5:584.2041015625, ndcg5:0.8544515042678252, pnl5:10.895198822021484 
train 11, step: 0, loss: 1.2586871012180976, grad_norm: 0.1885008798845027, ic: 0.20362708001833285
train 11, step: 500, loss: 0.6654025626463826, grad_norm: 0.037278749109525666, ic: 0.5246882033033573
train 11, step: 1000, loss: 0.9320995208161583, grad_norm: 0.11997190343583652, ic: 0.07887086393519818
train 11, step: 1500, loss: 1.0529560289884867, grad_norm: 0.07585441409604513, ic: 0.18152180448246916
train 11, step: 2000, loss: 0.7862556541888823, grad_norm: 0.005767766759079267, ic: 0.14949502593613317
Epoch 11: 2022-05-07 17:53:01.383776: train loss: 1.6265505271916023
Eval step 0: eval loss: 0.8319966484045705
Eval: 2022-05-07 17:53:31.046916: total loss: 1.071054606911323, mse:4.680411308641606, ic :0.16997182796599183, sharpe5:17.437134046554565, irr5:562.15966796875, ndcg5:0.8531211809366274, pnl5:7.8745222091674805 
train 12, step: 0, loss: 0.9608800411224365, grad_norm: 0.17562989646342492, ic: 0.3941116073075862
train 12, step: 500, loss: 0.9326917693616913, grad_norm: 0.07768606829537582, ic: 0.1808771294931028
train 12, step: 1000, loss: 2.8822149774830814, grad_norm: 0.9121454627666278, ic: 0.06659706992281245
train 12, step: 1500, loss: 0.9406509573733816, grad_norm: 0.11346448816812578, ic: -0.0966412618022135
train 12, step: 2000, loss: 0.8732784766805136, grad_norm: 0.003213417695903435, ic: 0.2055631272228152
Epoch 12: 2022-05-07 18:00:55.306298: train loss: 1.6258134415040932
Eval step 0: eval loss: 0.8291193778195798
Eval: 2022-05-07 18:01:25.018972: total loss: 1.0696377997236366, mse:4.679205972239586, ic :0.17181697681366395, sharpe5:17.866118924617766, irr5:569.9297485351562, ndcg5:0.8495090073212008, pnl5:7.107853889465332 
train 13, step: 0, loss: 2.0546404507243268, grad_norm: 0.7500791202976543, ic: 0.3805423879469343
train 13, step: 500, loss: 0.8332424606177524, grad_norm: 0.13018769775510153, ic: 0.49787090145223145
train 13, step: 1000, loss: 0.9570225658032717, grad_norm: 0.38545958133353536, ic: 0.48893302279368733
train 13, step: 1500, loss: 2.4171825828820257, grad_norm: 0.8745492010202162, ic: 0.021645671290622483
train 13, step: 2000, loss: 1.464171658487298, grad_norm: 0.048138792131045674, ic: 0.17577912773555515
Epoch 13: 2022-05-07 18:08:51.207949: train loss: 1.6257332330965637
Eval step 0: eval loss: 0.8246436164301237
Eval: 2022-05-07 18:09:21.173078: total loss: 1.071201911812078, mse:4.694941826818231, ic :0.16655957560295653, sharpe5:17.7300588619709, irr5:585.9420776367188, ndcg5:0.8544427669843166, pnl5:6.969725608825684 
train 14, step: 0, loss: 4.48854291085706, grad_norm: 1.4261400964590711, ic: 0.21197626546663115
train 14, step: 500, loss: 0.82820927331207, grad_norm: 0.005744101479758519, ic: 0.1084089840687632
train 14, step: 1000, loss: 1.853858941960896, grad_norm: 0.8896857675821963, ic: 0.360349483907781
train 14, step: 1500, loss: 1.1236842468357535, grad_norm: 0.07437116474238065, ic: -0.01804083436431638
train 14, step: 2000, loss: 1.1766534437989413, grad_norm: 0.38923346950659793, ic: 0.0676107396791428
Epoch 14: 2022-05-07 18:16:47.370102: train loss: 1.624204816298597
Eval step 0: eval loss: 0.8320034658192834
Eval: 2022-05-07 18:17:17.317983: total loss: 1.0695035889835902, mse:4.661789822787796, ic :0.17546955695104452, sharpe5:16.910525851249695, irr5:549.365966796875, ndcg5:0.8509685648236887, pnl5:5.429315090179443 
train 15, step: 0, loss: 3.416619801799611, grad_norm: 1.5355720541629438, ic: 0.10859790486584915
train 15, step: 500, loss: 1.2570871811357025, grad_norm: 0.03279291632872437, ic: 0.041187284891959226
train 15, step: 1000, loss: 1.3157195796811483, grad_norm: 0.13406168256514542, ic: 0.037078106892176096
train 15, step: 1500, loss: 0.8482210414616141, grad_norm: 0.21668436458232035, ic: 0.05383089476832201
train 15, step: 2000, loss: 1.4688781639995974, grad_norm: 0.6426568345748562, ic: 0.055434090295457907
Epoch 15: 2022-05-07 18:24:19.442640: train loss: 1.6228501277373586
Eval step 0: eval loss: 0.8372333876325407
Eval: 2022-05-07 18:24:51.336529: total loss: 1.072467095597982, mse:4.641531132902633, ic :0.185296213981706, sharpe5:17.659744420051574, irr5:586.5581665039062, ndcg5:0.8504925721561706, pnl5:5.487918853759766 
train 16, step: 0, loss: 0.6846920928592636, grad_norm: 0.631086328999497, ic: -0.039109425155007246
train 16, step: 500, loss: 1.5860140120434092, grad_norm: 0.7080866598547004, ic: 0.1816811266912665
train 16, step: 1000, loss: 0.881561279296875, grad_norm: 0.013034274735187179, ic: -0.09828170865602293
train 16, step: 1500, loss: 0.8439997198827818, grad_norm: 0.2774032281791885, ic: 0.1470670784003158
train 16, step: 2000, loss: 3.3058438919281583, grad_norm: 2.0937898860068214, ic: -0.002553899810555739
Epoch 16: 2022-05-07 18:32:28.654653: train loss: 1.6217861768643564
Eval step 0: eval loss: 0.8291361640954293
Eval: 2022-05-07 18:32:58.338446: total loss: 1.070412759616974, mse:4.662932521971427, ic :0.17417663380093854, sharpe5:17.611971387863157, irr5:569.8432006835938, ndcg5:0.8460111800317945, pnl5:4.9834794998168945 
train 17, step: 0, loss: 1.2816030649038461, grad_norm: 0.38462561940760626, ic: -0.11673942093751347
train 17, step: 500, loss: 1.723286648882114, grad_norm: 1.2072480346460124, ic: 0.23191769835343673
train 17, step: 1000, loss: 1.2826657826169805, grad_norm: 0.09642713822300521, ic: 0.15175651526685885
train 17, step: 1500, loss: 4.48480838997061, grad_norm: 1.9827871293214852, ic: 0.19702223418598175
train 17, step: 2000, loss: 1.287428486413584, grad_norm: 1.3207788370590277, ic: 0.08839711373919654
Epoch 17: 2022-05-07 18:40:35.038147: train loss: 1.6208480475784894
Eval step 0: eval loss: 0.8338818565183745
Eval: 2022-05-07 18:41:05.701610: total loss: 1.0686478198477383, mse:4.600655735082896, ic :0.19010159422818687, sharpe5:17.651004066467284, irr5:587.748291015625, ndcg5:0.849826440480825, pnl5:6.3398003578186035 
train 18, step: 0, loss: 1.4172213454587923, grad_norm: 1.6398169645434535, ic: 0.2538955218691128
train 18, step: 500, loss: 1.5154346530720337, grad_norm: 0.7952017830856825, ic: -0.058430134547349376
train 18, step: 1000, loss: 0.6597697720462329, grad_norm: 0.02390912182608335, ic: 0.5741610597624959
train 18, step: 1500, loss: 1.4199419701128686, grad_norm: 0.05846587816691924, ic: 0.23859143957500936
train 18, step: 2000, loss: 0.9134253240694666, grad_norm: 0.019416338244127595, ic: -0.05345855303126146
Epoch 18: 2022-05-07 18:48:51.208007: train loss: 1.6203913176916098
Eval step 0: eval loss: 0.8248967611877633
Eval: 2022-05-07 18:49:21.869072: total loss: 1.0667600690251742, mse:4.611197641378244, ic :0.18760942687976082, sharpe5:17.65470074415207, irr5:578.75146484375, ndcg5:0.8540449422485639, pnl5:5.2491888999938965 
train 19, step: 0, loss: 1.475419689360119, grad_norm: 1.1618322588322263, ic: 0.06373628278707955
train 19, step: 500, loss: 0.8600792355007595, grad_norm: 0.022609002673609116, ic: 0.2352959471095955
train 19, step: 1000, loss: 0.9531891402097571, grad_norm: 0.004925272197453792, ic: 0.20937991750172597
train 19, step: 1500, loss: 3.9664235440091358, grad_norm: 1.472531722628554, ic: 0.1209997491484693
train 19, step: 2000, loss: 1.0026115534855768, grad_norm: 0.17350491113958877, ic: 0.2470373086631086
Epoch 19: 2022-05-07 18:57:07.692184: train loss: 1.6223186759453914
Eval step 0: eval loss: 0.8271353815076725
Eval: 2022-05-07 18:57:38.026428: total loss: 1.067547323272236, mse:4.594257096429577, ic :0.19239873696993065, sharpe5:17.470230095386505, irr5:590.3882446289062, ndcg5:0.8474501596613545, pnl5:6.201769828796387 
train 20, step: 0, loss: 2.3226823045331026, grad_norm: 1.3619319924925235, ic: 0.05144233577796439
train 20, step: 500, loss: 3.2278473011363635, grad_norm: 0.7517203782291333, ic: 0.08731781676308734
train 20, step: 1000, loss: 0.9688526153564454, grad_norm: 0.14058943371399596, ic: 0.1634057349368995
train 20, step: 1500, loss: 1.7323403701665392, grad_norm: 3.4091800704231425, ic: 0.26783400752219566
train 20, step: 2000, loss: 1.0349861879333198, grad_norm: 0.07972868093173865, ic: 0.0011464076711126432
Epoch 20: 2022-05-07 19:05:08.810108: train loss: 1.6168801033796432
Eval step 0: eval loss: 0.8343306482152265
Eval: 2022-05-07 19:05:39.276161: total loss: 1.0674717902459507, mse:4.588128757965805, ic :0.19068979155938634, sharpe5:17.61711154818535, irr5:581.6364135742188, ndcg5:0.8623552576516018, pnl5:3.7525389194488525 
train 21, step: 0, loss: 1.0085710864499906, grad_norm: 0.3572360618608835, ic: 0.07160185223703691
train 21, step: 500, loss: 0.7673126490770188, grad_norm: 0.01741575149836154, ic: 0.19775651273119302
train 21, step: 1000, loss: 0.9425751535516036, grad_norm: 1.066003202540389, ic: 0.1567980059973143
train 21, step: 1500, loss: 0.9844201721674766, grad_norm: 0.2701671081512419, ic: 0.3166376410596245
train 21, step: 2000, loss: 0.9386873129066307, grad_norm: 0.10468256987304182, ic: 0.07758201933174119
Epoch 21: 2022-05-07 19:13:18.720423: train loss: 1.61697824004144
Eval step 0: eval loss: 0.8273815159048998
Eval: 2022-05-07 19:13:48.154051: total loss: 1.0670206897163228, mse:4.5976861246735075, ic :0.18543792245052665, sharpe5:17.71902369260788, irr5:573.7025756835938, ndcg5:0.8426649257476269, pnl5:7.513638973236084 
train 22, step: 0, loss: 1.0415040786656957, grad_norm: 0.1308652741500862, ic: 0.22083131463893124
train 22, step: 500, loss: 3.274194732914126, grad_norm: 1.6032712349014868, ic: -0.2228606212932599
train 22, step: 1000, loss: 1.1890066157875723, grad_norm: 0.023351794489842356, ic: 0.46542633492463725
train 22, step: 1500, loss: 0.9736167373971193, grad_norm: 0.15437661231449018, ic: 0.07938370718803582
train 22, step: 2000, loss: 1.7555573824581916, grad_norm: 2.886118776116206, ic: 0.11560611135368694
Epoch 22: 2022-05-07 19:21:34.634747: train loss: 1.6153310272109516
Eval step 0: eval loss: 0.8293660267386722
Eval: 2022-05-07 19:22:05.110127: total loss: 1.070199341685123, mse:4.627799648213283, ic :0.1733308180026579, sharpe5:15.34378720164299, irr5:507.4503479003906, ndcg5:0.8385705887835523, pnl5:5.574888706207275 
train 23, step: 0, loss: 0.9733916972487392, grad_norm: 0.037612405234590494, ic: 0.18651464732648487
train 23, step: 500, loss: 1.4187781125821723, grad_norm: 0.12262053099155593, ic: 0.05590677844984522
train 23, step: 1000, loss: 1.643142293294271, grad_norm: 0.05676108664028084, ic: 0.25956938306146243
train 23, step: 1500, loss: 1.1342456785423063, grad_norm: 2.616921385092374, ic: 0.11548059758839327
train 23, step: 2000, loss: 1.8910016418879907, grad_norm: 3.0223459606133316, ic: 0.44305136006720836
Epoch 23: 2022-05-07 19:29:46.778913: train loss: 1.6158964159423566
Eval step 0: eval loss: 0.8293694354460287
Eval: 2022-05-07 19:30:17.670373: total loss: 1.0669731283904833, mse:4.597797381672549, ic :0.18698379484242214, sharpe5:16.84955715537071, irr5:573.6000366210938, ndcg5:0.8397940941197113, pnl5:6.439020156860352 
train 24, step: 0, loss: 2.189522670305472, grad_norm: 0.10169470675663501, ic: 0.16572289727599782
train 24, step: 500, loss: 1.2123795446254864, grad_norm: 0.107862551643757, ic: 0.14267101021000805
train 24, step: 1000, loss: 0.9075437310094532, grad_norm: 0.0708620268441815, ic: 0.5299584763419027
train 24, step: 1500, loss: 2.623861616343776, grad_norm: 4.506246961298242, ic: -0.015923949008751707
train 24, step: 2000, loss: 0.9291503707276385, grad_norm: 0.059636963322881306, ic: 0.11200263771767932
Epoch 24: 2022-05-07 19:37:57.490765: train loss: 1.6114427571579797
Eval step 0: eval loss: 0.8231978099891333
Eval: 2022-05-07 19:38:28.093920: total loss: 1.066849330957935, mse:4.608582559498683, ic :0.18875435230174029, sharpe5:17.703841584920884, irr5:581.8484497070312, ndcg5:0.8512364726140599, pnl5:4.850369930267334 
train 25, step: 0, loss: 0.8388171221758869, grad_norm: 0.05434185009489082, ic: 0.6101782622382186
train 25, step: 500, loss: 0.867749170954494, grad_norm: 0.008107864946944297, ic: 0.23283025300501764
train 25, step: 1000, loss: 2.0944997143945163, grad_norm: 0.484511724095934, ic: 0.25461122965080135
train 25, step: 1500, loss: 1.1297841155964565, grad_norm: 0.4518496232407384, ic: 0.5353301932561753
train 25, step: 2000, loss: 1.0020832429736295, grad_norm: 0.6614606924318538, ic: 0.6012873243881655
Epoch 25: 2022-05-07 19:46:04.253991: train loss: 1.6142932582306007
Eval step 0: eval loss: 0.8223205502091017
Eval: 2022-05-07 19:46:35.358989: total loss: 1.0648876749384275, mse:4.593018086756633, ic :0.19305565432480112, sharpe5:17.74384099960327, irr5:596.9052734375, ndcg5:0.8368037125419603, pnl5:5.145827770233154 
train 26, step: 0, loss: 6.731237051966853, grad_norm: 1.639517399379032, ic: 0.1171564742902845
train 26, step: 500, loss: 3.954892203992377, grad_norm: 5.635682185588059, ic: 0.38203866687505017
train 26, step: 1000, loss: 1.269893819907771, grad_norm: 1.358793841672477, ic: 0.011729653911512322
train 26, step: 1500, loss: 0.8301084881146059, grad_norm: 0.21759764798138792, ic: 0.3099010617480413
train 26, step: 2000, loss: 0.9591699295584186, grad_norm: 0.45686511933997953, ic: 0.16817748132636265
Epoch 26: 2022-05-07 19:54:05.305279: train loss: 1.6116933941524079
Eval step 0: eval loss: 0.8251915179012447
Eval: 2022-05-07 19:54:34.564842: total loss: 1.0658988258487885, mse:4.594193348999033, ic :0.1862354825446103, sharpe5:17.395220078229904, irr5:565.4752807617188, ndcg5:0.8503756065473562, pnl5:7.892375946044922 
train 27, step: 0, loss: 0.8289803538602941, grad_norm: 0.02696448602120136, ic: 0.09388717898835637
train 27, step: 500, loss: 0.9408398899699966, grad_norm: 1.457472891323734, ic: 0.24565320137238753
train 27, step: 1000, loss: 0.7493089290893272, grad_norm: 0.6871503868206146, ic: 0.19929619574361163
train 27, step: 1500, loss: 0.6393364874141391, grad_norm: 0.08388187960080194, ic: 0.5142984145395687
train 27, step: 2000, loss: 1.3852132161458335, grad_norm: 0.09501253538122108, ic: 0.030795122823654975
Epoch 27: 2022-05-07 20:02:07.626445: train loss: 1.6128972175564218
Eval step 0: eval loss: 0.8280177865063553
Eval: 2022-05-07 20:02:38.246483: total loss: 1.0656714096755295, mse:4.5950811913002765, ic :0.18639793734073368, sharpe5:16.76271171569824, irr5:559.7073974609375, ndcg5:0.8444055553282293, pnl5:4.687231540679932 
train 28, step: 0, loss: 1.5354035955598455, grad_norm: 1.453765022667295, ic: 0.24776555874678316
train 28, step: 500, loss: 1.3922683434355785, grad_norm: 3.1213986865335874, ic: 0.19661010727471598
train 28, step: 1000, loss: 0.9071481596163617, grad_norm: 0.5907271207694673, ic: 0.5848529601969265
train 28, step: 1500, loss: 1.0334806523771367, grad_norm: 0.07295681391129191, ic: 0.04479011051386725
train 28, step: 2000, loss: 1.036040136419191, grad_norm: 0.3055511865474184, ic: 0.16039540414437115
Epoch 28: 2022-05-07 20:10:13.406040: train loss: 1.6076484062621685
Eval step 0: eval loss: 0.8224879627609655
Eval: 2022-05-07 20:10:43.471154: total loss: 1.075696299246359, mse:4.686143167524915, ic :0.17449337528411507, sharpe5:17.325857237577438, irr5:576.5812377929688, ndcg5:0.8475433931282436, pnl5:4.2519636154174805 
train 29, step: 0, loss: 0.9024557326217778, grad_norm: 0.06847355407098112, ic: 0.125787827840657
train 29, step: 500, loss: 1.097750723676967, grad_norm: 0.1930014394230704, ic: 0.6173513061219591
train 29, step: 1000, loss: 1.055459643910687, grad_norm: 0.9329725688968717, ic: 0.06196488702112973
train 29, step: 1500, loss: 2.337899963712432, grad_norm: 0.7275331731603274, ic: -0.019210689027233448
train 29, step: 2000, loss: 4.2871067376784335, grad_norm: 18.87385572207969, ic: 0.21946510023245847
Epoch 29: 2022-05-07 20:18:21.007434: train loss: 1.6111772992527655
Eval step 0: eval loss: 0.8310756542660036
Eval: 2022-05-07 20:18:50.889338: total loss: 1.0665414543103506, mse:4.590166037367948, ic :0.18878802923041343, sharpe5:16.90376626610756, irr5:550.1787109375, ndcg5:0.851316880757013, pnl5:4.000283718109131 
train 30, step: 0, loss: 1.001116947190835, grad_norm: 0.09167817998967281, ic: 0.5227613203851718
train 30, step: 500, loss: 1.3879478648610541, grad_norm: 2.876796940671153, ic: 0.12261703693497687
train 30, step: 1000, loss: 0.9763026381983901, grad_norm: 0.09763748210934206, ic: -0.040050283839450765
train 30, step: 1500, loss: 1.491307704539368, grad_norm: 3.716770951246449, ic: 0.1760337019334537
train 30, step: 2000, loss: 1.8390650030484887, grad_norm: 1.738421818973095, ic: 0.13016273995325697
Epoch 30: 2022-05-07 20:26:29.566328: train loss: 1.6067588952627032
Eval step 0: eval loss: 0.8263667501564146
Eval: 2022-05-07 20:27:01.113120: total loss: 1.0676044071428774, mse:4.637990260289981, ic :0.19195801345985616, sharpe5:17.65056704878807, irr5:594.5604248046875, ndcg5:0.8712985112030145, pnl5:3.738281011581421 
train 31, step: 0, loss: 1.0507302974698334, grad_norm: 0.848392104199967, ic: 0.3720319602722687
train 31, step: 500, loss: 1.4996049543467078, grad_norm: 1.8819343343396957, ic: 0.018335141132799937
train 31, step: 1000, loss: 4.466688240998243, grad_norm: 4.910843611353981, ic: 0.47577368519333607
train 31, step: 1500, loss: 0.7727192194261818, grad_norm: 0.10663667100453011, ic: 0.7046146963126247
train 31, step: 2000, loss: 1.2250503493659763, grad_norm: 1.8929626136471158, ic: 0.19613982594670387
Epoch 31: 2022-05-07 20:34:48.001430: train loss: 1.6039119988443744
Eval step 0: eval loss: 0.8361020183663724
Eval: 2022-05-07 20:35:19.011054: total loss: 1.067714080282943, mse:4.591609109651174, ic :0.18257941207734765, sharpe5:16.31011085629463, irr5:529.3639526367188, ndcg5:0.8546065452990002, pnl5:4.702590465545654 
train 32, step: 0, loss: 1.1238559832911146, grad_norm: 0.025084393958318584, ic: 0.18468296588474784
train 32, step: 500, loss: 1.4864507720226379, grad_norm: 2.67502530907563, ic: 0.10363524541061503
train 32, step: 1000, loss: 1.0349272738575788, grad_norm: 0.12331521174611647, ic: 0.5175759022080838
train 32, step: 1500, loss: 0.9748237196677155, grad_norm: 2.697397021348229, ic: 0.07161579870865833
train 32, step: 2000, loss: 0.9387789896283159, grad_norm: 0.09248850953236872, ic: 0.5666136543683047
Epoch 32: 2022-05-07 20:43:13.866507: train loss: 1.610186274224982
Eval step 0: eval loss: 0.8232304821275684
Eval: 2022-05-07 20:43:44.556008: total loss: 1.064301222660567, mse:4.590516906339786, ic :0.19352745501602214, sharpe5:17.09374220252037, irr5:581.7850952148438, ndcg5:0.8410174335803368, pnl5:4.477128505706787 
train 33, step: 0, loss: 1.297440618812995, grad_norm: 1.6453253730201363, ic: 0.2240112113759491
train 33, step: 500, loss: 0.9930329464882544, grad_norm: 0.07077130907255445, ic: 0.13021626874420011
train 33, step: 1000, loss: 1.052384126096932, grad_norm: 6.078631594284574, ic: 0.2301472714483465
train 33, step: 1500, loss: 0.8941709760527877, grad_norm: 0.12820666012885246, ic: 0.5548483249937541
train 33, step: 2000, loss: 0.7995684785434826, grad_norm: 0.10245648009718611, ic: 0.28165029660917595
Epoch 33: 2022-05-07 20:51:19.553377: train loss: 1.6053544423119024
Eval step 0: eval loss: 0.833729236470133
Eval: 2022-05-07 20:51:49.820711: total loss: 1.0681959715302782, mse:4.588045963282694, ic :0.1859419385210303, sharpe5:17.113315297365187, irr5:557.3630981445312, ndcg5:0.8446140158472119, pnl5:5.454026699066162 
train 34, step: 0, loss: 1.0320643385940753, grad_norm: 2.9234407449527033, ic: 0.6036606331679956
train 34, step: 500, loss: 0.7936209115777906, grad_norm: 0.5598478797450333, ic: 0.3046473971808749
train 34, step: 1000, loss: 3.147285426267281, grad_norm: 1.9660560685103035, ic: 0.3399188797797169
train 34, step: 1500, loss: 0.806376187821253, grad_norm: 0.46221119209758466, ic: 0.6985729614217107
train 34, step: 2000, loss: 5.5315087020720375, grad_norm: 50.29337426848565, ic: 0.4422516852094395
Epoch 34: 2022-05-07 20:59:30.826558: train loss: 1.6069023578394952
Eval step 0: eval loss: 0.8270712592202317
Eval: 2022-05-07 21:00:00.542889: total loss: 1.0679555339699638, mse:4.613674312011767, ic :0.18952181798928489, sharpe5:17.430455811023712, irr5:566.8208618164062, ndcg5:0.8518376990218218, pnl5:6.701313495635986 
train 35, step: 0, loss: 1.1458603084788601, grad_norm: 0.9424997737887493, ic: 0.5590284663596088
train 35, step: 500, loss: 1.1150255678914556, grad_norm: 1.0332705008948622, ic: 0.11438202520170396
train 35, step: 1000, loss: 1.833248713467025, grad_norm: 6.416796386232527, ic: 0.057744124158324604
train 35, step: 1500, loss: 1.5705019384398498, grad_norm: 1.9443395726909996, ic: 0.07305490597970442
train 35, step: 2000, loss: 0.7911665809345755, grad_norm: 0.16747861254197768, ic: 0.5506761967601786
Epoch 35: 2022-05-07 21:07:39.041866: train loss: 1.6053014146223814
Eval step 0: eval loss: 0.8314560788700276
Eval: 2022-05-07 21:08:09.925744: total loss: 1.0660014144277343, mse:4.590786019738549, ic :0.19149275884651806, sharpe5:17.138325573205947, irr5:561.0294189453125, ndcg5:0.8719516847465422, pnl5:6.2147603034973145 
train 36, step: 0, loss: 1.837945815345369, grad_norm: 5.392353098615757, ic: 0.11549237557628253
train 36, step: 500, loss: 0.839201365802305, grad_norm: 0.11911495164803487, ic: 0.15869909671020854
train 36, step: 1000, loss: 1.7774605823863636, grad_norm: 15.027869275399839, ic: 0.2827786480996873
train 36, step: 1500, loss: 0.7630704863649285, grad_norm: 0.04926454347680317, ic: 0.3923599976971323
train 36, step: 2000, loss: 1.100100177161934, grad_norm: 2.0007389240853697, ic: 0.7797107689400069
Epoch 36: 2022-05-07 21:15:51.996976: train loss: 1.6011306441609168
Eval step 0: eval loss: 0.8300268657591873
Eval: 2022-05-07 21:16:22.822776: total loss: 1.0664281630155497, mse:4.597449184532005, ic :0.18945706101770501, sharpe5:17.814972721338272, irr5:581.5112915039062, ndcg5:0.8508603257146754, pnl5:4.385183811187744 
train 37, step: 0, loss: 2.0181110843582504, grad_norm: 7.2552855367560145, ic: 0.23616137435091303
train 37, step: 500, loss: 2.378729616448308, grad_norm: 4.861134574455709, ic: -0.03539018406820851
train 37, step: 1000, loss: 1.0745903491248097, grad_norm: 0.5510278529656993, ic: 0.06404110330065696
train 37, step: 1500, loss: 1.9784901377918958, grad_norm: 3.450502821796409, ic: 0.6130960203548218
train 37, step: 2000, loss: 1.3080068189044332, grad_norm: 0.3006650577331916, ic: 0.21991892973385818
Epoch 37: 2022-05-07 21:24:02.127323: train loss: 1.6021087974928132
Eval step 0: eval loss: 0.8313330438290305
Eval: 2022-05-07 21:24:32.448334: total loss: 1.0676564526116907, mse:4.611369404074519, ic :0.19152403466325638, sharpe5:18.29673268675804, irr5:590.955078125, ndcg5:0.8498030096901806, pnl5:5.8773016929626465 
train 38, step: 0, loss: 1.3383005653939597, grad_norm: 1.534474917760367, ic: -0.08101801814207481
train 38, step: 500, loss: 0.9021205572434413, grad_norm: 0.119599192185601, ic: 0.25301823892274755
train 38, step: 1000, loss: 0.9267149672677866, grad_norm: 0.8999984592647976, ic: 0.1578940742805908
train 38, step: 1500, loss: 0.9478901578516515, grad_norm: 0.049570016395642666, ic: 0.2144631323464362
train 38, step: 2000, loss: 2.293540747144071, grad_norm: 5.4460199793748085, ic: -0.017351502472715843
Epoch 38: 2022-05-07 21:32:07.357981: train loss: 1.6023302739120813
Eval step 0: eval loss: 0.8297444575704689
Eval: 2022-05-07 21:32:38.307353: total loss: 1.0643016729806727, mse:4.589060586953385, ic :0.19332269868920318, sharpe5:18.197308326959607, irr5:594.59228515625, ndcg5:0.8575199888363646, pnl5:4.072938919067383 
train 39, step: 0, loss: 0.9725071151686736, grad_norm: 0.04275654384767222, ic: 0.059450206186662774
train 39, step: 500, loss: 0.8767568366881245, grad_norm: 0.3371153897260505, ic: 0.24640915323951124
train 39, step: 1000, loss: 0.9656105709729627, grad_norm: 0.596900089354824, ic: 0.1879622107196378
train 39, step: 1500, loss: 2.0706634279665215, grad_norm: 0.23607487215368145, ic: 0.2646484189030641
train 39, step: 2000, loss: 0.6060933720255727, grad_norm: 0.021596689822268113, ic: 0.13452389955550448
Epoch 39: 2022-05-07 21:40:22.360163: train loss: 1.5999231708976485
Eval step 0: eval loss: 0.8277693367607349
Eval: 2022-05-07 21:40:51.835932: total loss: 1.0644247818992454, mse:4.588613561568846, ic :0.1932438613779139, sharpe5:17.3166174352169, irr5:580.6967163085938, ndcg5:0.8505722652548207, pnl5:4.8263936042785645 
