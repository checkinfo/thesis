Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_20_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
61208
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795835827550821, grad_norm: 4.815870563376928, ic: 0.022698554444078554
train 0, step: 500, loss: 0.8631388499670309, grad_norm: 0.026572607289483616, ic: 0.045309122430310045
train 0, step: 1000, loss: 1.9500990492981896, grad_norm: 0.5127265503700597, ic: 0.015311613155437349
train 0, step: 1500, loss: 0.956370140347085, grad_norm: 0.04900928792061399, ic: 0.015371189670266577
train 0, step: 2000, loss: 1.0022277612322263, grad_norm: 0.15819921776239623, ic: 0.025446046577557828
Epoch 0: 2022-05-06 18:54:16.941780: train loss: 1.6484915993018554
Eval step 0: eval loss: 0.8363084059495192
Eval: 2022-05-06 18:54:40.677894: total loss: 1.0793449727517368, mse:4.822855533976745, ic :0.008208475018384911, sharpe5:7.911977587044238, irr5:225.21148681640625, ndcg5:0.8573771887553551, pnl5:2.7020838260650635 
train 1, step: 0, loss: 2.77476806640625, grad_norm: 0.8786315846460381, ic: 0.05705197029690105
train 1, step: 500, loss: 1.7553926039372247, grad_norm: 0.7704798590620893, ic: 0.09683609267570523
train 1, step: 1000, loss: 0.8777542699205915, grad_norm: 0.17541860873017562, ic: 0.08000843211478868
train 1, step: 1500, loss: 1.7132064643947558, grad_norm: 0.2066636009936959, ic: -0.03152081760845647
train 1, step: 2000, loss: 2.177240625, grad_norm: 0.8875717494866788, ic: -0.04497410368044572
Epoch 1: 2022-05-06 19:00:59.914912: train loss: 1.6467368668362126
Eval step 0: eval loss: 0.8345858510603266
Eval: 2022-05-06 19:01:23.859917: total loss: 1.0789813335871499, mse:4.823483282834702, ic :0.007895787399836122, sharpe5:7.422619473338127, irr5:210.81715393066406, ndcg5:0.8537722447372467, pnl5:2.627869129180908 
train 2, step: 0, loss: 2.1417661576704545, grad_norm: 0.009277787267492772, ic: 0.1316120145701607
train 2, step: 500, loss: 3.299967065849961, grad_norm: 0.2849523237132081, ic: 0.04870993691099984
train 2, step: 1000, loss: 2.072332412895115, grad_norm: 0.00020431627471022854, ic: 0.1929500129587877
train 2, step: 1500, loss: 1.4854866784947518, grad_norm: 0.05968823318381811, ic: -0.03799789048642218
train 2, step: 2000, loss: 3.2347671274038463, grad_norm: 0.7873027345607316, ic: 0.20634609977306054
Epoch 2: 2022-05-06 19:07:42.999852: train loss: 1.6464986320896458
Eval step 0: eval loss: 0.8357777409608799
Eval: 2022-05-06 19:08:08.782996: total loss: 1.0794696269773418, mse:4.823067343935421, ic :0.01059624786774194, sharpe5:7.208504458665847, irr5:203.80279541015625, ndcg5:0.8664198428951281, pnl5:2.822408676147461 
train 3, step: 0, loss: 1.5229625174669716, grad_norm: 0.5250195046705012, ic: -0.0013026425151581329
train 3, step: 500, loss: 1.5013734583007048, grad_norm: 0.3420871252313781, ic: 0.0947558679826518
train 3, step: 1000, loss: 3.6797735184585494, grad_norm: 0.7054721045387227, ic: -0.04686785463001503
train 3, step: 1500, loss: 1.980984202383388, grad_norm: 1.2349812064633676, ic: -0.06530199683997123
train 3, step: 2000, loss: 0.8987489442567568, grad_norm: 0.0010286827246818876, ic: 0.009347573830618287
Epoch 3: 2022-05-06 19:14:43.209512: train loss: 1.645969342525733
Eval step 0: eval loss: 0.8345558158464501
Eval: 2022-05-06 19:15:09.128388: total loss: 1.079308078476742, mse:4.824789149317161, ic :0.014910034322870996, sharpe5:7.468411737084389, irr5:207.1130828857422, ndcg5:0.8440579278381964, pnl5:2.7121896743774414 
train 4, step: 0, loss: 1.4314869658801022, grad_norm: 0.04425127721819764, ic: 0.12614005327387356
train 4, step: 500, loss: 1.649779312253937, grad_norm: 0.5607244904862125, ic: 0.02937448615920588
train 4, step: 1000, loss: 2.967988177043636, grad_norm: 0.7619004916541625, ic: 0.07264893315510894
train 4, step: 1500, loss: 2.1494933824498945, grad_norm: 0.4769722658202717, ic: 0.016325370470351284
train 4, step: 2000, loss: 1.0843512503948776, grad_norm: 0.3921126199687907, ic: 0.2316688189172469
Epoch 4: 2022-05-06 19:21:43.247858: train loss: 1.6453934360668816
Eval step 0: eval loss: 0.8490300233798735
Eval: 2022-05-06 19:22:09.283898: total loss: 1.0840409439064609, mse:4.82259454402365, ic :0.04766424877502573, sharpe5:10.17664907813072, irr5:302.3066711425781, ndcg5:0.8591714987659073, pnl5:2.723742723464966 
train 5, step: 0, loss: 1.3541266319735739, grad_norm: 0.14880081785810756, ic: 0.03484819306696596
train 5, step: 500, loss: 0.890304577530234, grad_norm: 0.009703774359710697, ic: 0.07383810493218129
train 5, step: 1000, loss: 0.9778542003412357, grad_norm: 0.14914728609569206, ic: 0.020769986805209216
train 5, step: 1500, loss: 1.534433548884954, grad_norm: 0.15904301424955078, ic: 0.016493707988355498
train 5, step: 2000, loss: 1.1083449078270011, grad_norm: 0.028138360664188095, ic: 0.12538588852849583
Epoch 5: 2022-05-06 19:28:49.439827: train loss: 1.645146620107958
Eval step 0: eval loss: 0.8396563354106296
Eval: 2022-05-06 19:29:15.501331: total loss: 1.080422448599652, mse:4.81252456534245, ic :0.053841548627822165, sharpe5:10.73461849451065, irr5:343.2230529785156, ndcg5:0.8526145319825487, pnl5:3.327960729598999 
train 6, step: 0, loss: 1.3381618274646132, grad_norm: 0.43039769382693055, ic: 0.11529548546862728
train 6, step: 500, loss: 1.0056779519704728, grad_norm: 0.0429723356245707, ic: 0.06130782240832687
train 6, step: 1000, loss: 1.1223062841611215, grad_norm: 0.09020570775043038, ic: 0.7407161034276376
train 6, step: 1500, loss: 1.5650855702802169, grad_norm: 0.7180170347651509, ic: 0.14905199767593183
train 6, step: 2000, loss: 0.8023916691947307, grad_norm: 0.04867182450319964, ic: 0.3602173561031482
Epoch 6: 2022-05-06 19:35:58.979017: train loss: 1.640207807466445
Eval step 0: eval loss: 0.8322510794668729
Eval: 2022-05-06 19:36:25.379287: total loss: 1.077688689099519, mse:4.748176319568568, ic :0.11992429580250677, sharpe5:11.555732147097586, irr5:384.4063720703125, ndcg5:0.84317117118625, pnl5:3.6467645168304443 
train 7, step: 0, loss: 0.9922325134277344, grad_norm: 0.04950505325654206, ic: 0.06338420886393588
train 7, step: 500, loss: 0.6501767700566347, grad_norm: 0.002294808606828779, ic: 0.039598357676747256
train 7, step: 1000, loss: 1.0252760853008378, grad_norm: 0.23536146491220883, ic: 0.05721970715529372
train 7, step: 1500, loss: 2.2484027498660235, grad_norm: 0.7151116620907693, ic: 0.4439045895488638
train 7, step: 2000, loss: 0.9177673044035238, grad_norm: 0.04786934358271793, ic: -0.04132756048170976
Epoch 7: 2022-05-06 19:43:07.362573: train loss: 1.632736914451915
Eval step 0: eval loss: 0.8329060658011722
Eval: 2022-05-06 19:43:33.209320: total loss: 1.073593775935754, mse:4.705202691213071, ic :0.14842655451143683, sharpe5:13.60689732670784, irr5:456.2281494140625, ndcg5:0.8396219830926036, pnl5:3.286454200744629 
train 8, step: 0, loss: 3.6013682489809784, grad_norm: 1.0830189228186735, ic: 0.16104996135667615
train 8, step: 500, loss: 2.7608000688639818, grad_norm: 0.8553059440900918, ic: 0.04070725495316936
train 8, step: 1000, loss: 3.053117923460145, grad_norm: 0.8775311175679916, ic: 0.11557762128321247
train 8, step: 1500, loss: 0.7172783829280981, grad_norm: 0.014822309821376238, ic: 0.44565337229084123
train 8, step: 2000, loss: 1.0819027429774737, grad_norm: 0.3526274415381108, ic: 0.5284140103584516
Epoch 8: 2022-05-06 19:50:11.356513: train loss: 1.62851216661318
Eval step 0: eval loss: 0.8280639648437499
Eval: 2022-05-06 19:50:37.251885: total loss: 1.0706794289283357, mse:4.685457596440639, ic :0.16440360252772032, sharpe5:16.710903364419938, irr5:530.8988037109375, ndcg5:0.8499772339542316, pnl5:5.635066986083984 
train 9, step: 0, loss: 5.430748168361244, grad_norm: 0.9074084768978588, ic: 0.12671621291397975
train 9, step: 500, loss: 1.340460609754144, grad_norm: 1.3050395913732304, ic: 0.32391102427425733
train 9, step: 1000, loss: 0.9278751868136802, grad_norm: 0.04045254266891338, ic: 0.0649771949234945
train 9, step: 1500, loss: 1.0867845313362943, grad_norm: 0.021613968122950886, ic: 0.4204717901536603
train 9, step: 2000, loss: 1.0725444306224645, grad_norm: 0.2899354387084018, ic: 0.2659472344868645
Epoch 9: 2022-05-06 19:57:13.880460: train loss: 1.626470506743289
Eval step 0: eval loss: 0.8329757835138961
Eval: 2022-05-06 19:57:39.742226: total loss: 1.0774082065347683, mse:4.700086889250121, ic :0.1579936494951377, sharpe5:17.99018464922905, irr5:586.2252197265625, ndcg5:0.8624240981775674, pnl5:9.609842300415039 
train 10, step: 0, loss: 7.164355753462099, grad_norm: 1.3465297746631792, ic: 0.24442057151623245
train 10, step: 500, loss: 1.1318270265588206, grad_norm: 0.107526985322097, ic: 0.05698746874305228
train 10, step: 1000, loss: 2.3500463567628453, grad_norm: 0.8981756768156004, ic: 0.1287422285247208
train 10, step: 1500, loss: 1.1129515016233766, grad_norm: 0.3250858056587366, ic: -0.009015054907169452
train 10, step: 2000, loss: 2.739798224081022, grad_norm: 1.0350924963600165, ic: 0.4310849724136838
Epoch 10: 2022-05-06 20:04:19.530194: train loss: 1.6279226405647298
Eval step 0: eval loss: 0.8280839668812565
Eval: 2022-05-06 20:04:45.679030: total loss: 1.0707672024152282, mse:4.68720660026819, ic :0.16599469039497286, sharpe5:17.648410338163377, irr5:559.5128173828125, ndcg5:0.835642456318398, pnl5:6.888942241668701 
train 11, step: 0, loss: 1.249501333804621, grad_norm: 0.03527359063461496, ic: 0.21497365620105885
train 11, step: 500, loss: 0.6580658064874653, grad_norm: 0.029741740845525586, ic: 0.5414670172426452
train 11, step: 1000, loss: 0.9304253561675598, grad_norm: 0.1204111278347029, ic: 0.0691880167498376
train 11, step: 1500, loss: 1.0536618818316543, grad_norm: 0.060269800527114545, ic: 0.18349549793361075
train 11, step: 2000, loss: 0.7853364628073164, grad_norm: 0.007661508762361446, ic: 0.14344555246202814
Epoch 11: 2022-05-06 20:11:27.028319: train loss: 1.626269530888273
Eval step 0: eval loss: 0.8283151158291622
Eval: 2022-05-06 20:11:53.299129: total loss: 1.0707294180528535, mse:4.6845020836582485, ic :0.16987989107986257, sharpe5:18.03408506155014, irr5:596.8798828125, ndcg5:0.8598890955796038, pnl5:8.829206466674805 
train 12, step: 0, loss: 0.9633736610412598, grad_norm: 0.13045108430143654, ic: 0.39485741912072614
train 12, step: 500, loss: 0.9306702609925267, grad_norm: 0.08493135368168556, ic: 0.1797611824771324
train 12, step: 1000, loss: 2.9388278062176556, grad_norm: 0.47431205035120605, ic: 0.2544940008762637
train 12, step: 1500, loss: 0.9480449040246572, grad_norm: 0.17643255641237077, ic: -0.12093900741618807
train 12, step: 2000, loss: 0.8728474504637934, grad_norm: 0.004806467852286289, ic: 0.24025781155795306
Epoch 12: 2022-05-06 20:18:37.632707: train loss: 1.6258888078435718
Eval step 0: eval loss: 0.8314419938339699
Eval: 2022-05-06 20:19:03.544453: total loss: 1.0702789455693376, mse:4.692604015069347, ic :0.16301875650302272, sharpe5:17.15374794602394, irr5:549.5789794921875, ndcg5:0.844505244606936, pnl5:6.317111968994141 
train 13, step: 0, loss: 2.071906503679382, grad_norm: 0.7455351749136709, ic: 0.350208941307704
train 13, step: 500, loss: 0.8280355705896146, grad_norm: 0.12310280628867906, ic: 0.5014044803759529
train 13, step: 1000, loss: 0.961790927144505, grad_norm: 0.397128457102015, ic: 0.4653243904093071
train 13, step: 1500, loss: 2.3881652974970726, grad_norm: 0.40146265111415125, ic: -0.023828264448636897
train 13, step: 2000, loss: 1.4773577894955252, grad_norm: 0.061584694208638935, ic: 0.18181817076699242
Epoch 13: 2022-05-06 20:25:42.833288: train loss: 1.6267582127329827
Eval step 0: eval loss: 0.8257850831878951
Eval: 2022-05-06 20:26:08.254876: total loss: 1.0699917581665779, mse:4.69572579135152, ic :0.16327559060436972, sharpe5:17.85391837477684, irr5:575.6519775390625, ndcg5:0.854078911188216, pnl5:7.199195384979248 
train 14, step: 0, loss: 4.511649811695593, grad_norm: 1.418727124269064, ic: 0.20492645477446433
train 14, step: 500, loss: 0.8289813587060397, grad_norm: 0.010805848572540239, ic: 0.11147729588923608
train 14, step: 1000, loss: 1.829233179693432, grad_norm: 0.3307607010504472, ic: 0.3770736057510984
train 14, step: 1500, loss: 1.1292705446060636, grad_norm: 0.06384328274525918, ic: -0.08299259673335148
train 14, step: 2000, loss: 1.1623522411089762, grad_norm: 0.2825721285474531, ic: 0.0724741745384343
Epoch 14: 2022-05-06 20:33:01.292191: train loss: 1.6238940110324307
Eval step 0: eval loss: 0.8332410195353661
Eval: 2022-05-06 20:33:28.078568: total loss: 1.0706131762858977, mse:4.682315608399692, ic :0.16772555102129544, sharpe5:16.889480895996094, irr5:553.1480102539062, ndcg5:0.8517445943702792, pnl5:5.627673149108887 
train 15, step: 0, loss: 3.462769789640078, grad_norm: 2.2385747624190344, ic: 0.08809810044875421
train 15, step: 500, loss: 1.2546671853893911, grad_norm: 0.10486962228489485, ic: 0.06809052343342668
train 15, step: 1000, loss: 1.316063857660061, grad_norm: 0.12601988872898207, ic: 0.03855688862751538
train 15, step: 1500, loss: 0.8550524613988681, grad_norm: 0.27070359806616673, ic: 0.07581568900508581
train 15, step: 2000, loss: 1.4578630382887983, grad_norm: 0.5624629348419947, ic: 0.057548862613248396
Epoch 15: 2022-05-06 20:39:58.372089: train loss: 1.6241096221723972
Eval step 0: eval loss: 0.8452444287572444
Eval: 2022-05-06 20:40:23.006898: total loss: 1.0748959214379576, mse:4.685301073107481, ic :0.169270597977201, sharpe5:15.806020422577857, irr5:531.115234375, ndcg5:0.8476196021901553, pnl5:3.4041686058044434 
train 16, step: 0, loss: 0.6893624179625643, grad_norm: 0.2903804440370071, ic: 0.02497022675100477
train 16, step: 500, loss: 1.6233548618233855, grad_norm: 1.2922857428837355, ic: 0.18951854906193513
train 16, step: 1000, loss: 0.8821019028172349, grad_norm: 0.022011109525480556, ic: -0.07388134992881255
train 16, step: 1500, loss: 0.8456063537357171, grad_norm: 0.3350978331924085, ic: 0.1495469263213658
train 16, step: 2000, loss: 3.351051034462737, grad_norm: 3.2519394028305952, ic: -0.00035956472142477033
Epoch 16: 2022-05-06 20:46:56.413674: train loss: 1.6232840463244507
Eval step 0: eval loss: 0.8310472269329556
Eval: 2022-05-06 20:47:21.177316: total loss: 1.0709236365169863, mse:4.700611482420304, ic :0.16702216125804928, sharpe5:17.09948964715004, irr5:539.6919555664062, ndcg5:0.8529101293936155, pnl5:8.795320510864258 
train 17, step: 0, loss: 1.2802643712698938, grad_norm: 0.3003004538377232, ic: -0.12055537027147228
train 17, step: 500, loss: 1.7291437743478997, grad_norm: 0.7872609092335145, ic: 0.2217866383347963
train 17, step: 1000, loss: 1.2844827273744037, grad_norm: 0.11623152032973962, ic: 0.15763989027345987
train 17, step: 1500, loss: 4.518830220196547, grad_norm: 1.9920320322437912, ic: 0.202778205702847
train 17, step: 2000, loss: 1.2724259770286397, grad_norm: 0.9271195851471614, ic: 0.10672128764606474
Epoch 17: 2022-05-06 20:53:53.093648: train loss: 1.6232316143611651
Eval step 0: eval loss: 0.8360536533110511
Eval: 2022-05-06 20:54:17.403857: total loss: 1.0729851850283316, mse:4.698209764583748, ic :0.1641808441489287, sharpe5:17.8709015250206, irr5:600.4976806640625, ndcg5:0.8510247800533585, pnl5:7.01122522354126 
train 18, step: 0, loss: 1.413574314792732, grad_norm: 0.6398742262637962, ic: 0.21903133644855813
train 18, step: 500, loss: 1.4893187983946226, grad_norm: 0.8609372834172201, ic: -0.026917918861479897
train 18, step: 1000, loss: 0.6678845382063355, grad_norm: 0.014234313971956028, ic: 0.502846949457786
train 18, step: 1500, loss: 1.4230317186860157, grad_norm: 0.08449769890966993, ic: 0.2035058184727267
train 18, step: 2000, loss: 0.9136171766147493, grad_norm: 0.009281404882486643, ic: -0.03696213840073155
Epoch 18: 2022-05-06 21:00:36.984467: train loss: 1.6218468758236713
Eval step 0: eval loss: 0.8241183539210681
Eval: 2022-05-06 21:01:00.944645: total loss: 1.0681057797778737, mse:4.678407021016243, ic :0.17410836962801818, sharpe5:18.57317380070686, irr5:603.0166625976562, ndcg5:0.8576685256845353, pnl5:9.232077598571777 
train 19, step: 0, loss: 1.4857644701760913, grad_norm: 0.9900905170601259, ic: -0.02100739856521879
train 19, step: 500, loss: 0.860253510651765, grad_norm: 0.0763551988414511, ic: 0.23452111350185562
train 19, step: 1000, loss: 0.9581301608769932, grad_norm: 0.04938760532805604, ic: 0.20414780802207158
train 19, step: 1500, loss: 3.9826078542252286, grad_norm: 1.3082025842636735, ic: 0.1460126466606421
train 19, step: 2000, loss: 1.004410400390625, grad_norm: 0.16587046133011218, ic: 0.22737392433825052
Epoch 19: 2022-05-06 21:07:31.442212: train loss: 1.6226514739174827
Eval step 0: eval loss: 0.8227168606757113
Eval: 2022-05-06 21:07:55.938464: total loss: 1.0692858669995744, mse:4.685438319198967, ic :0.1721863803984088, sharpe5:18.650611063241957, irr5:601.7373046875, ndcg5:0.8331448469160367, pnl5:11.158720970153809 
train 20, step: 0, loss: 2.266477272727273, grad_norm: 0.9156570579376171, ic: 0.031355745183406156
train 20, step: 500, loss: 3.2367215909090907, grad_norm: 0.6226300492423023, ic: 0.06814956287715308
train 20, step: 1000, loss: 0.9675758361816407, grad_norm: 0.2158512396199343, ic: 0.17729141333113344
train 20, step: 1500, loss: 1.6626473816759189, grad_norm: 1.5631002052822258, ic: 0.25770775863409445
train 20, step: 2000, loss: 1.022908196752651, grad_norm: 0.04572046144866054, ic: 0.049199433668080675
Epoch 20: 2022-05-06 21:14:22.542486: train loss: 1.6197410981231009
Eval step 0: eval loss: 0.8275141339156348
Eval: 2022-05-06 21:14:47.049671: total loss: 1.0683090929040588, mse:4.657635825147, ic :0.17967455439892308, sharpe5:17.63911567211151, irr5:588.2018432617188, ndcg5:0.8646685453178716, pnl5:5.227969169616699 
train 21, step: 0, loss: 1.0134457981741039, grad_norm: 0.3291907706728136, ic: 0.07871204478591054
train 21, step: 500, loss: 0.7646865169558905, grad_norm: 0.03087547957057211, ic: 0.2083987695400032
train 21, step: 1000, loss: 0.9485072754977042, grad_norm: 0.8109735916547578, ic: 0.18104325722740877
train 21, step: 1500, loss: 0.9920020656160007, grad_norm: 0.20447624859337232, ic: 0.31512471888049415
train 21, step: 2000, loss: 0.9399808797065338, grad_norm: 0.06091059099696346, ic: 0.07765090895828
Epoch 21: 2022-05-06 21:21:12.083960: train loss: 1.6221228159267402
Eval step 0: eval loss: 0.8336657573350237
Eval: 2022-05-06 21:21:37.024513: total loss: 1.0720670622344084, mse:4.69700823946717, ic :0.16166399668737486, sharpe5:17.82641653060913, irr5:576.629150390625, ndcg5:0.8497319180212975, pnl5:6.101362228393555 
train 22, step: 0, loss: 1.0377750720007946, grad_norm: 0.046950750290972953, ic: 0.2316410436366524
train 22, step: 500, loss: 3.240186142339939, grad_norm: 0.6997618644967596, ic: -0.22326423323641453
train 22, step: 1000, loss: 1.2042776824421966, grad_norm: 0.09630979802804535, ic: 0.45901256789875755
train 22, step: 1500, loss: 0.9737101739326132, grad_norm: 0.122074768824211, ic: 0.06383669075609884
train 22, step: 2000, loss: 1.7513167539151078, grad_norm: 1.258919210697509, ic: 0.15948424524232058
Epoch 22: 2022-05-06 21:28:06.014098: train loss: 1.6196287417217727
Eval step 0: eval loss: 0.8277910753095363
Eval: 2022-05-06 21:28:30.839829: total loss: 1.0692155124535372, mse:4.66466295331037, ic :0.17557860739193898, sharpe5:17.48079041004181, irr5:576.1276245117188, ndcg5:0.8427439079899006, pnl5:9.661026954650879 
train 23, step: 0, loss: 0.9779249031880404, grad_norm: 0.054712410905415135, ic: 0.1844621945411792
train 23, step: 500, loss: 1.4249568442160518, grad_norm: 0.18775339331866042, ic: 0.05234785018513883
train 23, step: 1000, loss: 1.6391385904947917, grad_norm: 0.11057254940497116, ic: 0.2529348392418607
train 23, step: 1500, loss: 1.1237319864315523, grad_norm: 0.7079565352613972, ic: 0.10489822842615229
train 23, step: 2000, loss: 1.818421020977675, grad_norm: 2.4266032599175587, ic: 0.3616531556435452
Epoch 23: 2022-05-06 21:35:02.770265: train loss: 1.6211836371528805
Eval step 0: eval loss: 0.8379346809347009
Eval: 2022-05-06 21:35:27.279531: total loss: 1.0735032374015505, mse:4.690006140513184, ic :0.1558238085893741, sharpe5:15.816583575010299, irr5:529.3058471679688, ndcg5:0.8382671387258268, pnl5:4.72537088394165 
train 24, step: 0, loss: 2.2222704565685905, grad_norm: 0.15479720266110858, ic: 0.12053098054790765
train 24, step: 500, loss: 1.2109420598249028, grad_norm: 0.17314207542480156, ic: 0.12763382085311492
train 24, step: 1000, loss: 0.9227507576436952, grad_norm: 0.048448609843475855, ic: 0.43239677259955145
train 24, step: 1500, loss: 2.6259854183841713, grad_norm: 2.3948172944131207, ic: -0.006746836920422307
train 24, step: 2000, loss: 0.9385865949088222, grad_norm: 0.3766322938910465, ic: 0.04487078269627168
Epoch 24: 2022-05-06 21:41:58.697317: train loss: 1.6154730425806518
Eval step 0: eval loss: 0.8274617813158588
Eval: 2022-05-06 21:42:23.284532: total loss: 1.0725061120656636, mse:4.702296272049475, ic :0.16395914399877426, sharpe5:17.467776364088056, irr5:581.9129028320312, ndcg5:0.8537588715063219, pnl5:5.427043437957764 
train 25, step: 0, loss: 0.8551376755173142, grad_norm: 0.07452969310491828, ic: 0.5545631929912938
train 25, step: 500, loss: 0.8650180782076096, grad_norm: 0.022932647453773197, ic: 0.24198346880899135
train 25, step: 1000, loss: 2.0885587112647563, grad_norm: 0.19056365103125078, ic: 0.2656558579882998
train 25, step: 1500, loss: 1.1544457049750507, grad_norm: 0.4844640905277148, ic: 0.4539390980149022
train 25, step: 2000, loss: 1.0199173683097675, grad_norm: 1.064513565311178, ic: 0.5598167294029768
Epoch 25: 2022-05-06 21:48:53.040760: train loss: 1.6190720135559762
Eval step 0: eval loss: 0.8275639139060853
Eval: 2022-05-06 21:49:17.506473: total loss: 1.0715540109190655, mse:4.683556009365408, ic :0.16818120417290186, sharpe5:18.009496668577192, irr5:593.2919921875, ndcg5:0.8522063076384825, pnl5:7.68017053604126 
train 26, step: 0, loss: 6.625553801417731, grad_norm: 1.3143061089401686, ic: 0.18002102476965548
train 26, step: 500, loss: 3.937973392298671, grad_norm: 2.8614653685350717, ic: 0.3550726455249825
train 26, step: 1000, loss: 1.2713759106406988, grad_norm: 1.1482452113791117, ic: -0.022400592176307726
train 26, step: 1500, loss: 0.832698444137518, grad_norm: 0.18458054601348808, ic: 0.3054592596712585
train 26, step: 2000, loss: 0.9526369108354642, grad_norm: 0.3222926464229041, ic: 0.19277199002273
Epoch 26: 2022-05-06 21:55:40.008580: train loss: 1.6169666969367842
Eval step 0: eval loss: 0.8278956518786221
Eval: 2022-05-06 21:56:03.440659: total loss: 1.068086014073894, mse:4.630996382208754, ic :0.18092769394807182, sharpe5:17.830376175642012, irr5:579.068359375, ndcg5:0.8501248151282275, pnl5:12.594095230102539 
train 27, step: 0, loss: 0.8303216911764706, grad_norm: 0.03237160129510183, ic: 0.09684117307815081
train 27, step: 500, loss: 0.9025438165567952, grad_norm: 1.0315311921912844, ic: 0.27066569816449493
train 27, step: 1000, loss: 0.7406005859375, grad_norm: 0.2978024304256174, ic: 0.20302693864089855
train 27, step: 1500, loss: 0.6467635826135746, grad_norm: 0.09417741346947037, ic: 0.478338016010633
train 27, step: 2000, loss: 1.384030793232726, grad_norm: 0.05790143134849898, ic: 0.028059214667496044
Epoch 27: 2022-05-06 22:02:27.555413: train loss: 1.617969900145136
Eval step 0: eval loss: 0.8306911134870587
Eval: 2022-05-06 22:02:59.219473: total loss: 1.0688164814305274, mse:4.615723363988171, ic :0.1820014837192194, sharpe5:17.32345080256462, irr5:574.6759643554688, ndcg5:0.860465670738867, pnl5:9.144194602966309 
train 28, step: 0, loss: 1.531588214888996, grad_norm: 0.45193694480683644, ic: 0.2677473741303796
train 28, step: 500, loss: 1.4082480228566938, grad_norm: 1.352105986041175, ic: 0.10218102116945788
train 28, step: 1000, loss: 0.917566314945376, grad_norm: 0.2171136518601011, ic: 0.5558513324932881
train 28, step: 1500, loss: 1.0402001156395688, grad_norm: 0.049306784583847615, ic: 0.018874549692897286
train 28, step: 2000, loss: 1.0441897339616086, grad_norm: 0.20805133670911735, ic: 0.13720896971764723
Epoch 28: 2022-05-06 22:10:44.367652: train loss: 1.611940225768621
Eval step 0: eval loss: 0.8270609687829293
Eval: 2022-05-06 22:11:14.802593: total loss: 1.082603759637653, mse:4.715683966752872, ic :0.1728096335571691, sharpe5:18.06591167807579, irr5:588.3140258789062, ndcg5:0.8441867775476427, pnl5:6.629848957061768 
train 29, step: 0, loss: 0.904472530385235, grad_norm: 0.047103529133654634, ic: 0.08905903989038012
train 29, step: 500, loss: 1.1159771904130578, grad_norm: 0.14021908306106787, ic: 0.5947207445385458
train 29, step: 1000, loss: 1.0457414230381812, grad_norm: 1.0802737919989682, ic: 0.06770747038331161
train 29, step: 1500, loss: 2.341345948599727, grad_norm: 0.9519970236245501, ic: -0.0017098698451291014
train 29, step: 2000, loss: 3.9285557122878085, grad_norm: 14.657287609987472, ic: 0.21799596658937417
Epoch 29: 2022-05-06 22:17:24.471788: train loss: 1.6144084854348342
Eval step 0: eval loss: 0.8332059034180715
Eval: 2022-05-06 22:17:47.876590: total loss: 1.0679847109352751, mse:4.589677198953797, ic :0.18850320956835914, sharpe5:17.298907814025878, irr5:581.6920776367188, ndcg5:0.8581197237175643, pnl5:5.080089569091797 
train 30, step: 0, loss: 1.0108280122018205, grad_norm: 0.07476950668025183, ic: 0.5143914625341978
train 30, step: 500, loss: 1.4324890086648894, grad_norm: 1.201927800124806, ic: 0.025552893747869735
train 30, step: 1000, loss: 0.9806544448390152, grad_norm: 0.11115425862808202, ic: -0.023765033428178005
train 30, step: 1500, loss: 1.4722023602788563, grad_norm: 1.136136915951405, ic: 0.16647094655428366
train 30, step: 2000, loss: 1.8428253664164116, grad_norm: 0.49049367685365625, ic: 0.09919033024905773
Epoch 30: 2022-05-06 22:25:28.543720: train loss: 1.6139927270501002
Eval step 0: eval loss: 0.8377749218955808
Eval: 2022-05-06 22:25:58.866638: total loss: 1.085138241571215, mse:4.926855223890141, ic :0.1663177468497089, sharpe5:17.464204548597333, irr5:590.8843383789062, ndcg5:0.8441089096410965, pnl5:5.717231273651123 
train 31, step: 0, loss: 1.0591436058059993, grad_norm: 0.7180044165618278, ic: 0.37041849619634337
train 31, step: 500, loss: 1.4966658227237655, grad_norm: 1.6771155920963752, ic: -0.005993945949889692
train 31, step: 1000, loss: 4.411683224775566, grad_norm: 2.9588024364784635, ic: 0.4751785741403126
train 31, step: 1500, loss: 0.7763286646450567, grad_norm: 0.04754225302212945, ic: 0.7067357059589595
train 31, step: 2000, loss: 1.2322044488506838, grad_norm: 1.2467263683111613, ic: 0.16706512863440448
Epoch 31: 2022-05-06 22:33:29.778172: train loss: 1.6087769415277433
Eval step 0: eval loss: 0.8381817157451923
Eval: 2022-05-06 22:34:01.002294: total loss: 1.0702706937716877, mse:4.615501461042329, ic :0.17892930347191138, sharpe5:16.909710463285446, irr5:560.75, ndcg5:0.8515824971080231, pnl5:3.367593765258789 
train 32, step: 0, loss: 1.133167293605076, grad_norm: 0.022492675757468637, ic: 0.17113222294657404
train 32, step: 500, loss: 1.4864172267162894, grad_norm: 1.0745491806851553, ic: 0.12614770280962667
train 32, step: 1000, loss: 1.0382756067000154, grad_norm: 0.11566719717252584, ic: 0.5109745938639898
train 32, step: 1500, loss: 0.9712391728940218, grad_norm: 1.601634795082719, ic: 0.0751137949433949
train 32, step: 2000, loss: 0.9454098700888727, grad_norm: 0.09160958335247507, ic: 0.5563600096610519
Epoch 32: 2022-05-06 22:41:53.266197: train loss: 1.6118217199293312
Eval step 0: eval loss: 0.8221002705355966
Eval: 2022-05-06 22:42:23.166990: total loss: 1.0667743579871298, mse:4.630773903694089, ic :0.1872764514540662, sharpe5:18.18187649488449, irr5:610.8866577148438, ndcg5:0.8401999999544212, pnl5:9.440062522888184 
train 33, step: 0, loss: 1.2729485156812455, grad_norm: 0.38148524223365876, ic: 0.23251582156226674
train 33, step: 500, loss: 0.9931404690782563, grad_norm: 0.1023639334832951, ic: 0.15599855705500118
train 33, step: 1000, loss: 1.0584091399132503, grad_norm: 1.914245067780716, ic: 0.21691000719372322
train 33, step: 1500, loss: 0.9004770733707741, grad_norm: 0.13687443516759729, ic: 0.4866512906992093
train 33, step: 2000, loss: 0.819887091731874, grad_norm: 0.07097307654747431, ic: 0.20780397226261652
Epoch 33: 2022-05-06 22:49:56.030014: train loss: 1.611176811215052
Eval step 0: eval loss: 0.8256200502996575
Eval: 2022-05-06 22:50:26.661981: total loss: 1.065811639287451, mse:4.578918705273236, ic :0.19786879938934665, sharpe5:17.986624184846878, irr5:618.3385620117188, ndcg5:0.8443322192580155, pnl5:6.031053066253662 
train 34, step: 0, loss: 1.0272925838176612, grad_norm: 0.872533749339033, ic: 0.5519072004915663
train 34, step: 500, loss: 0.806060697689698, grad_norm: 0.39907048314003063, ic: 0.2872457058457901
train 34, step: 1000, loss: 3.1848778321812596, grad_norm: 0.985544837659116, ic: 0.32390505887269877
train 34, step: 1500, loss: 0.8152401802508566, grad_norm: 0.23781285663017607, ic: 0.6864789500664104
train 34, step: 2000, loss: 6.257116576297444, grad_norm: 96.48887328209359, ic: 0.4520283766460911
Epoch 34: 2022-05-06 22:58:06.820964: train loss: 1.6128975793344893
Eval step 0: eval loss: 0.8222163595314146
Eval: 2022-05-06 22:58:35.259695: total loss: 1.0655487303714242, mse:4.595314024695328, ic :0.19455689502453336, sharpe5:17.870973415374756, irr5:614.1798706054688, ndcg5:0.8515947373565919, pnl5:6.240342140197754 
train 35, step: 0, loss: 1.1847137810202204, grad_norm: 0.7268246297719877, ic: 0.5554386518937597
train 35, step: 500, loss: 1.1709980500689903, grad_norm: 0.591066722628631, ic: 0.1345091919025963
train 35, step: 1000, loss: 1.769964845823381, grad_norm: 3.3284117803349718, ic: 0.07601276389444503
train 35, step: 1500, loss: 1.60999765037594, grad_norm: 1.6174110976064229, ic: 0.033220865422326315
train 35, step: 2000, loss: 0.7867079138118315, grad_norm: 0.1743127800946141, ic: 0.5756882633013211
Epoch 35: 2022-05-06 23:06:15.219996: train loss: 1.6090530795049032
Eval step 0: eval loss: 0.8328861280788988
Eval: 2022-05-06 23:06:45.104729: total loss: 1.0665003221408913, mse:4.580772789728503, ic :0.19462390810471009, sharpe5:17.659566586017608, irr5:598.6912841796875, ndcg5:0.8402202433500923, pnl5:5.852443695068359 
train 36, step: 0, loss: 1.846334939474588, grad_norm: 2.1153283576558843, ic: 0.08852723765711827
train 36, step: 500, loss: 0.837684962014874, grad_norm: 0.050005488119866666, ic: 0.1500635226231429
train 36, step: 1000, loss: 1.6753139204545453, grad_norm: 2.8829264233286587, ic: 0.2948885780226701
train 36, step: 1500, loss: 0.7686733471410007, grad_norm: 0.050141831387754604, ic: 0.3788708621581157
train 36, step: 2000, loss: 1.1194029937574463, grad_norm: 0.7483139883033871, ic: 0.7794611227064571
Epoch 36: 2022-05-06 23:14:15.956080: train loss: 1.6056687541724923
Eval step 0: eval loss: 0.8288502828841214
Eval: 2022-05-06 23:14:45.286559: total loss: 1.0681065338742604, mse:4.6050463962125745, ic :0.1878692155478233, sharpe5:17.439945337772368, irr5:588.3953857421875, ndcg5:0.8512846053552259, pnl5:8.920191764831543 
train 37, step: 0, loss: 1.9828454758087672, grad_norm: 2.895238313850193, ic: 0.20534042361357932
train 37, step: 500, loss: 2.3458375417388866, grad_norm: 3.735174862691183, ic: -0.04127932288320605
train 37, step: 1000, loss: 1.0700955790108924, grad_norm: 0.22234937123759052, ic: 0.07689422145131689
train 37, step: 1500, loss: 2.00032673450991, grad_norm: 1.112699267876749, ic: 0.6100302604284075
train 37, step: 2000, loss: 1.3078548393376246, grad_norm: 0.26895021040499373, ic: 0.22575479257093956
Epoch 37: 2022-05-06 23:22:15.026805: train loss: 1.6054988375914498
Eval step 0: eval loss: 0.8298769469507375
Eval: 2022-05-06 23:22:46.380112: total loss: 1.0662653425726305, mse:4.588491140572647, ic :0.193454762298055, sharpe5:17.79863280057907, irr5:591.6888427734375, ndcg5:0.8427992749610279, pnl5:5.227784633636475 
train 38, step: 0, loss: 1.3293236523139769, grad_norm: 0.464764661741531, ic: -0.08312165305307753
train 38, step: 500, loss: 0.8947429892457561, grad_norm: 0.12246712354189644, ic: 0.2603518084357747
train 38, step: 1000, loss: 0.8968286808300395, grad_norm: 0.40342208425289827, ic: 0.19899750146747372
train 38, step: 1500, loss: 0.9524110230697134, grad_norm: 0.048876736706827656, ic: 0.2109116247532874
train 38, step: 2000, loss: 2.2816250619571377, grad_norm: 5.171342402743867, ic: -0.0010032141071951717
Epoch 38: 2022-05-06 23:30:18.352822: train loss: 1.6060645778036131
Eval step 0: eval loss: 0.8254515443888303
Eval: 2022-05-06 23:30:48.281927: total loss: 1.0643977765118988, mse:4.581547249040912, ic :0.19781824561745276, sharpe5:18.707903891801834, irr5:620.4745483398438, ndcg5:0.8455372544309356, pnl5:8.66148567199707 
train 39, step: 0, loss: 0.9690142056993024, grad_norm: 0.013410584024274138, ic: 0.06604364941477338
train 39, step: 500, loss: 0.8882383893399308, grad_norm: 0.23293171471939034, ic: 0.2418297290113904
train 39, step: 1000, loss: 0.9355375779464966, grad_norm: 0.19150533917669615, ic: 0.25294772803943777
train 39, step: 1500, loss: 2.086454412638669, grad_norm: 0.10505455836168665, ic: 0.2200630065689887
train 39, step: 2000, loss: 0.6028644869112856, grad_norm: 0.058651243628856556, ic: 0.1807890603122958
Epoch 39: 2022-05-06 23:38:21.801991: train loss: 1.6044033049189927
Eval step 0: eval loss: 0.8297195675752436
Eval: 2022-05-06 23:38:51.557370: total loss: 1.067007006638294, mse:4.614367322310525, ic :0.18879417195091988, sharpe5:18.16768760919571, irr5:601.4766845703125, ndcg5:0.848348569794495, pnl5:11.731039047241211 
