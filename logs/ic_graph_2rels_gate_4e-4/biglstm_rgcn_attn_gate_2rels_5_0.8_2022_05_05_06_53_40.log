Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_5_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
32888
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795790397160868, grad_norm: 4.816657405557322, ic: 0.022771143722705188
train 0, step: 500, loss: 0.8631224758030331, grad_norm: 0.026533478524201404, ic: 0.0449061831577456
train 0, step: 1000, loss: 1.9501088720738844, grad_norm: 0.5126869276782424, ic: 0.01552964555297991
train 0, step: 1500, loss: 0.956497228569664, grad_norm: 0.04930699311385374, ic: 0.015144869805247532
train 0, step: 2000, loss: 1.0022102723969581, grad_norm: 0.15813638484903014, ic: 0.02568257012552532
Epoch 0: 2022-05-05 19:00:34.003521: train loss: 1.6484921835399335
Eval step 0: eval loss: 0.8363093706780163
Eval: 2022-05-05 19:00:57.214021: total loss: 1.0793448149674563, mse:4.822852621673272, ic :0.008269625252338703, sharpe5:7.9262421467900275, irr5:224.05067443847656, ndcg5:0.860296571007132, pnl5:2.6144354343414307 
train 1, step: 0, loss: 2.7747735792590724, grad_norm: 0.8785124745132588, ic: 0.057232531668868145
train 1, step: 500, loss: 1.7553667917125553, grad_norm: 0.7703374916147481, ic: 0.09705660522037876
train 1, step: 1000, loss: 0.8777592169102546, grad_norm: 0.17542241305989187, ic: 0.07997643898717235
train 1, step: 1500, loss: 1.7132394374102011, grad_norm: 0.20665156486824293, ic: -0.03157393933536753
train 1, step: 2000, loss: 2.1772296875, grad_norm: 0.8875310562074669, ic: -0.0457630968013273
Epoch 1: 2022-05-05 19:07:10.116504: train loss: 1.6467339168716124
Eval step 0: eval loss: 0.8345874589411552
Eval: 2022-05-05 19:07:33.827045: total loss: 1.0789842468834225, mse:4.823485775288287, ic :0.008041035404536578, sharpe5:7.540817147791385, irr5:212.47622680664062, ndcg5:0.8617279794433081, pnl5:2.5652177333831787 
train 2, step: 0, loss: 2.141768821022727, grad_norm: 0.00930786270560029, ic: 0.132635802676078
train 2, step: 500, loss: 3.3001424344679187, grad_norm: 0.28517566943583733, ic: 0.050034193086728
train 2, step: 1000, loss: 2.0723251167385057, grad_norm: 0.0002062504051888151, ic: 0.1887608250295742
train 2, step: 1500, loss: 1.485520038167939, grad_norm: 0.059718129555506226, ic: -0.03675969732086904
train 2, step: 2000, loss: 3.2351389723557693, grad_norm: 0.7876506906412801, ic: 0.2078780791459604
Epoch 2: 2022-05-05 19:13:46.483178: train loss: 1.6464912314413278
Eval step 0: eval loss: 0.8359031556655031
Eval: 2022-05-05 19:14:09.563327: total loss: 1.0794871676556768, mse:4.823016385378854, ic :0.011252361105200729, sharpe5:7.637527173757553, irr5:215.09629821777344, ndcg5:0.8417345430604701, pnl5:2.5038349628448486 
train 3, step: 0, loss: 1.5231369886941057, grad_norm: 0.525177473404042, ic: 0.0006821987924612669
train 3, step: 500, loss: 1.5015393095083691, grad_norm: 0.3424775045044386, ic: 0.09660071984043143
train 3, step: 1000, loss: 3.6790828408354925, grad_norm: 0.7059027947238979, ic: -0.04865420926538502
train 3, step: 1500, loss: 1.9820143079991817, grad_norm: 1.2945586379479774, ic: -0.06793686185388563
train 3, step: 2000, loss: 0.898846534522804, grad_norm: 0.0012606477344862242, ic: 0.00995526671498434
Epoch 3: 2022-05-05 19:20:22.188283: train loss: 1.6458816402886354
Eval step 0: eval loss: 0.8346220605365845
Eval: 2022-05-05 19:20:45.212866: total loss: 1.07926929294288, mse:4.824549379967431, ic :0.01708095638987118, sharpe5:7.807283002138138, irr5:217.5765380859375, ndcg5:0.854301579620733, pnl5:2.412874221801758 
train 4, step: 0, loss: 1.4319662786989797, grad_norm: 0.04375007140123848, ic: 0.11722811491608814
train 4, step: 500, loss: 1.6486331969734251, grad_norm: 0.5586241648492287, ic: 0.04022215693691524
train 4, step: 1000, loss: 2.961859726324314, grad_norm: 0.7819979292116377, ic: 0.080667900502979
train 4, step: 1500, loss: 2.1500103012921943, grad_norm: 0.4760896504526907, ic: 0.009743619162671146
train 4, step: 2000, loss: 1.081871495461946, grad_norm: 0.39368003038628624, ic: 0.23704489102141568
Epoch 4: 2022-05-05 19:26:58.493103: train loss: 1.6452090791231746
Eval step 0: eval loss: 0.8567687538074618
Eval: 2022-05-05 19:27:21.978342: total loss: 1.0880859354201524, mse:4.832799970273412, ic :0.046821147001311875, sharpe5:10.072931161522865, irr5:301.8880310058594, ndcg5:0.851668342640975, pnl5:2.8942599296569824 
train 5, step: 0, loss: 1.3684354026845638, grad_norm: 0.2072511459532192, ic: 0.03138204887513779
train 5, step: 500, loss: 0.8872699540914949, grad_norm: 0.007352643487658505, ic: 0.040727110013505
train 5, step: 1000, loss: 0.9802380792025862, grad_norm: 0.14765822270043477, ic: -0.002720218204210395
train 5, step: 1500, loss: 1.5291471379091692, grad_norm: 0.1542041489143873, ic: 0.03865326762516427
train 5, step: 2000, loss: 1.1038879252918359, grad_norm: 0.029536676024969073, ic: 0.1574732593736176
Epoch 5: 2022-05-05 19:33:32.833240: train loss: 1.6435049093890466
Eval step 0: eval loss: 0.8445225545804794
Eval: 2022-05-05 19:33:56.067403: total loss: 1.084995473451518, mse:4.802316072268363, ic :0.10790186555681702, sharpe5:11.476891130805015, irr5:381.8096923828125, ndcg5:0.8460585149855786, pnl5:2.229393720626831 
train 6, step: 0, loss: 1.3495149505955941, grad_norm: 0.46041974425579063, ic: 0.08624237658220121
train 6, step: 500, loss: 1.0077090299877967, grad_norm: 0.0431622040078634, ic: 0.02936543183907555
train 6, step: 1000, loss: 1.108359727750713, grad_norm: 0.07777337047087503, ic: 0.7556335785398722
train 6, step: 1500, loss: 1.5929053944989668, grad_norm: 1.1787482263932603, ic: 0.03301017910185383
train 6, step: 2000, loss: 0.8120592176866492, grad_norm: 0.0828450600306882, ic: 0.33813306299597073
Epoch 6: 2022-05-05 19:40:10.050586: train loss: 1.6371701996497043
Eval step 0: eval loss: 0.8284377006635273
Eval: 2022-05-05 19:40:33.747754: total loss: 1.073260645911934, mse:4.712304193816488, ic :0.1447689338032844, sharpe5:12.875798910856247, irr5:443.0848693847656, ndcg5:0.8522019197694192, pnl5:3.1949121952056885 
train 7, step: 0, loss: 0.9872185707092286, grad_norm: 0.0686648396994165, ic: 0.17065747843750417
train 7, step: 500, loss: 0.6516156877790179, grad_norm: 0.0025233314963516488, ic: 0.039589805448940536
train 7, step: 1000, loss: 1.0239948583575305, grad_norm: 0.3135582693585217, ic: 0.09675663877431886
train 7, step: 1500, loss: 2.252248291800643, grad_norm: 0.7858592735382376, ic: 0.45313233224715316
train 7, step: 2000, loss: 0.9162716965683058, grad_norm: 0.062137535410997785, ic: -0.03565314088266694
Epoch 7: 2022-05-05 19:46:47.552926: train loss: 1.6316866515280728
Eval step 0: eval loss: 0.8307003105653977
Eval: 2022-05-05 19:47:10.818712: total loss: 1.0726806304450642, mse:4.688635916953791, ic :0.1610605813453269, sharpe5:15.429167402386664, irr5:499.1230773925781, ndcg5:0.8524705535319522, pnl5:3.767595052719116 
train 8, step: 0, loss: 3.5869151239809782, grad_norm: 1.1380859564797956, ic: 0.16162317263224346
train 8, step: 500, loss: 2.7709474882456306, grad_norm: 1.2871159665663057, ic: 0.050284478376122245
train 8, step: 1000, loss: 3.0723208220108695, grad_norm: 0.9472711851330385, ic: 0.10659933731537333
train 8, step: 1500, loss: 0.7141491834685189, grad_norm: 0.046302073871861214, ic: 0.4626259801491578
train 8, step: 2000, loss: 1.0899555215764578, grad_norm: 0.3263118634517326, ic: 0.5094504994342146
Epoch 8: 2022-05-05 19:53:27.677708: train loss: 1.6286400606184044
Eval step 0: eval loss: 0.8253402147202646
Eval: 2022-05-05 19:53:51.441859: total loss: 1.0704395222763017, mse:4.679014584131175, ic :0.16668041009801843, sharpe5:17.048856135606766, irr5:547.4822387695312, ndcg5:0.8485627107129536, pnl5:6.319948673248291 
train 9, step: 0, loss: 5.435808537679426, grad_norm: 0.9852375535783376, ic: 0.1751045537088245
train 9, step: 500, loss: 1.350716768839713, grad_norm: 1.8596457766996382, ic: 0.30904023533557157
train 9, step: 1000, loss: 0.924106741969417, grad_norm: 0.02486144699687736, ic: 0.09489427138918725
train 9, step: 1500, loss: 1.087315510228737, grad_norm: 0.03524377239528094, ic: 0.41472494336327315
train 9, step: 2000, loss: 1.0578430051977687, grad_norm: 0.3071639258113069, ic: 0.28393576195300907
Epoch 9: 2022-05-05 20:00:06.331235: train loss: 1.6274648983727276
Eval step 0: eval loss: 0.8255819113664054
Eval: 2022-05-05 20:00:29.977705: total loss: 1.0703121413176546, mse:4.688494346342579, ic :0.16544548095869954, sharpe5:17.05417034626007, irr5:550.0087280273438, ndcg5:0.8424352009987239, pnl5:7.594915866851807 
train 10, step: 0, loss: 7.096624168640671, grad_norm: 2.297749014515632, ic: 0.23175387503027517
train 10, step: 500, loss: 1.133785421470271, grad_norm: 0.0995177048510896, ic: 0.04248708042699852
train 10, step: 1000, loss: 2.3936348195456287, grad_norm: 0.669322711608797, ic: 0.14374611994650344
train 10, step: 1500, loss: 1.1204102751496552, grad_norm: 0.39503846227862893, ic: 0.002372103289546722
train 10, step: 2000, loss: 2.7470948007693767, grad_norm: 1.0235510787698068, ic: 0.4382159968264703
Epoch 10: 2022-05-05 20:06:46.612927: train loss: 1.6271789305892153
Eval step 0: eval loss: 0.8253276089345692
Eval: 2022-05-05 20:07:10.469549: total loss: 1.069669045886572, mse:4.682699153001872, ic :0.16619840769341776, sharpe5:17.466750980615615, irr5:570.5594482421875, ndcg5:0.8454170885828246, pnl5:7.143219947814941 
train 11, step: 0, loss: 1.252622104299594, grad_norm: 0.16190917308720054, ic: 0.20730702158347472
train 11, step: 500, loss: 0.6665213146117063, grad_norm: 4.911398698912381, ic: 0.5216734317929564
train 11, step: 1000, loss: 0.9385610959398186, grad_norm: 0.1360127913566333, ic: 0.04101276363939205
train 11, step: 1500, loss: 1.057494983338473, grad_norm: 0.08578346719175217, ic: 0.1807239700719223
train 11, step: 2000, loss: 0.7862175674800059, grad_norm: 0.006297090453831214, ic: 0.13617865058201484
Epoch 11: 2022-05-05 20:13:14.873368: train loss: 1.626228491808246
Eval step 0: eval loss: 0.8285101196160432
Eval: 2022-05-05 20:13:37.719209: total loss: 1.0703796939787649, mse:4.675764310851813, ic :0.17637087430758824, sharpe5:18.600533756017683, irr5:594.0308837890625, ndcg5:0.8560013683800676, pnl5:7.746435642242432 
train 12, step: 0, loss: 0.964511235555013, grad_norm: 0.4897007363550299, ic: 0.40059897339696243
train 12, step: 500, loss: 0.9307304463554464, grad_norm: 0.08402786612768914, ic: 0.18101631539527357
train 12, step: 1000, loss: 2.9649856470193074, grad_norm: 0.34826177084261667, ic: 0.2785658965422186
train 12, step: 1500, loss: 0.9354803080969155, grad_norm: 0.2011849261795141, ic: -0.09841612113276502
train 12, step: 2000, loss: 0.873662574053531, grad_norm: 0.0042638053473321984, ic: 0.2153997116573375
Epoch 12: 2022-05-05 20:19:51.582612: train loss: 1.6249539296488558
Eval step 0: eval loss: 0.826153094951923
Eval: 2022-05-05 20:20:14.950713: total loss: 1.0681711205685673, mse:4.6570514285268745, ic :0.17903814743112031, sharpe5:18.09604130387306, irr5:588.7171630859375, ndcg5:0.8525930915595259, pnl5:8.593158721923828 
train 13, step: 0, loss: 2.0632231409219903, grad_norm: 0.6873123484781001, ic: 0.3916674177286793
train 13, step: 500, loss: 0.8264478238510402, grad_norm: 0.08312246292346284, ic: 0.5351674894941311
train 13, step: 1000, loss: 0.959487730704698, grad_norm: 0.4676944742676146, ic: 0.4921883068196041
train 13, step: 1500, loss: 2.4373551546643246, grad_norm: 0.9913660662656749, ic: 0.028988175566612095
train 13, step: 2000, loss: 1.489625526997931, grad_norm: 0.06026783903114491, ic: 0.03626613862007037
Epoch 13: 2022-05-05 20:26:28.850414: train loss: 1.625820185941027
Eval step 0: eval loss: 0.8227967401952713
Eval: 2022-05-05 20:26:52.369705: total loss: 1.0688294791339892, mse:4.664907927453227, ic :0.17255540151722376, sharpe5:17.226614495515822, irr5:549.097412109375, ndcg5:0.8517554204140998, pnl5:9.5973482131958 
train 14, step: 0, loss: 4.499605794778667, grad_norm: 1.2761699468280503, ic: 0.15504782540109188
train 14, step: 500, loss: 0.8277124994027141, grad_norm: 0.005984619212034041, ic: 0.09218468526318262
train 14, step: 1000, loss: 1.8426462693384587, grad_norm: 3.082126532421578, ic: 0.37853825694393956
train 14, step: 1500, loss: 1.124155664369113, grad_norm: 0.10286696668088299, ic: -0.06352666483595226
train 14, step: 2000, loss: 1.1573474999093678, grad_norm: 0.4997564379975687, ic: 0.06479701469963658
Epoch 14: 2022-05-05 20:33:03.787759: train loss: 1.6229432327934616
Eval step 0: eval loss: 0.829655509603036
Eval: 2022-05-05 20:33:27.116849: total loss: 1.069411964801765, mse:4.625294312129202, ic :0.17936528069264404, sharpe5:15.92696932196617, irr5:504.06494140625, ndcg5:0.8586606793045808, pnl5:4.263250827789307 
train 15, step: 0, loss: 3.4393140503404673, grad_norm: 1.2202398710825806, ic: 0.0703289121107971
train 15, step: 500, loss: 1.262220628441916, grad_norm: 0.0340252544590786, ic: 0.017199143949293068
train 15, step: 1000, loss: 1.3161340232787093, grad_norm: 0.11871445311123913, ic: 0.045025478988393264
train 15, step: 1500, loss: 0.8485641839936023, grad_norm: 0.21173467617774855, ic: 0.06058627220899836
train 15, step: 2000, loss: 1.4714398714271337, grad_norm: 0.706658716159025, ic: 0.033868982865717844
Epoch 15: 2022-05-05 20:39:40.544464: train loss: 1.621781832548303
Eval step 0: eval loss: 0.8327437341527265
Eval: 2022-05-05 20:40:03.860474: total loss: 1.0686089683805706, mse:4.583059601902445, ic :0.18988740406506507, sharpe5:17.996615052223206, irr5:586.8010864257812, ndcg5:0.8490620292453374, pnl5:8.89072322845459 
train 16, step: 0, loss: 0.708045458095185, grad_norm: 1.2385082351298753, ic: -0.09753490526781392
train 16, step: 500, loss: 1.610709824973531, grad_norm: 0.5122810967348401, ic: 0.19558568478460603
train 16, step: 1000, loss: 0.880926698626894, grad_norm: 0.009274701931791696, ic: -0.11903594262200312
train 16, step: 1500, loss: 0.836499658318558, grad_norm: 0.2840259533514624, ic: 0.18595419050908069
train 16, step: 2000, loss: 3.3191434071144714, grad_norm: 1.1185351528686858, ic: -0.007195462249575819
Epoch 16: 2022-05-05 20:46:20.196877: train loss: 1.619814895070826
Eval step 0: eval loss: 0.8265762891859851
Eval: 2022-05-05 20:46:43.550477: total loss: 1.0672567067843877, mse:4.592913964460098, ic :0.18682667018417853, sharpe5:17.09325221300125, irr5:549.7943725585938, ndcg5:0.8408131546938495, pnl5:7.261383533477783 
train 17, step: 0, loss: 1.279873487234748, grad_norm: 0.2900680325987456, ic: -0.12435717475789212
train 17, step: 500, loss: 1.755199070545393, grad_norm: 2.094526441359852, ic: 0.22382830622385658
train 17, step: 1000, loss: 1.2786913130467923, grad_norm: 0.0910242494765904, ic: 0.16143031604717087
train 17, step: 1500, loss: 4.5094829169728134, grad_norm: 1.604220129658474, ic: 0.2019256022005435
train 17, step: 2000, loss: 1.2956424491597056, grad_norm: 1.5323776041553805, ic: 0.10553290874461786
Epoch 17: 2022-05-05 20:52:55.857888: train loss: 1.6192888476275094
Eval step 0: eval loss: 0.8368164319760932
Eval: 2022-05-05 20:53:19.092557: total loss: 1.0701860869170847, mse:4.598855066920248, ic :0.18453049328758356, sharpe5:18.465606912374497, irr5:607.446533203125, ndcg5:0.8398747178921587, pnl5:7.0651044845581055 
train 18, step: 0, loss: 1.4285674496582415, grad_norm: 1.3734173343933358, ic: 0.12394290289241902
train 18, step: 500, loss: 1.5144903076763014, grad_norm: 0.7606663573105988, ic: -0.013677034490180685
train 18, step: 1000, loss: 0.6528098244863013, grad_norm: 0.0018213325144887992, ic: 0.5734993552413006
train 18, step: 1500, loss: 1.4290387577741095, grad_norm: 0.038221327574113416, ic: 0.17254590357801025
train 18, step: 2000, loss: 0.9106777701408241, grad_norm: 0.007072460653355397, ic: -0.016397216067392622
Epoch 18: 2022-05-05 20:59:33.646762: train loss: 1.6196404757480147
Eval step 0: eval loss: 0.8219841815397787
Eval: 2022-05-05 20:59:56.956819: total loss: 1.0651487454500714, mse:4.586295725967106, ic :0.1936892833158914, sharpe5:17.473504890203476, irr5:590.641845703125, ndcg5:0.8492947218376518, pnl5:11.185928344726562 
train 19, step: 0, loss: 1.4939849369109623, grad_norm: 0.8776549022604054, ic: 0.023250388028922073
train 19, step: 500, loss: 0.860346617522063, grad_norm: 0.020618930179522016, ic: 0.22914863139915692
train 19, step: 1000, loss: 0.9558311236474943, grad_norm: 0.03780237095370126, ic: 0.21810772582623664
train 19, step: 1500, loss: 3.948120717673928, grad_norm: 1.2537955676275958, ic: 0.14104140183473113
train 19, step: 2000, loss: 0.9995945387620192, grad_norm: 0.9498791942043769, ic: 0.12706272681399777
Epoch 19: 2022-05-05 21:06:18.870908: train loss: 1.6216593519477183
Eval step 0: eval loss: 0.83952300993233
Eval: 2022-05-05 21:06:42.611995: total loss: 1.0733291158614104, mse:4.60399669276377, ic :0.18050488211223914, sharpe5:18.24463677406311, irr5:589.1947021484375, ndcg5:0.8497200244656197, pnl5:7.973265171051025 
train 20, step: 0, loss: 2.3284650599061263, grad_norm: 3.0323373320657963, ic: 0.032486867103836664
train 20, step: 500, loss: 3.207177911931818, grad_norm: 1.4358371612074734, ic: 0.1277506139937855
train 20, step: 1000, loss: 0.9721565246582031, grad_norm: 0.5804472581263858, ic: 0.16566372158375325
train 20, step: 1500, loss: 1.8400822824942573, grad_norm: 1.2937512130710074, ic: 0.27102232944377846
train 20, step: 2000, loss: 1.032447005951774, grad_norm: 0.135764658391399, ic: -0.0024437493992854956
Epoch 20: 2022-05-05 21:13:20.441950: train loss: 1.6226186182041658
Eval step 0: eval loss: 0.8318374682025487
Eval: 2022-05-05 21:13:46.872561: total loss: 1.0677886328422606, mse:4.590162587466774, ic :0.18847298743033822, sharpe5:17.49431714773178, irr5:577.689208984375, ndcg5:0.8425127992454027, pnl5:7.889673709869385 
train 21, step: 0, loss: 1.009138792548627, grad_norm: 0.9843508854898029, ic: 0.041719752796783
train 21, step: 500, loss: 0.7692052790548949, grad_norm: 0.025093079115638097, ic: 0.2041230523821293
train 21, step: 1000, loss: 0.9446255867941337, grad_norm: 0.8971711462699645, ic: 0.1534731640788059
train 21, step: 1500, loss: 0.989727813962383, grad_norm: 0.7893768529382335, ic: 0.31314008493357814
train 21, step: 2000, loss: 0.9411539983907807, grad_norm: 0.11594150457460645, ic: 0.04872575328169111
Epoch 21: 2022-05-05 21:20:37.896872: train loss: 1.6194770129494613
Eval step 0: eval loss: 0.8285595137150947
Eval: 2022-05-05 21:21:05.128172: total loss: 1.067122711320159, mse:4.597398845136217, ic :0.18456564902855227, sharpe5:17.17065731406212, irr5:557.9659423828125, ndcg5:0.8318331399225274, pnl5:9.825016021728516 
train 22, step: 0, loss: 1.0393322443557997, grad_norm: 0.04125894782002204, ic: 0.22152885776086412
train 22, step: 500, loss: 3.263328291730183, grad_norm: 1.6159539236210734, ic: -0.1927500778144628
train 22, step: 1000, loss: 1.1979702458905348, grad_norm: 0.016413847991450264, ic: 0.4555024361813921
train 22, step: 1500, loss: 0.9706380208333333, grad_norm: 0.21422203575813342, ic: 0.08272095066965299
train 22, step: 2000, loss: 1.7285056600765307, grad_norm: 2.1992921372098273, ic: 0.20513781524044616
Epoch 22: 2022-05-05 21:28:06.648588: train loss: 1.6162885841381185
Eval step 0: eval loss: 0.8256118822650487
Eval: 2022-05-05 21:28:33.873804: total loss: 1.0666538183827885, mse:4.5969403705866, ic :0.18578785843979312, sharpe5:18.14275868296623, irr5:567.705810546875, ndcg5:0.8420009527214665, pnl5:6.152186870574951 
train 23, step: 0, loss: 0.981338017268552, grad_norm: 0.06415209944297552, ic: 0.19090338836615064
train 23, step: 500, loss: 1.4314836774553572, grad_norm: 0.14208351547017828, ic: 0.04702030907000972
train 23, step: 1000, loss: 1.655002644856771, grad_norm: 0.11143405977428904, ic: 0.25679928108002803
train 23, step: 1500, loss: 1.1309831316253358, grad_norm: 1.4202867203133607, ic: 0.08868076655967849
train 23, step: 2000, loss: 1.854913184420708, grad_norm: 2.2445955132686946, ic: 0.4349244996119933
Epoch 23: 2022-05-05 21:35:27.631624: train loss: 1.617614618915423
Eval step 0: eval loss: 0.831848594737882
Eval: 2022-05-05 21:35:51.926866: total loss: 1.0668859690691896, mse:4.585040533429857, ic :0.18700073867373185, sharpe5:16.671064757108688, irr5:550.4517211914062, ndcg5:0.8478135222897812, pnl5:5.257923603057861 
train 24, step: 0, loss: 2.198059539566154, grad_norm: 0.7685750384618416, ic: 0.16044052529043684
train 24, step: 500, loss: 1.219992362293288, grad_norm: 0.30137365927902204, ic: 0.11841610879630278
train 24, step: 1000, loss: 0.9128905920303005, grad_norm: 0.07833270791241104, ic: 0.5051335797198631
train 24, step: 1500, loss: 2.6119405898984955, grad_norm: 3.6728640041138254, ic: 0.043817876038432124
train 24, step: 2000, loss: 0.926958377903423, grad_norm: 2.322243641573832, ic: 0.10231724083471035
Epoch 24: 2022-05-05 21:42:18.736478: train loss: 1.615316758227488
Eval step 0: eval loss: 0.8235486495859128
Eval: 2022-05-05 21:42:43.144548: total loss: 1.067450595688687, mse:4.615484167239659, ic :0.1924654527173252, sharpe5:18.840734564065933, irr5:613.9052124023438, ndcg5:0.8552104847771564, pnl5:6.524245262145996 
train 25, step: 0, loss: 0.8429426863386825, grad_norm: 0.1010729044263093, ic: 0.6085926226488194
train 25, step: 500, loss: 0.868070850199443, grad_norm: 0.014967487963207097, ic: 0.22801436352401475
train 25, step: 1000, loss: 2.0893703440356055, grad_norm: 0.5067896607769424, ic: 0.26857671942604927
train 25, step: 1500, loss: 1.127031091424222, grad_norm: 0.5633223269085003, ic: 0.5394955484491573
train 25, step: 2000, loss: 0.9926576528344031, grad_norm: 0.65335821716487, ic: 0.5792210722629899
Epoch 25: 2022-05-05 21:49:11.154914: train loss: 1.6150829228903012
Eval step 0: eval loss: 0.826098941525619
Eval: 2022-05-05 21:49:35.214592: total loss: 1.065390422502751, mse:4.580597289549816, ic :0.19454780592085744, sharpe5:17.43708107471466, irr5:592.7169799804688, ndcg5:0.8475849687249978, pnl5:5.747541904449463 
train 26, step: 0, loss: 6.702389458117013, grad_norm: 0.745678940316892, ic: 0.1418732226744598
train 26, step: 500, loss: 3.895275697566458, grad_norm: 2.4736797324639217, ic: 0.30160253420079985
train 26, step: 1000, loss: 1.252086501667975, grad_norm: 1.0876989446147736, ic: -0.04026207633912318
train 26, step: 1500, loss: 0.8379310503095625, grad_norm: 0.17040169764652333, ic: 0.3024069774622688
train 26, step: 2000, loss: 0.9505047621828285, grad_norm: 0.10650645506431299, ic: 0.14478372920626364
Epoch 26: 2022-05-05 21:56:01.530645: train loss: 1.6181470723143718
Eval step 0: eval loss: 0.8296076590695798
Eval: 2022-05-05 21:56:26.331089: total loss: 1.068327233380405, mse:4.596591100516911, ic :0.1852601472297632, sharpe5:17.374464576244353, irr5:566.6737060546875, ndcg5:0.8797272941585687, pnl5:6.236461639404297 
train 27, step: 0, loss: 0.8302238434436274, grad_norm: 0.04669479272726583, ic: 0.08299404445228234
train 27, step: 500, loss: 0.9439881320254395, grad_norm: 0.9918362456503449, ic: 0.27089751478977153
train 27, step: 1000, loss: 0.7521140444490768, grad_norm: 0.3019022815775905, ic: 0.19328936940253805
train 27, step: 1500, loss: 0.6354036050989527, grad_norm: 0.10520101571127292, ic: 0.517441207330377
train 27, step: 2000, loss: 1.3827181434486524, grad_norm: 0.035708953013884166, ic: 0.012032092421525344
Epoch 27: 2022-05-05 22:02:49.045844: train loss: 1.6156819478258841
Eval step 0: eval loss: 0.8291375790305584
Eval: 2022-05-05 22:03:12.355317: total loss: 1.0679045078713634, mse:4.597561239427952, ic :0.1849192245310798, sharpe5:17.02970870733261, irr5:549.1027221679688, ndcg5:0.8332644269279048, pnl5:6.3110761642456055 
train 28, step: 0, loss: 1.5501567590190637, grad_norm: 1.1080189500774056, ic: 0.23395645751660624
train 28, step: 500, loss: 1.378839926722331, grad_norm: 1.3723287344262887, ic: 0.18378510780736154
train 28, step: 1000, loss: 0.9096061065591124, grad_norm: 0.27343122324362884, ic: 0.577105813426088
train 28, step: 1500, loss: 1.0294241619925213, grad_norm: 0.025924005863949388, ic: 0.061986044905292345
train 28, step: 2000, loss: 1.0434032978455714, grad_norm: 0.2824072802614088, ic: 0.1341543291348306
Epoch 28: 2022-05-05 22:09:49.869911: train loss: 1.612783542097135
Eval step 0: eval loss: 0.8235491641077779
Eval: 2022-05-05 22:10:15.065853: total loss: 1.075424065727725, mse:4.669965122048854, ic :0.17344471641713013, sharpe5:16.62188986301422, irr5:545.9722290039062, ndcg5:0.83255185454637, pnl5:4.989474773406982 
train 29, step: 0, loss: 0.9049349908488912, grad_norm: 0.05415332917889519, ic: 0.1131320559519578
train 29, step: 500, loss: 1.1077626396891516, grad_norm: 0.20950336438099204, ic: 0.6159347181232023
train 29, step: 1000, loss: 1.0613682967191098, grad_norm: 0.40384701336190715, ic: 0.09077398316840897
train 29, step: 1500, loss: 2.3536494163495316, grad_norm: 0.33292520749430266, ic: -0.018869023811920967
train 29, step: 2000, loss: 4.11101993513696, grad_norm: 8.6144562435634, ic: 0.2185091859601478
Epoch 29: 2022-05-05 22:16:57.350026: train loss: 1.614718832652125
Eval step 0: eval loss: 0.8308691380523906
Eval: 2022-05-05 22:17:24.814031: total loss: 1.0671035121027679, mse:4.587964960798228, ic :0.1901915323289489, sharpe5:17.72101770401001, irr5:576.7267456054688, ndcg5:0.8514951079521687, pnl5:4.787362098693848 
train 30, step: 0, loss: 1.016366004345182, grad_norm: 0.04072939519016557, ic: 0.5018963611962836
train 30, step: 500, loss: 1.4253250134587778, grad_norm: 1.7467782436139943, ic: 0.09321733197063743
train 30, step: 1000, loss: 0.9814342151988636, grad_norm: 0.8677470174035176, ic: -0.06791611222797422
train 30, step: 1500, loss: 1.5050658852771825, grad_norm: 2.4906197128552092, ic: 0.16878506840492222
train 30, step: 2000, loss: 1.8468253873374139, grad_norm: 0.5440643663378439, ic: 0.08929533700782716
Epoch 30: 2022-05-05 22:24:35.577666: train loss: 1.6158201035031658
Eval step 0: eval loss: 0.8329319848401278
Eval: 2022-05-05 22:25:02.616278: total loss: 1.0703220174842114, mse:4.642545533330765, ic :0.1927890092229764, sharpe5:16.858025082349776, irr5:585.7518920898438, ndcg5:0.844435454092921, pnl5:5.714717388153076 
train 31, step: 0, loss: 1.051792997960571, grad_norm: 0.19493841450071003, ic: 0.36839858219370925
train 31, step: 500, loss: 1.5007173514660495, grad_norm: 0.8791460453887042, ic: 0.01801240146976381
train 31, step: 1000, loss: 4.392155786494926, grad_norm: 3.0270806712656597, ic: 0.47685948703444714
train 31, step: 1500, loss: 0.7658393951294108, grad_norm: 0.15772976168336933, ic: 0.7140212194941915
train 31, step: 2000, loss: 1.2328942017714666, grad_norm: 1.537629097536626, ic: 0.16324944625989965
Epoch 31: 2022-05-05 22:32:06.506801: train loss: 1.6115614831705554
Eval step 0: eval loss: 0.8300361914679926
Eval: 2022-05-05 22:32:34.038482: total loss: 1.0664302081318042, mse:4.5846170279948115, ic :0.19052775455872623, sharpe5:16.779734594821928, irr5:549.0062255859375, ndcg5:0.8395977553315722, pnl5:3.873584032058716 
train 32, step: 0, loss: 1.1267757757696804, grad_norm: 0.10718224302868194, ic: 0.19817130641194536
train 32, step: 500, loss: 1.4840066744586613, grad_norm: 1.0845125125341573, ic: 0.14289654107349467
train 32, step: 1000, loss: 1.0526041566073083, grad_norm: 0.2932055384086127, ic: 0.5066877413568457
train 32, step: 1500, loss: 0.9617756975233601, grad_norm: 1.2453980720781754, ic: 0.06384441244990435
train 32, step: 2000, loss: 0.9451710008530614, grad_norm: 0.018438378435531155, ic: 0.5464341553492211
Epoch 32: 2022-05-05 22:39:47.003556: train loss: 1.6131339353628358
Eval step 0: eval loss: 0.8214977654315397
Eval: 2022-05-05 22:40:14.377729: total loss: 1.0642756269083982, mse:4.595193048875604, ic :0.19421941899911532, sharpe5:17.674763828516006, irr5:581.4270629882812, ndcg5:0.8285862271414017, pnl5:6.254603862762451 
train 33, step: 0, loss: 1.2702288699716522, grad_norm: 0.3944785163452544, ic: 0.2029306823748942
train 33, step: 500, loss: 0.9857035054788961, grad_norm: 0.018218646875416092, ic: 0.20076565686284992
train 33, step: 1000, loss: 1.053537121411944, grad_norm: 2.710730921577783, ic: 0.22044815170871696
train 33, step: 1500, loss: 0.8925559698899022, grad_norm: 0.024489583739112086, ic: 0.5436908997939034
train 33, step: 2000, loss: 0.8109139564864798, grad_norm: 0.06329686262080783, ic: 0.28141810560533775
Epoch 33: 2022-05-05 22:47:28.593759: train loss: 1.6135818259053643
Eval step 0: eval loss: 0.824882804782172
Eval: 2022-05-05 22:47:55.586177: total loss: 1.0664316291649436, mse:4.588146219779608, ic :0.19164526334273826, sharpe5:17.2382701921463, irr5:555.2265625, ndcg5:0.8440625902089767, pnl5:6.126031875610352 
train 34, step: 0, loss: 1.0057702577420193, grad_norm: 0.5913870553088054, ic: 0.6023283429426044
train 34, step: 500, loss: 0.7998696423451835, grad_norm: 0.5562248245958052, ic: 0.2923826637289333
train 34, step: 1000, loss: 3.158566148233487, grad_norm: 1.6333559718944834, ic: 0.3400290292795318
train 34, step: 1500, loss: 0.8098524273223331, grad_norm: 0.5854225493895976, ic: 0.6943646361671542
train 34, step: 2000, loss: 5.772815707300542, grad_norm: 35.502718685237404, ic: 0.4416882825035983
Epoch 34: 2022-05-05 22:55:00.376548: train loss: 1.6127739723474188
Eval step 0: eval loss: 0.8209604116586537
Eval: 2022-05-05 22:55:27.968675: total loss: 1.0642697864594737, mse:4.591097094390919, ic :0.1951731916695809, sharpe5:17.733170957565306, irr5:596.9668579101562, ndcg5:0.8357510756689943, pnl5:6.887380123138428 
train 35, step: 0, loss: 1.1992921357996322, grad_norm: 0.725533127377814, ic: 0.5528742222368589
train 35, step: 500, loss: 1.1844683044383688, grad_norm: 0.7869689065464079, ic: 0.08629604636160931
train 35, step: 1000, loss: 1.580574440601778, grad_norm: 3.6495089881119833, ic: 0.036506875380777876
train 35, step: 1500, loss: 1.6141482245653196, grad_norm: 1.4961843301052264, ic: 0.07177172211493318
train 35, step: 2000, loss: 0.7964315975413603, grad_norm: 0.32265780686165957, ic: 0.5555086717825576
Epoch 35: 2022-05-05 23:02:22.603490: train loss: 1.6099939512590191
Eval step 0: eval loss: 0.8261731613046628
Eval: 2022-05-05 23:02:49.073309: total loss: 1.0647525911637248, mse:4.581275736268896, ic :0.1944952102705336, sharpe5:17.481507421731948, irr5:578.6239624023438, ndcg5:0.8420656561296413, pnl5:6.690738201141357 
train 36, step: 0, loss: 1.797870917447508, grad_norm: 1.7512830167419746, ic: 0.11945517176565529
train 36, step: 500, loss: 0.8432547927748227, grad_norm: 0.116844492639574, ic: 0.11588748426893261
train 36, step: 1000, loss: 1.7155703125, grad_norm: 2.863733256205621, ic: 0.25690941916715215
train 36, step: 1500, loss: 0.7721069415255442, grad_norm: 0.07701451853271402, ic: 0.3743621852939868
train 36, step: 2000, loss: 1.1149138313517672, grad_norm: 1.2962667730771094, ic: 0.7765951595041322
Epoch 36: 2022-05-05 23:09:26.612943: train loss: 1.6060319760261472
Eval step 0: eval loss: 0.8292160436149894
Eval: 2022-05-05 23:09:51.929872: total loss: 1.0668035391838917, mse:4.593900276610025, ic :0.18702266349007102, sharpe5:17.044366772174833, irr5:574.8050537109375, ndcg5:0.8574762266916651, pnl5:5.660763263702393 
train 37, step: 0, loss: 2.0186100210566615, grad_norm: 4.63237371452723, ic: 0.1841926565440622
train 37, step: 500, loss: 2.3333768113402327, grad_norm: 3.4801460025726705, ic: -0.0557561157805851
train 37, step: 1000, loss: 1.08013144947441, grad_norm: 0.17705715955400142, ic: 0.016504726517292678
train 37, step: 1500, loss: 2.012195724894525, grad_norm: 2.986742779171222, ic: 0.6090120528589001
train 37, step: 2000, loss: 1.3140984722150124, grad_norm: 0.13738560589677107, ic: 0.23801201250149853
Epoch 37: 2022-05-05 23:16:21.496808: train loss: 1.6084277698970735
Eval step 0: eval loss: 0.8249430038403911
Eval: 2022-05-05 23:16:46.822837: total loss: 1.0680849210318029, mse:4.60688583088026, ic :0.18744436710749332, sharpe5:17.583065789937972, irr5:599.028076171875, ndcg5:0.8523726890939469, pnl5:8.47241497039795 
train 38, step: 0, loss: 1.3388262492854421, grad_norm: 0.8645100572542411, ic: -0.0837479490330479
train 38, step: 500, loss: 0.9110514322916666, grad_norm: 0.05941244547681363, ic: 0.2666183061556153
train 38, step: 1000, loss: 0.9047639266304348, grad_norm: 0.5134292916122817, ic: 0.11748563603445009
train 38, step: 1500, loss: 0.9515407738942673, grad_norm: 0.02950449226763155, ic: 0.21298718914305498
train 38, step: 2000, loss: 2.3172070828809477, grad_norm: 3.9401427206311923, ic: 0.007465483158217658
Epoch 38: 2022-05-05 23:23:19.299806: train loss: 1.6076479442538076
Eval step 0: eval loss: 0.8241397708937038
Eval: 2022-05-05 23:23:45.201462: total loss: 1.0646455433524733, mse:4.584390159010251, ic :0.1942170665722953, sharpe5:17.443657150268553, irr5:594.9097900390625, ndcg5:0.8534180538064629, pnl5:4.872620105743408 
train 39, step: 0, loss: 0.9683498184250765, grad_norm: 0.009689674572115363, ic: 0.07607301361520885
train 39, step: 500, loss: 0.8885115228370004, grad_norm: 0.39003060069601386, ic: 0.23592688531958417
train 39, step: 1000, loss: 0.9453955226877856, grad_norm: 0.28904782276378793, ic: 0.19747307932982086
train 39, step: 1500, loss: 2.0828754890550716, grad_norm: 0.20297309107250966, ic: 0.16576414810453755
train 39, step: 2000, loss: 0.6153671481598045, grad_norm: 0.10739279298442946, ic: 0.10662734255106057
Epoch 39: 2022-05-05 23:30:52.803615: train loss: 1.6054560533885058
Eval step 0: eval loss: 0.826193484918335
Eval: 2022-05-05 23:31:19.559519: total loss: 1.0644039519183834, mse:4.590227208316287, ic :0.19156814187585192, sharpe5:18.000822529792785, irr5:596.6473999023438, ndcg5:0.8431657783158378, pnl5:4.529235363006592 
