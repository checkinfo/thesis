Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
54552
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795790397160868, grad_norm: 4.81756586791436, ic: 0.0226022501578505
train 0, step: 500, loss: 0.8631021460825641, grad_norm: 0.026532766860179847, ic: 0.04494841311648896
train 0, step: 1000, loss: 1.9500952987838334, grad_norm: 0.5127551343204564, ic: 0.015205411324685817
train 0, step: 1500, loss: 0.9564121170948616, grad_norm: 0.049095184529397276, ic: 0.015051556023915398
train 0, step: 2000, loss: 1.0022315402267818, grad_norm: 0.15817840210396258, ic: 0.02515976287857186
Epoch 0: 2022-05-07 21:49:21.598541: train loss: 1.6484908856175349
Eval step 0: eval loss: 0.8363152876794652
Eval: 2022-05-07 21:49:51.733033: total loss: 1.079343899757147, mse:4.822844514313584, ic :0.008539281105011289, sharpe5:7.9187035918235775, irr5:224.28302001953125, ndcg5:0.8516990126077878, pnl5:2.6084134578704834 
train 1, step: 0, loss: 2.7747755481350804, grad_norm: 0.8786708794938692, ic: 0.057221034634122564
train 1, step: 500, loss: 1.755510756322766, grad_norm: 0.7705169119845149, ic: 0.09695202157211569
train 1, step: 1000, loss: 0.8777374234152518, grad_norm: 0.17533404134922878, ic: 0.07985754130666928
train 1, step: 1500, loss: 1.7131974845096982, grad_norm: 0.20657869971738452, ic: -0.03166956946860505
train 1, step: 2000, loss: 2.177323828125, grad_norm: 0.8876102514398222, ic: -0.04523626394481954
Epoch 1: 2022-05-07 21:57:23.632353: train loss: 1.64673959227889
Eval step 0: eval loss: 0.8346391040733666
Eval: 2022-05-07 21:57:53.864322: total loss: 1.0789681941899416, mse:4.823366303770061, ic :0.00860903176558545, sharpe5:7.627396782338619, irr5:214.6351318359375, ndcg5:0.8521737406217832, pnl5:2.528751850128174 
train 2, step: 0, loss: 2.141756036931818, grad_norm: 0.009263364167571486, ic: 0.13221121061072613
train 2, step: 500, loss: 3.2999934284526606, grad_norm: 0.28484471010357887, ic: 0.04614981365625082
train 2, step: 1000, loss: 2.0723526176364944, grad_norm: 0.0002016853894938628, ic: 0.1923221575457753
train 2, step: 1500, loss: 1.485507551586355, grad_norm: 0.05970687486781292, ic: -0.03763519375468087
train 2, step: 2000, loss: 3.2347618689903848, grad_norm: 0.7868357668584156, ic: 0.20166544090909128
Epoch 2: 2022-05-07 22:05:25.519089: train loss: 1.6465015542302839
Eval step 0: eval loss: 0.835995833916458
Eval: 2022-05-07 22:05:53.757255: total loss: 1.0793718150531666, mse:4.822446960455458, ic :0.012828533745871541, sharpe5:7.501811428070068, irr5:214.48892211914062, ndcg5:0.8567543524127881, pnl5:2.7750954627990723 
train 3, step: 0, loss: 1.5229264918381606, grad_norm: 0.5248969240007939, ic: -0.002232697891122624
train 3, step: 500, loss: 1.501124060144626, grad_norm: 0.3421145737011039, ic: 0.09336939061059304
train 3, step: 1000, loss: 3.6798651590385725, grad_norm: 0.7052644056912375, ic: -0.04808638317797355
train 3, step: 1500, loss: 1.983012647363441, grad_norm: 1.253025927416239, ic: -0.06813205092399655
train 3, step: 2000, loss: 0.8989804159628378, grad_norm: 0.0013689217114304887, ic: 0.010971090019769451
Epoch 3: 2022-05-07 22:13:30.002678: train loss: 1.6460121610203122
Eval step 0: eval loss: 0.8355144987116372
Eval: 2022-05-07 22:14:00.088025: total loss: 1.0788867552340593, mse:4.82222194348479, ic :0.019835124658930278, sharpe5:7.986747812032699, irr5:225.0284423828125, ndcg5:0.8529802401716038, pnl5:2.799513578414917 
train 4, step: 0, loss: 1.432578822544643, grad_norm: 0.04351652109735305, ic: 0.10979345946486158
train 4, step: 500, loss: 1.6497475931963583, grad_norm: 0.5606111501043063, ic: 0.02913666867247089
train 4, step: 1000, loss: 2.9717329071789256, grad_norm: 0.7648250454067871, ic: 0.07007717729243929
train 4, step: 1500, loss: 2.1454874159414556, grad_norm: 0.4902048124478283, ic: 0.01712883352182856
train 4, step: 2000, loss: 1.08740158437014, grad_norm: 0.3894635813945093, ic: 0.24296917977366392
Epoch 4: 2022-05-07 22:21:15.346105: train loss: 1.6456810071520012
Eval step 0: eval loss: 0.8461536532081467
Eval: 2022-05-07 22:21:45.589319: total loss: 1.0822767838540928, mse:4.820109374626689, ic :0.05223053501411773, sharpe5:10.375556445121765, irr5:311.568603515625, ndcg5:0.8424973607821329, pnl5:3.294952392578125 
train 5, step: 0, loss: 1.345658311267827, grad_norm: 0.12854597785018357, ic: 0.06802536557068525
train 5, step: 500, loss: 0.8888386689321471, grad_norm: 0.009988652121434623, ic: 0.04201194100132673
train 5, step: 1000, loss: 0.9812500000000001, grad_norm: 0.14654654535330097, ic: 0.0010150781970344348
train 5, step: 1500, loss: 1.5377733776799387, grad_norm: 0.17233237353326047, ic: 0.02346151568511451
train 5, step: 2000, loss: 1.1032178130891095, grad_norm: 0.029252629053720447, ic: 0.16738765052511395
Epoch 5: 2022-05-07 22:29:14.654821: train loss: 1.6435762843920632
Eval step 0: eval loss: 0.8429068272935326
Eval: 2022-05-07 22:29:44.564286: total loss: 1.081649937087535, mse:4.761738945715235, ic :0.11898846639010073, sharpe5:11.445960305929184, irr5:379.5939636230469, ndcg5:0.8411604158432848, pnl5:2.8540713787078857 
train 6, step: 0, loss: 1.3412048242499004, grad_norm: 0.43629892278721616, ic: 0.10712844110251174
train 6, step: 500, loss: 1.0080457982388975, grad_norm: 0.04330847959248417, ic: 0.03841873445132635
train 6, step: 1000, loss: 1.115922238890209, grad_norm: 0.07904276628446828, ic: 0.7654932543945765
train 6, step: 1500, loss: 1.5686792799263947, grad_norm: 0.7053390212826656, ic: 0.16296949534216648
train 6, step: 2000, loss: 0.8134422088452011, grad_norm: 0.07963337027075705, ic: 0.3489499780607108
Epoch 6: 2022-05-07 22:37:05.584622: train loss: 1.6368552626568211
Eval step 0: eval loss: 0.8298915465086604
Eval: 2022-05-07 22:37:35.275162: total loss: 1.0746109433455577, mse:4.734516637421529, ic :0.132416701498621, sharpe5:12.034915410876273, irr5:404.6100769042969, ndcg5:0.8323903521378098, pnl5:3.3300976753234863 
train 7, step: 0, loss: 0.9880189895629883, grad_norm: 0.05361325779663706, ic: 0.09121925382312163
train 7, step: 500, loss: 0.6498153361868351, grad_norm: 0.0008108565511249846, ic: 0.045495316722431026
train 7, step: 1000, loss: 1.0265176138256855, grad_norm: 0.231566156042407, ic: 0.11296986760939928
train 7, step: 1500, loss: 2.2496977680365755, grad_norm: 0.7201610083091889, ic: 0.43943040336708294
train 7, step: 2000, loss: 0.9217801440705776, grad_norm: 0.06109107246605205, ic: -0.045618123254242304
Epoch 7: 2022-05-07 22:45:00.498248: train loss: 1.6321026034460253
Eval step 0: eval loss: 0.8402955001975764
Eval: 2022-05-07 22:45:30.688362: total loss: 1.0779235994086502, mse:4.735823674791113, ic :0.13806681112576047, sharpe5:15.456174527406692, irr5:499.0829162597656, ndcg5:0.8583268459963, pnl5:3.394543409347534 
train 8, step: 0, loss: 3.5847259963768114, grad_norm: 1.0811267131092008, ic: 0.1823364591756778
train 8, step: 500, loss: 2.7716008797967326, grad_norm: 0.8578755141436591, ic: 0.04594470483865948
train 8, step: 1000, loss: 3.0547582653985508, grad_norm: 0.8730765993568099, ic: 0.117299189201691
train 8, step: 1500, loss: 0.7107930024660912, grad_norm: 0.0037314471673458726, ic: 0.470410626931062
train 8, step: 2000, loss: 1.0830777833242866, grad_norm: 0.36804286165504924, ic: 0.524895315761238
Epoch 8: 2022-05-07 22:53:08.629133: train loss: 1.6280584379301961
Eval step 0: eval loss: 0.8305563730736301
Eval: 2022-05-07 22:53:37.992641: total loss: 1.0711913530689225, mse:4.687369845225325, ic :0.16162053029242718, sharpe5:15.259321711063384, irr5:502.64849853515625, ndcg5:0.8520093654608939, pnl5:3.7770774364471436 
train 9, step: 0, loss: 5.433539626320774, grad_norm: 0.8399519816554535, ic: 0.12639675873088457
train 9, step: 500, loss: 1.3349275621582366, grad_norm: 2.5565652343213556, ic: 0.325233198298144
train 9, step: 1000, loss: 0.9362639629194889, grad_norm: 0.02798060103426401, ic: -0.04913171583665862
train 9, step: 1500, loss: 1.0848862390981682, grad_norm: 0.025108823153709114, ic: 0.43072658896938476
train 9, step: 2000, loss: 1.0681074760184246, grad_norm: 0.2783345605272163, ic: 0.27324874589248765
Epoch 9: 2022-05-07 23:01:02.464920: train loss: 1.6269480392759927
Eval step 0: eval loss: 0.8282815432774631
Eval: 2022-05-07 23:01:32.332833: total loss: 1.0706918119445934, mse:4.694451001052668, ic :0.1621973415845628, sharpe5:16.965919260978698, irr5:540.69140625, ndcg5:0.849542202675967, pnl5:6.255291938781738 
train 10, step: 0, loss: 7.082083921738338, grad_norm: 2.236809912938707, ic: 0.253528492188072
train 10, step: 500, loss: 1.1309511672758046, grad_norm: 0.17212104087941038, ic: 0.06332281060748175
train 10, step: 1000, loss: 2.3775829479006902, grad_norm: 0.708084348340028, ic: 0.14045671595227666
train 10, step: 1500, loss: 1.1185282917765829, grad_norm: 0.3273826666497168, ic: -0.02749183287878801
train 10, step: 2000, loss: 2.7397889482214097, grad_norm: 0.6764563816143527, ic: 0.4170604891776718
Epoch 10: 2022-05-07 23:09:01.752655: train loss: 1.626828768868395
Eval step 0: eval loss: 0.8303506929580479
Eval: 2022-05-07 23:09:30.585901: total loss: 1.0698750809476203, mse:4.680740873177245, ic :0.16745616816314718, sharpe5:16.43741353869438, irr5:532.951416015625, ndcg5:0.8590841931337488, pnl5:6.14749002456665 
train 11, step: 0, loss: 1.2524191258217325, grad_norm: 0.08512421024613716, ic: 0.2128596730983593
train 11, step: 500, loss: 0.6596436139860556, grad_norm: 0.03590240788520535, ic: 0.5340147577198366
train 11, step: 1000, loss: 0.9341928304371909, grad_norm: 0.1274028969583199, ic: 0.06998341380781742
train 11, step: 1500, loss: 1.0495760733621162, grad_norm: 0.04803892978245656, ic: 0.17883009183004386
train 11, step: 2000, loss: 0.7849474962665383, grad_norm: 0.004698826457190529, ic: 0.13828950898957626
Epoch 11: 2022-05-07 23:17:07.762138: train loss: 1.6260456087050683
Eval step 0: eval loss: 0.8320398039260076
Eval: 2022-05-07 23:17:37.279459: total loss: 1.0702741458802973, mse:4.670859020325028, ic :0.17427236772237967, sharpe5:17.55588745236397, irr5:563.2674560546875, ndcg5:0.8385901065146986, pnl5:9.11327075958252 
train 12, step: 0, loss: 0.9637653032938639, grad_norm: 0.21175975601883298, ic: 0.3982867596951343
train 12, step: 500, loss: 0.9292114157836815, grad_norm: 0.06690173186301725, ic: 0.18722867994540016
train 12, step: 1000, loss: 2.9600647361415207, grad_norm: 0.351188698633023, ic: 0.18886594816408694
train 12, step: 1500, loss: 0.9468213188130712, grad_norm: 0.11276387194672437, ic: -0.04718660658605448
train 12, step: 2000, loss: 0.8730465062075151, grad_norm: 0.003854216104341888, ic: 0.2189494893599031
Epoch 12: 2022-05-07 23:24:58.144328: train loss: 1.6248798253434606
Eval step 0: eval loss: 0.831419097610972
Eval: 2022-05-07 23:25:28.224756: total loss: 1.068954287598884, mse:4.659675948121284, ic :0.1758863791619212, sharpe5:16.823092045783994, irr5:546.2713012695312, ndcg5:0.8434914250889578, pnl5:5.047720909118652 
train 13, step: 0, loss: 2.0658499084279303, grad_norm: 0.7010791323547291, ic: 0.3951602685338925
train 13, step: 500, loss: 0.8325789843217087, grad_norm: 0.20236698631239503, ic: 0.5263278843838545
train 13, step: 1000, loss: 0.952595427852349, grad_norm: 0.35940259427739507, ic: 0.5059575777649912
train 13, step: 1500, loss: 2.414005895967506, grad_norm: 0.5055591831537312, ic: -0.02776514157105732
train 13, step: 2000, loss: 1.4640485544770014, grad_norm: 0.09351373798720451, ic: 0.20114750816098897
Epoch 13: 2022-05-07 23:32:51.733937: train loss: 1.6247402975403444
Eval step 0: eval loss: 0.8280051807206599
Eval: 2022-05-07 23:33:20.846586: total loss: 1.068675148774125, mse:4.650408415732098, ic :0.1782524581674085, sharpe5:17.963742241859435, irr5:565.8967895507812, ndcg5:0.8638619410649434, pnl5:8.74492073059082 
train 14, step: 0, loss: 4.508232223820203, grad_norm: 1.2747198498473848, ic: 0.20367750484832972
train 14, step: 500, loss: 0.8285250882489966, grad_norm: 0.005293078014844894, ic: 0.10414897424629979
train 14, step: 1000, loss: 1.8260649128582955, grad_norm: 0.17814821786167329, ic: 0.429602393964045
train 14, step: 1500, loss: 1.1281169130813384, grad_norm: 0.06854625683244979, ic: -0.06937040524824215
train 14, step: 2000, loss: 1.1604295628806554, grad_norm: 0.23135173590680919, ic: 0.06955340638453664
Epoch 14: 2022-05-07 23:40:48.690825: train loss: 1.6233278529921773
Eval step 0: eval loss: 0.8335607948745389
Eval: 2022-05-07 23:41:19.286817: total loss: 1.0687772162386948, mse:4.610072803836098, ic :0.1844843870410773, sharpe5:17.313017241954803, irr5:550.2553100585938, ndcg5:0.8452298063332301, pnl5:4.537659168243408 
train 15, step: 0, loss: 3.476721713886187, grad_norm: 1.5607698338639602, ic: 0.06480236009964348
train 15, step: 500, loss: 1.2592008341775769, grad_norm: 0.011423504621993598, ic: -0.047075985197089006
train 15, step: 1000, loss: 1.3216270880970529, grad_norm: 0.14201767807659774, ic: 0.023914885511750313
train 15, step: 1500, loss: 0.851602965828002, grad_norm: 0.20243643158573743, ic: 0.06538597106413024
train 15, step: 2000, loss: 1.4634695252742553, grad_norm: 0.6858801058146393, ic: 0.05762196642520902
Epoch 15: 2022-05-07 23:48:44.360635: train loss: 1.6213715929313897
Eval step 0: eval loss: 0.8393645371978727
Eval: 2022-05-07 23:49:14.206734: total loss: 1.0698431167116107, mse:4.587883104120696, ic :0.18873824111689247, sharpe5:17.483329274654388, irr5:567.8339233398438, ndcg5:0.8416311601711324, pnl5:5.304194927215576 
train 16, step: 0, loss: 0.6938291475684135, grad_norm: 0.6564144171950534, ic: 0.040248190264299336
train 16, step: 500, loss: 1.5939470960577689, grad_norm: 0.5723578915501208, ic: 0.18283333089355627
train 16, step: 1000, loss: 0.8816775235262784, grad_norm: 0.007440011962550226, ic: -0.09891617671558672
train 16, step: 1500, loss: 0.841906151497242, grad_norm: 0.3140095707297482, ic: 0.11739625259740608
train 16, step: 2000, loss: 3.321542073634909, grad_norm: 2.1369706953059175, ic: -0.004697679293712703
Epoch 16: 2022-05-07 23:56:47.256542: train loss: 1.6191019627754177
Eval step 0: eval loss: 0.8290600148593914
Eval: 2022-05-07 23:57:16.676183: total loss: 1.067427104170046, mse:4.594874642998542, ic :0.1852588768212607, sharpe5:16.995184310674667, irr5:550.828857421875, ndcg5:0.8578765318811089, pnl5:7.585377216339111 
train 17, step: 0, loss: 1.282865861240053, grad_norm: 0.35131726511450306, ic: -0.15100694777870471
train 17, step: 500, loss: 1.7507538586128049, grad_norm: 0.8395948953765403, ic: 0.21948225074178318
train 17, step: 1000, loss: 1.2746431870526247, grad_norm: 0.09515376631737969, ic: 0.1622251990073018
train 17, step: 1500, loss: 4.535909300950587, grad_norm: 1.5047214979444123, ic: 0.22344609233808901
train 17, step: 2000, loss: 1.2780736469520684, grad_norm: 1.0162047504359868, ic: 0.09198704168540554
Epoch 17: 2022-05-08 00:04:44.245815: train loss: 1.6182832709894739
Eval step 0: eval loss: 0.8328292734128029
Eval: 2022-05-08 00:05:14.620376: total loss: 1.0672237943539202, mse:4.583300940314669, ic :0.19353053483006935, sharpe5:18.203017934560776, irr5:611.5278930664062, ndcg5:0.8447143757993743, pnl5:6.636459827423096 
train 18, step: 0, loss: 1.4127926229961645, grad_norm: 0.8259594621929954, ic: 0.2538995741563464
train 18, step: 500, loss: 1.522948627610472, grad_norm: 0.6414546852263915, ic: -0.05919734530749969
train 18, step: 1000, loss: 0.6562575583261986, grad_norm: 0.0417529107264866, ic: 0.569351092064765
train 18, step: 1500, loss: 1.4263971002252254, grad_norm: 0.05102051671074195, ic: 0.16209077092062119
train 18, step: 2000, loss: 0.9128893226574941, grad_norm: 0.009502046494208438, ic: -0.04159454033314567
Epoch 18: 2022-05-08 00:12:50.305291: train loss: 1.619024316706811
Eval step 0: eval loss: 0.8273855034493546
Eval: 2022-05-08 00:13:20.425572: total loss: 1.0651502191291962, mse:4.591909517960167, ic :0.18808158052059515, sharpe5:17.591094807386398, irr5:571.908203125, ndcg5:0.8413319629789594, pnl5:8.334028244018555 
train 19, step: 0, loss: 1.4795943002852183, grad_norm: 0.8590669124664059, ic: 0.049698266068918355
train 19, step: 500, loss: 0.8621460243507667, grad_norm: 0.016958215353090538, ic: 0.22169283300317882
train 19, step: 1000, loss: 0.9612645038439636, grad_norm: 0.0347989091790383, ic: 0.17342858234221006
train 19, step: 1500, loss: 3.9628545958143007, grad_norm: 1.0093190739725248, ic: 0.12248696317572452
train 19, step: 2000, loss: 1.001748798076923, grad_norm: 0.15523606441009427, ic: 0.20067567415401094
Epoch 19: 2022-05-08 00:20:48.340842: train loss: 1.6189687383833617
Eval step 0: eval loss: 0.8306029373024235
Eval: 2022-05-08 00:21:17.595195: total loss: 1.06697102397296, mse:4.5848049788467575, ic :0.19083861374998015, sharpe5:17.123928583860398, irr5:583.0738525390625, ndcg5:0.8440246081564836, pnl5:6.611428737640381 
train 20, step: 0, loss: 2.297542382040514, grad_norm: 0.9523918363879735, ic: 0.056575846285837644
train 20, step: 500, loss: 3.2543288352272723, grad_norm: 0.6626167460273814, ic: 0.06285225777152764
train 20, step: 1000, loss: 0.9631694793701172, grad_norm: 0.10782695355294507, ic: 0.2001158555268306
train 20, step: 1500, loss: 1.7491400866194489, grad_norm: 1.7419828931657075, ic: 0.2692421753491583
train 20, step: 2000, loss: 1.0278045504944944, grad_norm: 0.0372373248341738, ic: 0.01299296496638251
Epoch 20: 2022-05-08 00:28:45.937327: train loss: 1.6162277399854297
Eval step 0: eval loss: 0.8353747417100237
Eval: 2022-05-08 00:29:15.592981: total loss: 1.0680090413288497, mse:4.586838783480524, ic :0.19061813654298862, sharpe5:17.51104489803314, irr5:586.1581420898438, ndcg5:0.8374053964318082, pnl5:6.615168571472168 
train 21, step: 0, loss: 1.0026479294789283, grad_norm: 0.4636677086721031, ic: 0.04000059134455066
train 21, step: 500, loss: 0.7653285338815334, grad_norm: 0.04690015501964622, ic: 0.19343186015882535
train 21, step: 1000, loss: 0.9369990700169613, grad_norm: 1.0113566866747683, ic: 0.17097045195336855
train 21, step: 1500, loss: 0.9896241113210943, grad_norm: 0.2882806389975065, ic: 0.31637590646862335
train 21, step: 2000, loss: 0.9414882385321497, grad_norm: 0.0988544687952174, ic: 0.08169709733531133
Epoch 21: 2022-05-08 00:36:37.202897: train loss: 1.6174367276966422
Eval step 0: eval loss: 0.8286887230184733
Eval: 2022-05-08 00:37:06.142061: total loss: 1.0670643727516058, mse:4.599442080599939, ic :0.1833165853080108, sharpe5:16.55678747177124, irr5:552.0685424804688, ndcg5:0.8594134528304188, pnl5:5.796192646026611 
train 22, step: 0, loss: 1.0332808844787253, grad_norm: 0.025582882144543694, ic: 0.24275161019462302
train 22, step: 500, loss: 3.2615276057545732, grad_norm: 0.9563364221996999, ic: -0.22708480218717597
train 22, step: 1000, loss: 1.191948722001445, grad_norm: 0.029426325807283164, ic: 0.4643457966257352
train 22, step: 1500, loss: 0.9706811222029321, grad_norm: 0.11726241763696796, ic: 0.09672319657535558
train 22, step: 2000, loss: 1.8509272361288265, grad_norm: 6.580502737247572, ic: 0.044079341926137626
Epoch 22: 2022-05-08 00:44:24.618423: train loss: 1.615465097415417
Eval step 0: eval loss: 0.8275000488795772
Eval: 2022-05-08 00:44:54.762996: total loss: 1.0665933228759032, mse:4.594874059888816, ic :0.1863924119157957, sharpe5:16.820693178176878, irr5:559.1591186523438, ndcg5:0.8516170021442404, pnl5:6.160068988800049 
train 23, step: 0, loss: 0.9773454385806917, grad_norm: 0.07382167347147794, ic: 0.19113842696312056
train 23, step: 500, loss: 1.420424145469486, grad_norm: 0.1789739173306785, ic: 0.08506230984244093
train 23, step: 1000, loss: 1.6433443196614583, grad_norm: 0.06157896887215891, ic: 0.2632699286254417
train 23, step: 1500, loss: 1.1242984938603222, grad_norm: 1.033433066172651, ic: 0.09492558749717082
train 23, step: 2000, loss: 1.8896858385657236, grad_norm: 1.9316789149845535, ic: 0.432220494565357
Epoch 23: 2022-05-08 00:54:53.689918: train loss: 1.6155085714196968
Eval step 0: eval loss: 0.8376030072773972
Eval: 2022-05-08 00:55:37.585062: total loss: 1.0673611155169227, mse:4.600843832953239, ic :0.18110110365267054, sharpe5:16.217686346769334, irr5:552.37451171875, ndcg5:0.8550758403591061, pnl5:4.776230812072754 
train 24, step: 0, loss: 2.1972143810907045, grad_norm: 0.14448727877925263, ic: 0.15627922376334263
train 24, step: 500, loss: 1.2097276264591441, grad_norm: 0.08134060664193796, ic: 0.13846007292849166
train 24, step: 1000, loss: 0.9112363373565159, grad_norm: 0.06117503889617469, ic: 0.5205656703269564
train 24, step: 1500, loss: 2.6119665537407255, grad_norm: 2.1788450207484753, ic: 0.0035067920667851554
train 24, step: 2000, loss: 0.9294367932457213, grad_norm: 0.0668285282304199, ic: 0.0912703687703589
Epoch 24: 2022-05-08 01:07:11.599466: train loss: 1.6108832771505188
Eval step 0: eval loss: 0.8240103686446258
Eval: 2022-05-08 01:07:40.907107: total loss: 1.0658635173484683, mse:4.6048849225592905, ic :0.19007315211015985, sharpe5:17.254786055088044, irr5:589.6464233398438, ndcg5:0.8559144241698056, pnl5:6.328096389770508 
train 25, step: 0, loss: 0.8345798801731419, grad_norm: 0.04813971776397641, ic: 0.6150676817037924
train 25, step: 500, loss: 0.8644750312736027, grad_norm: 0.03517869445882492, ic: 0.26060590864478184
train 25, step: 1000, loss: 2.127872417650419, grad_norm: 0.23753373770461217, ic: 0.2448894130081455
train 25, step: 1500, loss: 1.1241052031355718, grad_norm: 0.8556031832446643, ic: 0.5273125159285179
train 25, step: 2000, loss: 1.004143867783874, grad_norm: 0.567613939282256, ic: 0.602780293726942
Epoch 25: 2022-05-08 01:15:33.619961: train loss: 1.6149078678003184
Eval step 0: eval loss: 0.8250232049361169
Eval: 2022-05-08 01:16:05.443627: total loss: 1.0645820264264803, mse:4.581425578142392, ic :0.19618288820041943, sharpe5:18.206786502599716, irr5:609.0062255859375, ndcg5:0.8540853350079088, pnl5:4.670085430145264 
train 26, step: 0, loss: 6.739595553364616, grad_norm: 2.212073848577835, ic: 0.12286055508602725
train 26, step: 500, loss: 3.874636938648358, grad_norm: 2.431891383509094, ic: 0.36969994715686233
train 26, step: 1000, loss: 1.2627348847441622, grad_norm: 1.19386996128592, ic: -0.00010498849720485658
train 26, step: 1500, loss: 0.8365199598641991, grad_norm: 0.20559449974102378, ic: 0.2991171775460161
train 26, step: 2000, loss: 0.9549110106461448, grad_norm: 0.31232891664065604, ic: 0.1621978626456499
Epoch 26: 2022-05-08 01:23:45.671507: train loss: 1.6142681075934397
Eval step 0: eval loss: 0.8270380725599314
Eval: 2022-05-08 01:24:17.494506: total loss: 1.0650917399499231, mse:4.586885864260747, ic :0.1905608306375243, sharpe5:17.50938763618469, irr5:570.5045166015625, ndcg5:0.8652474931524272, pnl5:4.381860256195068 
train 27, step: 0, loss: 0.8263042853860294, grad_norm: 0.027329681538135597, ic: 0.12936107104099503
train 27, step: 500, loss: 0.916228567127282, grad_norm: 1.205250295961263, ic: 0.25407594839023745
train 27, step: 1000, loss: 0.7458145837864947, grad_norm: 0.4219347900202557, ic: 0.18365127570588052
train 27, step: 1500, loss: 0.638319146256971, grad_norm: 0.1447443565924287, ic: 0.5215980246635492
train 27, step: 2000, loss: 1.383958311088174, grad_norm: 0.05328514492817367, ic: -0.012832167423616071
Epoch 27: 2022-05-08 01:31:55.905862: train loss: 1.6121408967787312
Eval step 0: eval loss: 0.833643890155756
Eval: 2022-05-08 01:32:25.959633: total loss: 1.0666055999411048, mse:4.591566716898617, ic :0.18858129548894945, sharpe5:16.53917244315147, irr5:562.7725830078125, ndcg5:0.8513424967375175, pnl5:4.344521999359131 
train 28, step: 0, loss: 1.5490588803088803, grad_norm: 1.3930817064646024, ic: 0.2199484112465217
train 28, step: 500, loss: 1.3989552582622975, grad_norm: 2.292113900776835, ic: 0.15122153589248066
train 28, step: 1000, loss: 0.9107224379128557, grad_norm: 0.23539053031103846, ic: 0.5761109311110721
train 28, step: 1500, loss: 1.0332433408605284, grad_norm: 0.04416098684283115, ic: 0.03943437528551858
train 28, step: 2000, loss: 1.0401090844277223, grad_norm: 0.2676100826465648, ic: 0.13734702647514288
Epoch 28: 2022-05-08 01:40:08.186728: train loss: 1.6102588752005498
Eval step 0: eval loss: 0.8262078915305584
Eval: 2022-05-08 01:40:40.244200: total loss: 1.0674478327937609, mse:4.6210891921422315, ic :0.19151100666935147, sharpe5:17.94596640586853, irr5:596.2200317382812, ndcg5:0.8632787677432039, pnl5:6.4678497314453125 
train 29, step: 0, loss: 0.90185759734529, grad_norm: 0.054209810505372116, ic: 0.1382014596189115
train 29, step: 500, loss: 1.1075908954326923, grad_norm: 0.10119467075051253, ic: 0.6068472220598379
train 29, step: 1000, loss: 1.0630644463797487, grad_norm: 0.9347466131965669, ic: 0.09247913061873167
train 29, step: 1500, loss: 2.386652807255074, grad_norm: 0.6721121001559371, ic: 0.016726981165217786
train 29, step: 2000, loss: 4.223301263503086, grad_norm: 4.267753580104824, ic: 0.23265665187907014
Epoch 29: 2022-05-08 01:48:25.129717: train loss: 1.6108384541945275
Eval step 0: eval loss: 0.8326845641382375
Eval: 2022-05-08 01:48:55.851116: total loss: 1.0661296166684027, mse:4.58190679880294, ic :0.19412874503592695, sharpe5:17.648567361831663, irr5:601.2099609375, ndcg5:0.8558072454785315, pnl5:4.937716960906982 
train 30, step: 0, loss: 1.0082053012692247, grad_norm: 0.09778258871207796, ic: 0.5077618413107069
train 30, step: 500, loss: 1.4120903484541634, grad_norm: 2.7069413514014435, ic: 0.17013335663685053
train 30, step: 1000, loss: 0.9848645759351325, grad_norm: 0.1851473973814517, ic: -0.03251378884790509
train 30, step: 1500, loss: 1.500594463460766, grad_norm: 3.5600373700174197, ic: 0.15483569273999684
train 30, step: 2000, loss: 1.8441418952037107, grad_norm: 1.3649265465600315, ic: 0.10784718967175465
Epoch 30: 2022-05-08 01:56:36.453191: train loss: 1.6135149558715203
Eval step 0: eval loss: 0.8291013052390673
Eval: 2022-05-08 01:57:06.800317: total loss: 1.0698680753525773, mse:4.6536552249331145, ic :0.1923096479378205, sharpe5:17.559758180379866, irr5:598.690673828125, ndcg5:0.8621685881559775, pnl5:6.654580116271973 
train 31, step: 0, loss: 1.054388921492607, grad_norm: 0.45818263573004614, ic: 0.34949739351953685
train 31, step: 500, loss: 1.485179759837963, grad_norm: 2.1457741784767, ic: -0.004459211816752774
train 31, step: 1000, loss: 4.458842579039812, grad_norm: 6.846494384792543, ic: 0.4545218844366078
train 31, step: 1500, loss: 0.7690120854485686, grad_norm: 0.11480178430625067, ic: 0.7116787186894823
train 31, step: 2000, loss: 1.2194319611987081, grad_norm: 1.5150249586179254, ic: 0.1523493948800738
Epoch 31: 2022-05-08 02:04:47.423540: train loss: 1.6081245004683258
Eval step 0: eval loss: 0.8356772805667149
Eval: 2022-05-08 02:05:18.691400: total loss: 1.0668836105715127, mse:4.588664689955679, ic :0.18983865423375307, sharpe5:17.677690522670744, irr5:595.6615600585938, ndcg5:0.8405609836918053, pnl5:5.314683437347412 
train 32, step: 0, loss: 1.1289270866316738, grad_norm: 0.013037303537787575, ic: 0.18074665361023018
train 32, step: 500, loss: 1.4849521907295768, grad_norm: 0.7418046187985958, ic: 0.12163024035736474
train 32, step: 1000, loss: 1.0294860915288937, grad_norm: 0.1164277907216048, ic: 0.5146605634885463
train 32, step: 1500, loss: 0.9780696167643974, grad_norm: 1.5518355198155336, ic: 0.08121594211685085
train 32, step: 2000, loss: 0.9408429644123998, grad_norm: 0.029331580598170266, ic: 0.551225658105603
Epoch 32: 2022-05-08 02:12:54.843502: train loss: 1.609393055221226
Eval step 0: eval loss: 0.8229001590901607
Eval: 2022-05-08 02:13:25.747287: total loss: 1.0634775639069105, mse:4.589330388841959, ic :0.2004628650105933, sharpe5:18.576770210266112, irr5:637.6128540039062, ndcg5:0.8590705415784476, pnl5:6.76857328414917 
train 33, step: 0, loss: 1.278115016423686, grad_norm: 0.7878545439342473, ic: 0.2035690140511207
train 33, step: 500, loss: 0.9837063642928763, grad_norm: 0.04778943978008693, ic: 0.20675680470624824
train 33, step: 1000, loss: 1.0520571660637728, grad_norm: 1.9203880915933949, ic: 0.21875632277528217
train 33, step: 1500, loss: 0.8974581862118179, grad_norm: 0.23736522991847925, ic: 0.5417020083044475
train 33, step: 2000, loss: 0.8024873325450674, grad_norm: 0.06011692551751193, ic: 0.29069647624717154
Epoch 33: 2022-05-08 02:21:15.751243: train loss: 1.6114942716321967
Eval step 0: eval loss: 0.8268927201330347
Eval: 2022-05-08 02:21:46.204968: total loss: 1.0643320944909138, mse:4.574850282480307, ic :0.20183408311412515, sharpe5:18.241401708126066, irr5:633.789794921875, ndcg5:0.852994301231471, pnl5:4.574471950531006 
train 34, step: 0, loss: 1.0204493610665337, grad_norm: 0.8135891570782467, ic: 0.5880818069465501
train 34, step: 500, loss: 0.7884924652379587, grad_norm: 0.22847914490918592, ic: 0.25256028520947627
train 34, step: 1000, loss: 3.1732655889976957, grad_norm: 1.5061398585500896, ic: 0.3377471607378605
train 34, step: 1500, loss: 0.8142088388799911, grad_norm: 0.5755644188028168, ic: 0.6940927295693724
train 34, step: 2000, loss: 5.905418673750969, grad_norm: 30.52307974128831, ic: 0.44754362013688637
Epoch 34: 2022-05-08 02:29:22.443147: train loss: 1.606417794025413
Eval step 0: eval loss: 0.821873302077845
Eval: 2022-05-08 02:29:53.045581: total loss: 1.0646931573478988, mse:4.592726430174717, ic :0.1981090456024483, sharpe5:18.286200749874116, irr5:631.369384765625, ndcg5:0.8402051399040943, pnl5:3.950199842453003 
train 35, step: 0, loss: 1.1769058766084557, grad_norm: 1.0200973380192324, ic: 0.563408128232504
train 35, step: 500, loss: 1.1716679895109363, grad_norm: 0.4304337146048759, ic: 0.09077329225939434
train 35, step: 1000, loss: 1.8010117840615048, grad_norm: 4.370677035616236, ic: 0.04472479488991217
train 35, step: 1500, loss: 1.621321369830827, grad_norm: 1.4618174440495189, ic: 0.0575106435375952
train 35, step: 2000, loss: 0.7887981231199865, grad_norm: 0.13163102202135088, ic: 0.5608639223858694
Epoch 35: 2022-05-08 02:37:30.954034: train loss: 1.614492155590474
Eval step 0: eval loss: 0.8305254374464897
Eval: 2022-05-08 02:38:00.866249: total loss: 1.0667868487720558, mse:4.602372297504485, ic :0.19556586107192545, sharpe5:18.67106765270233, irr5:617.9063110351562, ndcg5:0.8308801176868097, pnl5:5.251504898071289 
train 36, step: 0, loss: 1.8424219823145604, grad_norm: 3.450799999741049, ic: 0.0978946342077523
train 36, step: 500, loss: 0.8375194119693165, grad_norm: 0.18998592983227486, ic: 0.1574289169122199
train 36, step: 1000, loss: 1.6547982954545453, grad_norm: 3.29751902743402, ic: 0.2798460049129488
train 36, step: 1500, loss: 0.7726569321342999, grad_norm: 0.0999940537475698, ic: 0.3787281676758958
train 36, step: 2000, loss: 1.1049439155269558, grad_norm: 1.8093803638963448, ic: 0.7694562213617745
Epoch 36: 2022-05-08 02:45:47.193200: train loss: 1.6087153778056198
Eval step 0: eval loss: 0.8290113925431375
Eval: 2022-05-08 02:46:18.466707: total loss: 1.0652118668138613, mse:4.589307262377649, ic :0.1970837846630193, sharpe5:18.30495467305183, irr5:624.7547607421875, ndcg5:0.8549917695584041, pnl5:5.164560794830322 
train 37, step: 0, loss: 1.99640481431853, grad_norm: 5.00163470284554, ic: 0.19210013353663002
train 37, step: 500, loss: 2.3317373253976346, grad_norm: 3.507235935317369, ic: -0.0644319405958562
train 37, step: 1000, loss: 1.06828598417285, grad_norm: 0.2926604668076983, ic: 0.0850977129563152
train 37, step: 1500, loss: 1.997258879513344, grad_norm: 2.8510691125920156, ic: 0.614318127730592
train 37, step: 2000, loss: 1.3169967296511629, grad_norm: 0.30622644349306466, ic: 0.1495466662119438
Epoch 37: 2022-05-08 02:54:00.806454: train loss: 1.6050743532088185
Eval step 0: eval loss: 0.8291212429613408
Eval: 2022-05-08 02:54:33.052636: total loss: 1.064754543484009, mse:4.581551383343484, ic :0.19932703007689118, sharpe5:18.640050748586653, irr5:626.19482421875, ndcg5:0.8648368897170617, pnl5:4.383070468902588 
train 38, step: 0, loss: 1.3441158387719132, grad_norm: 0.5048961511194993, ic: -0.08469065233044146
train 38, step: 500, loss: 0.9102323555652005, grad_norm: 0.11006292584392191, ic: 0.2550302190932284
train 38, step: 1000, loss: 0.9026811079545455, grad_norm: 0.2737654635671428, ic: 0.17642527642729472
train 38, step: 1500, loss: 0.945870853502278, grad_norm: 0.020623850707024063, ic: 0.21588352441900244
train 38, step: 2000, loss: 2.2838520153771715, grad_norm: 3.850155737367267, ic: 0.03006490607023147
Epoch 38: 2022-05-08 03:02:17.125891: train loss: 1.6054972856499052
Eval step 0: eval loss: 0.828329265180453
Eval: 2022-05-08 03:02:45.904550: total loss: 1.0635842156354949, mse:4.584870196658184, ic :0.1976653215683424, sharpe5:19.656029827594757, irr5:649.5463256835938, ndcg5:0.8575740286943949, pnl5:5.2340497970581055 
train 39, step: 0, loss: 0.9613998366422974, grad_norm: 0.02944078312646652, ic: 0.12114424601039968
train 39, step: 500, loss: 0.8816911607957821, grad_norm: 0.28348774471197574, ic: 0.2245941500627955
train 39, step: 1000, loss: 0.9317285751261425, grad_norm: 0.3406652447897754, ic: 0.25192508507441336
train 39, step: 1500, loss: 2.0763229636118266, grad_norm: 1.0525475721691326, ic: 0.22565158092976867
train 39, step: 2000, loss: 0.6140486730783471, grad_norm: 0.1418613908159847, ic: 0.09352692249116032
Epoch 39: 2022-05-08 03:10:35.548721: train loss: 1.6059591932033117
Eval step 0: eval loss: 0.828700042499506
Eval: 2022-05-08 03:11:06.405083: total loss: 1.0647594741484023, mse:4.600895831545244, ic :0.19377716572798273, sharpe5:19.694074959754943, irr5:651.4788818359375, ndcg5:0.8529719977026622, pnl5:11.736308097839355 
