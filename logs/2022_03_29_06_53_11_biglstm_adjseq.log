Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='BiGLSTM', dataset_type='AdjSeqTimeDataset', seed=10086, num_days=8, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=1, num_heads=1, gnn_layers=2, print_inteval=500, relation_num=1, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=True)
689852
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8224926329495613, grad_norm: 0.12881675831193562, ic: -0.027662175716501865
train 0, step: 500, loss: 1.0990286091811785, grad_norm: 0.015080673962716354, ic: 0.0020368968625715314
train 0, step: 1000, loss: 0.6800792429706838, grad_norm: 6.244395673297585e-05, ic: 0.03028629570282436
train 0, step: 1500, loss: 0.8241586213064666, grad_norm: 0.16989515042236913, ic: 0.0784484229572727
train 0, step: 2000, loss: 2.3494477982954542, grad_norm: 0.7191317774948127, ic: 0.018428382066556226
Epoch 0: train loss: 1.6302736031169487
Eval step 0: eval loss: 0.8350763225505199
Eval: total loss: 1.073602770400782, mse:4.628853556000368, ic :0.0030719810527853673, sharpe5:2.213708560913801, irr5:29.065183639526367, ndcg5:0.838002769349774 
train 1, step: 0, loss: 2.273630746105995, grad_norm: 0.04232632590309402, ic: 0.026402250792348175
train 1, step: 500, loss: 0.5954578077936747, grad_norm: 0.013749929980750034, ic: -0.04157453654876227
train 1, step: 1000, loss: 1.1876858556694665, grad_norm: 0.09692663495170956, ic: 0.07842025737971768
train 1, step: 1500, loss: 2.221692558512029, grad_norm: 0.8577506387761425, ic: 0.056288581615356666
train 1, step: 2000, loss: 1.1321815127418156, grad_norm: 0.012530028232412019, ic: -0.036980887316990965
Epoch 1: train loss: 1.628532889047392
Eval step 0: eval loss: 0.836204396228278
Eval: total loss: 1.0739228595751509, mse:4.628354959085193, ic :0.0042755589337813826, sharpe5:1.3856175237894057, irr5:17.77326011657715, ndcg5:0.8548360116019996 
train 2, step: 0, loss: 1.9853921274038462, grad_norm: 0.24052123061704783, ic: 0.07109531990352375
train 2, step: 500, loss: 1.084444780006866, grad_norm: 0.41682682108527114, ic: 0.14571194608968602
train 2, step: 1000, loss: 1.0938662644397201, grad_norm: 0.06069120836049383, ic: 0.0899721711027953
train 2, step: 1500, loss: 1.2170767112759477, grad_norm: 0.2762062517000585, ic: 0.2575870867343224
train 2, step: 2000, loss: 2.7148810518525592, grad_norm: 4.635460939072903, ic: 0.10970556321559943
Epoch 2: train loss: 1.6285160884736474
Eval step 0: eval loss: 0.8376610119635334
Eval: total loss: 1.0744261297889448, mse:4.628233840469882, ic :0.009809935413081472, sharpe5:10.318900224566459, irr5:212.56849670410156, ndcg5:0.8520401305584994 
train 3, step: 0, loss: 0.8181649387472927, grad_norm: 0.14925936046560737, ic: 0.14581872540025134
train 3, step: 500, loss: 1.2290445036660023, grad_norm: 0.06945355695408442, ic: 0.004582539709087159
train 3, step: 1000, loss: 1.3800141220792719, grad_norm: 0.3590898387332726, ic: -0.002427915622221202
train 3, step: 1500, loss: 1.6346925882866297, grad_norm: 0.2726856475269131, ic: -0.015050018235502787
train 3, step: 2000, loss: 1.1125282985753298, grad_norm: 0.27244942735602434, ic: -0.07137793180250328
Epoch 3: train loss: 1.6269232523658894
Eval step 0: eval loss: 0.841155796944938
Eval: total loss: 1.0759237276140794, mse:4.622225311790173, ic :0.07442538303832792, sharpe5:9.682600591778755, irr5:192.64637756347656, ndcg5:0.8382996667060796 
train 4, step: 0, loss: 1.093665823383809, grad_norm: 0.36051942521580216, ic: -0.008281358935287114
train 4, step: 500, loss: 0.9849553732956278, grad_norm: 0.03522113225844623, ic: 0.22708556966337753
train 4, step: 1000, loss: 1.0933011492681697, grad_norm: 0.020715077176356227, ic: 0.27572747180922247
train 4, step: 1500, loss: 1.3044241304307895, grad_norm: 0.3120439262248607, ic: 0.043245404257147846
train 4, step: 2000, loss: 1.3367657738524827, grad_norm: 0.14285616415728694, ic: -0.0034139612289721235
Epoch 4: train loss: 1.6244757723819272
Eval step 0: eval loss: 0.8323618490611835
Eval: total loss: 1.0718286027893462, mse:4.60994340142477, ic :0.07430765686991234, sharpe5:10.293881435394287, irr5:207.3849639892578, ndcg5:0.8370335430883985 
train 5, step: 0, loss: 2.261218384449923, grad_norm: 0.00385967308213366, ic: 0.01607126839600767
train 5, step: 500, loss: 1.7977955492236946, grad_norm: 0.6612993942143564, ic: -0.0061876263923995485
train 5, step: 1000, loss: 4.469363247551223, grad_norm: 0.7468766476964992, ic: -0.025608969091599758
train 5, step: 1500, loss: 0.9288075399055755, grad_norm: 0.02503780378024304, ic: 0.11422827719396865
train 5, step: 2000, loss: 2.2428121747255867, grad_norm: 0.6377597551043999, ic: 0.019154442731302714
Epoch 5: train loss: 1.6252052924129397
Eval step 0: eval loss: 0.8335305485617429
Eval: total loss: 1.0725040975053355, mse:4.610118245490202, ic :0.07329137995302741, sharpe5:10.260634038448334, irr5:207.06809997558594, ndcg5:0.829257566858335 
train 6, step: 0, loss: 1.9929105247641508, grad_norm: 0.07063464412119685, ic: 0.2394589551136112
train 6, step: 500, loss: 1.0057516257856145, grad_norm: 0.00014511066534503986, ic: 0.019154394418489346
train 6, step: 1000, loss: 1.5534399580648217, grad_norm: 0.5181450614148305, ic: 0.020389207316554706
train 6, step: 1500, loss: 1.084519107784845, grad_norm: 0.004084750268004684, ic: 0.12274877615670657
train 6, step: 2000, loss: 1.0758741092377198, grad_norm: 0.1408475089832169, ic: 0.16900527205002908
Epoch 6: train loss: 1.6241205717846467
Eval step 0: eval loss: 0.831667096066186
Eval: total loss: 1.0720364570714065, mse:4.606054124816022, ic :0.0738993914147164, sharpe5:10.000740948915482, irr5:202.982177734375, ndcg5:0.8192336018326005 
train 7, step: 0, loss: 1.155506158146334, grad_norm: 0.13435733708091616, ic: 0.1962569801935705
train 7, step: 500, loss: 1.412790136613856, grad_norm: 0.03369532414485918, ic: 0.11248651237717622
train 7, step: 1000, loss: 1.0320673403532608, grad_norm: 0.0976426055623561, ic: -0.04167207934016702
train 7, step: 1500, loss: 1.8794728851935125, grad_norm: 0.6298080557972373, ic: 0.04428687903787577
train 7, step: 2000, loss: 2.9722553473475495, grad_norm: 0.6886365124383333, ic: 0.04209317739586906
Epoch 7: train loss: 1.623276351689286
Eval step 0: eval loss: 0.8377486917456556
Eval: total loss: 1.074618145471188, mse:4.607691362516412, ic :0.0758237871669166, sharpe5:10.170100434422492, irr5:197.30621337890625, ndcg5:0.8389936826287195 
train 8, step: 0, loss: 1.2621201321539752, grad_norm: 0.0010336793412768185, ic: -0.039598046478161876
train 8, step: 500, loss: 0.6375579833984375, grad_norm: 0.07679506236668365, ic: 0.024834386828235838
train 8, step: 1000, loss: 1.911329694075172, grad_norm: 0.5446525110891097, ic: 0.20630384664489684
train 8, step: 1500, loss: 1.4161936783417226, grad_norm: 0.19891114195461063, ic: 0.12700999192664944
train 8, step: 2000, loss: 3.8996047522189348, grad_norm: 1.0120395613143798, ic: -0.0619852223486614
Epoch 8: train loss: 1.6225066115779496
Eval step 0: eval loss: 0.8306461794242035
Eval: total loss: 1.0705937027985084, mse:4.595941480830948, ic :0.07092818698375405, sharpe5:10.254510115385054, irr5:192.1240692138672, ndcg5:0.8221714814107777 
train 9, step: 0, loss: 0.7909511637538982, grad_norm: 0.025562351783972793, ic: 0.23948820361424078
train 9, step: 500, loss: 1.0555857382015306, grad_norm: 0.19294115052575378, ic: -0.008029153525385128
train 9, step: 1000, loss: 0.800802819164128, grad_norm: 0.03138783418653081, ic: -0.0497548297318306
train 9, step: 1500, loss: 1.0471860216809559, grad_norm: 0.09967725394196209, ic: 0.024317827421828107
train 9, step: 2000, loss: 6.681746157987471, grad_norm: 0.4204472742904768, ic: 0.14145630282189325
Epoch 9: train loss: 1.6216460609972312
Eval step 0: eval loss: 0.8323794621552462
Eval: total loss: 1.070438400462124, mse:4.588551083982062, ic :0.07976063360429102, sharpe5:11.633008602261542, irr5:214.36509704589844, ndcg5:0.8350673434858319 
train 10, step: 0, loss: 0.9140071055335457, grad_norm: 0.027724862792745344, ic: 0.26007646519503685
train 10, step: 500, loss: 3.9507453345149925, grad_norm: 1.0238318655303476, ic: 0.10304847259969575
train 10, step: 1000, loss: 1.3026927473358294, grad_norm: 0.500907430161814, ic: 0.3490429803462471
train 10, step: 1500, loss: 1.4286287837155418, grad_norm: 0.005101842092692422, ic: -0.05279279543339402
train 10, step: 2000, loss: 1.3031184135189855, grad_norm: 0.603731442433371, ic: 0.07682566936408727
Epoch 10: train loss: 1.6212597847779264
Eval step 0: eval loss: 0.8285780550618747
Eval: total loss: 1.0698399642360772, mse:4.593789835954982, ic :0.07834727073569596, sharpe5:11.999763865470886, irr5:209.0758056640625, ndcg5:0.8293276166840722 
train 11, step: 0, loss: 2.2771941624796304, grad_norm: 0.10232121792447019, ic: -0.14293945792779128
train 11, step: 500, loss: 1.8690691692073174, grad_norm: 0.26519047717798894, ic: 0.13354419963700945
train 11, step: 1000, loss: 3.0188743484799776, grad_norm: 1.0142667780651662, ic: 0.09423293812086375
train 11, step: 1500, loss: 1.0056283412835536, grad_norm: 0.12792682709583117, ic: -0.026259810129494254
train 11, step: 2000, loss: 1.1717690838112516, grad_norm: 0.49014947793236696, ic: 0.21735598054085561
Epoch 11: train loss: 1.6214435114769208
Eval step 0: eval loss: 0.8273633943852027
Eval: total loss: 1.0699283442356387, mse:4.5937565112702465, ic :0.08771443094631749, sharpe5:14.986057961583137, irr5:296.20721435546875, ndcg5:0.8355517217073949 
train 12, step: 0, loss: 1.330732571148431, grad_norm: 0.631548831530292, ic: 0.02759906333265228
train 12, step: 500, loss: 1.919918116098922, grad_norm: 0.2522920474408069, ic: 0.07973529823791221
train 12, step: 1000, loss: 1.3423631561468845, grad_norm: 0.19717060746597548, ic: 0.2678462611898485
train 12, step: 1500, loss: 1.1270475830635949, grad_norm: 0.24657497553989777, ic: 0.053587702118731595
train 12, step: 2000, loss: 2.831556241326091, grad_norm: 2.5239483378378385, ic: 0.06961913028345595
Epoch 12: train loss: 1.617084152689857
Eval step 0: eval loss: 0.8242944091668312
Eval: total loss: 1.0681620611498273, mse:4.577331117014401, ic :0.10359934316944208, sharpe5:15.779676391482353, irr5:316.3411560058594, ndcg5:0.8177021246371866 
train 13, step: 0, loss: 1.885013796248526, grad_norm: 0.09762848834909485, ic: 0.2239059775216542
train 13, step: 500, loss: 2.852227525855211, grad_norm: 0.8986146637551463, ic: 0.08813566265041797
train 13, step: 1000, loss: 0.9776877963041104, grad_norm: 0.001236727776578245, ic: -0.01250917944818749
train 13, step: 1500, loss: 0.9581357310480038, grad_norm: 0.0005452078729894628, ic: 0.08176869835930597
train 13, step: 2000, loss: 0.7426388428329356, grad_norm: 0.0122224385175241, ic: 0.05685327455227603
Epoch 13: train loss: 1.6162748550843422
Eval step 0: eval loss: 0.8191902759264744
Eval: total loss: 1.0695795364487926, mse:4.59305630315708, ic :0.08917045194143806, sharpe5:18.111944206953048, irr5:337.916259765625, ndcg5:0.8354301443770031 
train 14, step: 0, loss: 2.238100070968052, grad_norm: 0.5817957681293618, ic: 0.1414433268580266
train 14, step: 500, loss: 0.7727776126585145, grad_norm: 0.0034078328319712002, ic: 0.018452356306313842
train 14, step: 1000, loss: 3.941685792326799, grad_norm: 0.9360770041609423, ic: 0.17901310172446158
train 14, step: 1500, loss: 1.379526829850235, grad_norm: 0.729847983687303, ic: 0.24167059965585783
train 14, step: 2000, loss: 1.3221150716145833, grad_norm: 0.3150793371969341, ic: -0.07935424785029153
Epoch 14: train loss: 1.616209017298031
Eval step 0: eval loss: 0.8290399809521458
Eval: total loss: 1.068769695286164, mse:4.568570555426526, ic :0.1149757505140759, sharpe5:17.712356808185575, irr5:353.801025390625, ndcg5:0.8232805045154901 
train 15, step: 0, loss: 0.9165070134943182, grad_norm: 0.07712782080248601, ic: 0.24209158289494645
train 15, step: 500, loss: 2.2807632017606303, grad_norm: 0.15133408118139674, ic: 0.20555362157071433
train 15, step: 1000, loss: 1.1340547112066857, grad_norm: 0.24791593345094337, ic: 0.21981918970180572
train 15, step: 1500, loss: 0.8651373092639834, grad_norm: 0.00224370721945616, ic: 0.0077422645535100846
train 15, step: 2000, loss: 2.8703287846853422, grad_norm: 0.6069408260949201, ic: 0.1673716182886639
Epoch 15: train loss: 1.6153442018500288
Eval step 0: eval loss: 0.8291922635062862
Eval: total loss: 1.0692291580123168, mse:4.571711157050607, ic :0.1170651101336569, sharpe5:17.178548226356504, irr5:316.70965576171875, ndcg5:0.8325800459773801 
train 16, step: 0, loss: 0.9126090069968392, grad_norm: 0.00995219150377376, ic: 0.0011764252142639968
train 16, step: 500, loss: 1.4311129512801348, grad_norm: 0.27285621659682585, ic: 0.099879661172674
train 16, step: 1000, loss: 0.9736324283290716, grad_norm: 0.1331652137706735, ic: 0.020878662926749655
train 16, step: 1500, loss: 2.7412094008162864, grad_norm: 1.8117852113842607, ic: 0.05574586760838169
train 16, step: 2000, loss: 1.8829052615955941, grad_norm: 0.626831430504304, ic: -0.0032736338574933235
Epoch 16: train loss: 1.6144723494261786
Eval step 0: eval loss: 0.8259198920484465
Eval: total loss: 1.067499015055155, mse:4.5694948284278984, ic :0.12090985773052813, sharpe5:17.547271960973738, irr5:340.27252197265625, ndcg5:0.8295835948198422 
train 17, step: 0, loss: 0.7618291646289372, grad_norm: 0.11637662094089696, ic: 0.17890639652848
train 17, step: 500, loss: 1.8088776344476745, grad_norm: 0.28891102657771134, ic: 0.13739669377296404
train 17, step: 1000, loss: 2.024959413072009, grad_norm: 0.7762324249672017, ic: 0.04759686222091153
train 17, step: 1500, loss: 1.1222737009932355, grad_norm: 0.1515098737861482, ic: 0.023697255524308933
train 17, step: 2000, loss: 1.5236451253725614, grad_norm: 0.4384255964215495, ic: 0.1323023063868914
Epoch 17: train loss: 1.615070555231788
Eval step 0: eval loss: 0.8319127151625855
Eval: total loss: 1.0681122265864307, mse:4.566054038014133, ic :0.12299471400680193, sharpe5:17.900166574716568, irr5:352.907470703125, ndcg5:0.8260571933705935 
train 18, step: 0, loss: 1.0218842765971425, grad_norm: 0.18343892527886826, ic: 0.02668507453142348
train 18, step: 500, loss: 1.5612212832611387, grad_norm: 0.2672106736737779, ic: 0.00418600378374807
train 18, step: 1000, loss: 1.7495614288279966, grad_norm: 0.34221494255607254, ic: 0.15792790901840797
train 18, step: 1500, loss: 0.7657306470053108, grad_norm: 0.027863442696260687, ic: 0.06617618807720758
train 18, step: 2000, loss: 3.25648776651016, grad_norm: 0.7420879813371423, ic: -0.021250504359388647
Epoch 18: train loss: 1.6150181646042208
Eval step 0: eval loss: 0.830579133960308
Eval: total loss: 1.0681030763801997, mse:4.559760249933773, ic :0.12746051017072865, sharpe5:18.210799119472505, irr5:358.8373718261719, ndcg5:0.83304299743131 
train 19, step: 0, loss: 2.3328902279612977, grad_norm: 0.1522045824537825, ic: 0.10532545517356001
train 19, step: 500, loss: 9.17291422143975, grad_norm: 1.5940296895016837, ic: 0.0072739415082707735
train 19, step: 1000, loss: 1.2584186805951025, grad_norm: 0.19615291762636838, ic: 0.004550913529282718
train 19, step: 1500, loss: 0.7468622947225766, grad_norm: 0.03535460915834543, ic: 0.18625036638009693
train 19, step: 2000, loss: 1.3082301402563736, grad_norm: 0.1955674546822848, ic: -0.025170593572382535
Epoch 19: train loss: 1.6149750015176776
Eval step 0: eval loss: 0.83073366636223
Eval: total loss: 1.068178551632905, mse:4.560054986577262, ic :0.12550377701847337, sharpe5:18.997814987897872, irr5:348.3966064453125, ndcg5:0.8392362486796667 
