Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=8, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='BaseLSTM', dataset_type='TimeDataset', seed=10086, num_days=8, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=1, num_heads=1, gnn_layers=2, print_inteval=500, mask_type='soft', shuffle=True, input_graph=False, use_adj=False, mask_adj=False)
431830
BaseLSTM(
  (rnn1): LSTM(9, 128, batch_first=True, dropout=0.3, bidirectional=True)
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (predict): Linear(in_features=128, out_features=1, bias=True)
  (relu): PReLU(num_parameters=1)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 1.0604989711077046, grad_norm: 0.029915942103754433, ic: -0.005231022303330004
Epoch 0: train loss: 1.6094557752585437
Eval step 0: eval loss: 1.1438009969072842
Eval: total loss: 1.0736281280472606, ic :0.005511821611778505 
train 1, step: 0, loss: 2.3206067517275213, grad_norm: 0.13195322939454748, ic: 0.0603367783320973
Epoch 1: train loss: 1.610142623654409
Eval step 0: eval loss: 1.1463530271188391
Eval: total loss: 1.0742748483697837, ic :0.008953693832861298 
train 2, step: 0, loss: 1.2198906023738105, grad_norm: 0.0035719233397300243, ic: 0.05772078938536101
Epoch 2: train loss: 1.608276124974787
Eval step 0: eval loss: 1.1531711645390537
Eval: total loss: 1.0766755597637898, ic :0.013528029812863204 
train 3, step: 0, loss: 1.628108579012257, grad_norm: 0.00022885825252664946, ic: 0.061494408500526135
Epoch 3: train loss: 1.6069206604575674
Eval step 0: eval loss: 1.1442413079061655
Eval: total loss: 1.0737788810224473, ic :0.014826802890108157 
train 4, step: 0, loss: 1.261365249203498, grad_norm: 0.0002952019819749877, ic: 0.03368452962097355
Epoch 4: train loss: 1.6106085745628616
Eval step 0: eval loss: 1.143320844574587
Eval: total loss: 1.0735471287082288, ic :0.015218839229799748 
train 5, step: 0, loss: 1.1042764743634421, grad_norm: 0.0923317047961753, ic: -0.002681505565045303
Epoch 5: train loss: 1.6088503385725152
Eval step 0: eval loss: 1.140427849657827
Eval: total loss: 1.0731192496198076, ic :0.01846794108879231 
train 6, step: 0, loss: 1.3959855962259102, grad_norm: 0.3164794585859974, ic: 0.05003742848450037
Epoch 6: train loss: 1.6150446001999552
Eval step 0: eval loss: 1.1474390247664012
Eval: total loss: 1.074391264789793, ic :0.029517945519656495 
train 7, step: 0, loss: 1.3620813634842273, grad_norm: 0.1965949432139791, ic: 0.040809400651088665
Epoch 7: train loss: 1.6129767393066041
Eval step 0: eval loss: 1.1481119577465946
Eval: total loss: 1.074710501130278, ic :0.032010386442219714 
train 8, step: 0, loss: 1.6107653781038374, grad_norm: 0.010326667469730204, ic: 0.06207620601981065
Epoch 8: train loss: 1.6058880640440432
Eval step 0: eval loss: 1.1422644066263077
Eval: total loss: 1.07322590929827, ic :0.029713981054123196 
train 9, step: 0, loss: 1.087698300736079, grad_norm: 0.0592987908608427, ic: 0.009592872367548376
Epoch 9: train loss: 1.6089328474686084
Eval step 0: eval loss: 1.1399443557939066
Eval: total loss: 1.07288056430437, ic :0.03203556895420404 
train 10, step: 0, loss: 2.283213238886175, grad_norm: 0.004461263123739882, ic: 0.10909167763339278
Epoch 10: train loss: 1.612179254895575
Eval step 0: eval loss: 1.1451520036849376
Eval: total loss: 1.0733482464809225, ic :0.04144869203392546 
train 11, step: 0, loss: 1.4829640338104106, grad_norm: 0.0021795829748194884, ic: 0.09167173077007948
Epoch 11: train loss: 1.6063641772480814
Eval step 0: eval loss: 1.1426808128248995
Eval: total loss: 1.0726948925385063, ic :0.041225776224801544 
train 12, step: 0, loss: 1.7877287387552068, grad_norm: 0.00545042214627557, ic: 0.08150054126633846
Epoch 12: train loss: 1.6090584634443985
Eval step 0: eval loss: 1.1399902375880107
Eval: total loss: 1.0731083668342063, ic :0.03522533238030486 
train 13, step: 0, loss: 1.3920891387779732, grad_norm: 0.003121913425360938, ic: -0.016064601191367524
Epoch 13: train loss: 1.6183370371038874
Eval step 0: eval loss: 1.1397330682042508
Eval: total loss: 1.0727378051139334, ic :0.03413797234324544 
train 14, step: 0, loss: 1.272026409030982, grad_norm: 0.025260084088694675, ic: -0.00044095368918567295
Epoch 14: train loss: 1.6087220643054905
Eval step 0: eval loss: 1.138203418026584
Eval: total loss: 1.0733976168069463, ic :0.03152670446007295 
train 15, step: 0, loss: 1.992473140042621, grad_norm: 0.017946863242438323, ic: 0.0233184531413571
Epoch 15: train loss: 1.6118791159283017
Eval step 0: eval loss: 1.1442906597519247
Eval: total loss: 1.0729722055483066, ic :0.04411619621187411 
train 16, step: 0, loss: 1.7446563081051385, grad_norm: 0.09377775164275044, ic: 0.021685199933808703
Epoch 16: train loss: 1.607084602628468
Eval step 0: eval loss: 1.1439369715239849
Eval: total loss: 1.072848086739986, ic :0.044997428347736165 
train 17, step: 0, loss: 1.608835170121578, grad_norm: 0.007021568613646301, ic: 0.012849266372034932
Epoch 17: train loss: 1.6073467390633207
Eval step 0: eval loss: 1.1415799068072645
Eval: total loss: 1.0728893770496826, ic :0.03873297574695574 
train 18, step: 0, loss: 2.0400828789957637, grad_norm: 0.003856740032018395, ic: -0.01600700651234582
Epoch 18: train loss: 1.6056657652222326
Eval step 0: eval loss: 1.1416607461587813
Eval: total loss: 1.072611229015022, ic :0.049184936822540215 
train 19, step: 0, loss: 1.4648492252518617, grad_norm: 0.0018225426972160923, ic: 0.10647132618099459
Epoch 19: train loss: 1.6062637851637365
Eval step 0: eval loss: 1.1465413095512271
Eval: total loss: 1.0735168133050905, ic :0.04797271755261758 
