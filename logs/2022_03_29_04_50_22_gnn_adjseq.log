Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='GNNModel', dataset_type='AdjSeqTimeDataset', seed=10086, num_days=1, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=3, num_heads=1, gnn_layers=2, print_inteval=500, relation_num=1, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=True)
612789
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
    (1): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.846197950983622, grad_norm: 0.00026527976980683795, ic: 0.07376071679729188
train 0, step: 500, loss: 0.9487475718064117, grad_norm: 0.14267057953791537, ic: -0.014420098245642991
train 0, step: 1000, loss: 1.016800025201613, grad_norm: 0.023243594021159095, ic: -0.11119121649971873
train 0, step: 1500, loss: 0.9321479772216957, grad_norm: 0.11192188725388234, ic: 0.21753264380304949
train 0, step: 2000, loss: 1.0570099695977975, grad_norm: 0.35012242260400256, ic: 0.08706494523912398
Epoch 0: train loss: 1.6294511120926938
Eval step 0: eval loss: 1.0080592118796077
Eval: total loss: 1.0856897501967178, ic :0.029125765358108485, sharpe5:0.5673052893206477, irr5:5.600896835327148, ndcg5:0.8296163489622966 
train 1, step: 0, loss: 0.9427355566058719, grad_norm: 0.08108813894603376, ic: 0.11879378830874214
train 1, step: 500, loss: 1.6788434498604277, grad_norm: 0.11492594761347223, ic: -0.08099382100132875
train 1, step: 1000, loss: 1.0689619525623104, grad_norm: 0.390115081296333, ic: 0.04880460284993791
train 1, step: 1500, loss: 0.8835188289664457, grad_norm: 0.12719328483553338, ic: -0.09419914528320725
train 1, step: 2000, loss: 4.657376567398119, grad_norm: 2.0657587743867936, ic: 0.06259333167559512
Epoch 1: train loss: 1.6271290414685537
Eval step 0: eval loss: 0.997816490587151
Eval: total loss: 1.0837137843670408, ic :0.047738777709031524, sharpe5:1.9151843382418154, irr5:19.4160213470459, ndcg5:0.8497845315018739 
train 2, step: 0, loss: 0.9546995677875475, grad_norm: 0.12041129409149028, ic: 0.009211540735813368
train 2, step: 500, loss: 1.2831821053301338, grad_norm: 0.09476254882593332, ic: 0.2053622869029581
train 2, step: 1000, loss: 1.0486337661743164, grad_norm: 0.010836286439352402, ic: 0.009239818310881654
train 2, step: 1500, loss: 0.9493362808485463, grad_norm: 0.3193317937819234, ic: -0.010140438268263014
train 2, step: 2000, loss: 1.702733273613722, grad_norm: 0.0837158409364978, ic: 0.09930148972671499
Epoch 2: train loss: 1.6263138150117293
Eval step 0: eval loss: 0.9893870823254014
Eval: total loss: 1.0841977925985968, ic :0.045981340563319706, sharpe5:1.4343520894646644, irr5:14.246757507324219, ndcg5:0.8574379758944267 
train 3, step: 0, loss: 0.8377800436580882, grad_norm: 0.06274486307756653, ic: 0.2370468740558919
train 3, step: 500, loss: 1.1795238515685893, grad_norm: 0.004408411196490862, ic: 0.104165747820799
train 3, step: 1000, loss: 0.9042266417886609, grad_norm: 0.027754153655893368, ic: 0.20688663502858826
train 3, step: 1500, loss: 4.3776549359709485, grad_norm: 0.7601162567344939, ic: 0.09053782016920198
train 3, step: 2000, loss: 1.640134658077522, grad_norm: 0.3830462939681373, ic: -0.008505122119801433
Epoch 3: train loss: 1.6253785233424438
Eval step 0: eval loss: 0.9964234491220707
Eval: total loss: 1.0857849909824815, ic :0.048939126781492336, sharpe5:0.9764454736188054, irr5:11.244081497192383, ndcg5:0.8391311383724981 
train 4, step: 0, loss: 1.2871001779216609, grad_norm: 0.22011287565300555, ic: 0.015657211735278988
train 4, step: 500, loss: 0.8308364122737639, grad_norm: 0.04013434316057508, ic: -0.03199062649336374
train 4, step: 1000, loss: 1.8631783063730518, grad_norm: 0.18370633277065207, ic: -0.04952948669666855
train 4, step: 1500, loss: 1.1734638261284007, grad_norm: 0.011594723915721455, ic: 0.17542703665454346
train 4, step: 2000, loss: 6.63335585056391, grad_norm: 1.0253514558886443, ic: -0.050850728544704286
Epoch 4: train loss: 1.6260692481428247
Eval step 0: eval loss: 0.9927529188882306
Eval: total loss: 1.083540920437653, ic :0.05339258414511358, sharpe5:1.7209608406573533, irr5:18.12190055847168, ndcg5:0.8518371546223446 
train 5, step: 0, loss: 3.743458589594415, grad_norm: 0.6269866043514027, ic: 0.15000885023036076
train 5, step: 500, loss: 3.4333670613950806, grad_norm: 0.7342357563020396, ic: -0.04871630882995526
train 5, step: 1000, loss: 1.0407282429620512, grad_norm: 0.04855177977371601, ic: 0.07555404446253086
train 5, step: 1500, loss: 1.4823039342650541, grad_norm: 0.9386219811204543, ic: -0.054565921068713566
train 5, step: 2000, loss: 1.7787291984247968, grad_norm: 0.4647188713654229, ic: 0.012958659982149618
Epoch 5: train loss: 1.6265493787988057
Eval step 0: eval loss: 0.9968245005595049
Eval: total loss: 1.085580994475101, ic :0.051338036709906276, sharpe5:0.6406224541738629, irr5:6.775381565093994, ndcg5:0.847243204555781 
train 6, step: 0, loss: 1.1019947045313412, grad_norm: 0.024847482997596104, ic: -0.05011797540793077
train 6, step: 500, loss: 1.8489217768270503, grad_norm: 0.1081799254692439, ic: 0.18351099371621996
train 6, step: 1000, loss: 0.8865971599011605, grad_norm: 0.18371360275367918, ic: 0.0654064620087114
train 6, step: 1500, loss: 0.844423814447654, grad_norm: 0.003506549572528647, ic: 0.09011986370119629
train 6, step: 2000, loss: 1.9277138557879379, grad_norm: 0.26180079276664997, ic: 0.04714088456622824
Epoch 6: train loss: 1.6248744698662176
Eval step 0: eval loss: 0.9879143319674828
Eval: total loss: 1.083400764903226, ic :0.049810096333616084, sharpe5:1.480227003470063, irr5:15.189260482788086, ndcg5:0.8545601090263282 
train 7, step: 0, loss: 0.8142683653470292, grad_norm: 0.08030769791781146, ic: -0.004922403187012129
train 7, step: 500, loss: 1.126645991569405, grad_norm: 0.01697358088125072, ic: 0.028392792561194945
train 7, step: 1000, loss: 1.1022149575099283, grad_norm: 0.028870885947257275, ic: -0.07228292723800271
train 7, step: 1500, loss: 1.0722986246982884, grad_norm: 0.0658443839597989, ic: 0.17507304119456293
train 7, step: 2000, loss: 0.7110839061874499, grad_norm: 0.019305527359666525, ic: -0.028362228075386516
Epoch 7: train loss: 1.6258730869392142
Eval step 0: eval loss: 1.001195633392575
Eval: total loss: 1.09502773428181, ic :0.03522672774188449, sharpe5:2.936117466688156, irr5:29.197343826293945, ndcg5:0.8434779023581603 
train 8, step: 0, loss: 2.0261591101554197, grad_norm: 0.3589061735087694, ic: -0.09564606854193214
train 8, step: 500, loss: 0.9578826272898706, grad_norm: 0.053111797567547386, ic: 0.1729913712230192
train 8, step: 1000, loss: 0.7483875385707612, grad_norm: 0.040967851706337086, ic: 0.08922727365355204
train 8, step: 1500, loss: 0.8420278033088234, grad_norm: 0.03826065950519658, ic: 0.16892165101445922
train 8, step: 2000, loss: 1.4629604204963236, grad_norm: 0.7742225360909418, ic: -0.0118650269649929
Epoch 8: train loss: 1.625346118930061
Eval step 0: eval loss: 0.9958953777070167
Eval: total loss: 1.0854047125338961, ic :0.05216206500567759, sharpe5:4.5397968801856035, irr5:48.9305419921875, ndcg5:0.8608499456132461 
train 9, step: 0, loss: 0.7824792488846946, grad_norm: 0.006691251441363373, ic: 0.08880444677165855
train 9, step: 500, loss: 1.742575027993719, grad_norm: 0.04236385298826688, ic: -0.11268052345545539
train 9, step: 1000, loss: 0.7806170728647563, grad_norm: 0.010741428299078546, ic: 0.11929468211017659
train 9, step: 1500, loss: 0.9872251729249012, grad_norm: 0.03614559364147028, ic: 0.011980592073083274
train 9, step: 2000, loss: 4.471871170343137, grad_norm: 1.9770282429117036, ic: 0.24374371891347554
Epoch 9: train loss: 1.6243575709310012
Eval step 0: eval loss: 0.9884704300577606
Eval: total loss: 1.0841680351474299, ic :0.049475544866598026, sharpe5:3.077176746726036, irr5:36.630916595458984, ndcg5:0.8340570477220721 
train 10, step: 0, loss: 0.780773504849138, grad_norm: 0.0013497063963262813, ic: 0.10151281466246023
train 10, step: 500, loss: 0.7853202336951147, grad_norm: 0.07594657375785137, ic: -0.020107209910242726
train 10, step: 1000, loss: 1.0078659415804725, grad_norm: 0.38685129267477164, ic: 0.0014447449745205096
train 10, step: 1500, loss: 1.2982999674479168, grad_norm: 0.13988554332088915, ic: 0.09449272637882317
train 10, step: 2000, loss: 1.2702068766407513, grad_norm: 0.44247425767432397, ic: -0.17225043742480692
Epoch 10: train loss: 1.6190402895017453
Eval step 0: eval loss: 0.9988419069246971
Eval: total loss: 1.080411119679088, ic :0.0892211227116222, sharpe5:16.563792997598647, irr5:267.26422119140625, ndcg5:0.8361336297039297 
train 11, step: 0, loss: 1.528909519045563, grad_norm: 0.05697889924901367, ic: 0.17531046808356007
train 11, step: 500, loss: 1.3041346105653568, grad_norm: 0.31374084481187264, ic: 0.1465569400876146
train 11, step: 1000, loss: 2.231948773477979, grad_norm: 0.7724235205651715, ic: 0.28404422728141476
train 11, step: 1500, loss: 1.3841975924489591, grad_norm: 1.037554569544617, ic: 0.029390258325748566
train 11, step: 2000, loss: 3.705401620820373, grad_norm: 3.284014329398052, ic: 0.2269717058280985
Epoch 11: train loss: 1.6161606366639014
Eval step 0: eval loss: 1.006200837611901
Eval: total loss: 1.0852067385075914, ic :0.09115305246154991, sharpe5:16.24157286286354, irr5:254.8573455810547, ndcg5:0.8326955434599699 
train 12, step: 0, loss: 0.9957240896659145, grad_norm: 0.08036780183851991, ic: 0.07430171297997268
train 12, step: 500, loss: 1.2061938718004654, grad_norm: 0.11803940110333572, ic: 0.10069428690735147
train 12, step: 1000, loss: 1.2093601600796569, grad_norm: 0.7280975138615668, ic: 0.22651703428204223
train 12, step: 1500, loss: 0.7503261256862331, grad_norm: 0.045611710028508834, ic: 0.38322791199068607
train 12, step: 2000, loss: 2.698769336909204, grad_norm: 0.6354300341735184, ic: -0.04885211355298716
Epoch 12: train loss: 1.616022033990207
Eval step 0: eval loss: 1.0006944958695365
Eval: total loss: 1.0809799076703763, ic :0.10082970564496875, sharpe5:16.822206659317015, irr5:273.7202453613281, ndcg5:0.8320101943473721 
train 13, step: 0, loss: 1.324259315764658, grad_norm: 0.5970276404098511, ic: 0.17325458287709525
train 13, step: 500, loss: 1.430488853512652, grad_norm: 0.1373343231964783, ic: -0.09486974152525483
train 13, step: 1000, loss: 2.164250117409626, grad_norm: 0.8693284609015789, ic: 0.17255611644390131
train 13, step: 1500, loss: 1.5782617417640417, grad_norm: 1.1899218077347553, ic: -0.11186010549720717
train 13, step: 2000, loss: 0.7164316414068754, grad_norm: 0.027548723366981626, ic: -0.04613703331424125
Epoch 13: train loss: 1.6152312682834589
Eval step 0: eval loss: 1.0041061650457477
Eval: total loss: 1.0818426244498593, ic :0.09874977990783967, sharpe5:16.069174118041992, irr5:272.72088623046875, ndcg5:0.8332050456066713 
train 14, step: 0, loss: 1.9665293689949133, grad_norm: 0.9601424572839974, ic: 0.16513361101176952
train 14, step: 500, loss: 2.744502717000575, grad_norm: 1.9236733029480442, ic: -0.1429117184825892
train 14, step: 1000, loss: 1.052434458374506, grad_norm: 0.12529776602715934, ic: 0.17883720425526375
train 14, step: 1500, loss: 3.022989497203308, grad_norm: 1.0889749602166596, ic: 0.11622871455141036
train 14, step: 2000, loss: 0.7888072029107959, grad_norm: 0.026026133510330257, ic: -0.04910240554614052
Epoch 14: train loss: 1.615216189137545
Eval step 0: eval loss: 1.0016144907681674
Eval: total loss: 1.0813574528061736, ic :0.10057238535817443, sharpe5:17.372724072933195, irr5:287.8536682128906, ndcg5:0.8331791046427329 
train 15, step: 0, loss: 1.2369364707122947, grad_norm: 0.2669207565896044, ic: 0.3373132978001768
train 15, step: 500, loss: 1.809505907304448, grad_norm: 1.2445109446525815, ic: -0.027840282829184555
train 15, step: 1000, loss: 1.726839043728547, grad_norm: 0.5478137401029682, ic: 0.1874709223209195
train 15, step: 1500, loss: 1.3975646082354132, grad_norm: 0.4625311123538521, ic: -0.09462197489261673
train 15, step: 2000, loss: 3.117460161726051, grad_norm: 0.07969097925581906, ic: 0.1956116202844346
Epoch 15: train loss: 1.6142385546787652
Eval step 0: eval loss: 1.002010335415021
Eval: total loss: 1.081793479902003, ic :0.09734680133264563, sharpe5:16.921033194065092, irr5:292.94189453125, ndcg5:0.8296560417179057 
train 16, step: 0, loss: 1.8084180216802168, grad_norm: 0.5685797069838854, ic: 0.18100493916578672
train 16, step: 500, loss: 1.3298305724718555, grad_norm: 0.4115747388081156, ic: 0.3474103706707804
train 16, step: 1000, loss: 0.907005316070725, grad_norm: 0.022725684897751834, ic: -0.008867602695815553
train 16, step: 1500, loss: 1.3826791562387266, grad_norm: 1.4663032204634765, ic: 0.22910877291417964
train 16, step: 2000, loss: 1.2441971488620924, grad_norm: 0.12151548354813596, ic: 0.2357196363549117
Epoch 16: train loss: 1.6143195535510289
Eval step 0: eval loss: 1.0073516026115719
Eval: total loss: 1.0816674189159277, ic :0.10365777848949069, sharpe5:16.944138375520705, irr5:290.3554382324219, ndcg5:0.842815491610827 
train 17, step: 0, loss: 1.5388248524767287, grad_norm: 0.43884766961251137, ic: 0.10111702132312309
train 17, step: 500, loss: 2.226435413099315, grad_norm: 0.9483600260643541, ic: 0.06795137311786854
train 17, step: 1000, loss: 0.9560431739013934, grad_norm: 0.01680892719101154, ic: 0.1767924537094072
train 17, step: 1500, loss: 0.8352101755666209, grad_norm: 0.23962737529624167, ic: 0.2734480513808476
train 17, step: 2000, loss: 3.12548137703751, grad_norm: 1.8681279646002507, ic: 0.016280684639150174
Epoch 17: train loss: 1.6143860956980947
Eval step 0: eval loss: 1.0098983017377565
Eval: total loss: 1.0830378483507808, ic :0.1014004761130745, sharpe5:16.158461937904356, irr5:283.37115478515625, ndcg5:0.8570046770619671 
train 18, step: 0, loss: 1.074594754260788, grad_norm: 0.2339724940982196, ic: -0.030382452523925227
train 18, step: 500, loss: 0.7969166236587717, grad_norm: 0.10265869144120632, ic: -0.023807587916103296
train 18, step: 1000, loss: 1.1923264865615546, grad_norm: 0.9406491765114993, ic: 0.03823613415097291
train 18, step: 1500, loss: 0.906962303598975, grad_norm: 0.009806464688031497, ic: 0.08187442921456291
train 18, step: 2000, loss: 1.7243606757326246, grad_norm: 0.7804102832446179, ic: 0.09430576406467829
Epoch 18: train loss: 1.613681564196097
Eval step 0: eval loss: 0.997066327055358
Eval: total loss: 1.080626209962642, ic :0.0974352999952476, sharpe5:18.18712259888649, irr5:294.8181457519531, ndcg5:0.840344816129153 
train 19, step: 0, loss: 1.0786271072889158, grad_norm: 0.8331804410900433, ic: 0.046270045911890115
train 19, step: 500, loss: 2.2523889309022485, grad_norm: 1.1074320878152053, ic: 0.21217486590674098
train 19, step: 1000, loss: 1.2950595238095237, grad_norm: 0.10716053553815535, ic: 0.2038679345933621
train 19, step: 1500, loss: 1.5061268302964868, grad_norm: 0.15185775088095216, ic: 0.1777245979334966
train 19, step: 2000, loss: 1.1484872599594464, grad_norm: 0.372189096109094, ic: 0.19277433068479308
Epoch 19: train loss: 1.6146816217722726
Eval step 0: eval loss: 1.0119013090771458
Eval: total loss: 1.0846837158168767, ic :0.08721323675084723, sharpe5:13.84686827480793, irr5:223.23458862304688, ndcg5:0.8522843878371349 
