Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjTimeDataset', dout=0.3, epochs=20, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=1, mask_adj=True, mask_type='soft', model_type='BiGLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
54590
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8208710336868479, grad_norm: 0.13168332720693687, ic: -0.02304773472706633
train 0, step: 500, loss: 1.152311362424984, grad_norm: 0.014446543774220121, ic: 0.0716439110391145
train 0, step: 1000, loss: 0.7121996737942837, grad_norm: 9.801185549478023e-05, ic: 0.09297214210691299
train 0, step: 1500, loss: 0.8560795547938583, grad_norm: 0.16706018318477717, ic: 0.05719778960414934
train 0, step: 2000, loss: 2.3867760065777497, grad_norm: 0.7208784237891436, ic: 0.10155669306154025
Epoch 0: train loss: 1.6485105513411238
Eval step 0: eval loss: 0.8353027086489067
Eval: total loss: 1.0790276795317333, mse:4.8236522346183115, ic :0.009980005847162394, sharpe5:7.571967900395393, irr5:212.4063262939453, ndcg5:0.8578755517108507 
train 1, step: 0, loss: 2.3076617498293683, grad_norm: 0.043084112998031485, ic: 0.05424695205475892
train 1, step: 500, loss: 0.626982877788374, grad_norm: 0.045699466642559694, ic: -0.08370086330347037
train 1, step: 1000, loss: 1.1951594292363035, grad_norm: 0.09873755512797869, ic: 0.05214530945017625
train 1, step: 1500, loss: 2.2702461049289657, grad_norm: 0.8625510900410174, ic: 0.0723028545592504
train 1, step: 2000, loss: 1.1671662972959589, grad_norm: 0.013002100811136718, ic: -0.009637971435750799
Epoch 1: train loss: 1.6469032313017613
Eval step 0: eval loss: 0.8362027360214699
Eval: total loss: 1.0792758288325597, mse:4.822991019669121, ic :0.01720688494695179, sharpe5:8.406014249324798, irr5:231.74168395996094, ndcg5:0.8498715094686681 
train 2, step: 0, loss: 2.0062583640769676, grad_norm: 0.2388724528391829, ic: 0.07665395865719137
train 2, step: 500, loss: 1.1513923817722205, grad_norm: 0.43004220466024745, ic: 0.4180761694336621
train 2, step: 1000, loss: 1.1169206042355684, grad_norm: 0.043959141907476434, ic: 0.06833105860526803
train 2, step: 1500, loss: 1.278964381506516, grad_norm: 0.27167911339110756, ic: -0.37745800888925085
train 2, step: 2000, loss: 2.632434325050249, grad_norm: 12.778087279024309, ic: -0.02257824669823239
Epoch 2: train loss: 1.6472115701418062
Eval step 0: eval loss: 0.8392452967556309
Eval: total loss: 1.0803937180397218, mse:4.823293593631888, ic :0.0108123643260964, sharpe5:11.30923904120922, irr5:397.50177001953125, ndcg5:0.847355613308548 
train 3, step: 0, loss: 0.8460483101959417, grad_norm: 0.20206454540056962, ic: 0.07403240967399513
train 3, step: 500, loss: 1.252599184204932, grad_norm: 0.07894018240178519, ic: -0.01442206099598983
train 3, step: 1000, loss: 1.3727183031871653, grad_norm: 0.46050417537656513, ic: -0.0008771466912767694
train 3, step: 1500, loss: 1.716733834869175, grad_norm: 0.2735688019065625, ic: 0.09019659995493362
train 3, step: 2000, loss: 1.1168701574083608, grad_norm: 0.3723237217618611, ic: 0.04015771567402972
Epoch 3: train loss: 1.6467428300439853
Eval step 0: eval loss: 0.8399118598318953
Eval: total loss: 1.0806579803644603, mse:4.823694262282189, ic :0.002666478124551055, sharpe5:-2.9422998006641863, irr5:-44.20954895019531, ndcg5:0.8492648657321088 
train 4, step: 0, loss: 1.1290239149810826, grad_norm: 0.42428638720978634, ic: -0.021007158310682488
train 4, step: 500, loss: 1.0532173101049271, grad_norm: 0.02561130567808806, ic: 0.19430381160799162
train 4, step: 1000, loss: 1.1711748401511013, grad_norm: 0.002430817686240759, ic: 0.4018327721045267
train 4, step: 1500, loss: 1.3334139163217573, grad_norm: 0.7238171989375455, ic: 0.06779015362627164
train 4, step: 2000, loss: 1.3803067498828576, grad_norm: 0.14644510929688223, ic: 0.3523806474903759
Epoch 4: train loss: 1.6473454037473636
Eval step 0: eval loss: 0.8361813190488342
Eval: total loss: 1.07904767136457, mse:4.8072784991509, ic :0.12892076070715805, sharpe5:11.700344324111938, irr5:404.2629089355469, ndcg5:0.8440753656988624 
train 5, step: 0, loss: 2.2792583264802633, grad_norm: 0.003699359099861951, ic: 0.0034980035543556547
train 5, step: 500, loss: 1.7961080044130735, grad_norm: 0.6704723672875881, ic: 0.014141513259182906
train 5, step: 1000, loss: 4.4864931085999284, grad_norm: 0.7522995319251491, ic: -0.04509487131986173
train 5, step: 1500, loss: 0.9464028866452103, grad_norm: 0.13323939645422925, ic: -0.03536525056403916
train 5, step: 2000, loss: 2.2486131558605553, grad_norm: 0.6513921513600234, ic: -0.01458163997503371
Epoch 5: train loss: 1.6467941852058305
Eval step 0: eval loss: 0.8379994463744731
Eval: total loss: 1.079912789236344, mse:4.822556676806948, ic :0.02205534489842028, sharpe5:12.00196408867836, irr5:392.5057067871094, ndcg5:0.8429691742479933 
train 6, step: 0, loss: 2.0417768998579544, grad_norm: 0.02821127392404989, ic: 0.177668533729598
train 6, step: 500, loss: 1.0158824028238997, grad_norm: 0.0026353664772981213, ic: -0.014366956550153631
train 6, step: 1000, loss: 1.5687125564794664, grad_norm: 0.5251334838693531, ic: -0.0007388303156630697
train 6, step: 1500, loss: 1.1148943219866072, grad_norm: 0.010078759685129123, ic: 0.4714637634108264
train 6, step: 2000, loss: 1.0852928750998934, grad_norm: 0.15320161327041285, ic: 0.113152225705652
Epoch 6: train loss: 1.6451284901572887
Eval step 0: eval loss: 0.8339987816122233
Eval: total loss: 1.077652060032456, mse:4.759842723963121, ic :0.12305419716938223, sharpe5:11.735000204443931, irr5:400.9820251464844, ndcg5:0.8498901397641997 
train 7, step: 0, loss: 1.242860996670726, grad_norm: 0.26236640179581017, ic: 0.47025274241566156
train 7, step: 500, loss: 1.411445134180317, grad_norm: 0.049318677454474026, ic: 0.11231211948767537
train 7, step: 1000, loss: 1.0678456065418958, grad_norm: 0.09609131439396205, ic: -0.10056198207861211
train 7, step: 1500, loss: 1.9259251461753362, grad_norm: 0.6863222698127256, ic: -0.04457488678051565
train 7, step: 2000, loss: 2.9951156988376524, grad_norm: 0.6978563341571129, ic: -0.041946337947086056
Epoch 7: train loss: 1.6420403950252909
Eval step 0: eval loss: 0.8443885216346153
Eval: total loss: 1.084219679885552, mse:4.816513698292678, ic :0.07342958308595209, sharpe5:11.597687736153603, irr5:402.01971435546875, ndcg5:0.845997423641846 
train 8, step: 0, loss: 1.266310162825172, grad_norm: 0.0002394102902722824, ic: -0.043245845164409435
train 8, step: 500, loss: 0.6335560455564085, grad_norm: 0.07928616558924947, ic: 0.009030229540078306
train 8, step: 1000, loss: 1.9267478514121439, grad_norm: 1.316289075655212, ic: 0.30808785925531856
train 8, step: 1500, loss: 1.4300526874301287, grad_norm: 0.20574778019910597, ic: 0.11193998269157959
train 8, step: 2000, loss: 4.0692818186517785, grad_norm: 1.048868996742525, ic: -0.028036825601189287
Epoch 8: train loss: 1.6422950302188892
Eval step 0: eval loss: 0.8312548365055321
Eval: total loss: 1.0739540915521812, mse:4.639621171880055, ic :0.1374303220813273, sharpe5:11.498178243041037, irr5:399.4407958984375, ndcg5:0.8407069243515135 
train 9, step: 0, loss: 0.787514353229239, grad_norm: 0.021650289947617477, ic: 0.2357834443434747
train 9, step: 500, loss: 1.0756619936460026, grad_norm: 0.19272719035665728, ic: -0.03060480894840064
train 9, step: 1000, loss: 0.8008770425811068, grad_norm: 0.03188550876857057, ic: -0.03476851229426038
train 9, step: 1500, loss: 1.0542127379377035, grad_norm: 0.10082964266168047, ic: -0.04790765740364701
train 9, step: 2000, loss: 6.690571647863418, grad_norm: 0.25592716058137643, ic: 0.09956059170357731
Epoch 9: train loss: 1.6372493284652192
Eval step 0: eval loss: 0.8340665055527199
Eval: total loss: 1.07427173539578, mse:4.630322953274521, ic :0.1379626900567338, sharpe5:11.544983593225478, irr5:401.3159484863281, ndcg5:0.8477520393232136 
train 10, step: 0, loss: 0.918930108012678, grad_norm: 0.04242972722235364, ic: 0.5175266845330257
train 10, step: 500, loss: 3.9711663571396314, grad_norm: 1.1619106601964597, ic: 0.09879494910495965
train 10, step: 1000, loss: 1.3211611882309242, grad_norm: 0.4895006626602858, ic: 0.32298038465898593
train 10, step: 1500, loss: 1.418311775958238, grad_norm: 0.005570607306090199, ic: -0.07771930096297383
train 10, step: 2000, loss: 1.3089353912888342, grad_norm: 0.6206129525193961, ic: -0.037901162798694266
Epoch 10: train loss: 1.6369619769996362
Eval step 0: eval loss: 0.8281931741471285
Eval: total loss: 1.0723387634436325, mse:4.626397176294464, ic :0.14303464260209106, sharpe5:11.802252684831618, irr5:406.9198303222656, ndcg5:0.868652455475145 
train 11, step: 0, loss: 2.283282520325203, grad_norm: 0.11214496107969087, ic: 0.005446731162620091
train 11, step: 500, loss: 1.8708918321037797, grad_norm: 0.2877570299110286, ic: 0.13431996553725015
train 11, step: 1000, loss: 3.0068501610319873, grad_norm: 1.1080181671477616, ic: 0.09969122813100649
train 11, step: 1500, loss: 0.9835613290692989, grad_norm: 0.22063010376572545, ic: -0.021144985219774252
train 11, step: 2000, loss: 1.205755101854556, grad_norm: 0.9626966857175828, ic: 0.6684772900433814
Epoch 11: train loss: 1.6382188136674916
Eval step 0: eval loss: 0.8289505503325869
Eval: total loss: 1.0729806156156767, mse:4.636335348550294, ic :0.13766003119035278, sharpe5:11.872846174836159, irr5:400.499267578125, ndcg5:0.8636002966040468 
train 12, step: 0, loss: 1.38708309201249, grad_norm: 1.4175229061521524, ic: -0.17937371417112435
train 12, step: 500, loss: 1.9545072115384616, grad_norm: 0.33407749627634825, ic: 0.01774776025335525
train 12, step: 1000, loss: 1.365606732002808, grad_norm: 0.44073308079383006, ic: 0.49533820190096156
train 12, step: 1500, loss: 1.1217244776283823, grad_norm: 0.28142695082243574, ic: 0.0869351861708466
train 12, step: 2000, loss: 2.817150972089695, grad_norm: 0.4287278596754994, ic: 0.0005199287323859186
Epoch 12: train loss: 1.6354081772836446
Eval step 0: eval loss: 0.8295764661815068
Eval: total loss: 1.0729410977108218, mse:4.6363626907641855, ic :0.13621357938777628, sharpe5:11.215548883080482, irr5:370.0815124511719, ndcg5:0.8434035405594011 
train 13, step: 0, loss: 1.9154036355564326, grad_norm: 0.0976816170414408, ic: 0.1039846959317913
train 13, step: 500, loss: 2.844774618638834, grad_norm: 1.2821162176467529, ic: 0.27010747411631275
train 13, step: 1000, loss: 1.0179755274053628, grad_norm: 0.010100718378067565, ic: 0.38595125816115433
train 13, step: 1500, loss: 0.9715616954667331, grad_norm: 0.004250270439443137, ic: 0.07691607422042682
train 13, step: 2000, loss: 0.8076968181338299, grad_norm: 0.09750529666944231, ic: 0.9306331540569157
Epoch 13: train loss: 1.6356337889348729
Eval step 0: eval loss: 0.825484023581566
Eval: total loss: 1.071397726928307, mse:4.632076536150128, ic :0.14893571228050184, sharpe5:12.13447692990303, irr5:401.0220947265625, ndcg5:0.8522969737635185 
train 14, step: 0, loss: 2.2503925030048078, grad_norm: 0.7053936986243895, ic: 0.09057527303996878
train 14, step: 500, loss: 0.7782638723859315, grad_norm: 0.003227710833146453, ic: -0.03770093212931305
train 14, step: 1000, loss: 3.968456978831186, grad_norm: 1.2102016774436217, ic: 0.10492610660721788
train 14, step: 1500, loss: 1.3881722873945253, grad_norm: 0.4327458231927744, ic: 0.09860319153922936
train 14, step: 2000, loss: 1.3276994479827517, grad_norm: 0.08597815848644538, ic: 0.1849724682324518
Epoch 14: train loss: 1.6339795984077832
Eval step 0: eval loss: 0.8312369568707191
Eval: total loss: 1.0714662637774273, mse:4.6200101720417495, ic :0.15572658202687376, sharpe5:11.891829957962036, irr5:392.345458984375, ndcg5:0.8394800921246087 
train 15, step: 0, loss: 0.9263019534084724, grad_norm: 0.1914102345787454, ic: 0.5434885361505843
train 15, step: 500, loss: 2.352720912866527, grad_norm: 0.07627725225968104, ic: 0.41688044847999833
train 15, step: 1000, loss: 1.162372295673077, grad_norm: 0.2964398910124453, ic: 0.5106142771398222
train 15, step: 1500, loss: 0.8616956345578457, grad_norm: 0.0026484590806627093, ic: 0.032743931540968906
train 15, step: 2000, loss: 2.8930321772935472, grad_norm: 0.608183513740857, ic: 0.07190650548657486
Epoch 15: train loss: 1.6334419907598812
Eval step 0: eval loss: 0.8327663087995587
Eval: total loss: 1.0736435968019205, mse:4.662404687841079, ic :0.15320508349819884, sharpe5:12.059656097888945, irr5:391.1877136230469, ndcg5:0.8495276125402706 
train 16, step: 0, loss: 0.9211406552357753, grad_norm: 0.01879773873028219, ic: 0.004420016549288301
train 16, step: 500, loss: 1.4361856322778042, grad_norm: 0.008161219148993929, ic: 0.040458659523120395
train 16, step: 1000, loss: 0.9777981485424072, grad_norm: 0.17457539789372262, ic: 0.055995979646823214
train 16, step: 1500, loss: 2.71470560773413, grad_norm: 0.57655911087894, ic: 0.021648459007433212
train 16, step: 2000, loss: 1.9644022108633306, grad_norm: 1.0043524116919422, ic: -0.08969757024855123
Epoch 16: train loss: 1.6334306888007988
Eval step 0: eval loss: 0.8260087715687565
Eval: total loss: 1.0724408808791086, mse:4.648676647163424, ic :0.1370835950935277, sharpe5:11.23444185733795, irr5:351.29327392578125, ndcg5:0.8555940286309823 
train 17, step: 0, loss: 0.7645784621189627, grad_norm: 0.14274729193797184, ic: -0.015615754836162067
train 17, step: 500, loss: 1.816738405323584, grad_norm: 0.34037000280171803, ic: 0.11953692701533931
train 17, step: 1000, loss: 2.027984328497024, grad_norm: 0.9192257309106868, ic: 0.009556470690822022
train 17, step: 1500, loss: 1.1203740553950248, grad_norm: 0.1776263757034311, ic: 0.06474363303448374
train 17, step: 2000, loss: 1.5439146849089762, grad_norm: 0.5372968854971922, ic: 0.13027593298058004
Epoch 17: train loss: 1.633922801000712
Eval step 0: eval loss: 0.8449315994632507
Eval: total loss: 1.078656855728209, mse:4.638518482011763, ic :0.14931622619784185, sharpe5:13.00809702694416, irr5:418.559326171875, ndcg5:0.8509810706616349 
train 18, step: 0, loss: 1.0256261664911945, grad_norm: 0.08635654369335725, ic: 0.0017119086780745824
train 18, step: 500, loss: 1.63874106235482, grad_norm: 4.831086458292609, ic: 0.22835293578242977
train 18, step: 1000, loss: 1.767155843968262, grad_norm: 0.3315828871934082, ic: 0.009264396098243987
train 18, step: 1500, loss: 0.7910180260683025, grad_norm: 0.06670544862641911, ic: 0.14416640001687317
train 18, step: 2000, loss: 3.259651997304719, grad_norm: 1.1162322868321795, ic: 0.020096493286375996
Epoch 18: train loss: 1.6340170511152403
Eval step 0: eval loss: 0.8345488054860378
Eval: total loss: 1.0734101349841965, mse:4.622024302711737, ic :0.15026167719323993, sharpe5:12.422507525682448, irr5:394.5571594238281, ndcg5:0.8434932768538916 
train 19, step: 0, loss: 2.375885319952111, grad_norm: 0.10592884542924697, ic: 0.08274226767613332
train 19, step: 500, loss: 9.175953088177534, grad_norm: 1.7317097513290212, ic: -0.004625473047771357
train 19, step: 1000, loss: 1.2766119759583492, grad_norm: 0.23937670300902483, ic: 0.037993770724855255
train 19, step: 1500, loss: 0.7486226461297986, grad_norm: 0.034710697721252945, ic: 0.10735980093864941
train 19, step: 2000, loss: 1.3103647914288432, grad_norm: 0.2398084977221464, ic: 0.04267808622798695
Epoch 19: train loss: 1.6337020445108834
Eval step 0: eval loss: 0.8363901506108403
Eval: total loss: 1.0740182921882897, mse:4.620857756141451, ic :0.15401085677156431, sharpe5:12.029709035754204, irr5:402.2958679199219, ndcg5:0.8593864337889388 
