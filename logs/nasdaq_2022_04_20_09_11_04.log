Namespace(adj_path='./data/graphs/NASDAQ_1026_1026.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='AdjSeqTimeDataset', dout=0.3, epochs=60, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=5, input_graph=True, label_cnt=1, lr=0.001, lstm_layers=1, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.8/', stock_num=1931, test_mask_path='./data/NASDAQ/test_mask_237_1026.npy', test_path='./data/NASDAQ/test_237_1026_6.npy', top_stocks=5, train_mask_path='./data/NASDAQ/train_mask_756_1026.npy', train_path='./data/NASDAQ/train_756_1026_6.npy', use_adj=True)
44855
BiGLSTM(
  (input_to_hidden): Linear(in_features=5, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.0025261410046368837, grad_norm: 0.0003002447013015416, ic: 0.10355073434733245
train 0, step: 500, loss: 0.0001739077124511823, grad_norm: 8.08999611215319e-05, ic: -0.03659676540311614
Epoch 0: 2022-04-20 21:12:04.018851: train loss: 0.00043414833599381966
Eval step 0: eval loss: 0.00016886257799342275
Eval: 2022-04-20 21:12:14.033371: total loss: 0.00019143321908853501, mse:0.0003828664384006938, ic :0.02283357309833986, sharpe5:-0.8693041336163878, irr5:-1.1887435913085938, ndcg5:0.856532361186046, pnl5:0.9495620131492615 
train 1, step: 0, loss: 0.0005081444978713989, grad_norm: 0.0002696890166483027, ic: 0.06341856067702803
train 1, step: 500, loss: 0.0004294979153200984, grad_norm: 4.866334437302315e-05, ic: -0.005770500751350562
Epoch 1: 2022-04-20 21:13:08.294360: train loss: 0.00028140653932025067
Eval step 0: eval loss: 0.00015057754353620112
Eval: 2022-04-20 21:13:18.359070: total loss: 0.0001970846099657533, mse:0.00039416922018900406, ic :0.027052632322119984, sharpe5:0.31183673774823545, irr5:-0.9135718941688538, ndcg5:0.8449474135110596, pnl5:1.18625009059906 
