Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjSeqTimeDataset', dout=0.3, epochs=20, gnn_layers=1, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=1, mask_adj=True, mask_type='soft', model_type='BiGLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
85948
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8182358299226044, grad_norm: 0.09397914205085178, ic: -0.002059155135218477
train 0, step: 500, loss: 1.1523219269435012, grad_norm: 0.013911955098296151, ic: 0.04780246395669311
train 0, step: 1000, loss: 0.7122287561397741, grad_norm: 7.291863903571033e-05, ic: 0.10779606364868155
train 0, step: 1500, loss: 0.8566559895430461, grad_norm: 0.16826841320467897, ic: 0.13695935642668594
train 0, step: 2000, loss: 2.387917048377185, grad_norm: 0.7241118631832597, ic: 0.1352399627335259
Epoch 0: train loss: 1.648480720351997
Eval step 0: eval loss: 0.8351689329639752
Eval: total loss: 1.0789949924142381, mse:4.823704685786453, ic :0.009830462298614707, sharpe5:7.290479799807072, irr5:205.42933654785156, ndcg5:0.8354728732529917 
train 1, step: 0, loss: 2.308128511663904, grad_norm: 0.043316665035539714, ic: 0.02597643659436834
train 1, step: 500, loss: 0.616466229252981, grad_norm: 0.01890413208166691, ic: 0.0008731275351159355
train 1, step: 1000, loss: 1.1954355360586433, grad_norm: 0.09936827179232258, ic: 0.10041196687688711
train 1, step: 1500, loss: 2.269653220500307, grad_norm: 0.8638791263088442, ic: 0.04236143466048446
train 1, step: 2000, loss: 1.1677621397087956, grad_norm: 0.013452884803863576, ic: 0.013151066507981247
Epoch 1: train loss: 1.6468527383791227
Eval step 0: eval loss: 0.8361408647671891
Eval: total loss: 1.0792566732673652, mse:4.8229690650800405, ic :0.01879240025916457, sharpe5:9.51495606958866, irr5:275.6343688964844, ndcg5:0.8440925474335579 
train 2, step: 0, loss: 2.0065317036193093, grad_norm: 0.23698265157385073, ic: 0.08180827434540872
train 2, step: 500, loss: 1.1674065113702563, grad_norm: 0.4765506781214404, ic: 0.02250750430348158
train 2, step: 1000, loss: 1.0943715823248648, grad_norm: 0.014552131766404429, ic: 0.12266071100112635
train 2, step: 1500, loss: 1.282717330502921, grad_norm: 0.2774591226526223, ic: -0.30921360026164335
train 2, step: 2000, loss: 2.4139198665414434, grad_norm: 0.9940372984393395, ic: 0.12493183836176976
Epoch 2: train loss: 1.6470065776005025
Eval step 0: eval loss: 0.8383122756684668
Eval: total loss: 1.0799964461554223, mse:4.822016325062366, ic :0.03203687681747991, sharpe5:11.66135137438774, irr5:390.53118896484375, ndcg5:0.843184952265735 
train 3, step: 0, loss: 0.8502315251800651, grad_norm: 0.14931260940095364, ic: 0.32310795777086065
train 3, step: 500, loss: 1.251858273691421, grad_norm: 0.06859827787589935, ic: 0.04146861835259899
train 3, step: 1000, loss: 1.4037693668706963, grad_norm: 0.3930166665625263, ic: 0.021343176026592272
train 3, step: 1500, loss: 1.7165960045239685, grad_norm: 0.2757138634673366, ic: 0.461796889690542
train 3, step: 2000, loss: 1.1097113469483628, grad_norm: 0.283610560056573, ic: -0.0751061124859333
Epoch 3: train loss: 1.6433876145274595
Eval step 0: eval loss: 0.8392813132861894
Eval: total loss: 1.0793703077337078, mse:4.7368970340065, ic :0.1246855655275293, sharpe5:11.373380371928214, irr5:385.106689453125, ndcg5:0.8564963932797997 
train 4, step: 0, loss: 1.11971374440665, grad_norm: 0.37166013312173374, ic: -0.00648055951457433
train 4, step: 500, loss: 1.0239492448867094, grad_norm: 0.07001905879726236, ic: 0.5116190045848934
train 4, step: 1000, loss: 1.1321223553347763, grad_norm: 0.015627610082034067, ic: 0.3613567241200548
train 4, step: 1500, loss: 1.3093581244489616, grad_norm: 0.32601994425488345, ic: 0.04417070567761147
train 4, step: 2000, loss: 1.3739558553200693, grad_norm: 0.15074871315829602, ic: 0.38775945360730224
Epoch 4: train loss: 1.6401678943386435
Eval step 0: eval loss: 0.8318356030607876
Eval: total loss: 1.0749129932387758, mse:4.67003305013306, ic :0.1371470028919652, sharpe5:11.31844762802124, irr5:388.1796875, ndcg5:0.8564384961823178 
train 5, step: 0, loss: 2.2779612821691178, grad_norm: 0.003775742671872338, ic: 0.03265903718487338
train 5, step: 500, loss: 1.7833302171924446, grad_norm: 0.6852706348222044, ic: 0.012887897287324637
train 5, step: 1000, loss: 4.469707957966391, grad_norm: 0.7788431305056056, ic: 0.10086549072401044
train 5, step: 1500, loss: 0.9338795270106589, grad_norm: 0.027863450793349336, ic: 0.10655915714849568
train 5, step: 2000, loss: 2.260584159920223, grad_norm: 0.662887595027132, ic: 0.004960381201048813
Epoch 5: train loss: 1.6358288809667194
Eval step 0: eval loss: 0.8345046852361038
Eval: total loss: 1.073031955929144, mse:4.622524754112032, ic :0.1427313285354369, sharpe5:11.252721871733664, irr5:382.7681579589844, ndcg5:0.8540617606119454 
train 6, step: 0, loss: 1.9733080314867424, grad_norm: 0.13383968510916994, ic: 0.26304868756445776
train 6, step: 500, loss: 1.0154914099672878, grad_norm: 0.00029478237201208915, ic: -0.007553718277586137
train 6, step: 1000, loss: 1.5677781318011923, grad_norm: 0.5288719211027951, ic: 0.06236854199571311
train 6, step: 1500, loss: 1.0810924412048968, grad_norm: 0.06022132388932585, ic: 0.4921340650853457
train 6, step: 2000, loss: 1.0991416447456046, grad_norm: 0.15936145850593333, ic: -0.007345242804696138
Epoch 6: train loss: 1.6307313958080596
Eval step 0: eval loss: 0.8254441481370192
Eval: total loss: 1.0699358997416712, mse:4.604899792049962, ic :0.1679405079610515, sharpe5:15.619472520947456, irr5:507.14630126953125, ndcg5:0.8602440641213996 
train 7, step: 0, loss: 1.1855820152910905, grad_norm: 0.2288669153150575, ic: 0.584773773371652
train 7, step: 500, loss: 1.4130363751174813, grad_norm: 0.591339765244162, ic: 0.13436255361446728
train 7, step: 1000, loss: 1.066400884271978, grad_norm: 0.10207068106401922, ic: 0.07087730910555
train 7, step: 1500, loss: 1.9466162572933148, grad_norm: 0.728703680667, ic: -0.10157124541448884
train 7, step: 2000, loss: 2.994045350609756, grad_norm: 0.7231366847672993, ic: 0.01857714672598221
Epoch 7: train loss: 1.6279645872038118
Eval step 0: eval loss: 0.8325295644263698
Eval: total loss: 1.0723588109726603, mse:4.6024796394782, ic :0.17370161663396633, sharpe5:17.12793552517891, irr5:559.714599609375, ndcg5:0.8492424729985255 
train 8, step: 0, loss: 1.2643868315739766, grad_norm: 0.0007395829753630208, ic: 0.053020298210576564
train 8, step: 500, loss: 0.6310020483050218, grad_norm: 0.08688414397217573, ic: 0.015953852654601725
train 8, step: 1000, loss: 1.8977443586412623, grad_norm: 0.88009537118162, ic: 0.45500011002953744
train 8, step: 1500, loss: 1.4155939399629682, grad_norm: 0.21121439141951479, ic: 0.17435552587990538
train 8, step: 2000, loss: 4.064684139784946, grad_norm: 1.090337175502065, ic: -0.056127596805042826
Epoch 8: train loss: 1.627284273984495
Eval step 0: eval loss: 0.8235815146700474
Eval: total loss: 1.0676248970081748, mse:4.5901830922931, ic :0.1841698287977938, sharpe5:18.15704972863197, irr5:593.0145874023438, ndcg5:0.8519183094836884 
train 9, step: 0, loss: 0.755759614966991, grad_norm: 0.01751682011442139, ic: 0.36095778255594413
train 9, step: 500, loss: 1.0681911319881292, grad_norm: 0.2104803712956739, ic: -0.058614143395827344
train 9, step: 1000, loss: 0.794931379711355, grad_norm: 0.034782684420035896, ic: 0.13386418791239252
train 9, step: 1500, loss: 1.0338421054305782, grad_norm: 0.13479902746382125, ic: 0.125409851614881
train 9, step: 2000, loss: 6.6820329660043924, grad_norm: 0.3102876870098054, ic: 0.13088417663059965
Epoch 9: train loss: 1.6259496459151745
Eval step 0: eval loss: 0.8243254489717794
Eval: total loss: 1.0671697565072569, mse:4.593144768864408, ic :0.18453402560906196, sharpe5:17.391447726488114, irr5:581.5806884765625, ndcg5:0.845342434337896 
train 10, step: 0, loss: 0.8988059554094381, grad_norm: 0.0381254125349155, ic: 0.5516923950180506
train 10, step: 500, loss: 3.98675537109375, grad_norm: 2.101732873730249, ic: 0.1321697153199476
train 10, step: 1000, loss: 1.2883078920329276, grad_norm: 0.5692824855542706, ic: 0.3830528144180918
train 10, step: 1500, loss: 1.3945777130708905, grad_norm: 0.02074361056121665, ic: 0.1575391007285149
train 10, step: 2000, loss: 1.2892200309927642, grad_norm: 0.7298379197033665, ic: 0.1341598084708769
Epoch 10: train loss: 1.6244954444751831
Eval step 0: eval loss: 0.8191431847462789
Eval: total loss: 1.0660720788174267, mse:4.604055274179291, ic :0.1847687541406388, sharpe5:17.90616374373436, irr5:586.7278442382812, ndcg5:0.8441710218644128 
train 11, step: 0, loss: 2.3087758299457994, grad_norm: 0.14507981435089518, ic: -0.22862289465696603
train 11, step: 500, loss: 1.8729714957725463, grad_norm: 0.2985919203135027, ic: 0.12943972520080896
train 11, step: 1000, loss: 3.001979051438053, grad_norm: 1.1329499428036176, ic: 0.1327725752586657
train 11, step: 1500, loss: 1.0230283037872683, grad_norm: 0.15458362666716732, ic: 0.058459538921683096
train 11, step: 2000, loss: 1.1859769732038552, grad_norm: 0.7931129089883409, ic: 0.6907005732731326
Epoch 11: train loss: 1.624467653107179
Eval step 0: eval loss: 0.8221525588201396
Eval: total loss: 1.066430554809822, mse:4.590674702372612, ic :0.1865425861190195, sharpe5:17.781905050277707, irr5:582.9969482421875, ndcg5:0.8504342107985395 
train 12, step: 0, loss: 1.3820118131798953, grad_norm: 0.8722713922967125, ic: -0.1589172160981938
train 12, step: 500, loss: 1.971411884014423, grad_norm: 0.9200316767549847, ic: 0.08085254906016273
train 12, step: 1000, loss: 1.3361691593846825, grad_norm: 0.3646111452321589, ic: 0.5354043687933013
train 12, step: 1500, loss: 1.1150947198635195, grad_norm: 0.2838227000363449, ic: 0.21223189281492036
train 12, step: 2000, loss: 2.8079455659589696, grad_norm: 0.4142739758853351, ic: 0.21604393081708081
Epoch 12: train loss: 1.6237816334761983
Eval step 0: eval loss: 0.821489147190299
Eval: total loss: 1.0659421557990674, mse:4.601434671877487, ic :0.18736628104868822, sharpe5:17.3868694460392, irr5:587.5112915039062, ndcg5:0.8459800641779267 
train 13, step: 0, loss: 1.881897984288674, grad_norm: 0.10961758717997883, ic: 0.22410642396809807
train 13, step: 500, loss: 2.8303888494318183, grad_norm: 1.4978113992232878, ic: 0.2743710304385348
train 13, step: 1000, loss: 1.0112621415676755, grad_norm: 0.062441993148913855, ic: 0.3413813873165871
train 13, step: 1500, loss: 0.9548145198604784, grad_norm: 0.0038240677175403366, ic: 0.1989965809186995
train 13, step: 2000, loss: 0.8156810263743858, grad_norm: 0.10459540910192332, ic: 0.9211929805128048
Epoch 13: train loss: 1.6230013230213147
Eval step 0: eval loss: 0.8171782900586143
Eval: total loss: 1.0683402684362886, mse:4.644193066641794, ic :0.18184034985378725, sharpe5:17.856029680967332, irr5:581.333984375, ndcg5:0.849550902021009 
train 14, step: 0, loss: 2.2257444411057694, grad_norm: 0.8197450835769162, ic: 0.14479522026763025
train 14, step: 500, loss: 0.7724158225403992, grad_norm: 0.0026322540365544657, ic: 0.03124668018639782
train 14, step: 1000, loss: 3.949567859299517, grad_norm: 1.0461172331070794, ic: 0.1894188022944737
train 14, step: 1500, loss: 1.3554240994775315, grad_norm: 0.5034744173935723, ic: 0.27838533438489643
train 14, step: 2000, loss: 1.3687401453149781, grad_norm: 0.7981331179051705, ic: -0.03609950744697736
Epoch 14: train loss: 1.622435062750463
Eval step 0: eval loss: 0.8229610656159444
Eval: total loss: 1.0652821571036222, mse:4.585342303547648, ic :0.18984476539457915, sharpe5:17.788320318460464, irr5:585.5092163085938, ndcg5:0.8488200165425908 
train 15, step: 0, loss: 0.9179164847718614, grad_norm: 0.14968496758763516, ic: 0.5523292681616482
train 15, step: 500, loss: 2.292632568545316, grad_norm: 1.2884565750632724, ic: 0.4475331353803947
train 15, step: 1000, loss: 1.107487221901982, grad_norm: 0.9856207921777589, ic: 0.5519528745931414
train 15, step: 1500, loss: 0.8526357169571619, grad_norm: 0.01768981809999812, ic: 0.18176041442087554
train 15, step: 2000, loss: 2.931338787838555, grad_norm: 1.2954457288748817, ic: 0.19448120883331893
Epoch 15: train loss: 1.6228206734616444
Eval step 0: eval loss: 0.8262441010068163
Eval: total loss: 1.0664895858244068, mse:4.579552749004183, ic :0.19153501318718255, sharpe5:17.770701506137847, irr5:591.0036010742188, ndcg5:0.8384682458568732 
train 16, step: 0, loss: 0.9208822587079032, grad_norm: 0.012127537262278654, ic: 0.02216305932321778
train 16, step: 500, loss: 1.4166615301211976, grad_norm: 0.06767784323201387, ic: 0.1486678734842696
train 16, step: 1000, loss: 0.974835291857968, grad_norm: 0.2026818848034438, ic: -0.004082188131016077
train 16, step: 1500, loss: 2.8121513916780954, grad_norm: 5.043534887724652, ic: 0.17979288140999927
train 16, step: 2000, loss: 1.9580348533920096, grad_norm: 0.8294213625056033, ic: -0.10561122806364774
Epoch 16: train loss: 1.6221044199861228
Eval step 0: eval loss: 0.819634681757936
Eval: total loss: 1.0685875382371144, mse:4.646262641988229, ic :0.19100923477508244, sharpe5:18.07652118086815, irr5:604.237060546875, ndcg5:0.8519574129987657 
train 17, step: 0, loss: 0.7401434263348673, grad_norm: 0.3267906031246376, ic: 0.17722058361922116
train 17, step: 500, loss: 1.797761479908351, grad_norm: 0.3423838168110353, ic: 0.09128080308489987
train 17, step: 1000, loss: 2.06348392547123, grad_norm: 1.5092476639190955, ic: 0.03918321944780308
train 17, step: 1500, loss: 1.1172495570538432, grad_norm: 0.1658939877803053, ic: 0.0074651099056673165
train 17, step: 2000, loss: 1.5422752286860129, grad_norm: 0.7965783196922025, ic: 0.15925800587366598
Epoch 17: train loss: 1.622392738248965
Eval step 0: eval loss: 0.8406421593042017
Eval: total loss: 1.0738628929379541, mse:4.607568397540677, ic :0.19027836652486746, sharpe5:17.479899348020552, irr5:585.614013671875, ndcg5:0.8510138837125442 
train 18, step: 0, loss: 1.0301246350976263, grad_norm: 0.3027192408829668, ic: 0.015040925226670733
train 18, step: 500, loss: 1.6152550744954994, grad_norm: 0.31972726739149626, ic: 0.18904779360489427
train 18, step: 1000, loss: 1.732486470958299, grad_norm: 0.5464229933462306, ic: 0.12184563795061404
train 18, step: 1500, loss: 0.7861705572937155, grad_norm: 0.03407579248780707, ic: 0.08431005733961736
train 18, step: 2000, loss: 3.2588850103615745, grad_norm: 0.968321535940118, ic: 0.041740682336429184
Epoch 18: train loss: 1.622315114647002
Eval step 0: eval loss: 0.8294840451914844
Eval: total loss: 1.0674740598390826, mse:4.579295077294247, ic :0.19453973917772688, sharpe5:18.258687554597852, irr5:600.9254150390625, ndcg5:0.8474067310101393 
train 19, step: 0, loss: 2.327104915949961, grad_norm: 0.28640321614990644, ic: 0.1107083991805902
train 19, step: 500, loss: 9.244729017331109, grad_norm: 1.575763385222988, ic: 0.01612851881606493
train 19, step: 1000, loss: 1.2879822841641395, grad_norm: 1.0014664192693263, ic: 0.07405048400512103
train 19, step: 1500, loss: 0.7319495681554936, grad_norm: 0.026451149246089607, ic: 0.1457160997406607
train 19, step: 2000, loss: 1.296068258300038, grad_norm: 0.28250001740570957, ic: 0.01321246934506035
Epoch 19: train loss: 1.6221582405434591
Eval step 0: eval loss: 0.8234664547179597
Eval: total loss: 1.0655219155002713, mse:4.582612737306313, ic :0.19369521981903254, sharpe5:17.594795268774032, irr5:598.5458374023438, ndcg5:0.8338144049100542 
