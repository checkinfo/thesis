Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='BiGLSTM', dataset_type='AdjSeqTimeDataset', seed=10086, num_days=8, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=1, num_heads=1, gnn_layers=2, print_inteval=500, relation_num=1, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=True)
689852
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8224926329495613, grad_norm: 0.12881675831193562, ic: -0.027662175716501865
train 0, step: 500, loss: 1.0990286091811785, grad_norm: 0.015080673962716354, ic: 0.0020368968625715314
train 0, step: 1000, loss: 0.6800792429706838, grad_norm: 6.244395673297585e-05, ic: 0.03028629570282436
train 0, step: 1500, loss: 0.8241586213064666, grad_norm: 0.16989515042236913, ic: 0.0784484229572727
train 0, step: 2000, loss: 2.3494477982954542, grad_norm: 0.7191317774948127, ic: 0.018428382066556226
Epoch 0: train loss: 1.6302736031169487
Eval step 0: eval loss: 0.8350763225505199
Eval: total loss: 1.073602770400782, mse:4.628853556000368, ic :0.0030719810527853673, sharpe5:2.213708560913801, irr5:29.065183639526367, ndcg5:0.838002769349774 
train 1, step: 0, loss: 2.273630746105995, grad_norm: 0.04232632590309402, ic: 0.026402250792348175
train 1, step: 500, loss: 0.5954578077936747, grad_norm: 0.013749929980750034, ic: -0.04157453654876227
train 1, step: 1000, loss: 1.1876858556694665, grad_norm: 0.09692663495170956, ic: 0.07842025737971768
train 1, step: 1500, loss: 2.221692558512029, grad_norm: 0.8577506387761425, ic: 0.056288581615356666
train 1, step: 2000, loss: 1.1321815127418156, grad_norm: 0.012530028232412019, ic: -0.036980887316990965
Epoch 1: train loss: 1.628532889047392
Eval step 0: eval loss: 0.836204396228278
Eval: total loss: 1.0739228595751509, mse:4.628354959085193, ic :0.0042755589337813826, sharpe5:1.3856175237894057, irr5:17.77326011657715, ndcg5:0.8548360116019996 
train 2, step: 0, loss: 1.9853921274038462, grad_norm: 0.24052123061704783, ic: 0.07109531990352375
train 2, step: 500, loss: 1.084444780006866, grad_norm: 0.41682682108527114, ic: 0.14571194608968602
train 2, step: 1000, loss: 1.0938662644397201, grad_norm: 0.06069120836049383, ic: 0.0899721711027953
train 2, step: 1500, loss: 1.2170767112759477, grad_norm: 0.2762062517000585, ic: 0.2575870867343224
train 2, step: 2000, loss: 2.7148810518525592, grad_norm: 4.635460939072903, ic: 0.10970556321559943
Epoch 2: train loss: 1.6285160884736474
Eval step 0: eval loss: 0.8376610119635334
Eval: total loss: 1.0744261297889448, mse:4.628233840469882, ic :0.009809935413081472, sharpe5:10.318900224566459, irr5:212.56849670410156, ndcg5:0.8520401305584994 
train 3, step: 0, loss: 0.8181649387472927, grad_norm: 0.14925936046560737, ic: 0.14581872540025134
train 3, step: 500, loss: 1.2290445036660023, grad_norm: 0.06945355695408442, ic: 0.004582539709087159
train 3, step: 1000, loss: 1.3800141220792719, grad_norm: 0.3590898387332726, ic: -0.002427915622221202
train 3, step: 1500, loss: 1.6346925882866297, grad_norm: 0.2726856475269131, ic: -0.015050018235502787
train 3, step: 2000, loss: 1.1125282985753298, grad_norm: 0.27244942735602434, ic: -0.07137793180250328
Epoch 3: train loss: 1.6269232523658894
Eval step 0: eval loss: 0.841155796944938
Eval: total loss: 1.0759237276140794, mse:4.622225311790173, ic :0.07442538303832792, sharpe5:9.682600591778755, irr5:192.64637756347656, ndcg5:0.8382996667060796 
train 4, step: 0, loss: 1.093665823383809, grad_norm: 0.36051942521580216, ic: -0.008281358935287114
train 4, step: 500, loss: 0.9849553732956278, grad_norm: 0.03522113225844623, ic: 0.22708556966337753
train 4, step: 1000, loss: 1.0933011492681697, grad_norm: 0.020715077176356227, ic: 0.27572747180922247
train 4, step: 1500, loss: 1.3044241304307895, grad_norm: 0.3120439262248607, ic: 0.043245404257147846
train 4, step: 2000, loss: 1.3367657738524827, grad_norm: 0.14285616415728694, ic: -0.0034139612289721235
Epoch 4: train loss: 1.6244757723819272
Eval step 0: eval loss: 0.8323618490611835
Eval: total loss: 1.0718286027893462, mse:4.60994340142477, ic :0.07430765686991234, sharpe5:10.293881435394287, irr5:207.3849639892578, ndcg5:0.8370335430883985 
train 5, step: 0, loss: 2.261218384449923, grad_norm: 0.00385967308213366, ic: 0.01607126839600767
train 5, step: 500, loss: 1.7977955492236946, grad_norm: 0.6612993942143564, ic: -0.0061876263923995485
train 5, step: 1000, loss: 4.469363247551223, grad_norm: 0.7468766476964992, ic: -0.025608969091599758
train 5, step: 1500, loss: 0.9288075399055755, grad_norm: 0.02503780378024304, ic: 0.11422827719396865
train 5, step: 2000, loss: 2.2428121747255867, grad_norm: 0.6377597551043999, ic: 0.019154442731302714
Epoch 5: train loss: 1.6252052924129397
Eval step 0: eval loss: 0.8335305485617429
Eval: total loss: 1.0725040975053355, mse:4.610118245490202, ic :0.07329137995302741, sharpe5:10.260634038448334, irr5:207.06809997558594, ndcg5:0.829257566858335 
train 6, step: 0, loss: 1.9929105247641508, grad_norm: 0.07063464412119685, ic: 0.2394589551136112
train 6, step: 500, loss: 1.0057516257856145, grad_norm: 0.00014511066534503986, ic: 0.019154394418489346
train 6, step: 1000, loss: 1.5534399580648217, grad_norm: 0.5181450614148305, ic: 0.020389207316554706
train 6, step: 1500, loss: 1.084519107784845, grad_norm: 0.004084750268004684, ic: 0.12274877615670657
train 6, step: 2000, loss: 1.0758741092377198, grad_norm: 0.1408475089832169, ic: 0.16900527205002908
Epoch 6: train loss: 1.6241205717846467
Eval step 0: eval loss: 0.831667096066186
Eval: total loss: 1.0720364570714065, mse:4.606054124816022, ic :0.0738993914147164, sharpe5:10.000740948915482, irr5:202.982177734375, ndcg5:0.8192336018326005 
train 7, step: 0, loss: 1.155506158146334, grad_norm: 0.13435733708091616, ic: 0.1962569801935705
train 7, step: 500, loss: 1.412790136613856, grad_norm: 0.03369532414485918, ic: 0.11248651237717622
train 7, step: 1000, loss: 1.0320673403532608, grad_norm: 0.0976426055623561, ic: -0.04167207934016702
train 7, step: 1500, loss: 1.8794728851935125, grad_norm: 0.6298080557972373, ic: 0.04428687903787577
train 7, step: 2000, loss: 2.9722553473475495, grad_norm: 0.6886365124383333, ic: 0.04209317739586906
Epoch 7: train loss: 1.623276351689286
Eval step 0: eval loss: 0.8377486917456556
Eval: total loss: 1.074618145471188, mse:4.607691362516412, ic :0.0758237871669166, sharpe5:10.170100434422492, irr5:197.30621337890625, ndcg5:0.8389936826287195 
train 8, step: 0, loss: 1.2621201321539752, grad_norm: 0.0010336793412768185, ic: -0.039598046478161876
train 8, step: 500, loss: 0.6375579833984375, grad_norm: 0.07679506236668365, ic: 0.024834386828235838
train 8, step: 1000, loss: 1.911329694075172, grad_norm: 0.5446525110891097, ic: 0.20630384664489684
train 8, step: 1500, loss: 1.4161936783417226, grad_norm: 0.19891114195461063, ic: 0.12700999192664944
train 8, step: 2000, loss: 3.8996047522189348, grad_norm: 1.0120395613143798, ic: -0.0619852223486614
Epoch 8: train loss: 1.6225066115779496
Eval step 0: eval loss: 0.8306461794242035
Eval: total loss: 1.0705937027985084, mse:4.595941480830948, ic :0.07092818698375405, sharpe5:10.254510115385054, irr5:192.1240692138672, ndcg5:0.8221714814107777 
train 9, step: 0, loss: 0.7909511637538982, grad_norm: 0.025562351783972793, ic: 0.23948820361424078
train 9, step: 500, loss: 1.0555857382015306, grad_norm: 0.19294115052575378, ic: -0.008029153525385128
train 9, step: 1000, loss: 0.800802819164128, grad_norm: 0.03138783418653081, ic: -0.0497548297318306
train 9, step: 1500, loss: 1.0471860216809559, grad_norm: 0.09967725394196209, ic: 0.024317827421828107
train 9, step: 2000, loss: 6.681746157987471, grad_norm: 0.4204472742904768, ic: 0.14145630282189325
Epoch 9: train loss: 1.6216460609972312
Eval step 0: eval loss: 0.8323794621552462
Eval: total loss: 1.070438400462124, mse:4.588551083982062, ic :0.07976063360429102, sharpe5:11.633008602261542, irr5:214.36509704589844, ndcg5:0.8350673434858319 
