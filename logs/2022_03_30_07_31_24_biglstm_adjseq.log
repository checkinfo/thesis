Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjSeqTimeDataset', dout=0.3, epochs=20, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=1, mask_adj=True, mask_type='soft', model_type='BiGLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
85970
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8208717040345277, grad_norm: 0.13168714007082594, ic: -0.02305628950909633
train 0, step: 500, loss: 1.1522991785277164, grad_norm: 0.014439842134418226, ic: 0.07164622758676398
train 0, step: 1000, loss: 0.7122004291798809, grad_norm: 9.758839113463836e-05, ic: 0.09291486057938288
train 0, step: 1500, loss: 0.8560681484713413, grad_norm: 0.16728811315704092, ic: 0.06080294640742216
train 0, step: 2000, loss: 2.3866956339903496, grad_norm: 0.7219933558843588, ic: 0.10032658592527247
Epoch 0: train loss: 1.648511888693393
Eval step 0: eval loss: 0.8352967273322246
Eval: total loss: 1.0790261837931983, mse:4.82366037017552, ic :0.009865353220178502, sharpe5:7.60066539466381, irr5:213.1649627685547, ndcg5:0.8418355877989908 
train 1, step: 0, loss: 2.3076769848137677, grad_norm: 0.04311962303194317, ic: 0.053125286788005044
train 1, step: 500, loss: 0.6241592246867421, grad_norm: 0.03789773111473518, ic: -0.09137968966768728
train 1, step: 1000, loss: 1.1952938610994364, grad_norm: 0.09890978984745968, ic: 0.05111020563184029
train 1, step: 1500, loss: 2.269905346100777, grad_norm: 0.8630165213746862, ic: 0.07219504973856107
train 1, step: 2000, loss: 1.1660964893274564, grad_norm: 0.011921426150802472, ic: -0.005896592891504862
Epoch 1: train loss: 1.6468645393443553
Eval step 0: eval loss: 0.8363366403368677
Eval: total loss: 1.0793154397034628, mse:4.822850085365305, ic :0.01966889988862485, sharpe5:9.628413237333298, irr5:264.0504150390625, ndcg5:0.8474248387680969 
train 2, step: 0, loss: 2.0054828031563465, grad_norm: 0.2387679398539105, ic: 0.09259078701907128
train 2, step: 500, loss: 1.147324319527089, grad_norm: 0.42382826431393095, ic: 0.5064256547226436
train 2, step: 1000, loss: 1.131021890737748, grad_norm: 0.058006988526137726, ic: 0.21971036650078057
train 2, step: 1500, loss: 1.2786074593225734, grad_norm: 0.27249246573413377, ic: 0.4806367250958302
train 2, step: 2000, loss: 2.5683507758661945, grad_norm: 2.9266823519502174, ic: 0.13039579665792483
Epoch 2: train loss: 1.646802884769164
Eval step 0: eval loss: 0.8385220719589699
Eval: total loss: 1.0800845531557135, mse:4.821074043965529, ic :0.030790428747900302, sharpe5:11.511449580788613, irr5:398.989990234375, ndcg5:0.8384848802341347 
train 3, step: 0, loss: 0.8504328580003098, grad_norm: 0.14824954559902068, ic: 0.2878003379825582
train 3, step: 500, loss: 1.2499976010369427, grad_norm: 0.06790575340132336, ic: 0.040820873026615107
train 3, step: 1000, loss: 1.4026534555518362, grad_norm: 0.4018607891449022, ic: 0.017306594962236084
train 3, step: 1500, loss: 1.3855522278158423, grad_norm: 1.9700543735763099, ic: -0.015141303957935729
train 3, step: 2000, loss: 1.0939104812345553, grad_norm: 0.29088664446807866, ic: -0.05305752400260614
Epoch 3: train loss: 1.6463647520964846
Eval step 0: eval loss: 0.8434650835171891
Eval: total loss: 1.0819723964931938, mse:4.79470755691799, ic :0.11877258812210174, sharpe5:11.12328801870346, irr5:391.4949645996094, ndcg5:0.8534289034090843 
train 4, step: 0, loss: 1.1063435635550058, grad_norm: 0.38593361603967324, ic: 0.010485802425869049
train 4, step: 500, loss: 1.0337563767630398, grad_norm: 0.034057419677943096, ic: 0.49721256621594223
train 4, step: 1000, loss: 1.1360853306783794, grad_norm: 0.013393089689415815, ic: 0.41176835155068864
train 4, step: 1500, loss: 1.2754035399625294, grad_norm: 0.2788259632456813, ic: 0.059362967540658285
train 4, step: 2000, loss: 1.3771888178525087, grad_norm: 0.14808229046122928, ic: 0.3742125414198649
Epoch 4: train loss: 1.642280756735697
Eval step 0: eval loss: 0.8316796386204227
Eval: total loss: 1.0761409713681596, mse:4.736016648238715, ic :0.12612666900421382, sharpe5:11.290697951316833, irr5:394.2689208984375, ndcg5:0.8343413628687768 
train 5, step: 0, loss: 2.2783703877829917, grad_norm: 0.0034700991242790206, ic: 0.019516119684387913
train 5, step: 500, loss: 1.8002856290793117, grad_norm: 0.6652229622901843, ic: -0.026010249322611263
train 5, step: 1000, loss: 4.457993420313623, grad_norm: 0.763489612608115, ic: 0.04164859723019197
train 5, step: 1500, loss: 0.9316002728535091, grad_norm: 0.02437243379541026, ic: 0.11287135437798941
train 5, step: 2000, loss: 2.2726462557236595, grad_norm: 0.6760462895139142, ic: 0.019239733407943143
Epoch 5: train loss: 1.6401500904861739
Eval step 0: eval loss: 0.8349143089559733
Eval: total loss: 1.0748049471888645, mse:4.635552198706614, ic :0.1385080270838957, sharpe5:11.45059817969799, irr5:399.0111083984375, ndcg5:0.8471709416980724 
train 6, step: 0, loss: 1.9713308623342802, grad_norm: 0.04800047817250484, ic: 0.26521434314737136
train 6, step: 500, loss: 1.0147814467064828, grad_norm: 0.00012967879848739238, ic: 0.006853452357368037
train 6, step: 1000, loss: 1.5691095391157268, grad_norm: 0.526392954436472, ic: 0.039193770770585015
train 6, step: 1500, loss: 1.0895776762702503, grad_norm: 0.025735298236960014, ic: 0.4740539975435986
train 6, step: 2000, loss: 1.1082022925546084, grad_norm: 0.24113967256632263, ic: -0.03888386165087819
Epoch 6: train loss: 1.6376236199677154
Eval step 0: eval loss: 0.831742603233667
Eval: total loss: 1.0760187172062408, mse:4.651921712440052, ic :0.10548657577267155, sharpe5:11.185395609140395, irr5:360.8050537109375, ndcg5:0.8403412941120012 
train 7, step: 0, loss: 1.2069219928504087, grad_norm: 0.18008301897528534, ic: 0.5777191663059222
train 7, step: 500, loss: 1.4217378511848817, grad_norm: 0.037286349878059885, ic: 0.11431279179150332
train 7, step: 1000, loss: 1.0834435862686422, grad_norm: 0.12115406689316331, ic: -0.12808347069744713
train 7, step: 1500, loss: 1.956156477143493, grad_norm: 0.7590958161230676, ic: -0.10227627520288755
train 7, step: 2000, loss: 2.99599977818931, grad_norm: 0.7145513023802869, ic: 0.027834167092498155
Epoch 7: train loss: 1.6374803215735423
Eval step 0: eval loss: 0.8382300808005136
Eval: total loss: 1.0766108862497525, mse:4.637511317808451, ic :0.12712972845220244, sharpe5:11.251865808963775, irr5:368.195068359375, ndcg5:0.8433281190761494 
train 8, step: 0, loss: 1.266475195910243, grad_norm: 0.0004792441668225952, ic: -0.01837807011334562
train 8, step: 500, loss: 0.6298767186571415, grad_norm: 0.07554874340886716, ic: 0.015058752406357623
train 8, step: 1000, loss: 1.918763005797729, grad_norm: 0.5442877474809663, ic: 0.4354166446170675
train 8, step: 1500, loss: 1.432709712566378, grad_norm: 0.21465462886743464, ic: 0.11547224899914417
train 8, step: 2000, loss: 4.069853701406121, grad_norm: 1.0661943577647155, ic: -0.06211741969947371
Epoch 8: train loss: 1.63491260029486
Eval step 0: eval loss: 0.8314517697494073
Eval: total loss: 1.0733557963079987, mse:4.63223122424536, ic :0.12459847486040544, sharpe5:10.574963260889053, irr5:337.4560546875, ndcg5:0.8376251293429631 
train 9, step: 0, loss: 0.781833969444927, grad_norm: 0.01060550386759192, ic: 0.2676689912901638
train 9, step: 500, loss: 1.0658382068980825, grad_norm: 0.19679569478440834, ic: -0.02136726924544899
train 9, step: 1000, loss: 0.8021709878936069, grad_norm: 0.03134397912662274, ic: -0.04680630188436089
train 9, step: 1500, loss: 1.0477467887952974, grad_norm: 0.10211258024671206, ic: 0.009605323780610422
train 9, step: 2000, loss: 6.683743510383386, grad_norm: 0.2503925643629991, ic: 0.08829701894039149
Epoch 9: train loss: 1.6357676532859273
Eval step 0: eval loss: 0.8324712305099117
Eval: total loss: 1.0736774583326112, mse:4.632000385859607, ic :0.14147612481452929, sharpe5:11.179681271910667, irr5:381.7373962402344, ndcg5:0.8500153113467576 
train 10, step: 0, loss: 0.924245524378151, grad_norm: 0.03638382870990105, ic: 0.5117136052806348
train 10, step: 500, loss: 3.9556546073428467, grad_norm: 1.020656246713168, ic: 0.09849231086611876
train 10, step: 1000, loss: 1.3169458307371549, grad_norm: 0.49264002603157653, ic: 0.3309433572221967
train 10, step: 1500, loss: 1.426018127860412, grad_norm: 0.008054536944258981, ic: -0.08499424979420843
train 10, step: 2000, loss: 1.308025601184944, grad_norm: 0.6196180539604761, ic: 0.07960418080721786
Epoch 10: train loss: 1.636274864053125
Eval step 0: eval loss: 0.8280201661699815
Eval: total loss: 1.0713844096805847, mse:4.620511492242275, ic :0.14325252598626434, sharpe5:11.7886323004961, irr5:399.2562255859375, ndcg5:0.8292094411267471 
train 11, step: 0, loss: 2.282536998221545, grad_norm: 0.10440546830365419, ic: -0.09112928993485693
train 11, step: 500, loss: 1.872939375414456, grad_norm: 0.26772468035081415, ic: 0.13534276185201322
train 11, step: 1000, loss: 3.014346142434089, grad_norm: 1.045390861264532, ic: 0.10121295954818182
train 11, step: 1500, loss: 1.0318790408126006, grad_norm: 0.19543429359871384, ic: 0.024871305106068504
train 11, step: 2000, loss: 1.1947019202686917, grad_norm: 0.7278769640539283, ic: 0.6761161645483074
Epoch 11: train loss: 1.6348779924192869
Eval step 0: eval loss: 0.8301244319678609
Eval: total loss: 1.0723098797526112, mse:4.6263638734791535, ic :0.13515112483893313, sharpe5:11.64043506503105, irr5:401.77008056640625, ndcg5:0.8672297685402907 
train 12, step: 0, loss: 1.4026282200594278, grad_norm: 1.001481345519079, ic: -0.17844278935019453
train 12, step: 500, loss: 1.947509765625, grad_norm: 0.3378471506082692, ic: 0.027991094476691014
train 12, step: 1000, loss: 1.390909137326927, grad_norm: 0.17035571966546953, ic: 0.39334521789340093
train 12, step: 1500, loss: 1.1303837473799543, grad_norm: 0.2556098821570166, ic: 0.035648135256988324
train 12, step: 2000, loss: 2.82225742485687, grad_norm: 0.3525443936685435, ic: -0.03882469584094443
Epoch 12: train loss: 1.6366866239888214
Eval step 0: eval loss: 0.830296153640345
Eval: total loss: 1.0739264041169476, mse:4.641707611425904, ic :0.1271930364313824, sharpe5:11.563007261753082, irr5:396.59576416015625, ndcg5:0.8547675024272224 
train 13, step: 0, loss: 1.9161512800907656, grad_norm: 0.10273598873697201, ic: 0.11638878449032264
train 13, step: 500, loss: 2.8881042721714425, grad_norm: 1.0050959083315725, ic: 0.26378750690934516
train 13, step: 1000, loss: 1.0027261727991916, grad_norm: 0.08240876455933907, ic: 0.3864653261415584
train 13, step: 1500, loss: 0.970896472510915, grad_norm: 0.0009384033954549173, ic: 0.07765568022134714
train 13, step: 2000, loss: 0.8098505443476147, grad_norm: 0.06533271107835312, ic: 0.932112010540327
Epoch 13: train loss: 1.6342824300367291
Eval step 0: eval loss: 0.828688144181375
Eval: total loss: 1.071497039160672, mse:4.620610762121628, ic :0.14061169258803433, sharpe5:11.59774922132492, irr5:391.29388427734375, ndcg5:0.8549629092282788 
train 14, step: 0, loss: 2.2943939678485576, grad_norm: 0.5584096936259589, ic: 0.07016707307491737
train 14, step: 500, loss: 0.782656175736692, grad_norm: 0.0030799068769968903, ic: -0.040625083623280274
train 14, step: 1000, loss: 3.943476268954643, grad_norm: 0.9341378655590092, ic: 0.16622024517383252
train 14, step: 1500, loss: 1.3905975964604593, grad_norm: 0.5492162145773176, ic: 0.1087917630583843
train 14, step: 2000, loss: 1.3308075459394826, grad_norm: 0.08942942368870477, ic: 0.0013035421781540692
Epoch 14: train loss: 1.6340741487771815
Eval step 0: eval loss: 0.8333633471087987
Eval: total loss: 1.0726797095935692, mse:4.622556076963514, ic :0.14014690475254374, sharpe5:11.657750235199927, irr5:404.8592529296875, ndcg5:0.8403668577342281 
train 15, step: 0, loss: 0.9289983962547623, grad_norm: 0.16358768580690516, ic: 0.5408702669902868
train 15, step: 500, loss: 2.3553835889899086, grad_norm: 0.18835628398928397, ic: 0.4150929036079135
train 15, step: 1000, loss: 1.1557438968676415, grad_norm: 0.346272041204239, ic: 0.5111237843161499
train 15, step: 1500, loss: 0.869110849490644, grad_norm: 0.00997992306965435, ic: -0.04665630754690235
train 15, step: 2000, loss: 2.946869398898035, grad_norm: 0.7228515572224213, ic: -0.004296565925337963
Epoch 15: train loss: 1.6340558095705209
Eval step 0: eval loss: 0.8341650364898906
Eval: total loss: 1.0726312009323096, mse:4.618292830491205, ic :0.14486202404124007, sharpe5:11.632665231227874, irr5:396.4867248535156, ndcg5:0.8526807822341026 
train 16, step: 0, loss: 0.9221273626340982, grad_norm: 0.01063064255014292, ic: 0.011665780996889717
train 16, step: 500, loss: 1.4359124361335551, grad_norm: 0.006674627888278544, ic: 0.038472288892858586
train 16, step: 1000, loss: 0.98011256695547, grad_norm: 0.17627414299854977, ic: 0.023649482048556505
train 16, step: 1500, loss: 2.745424085276254, grad_norm: 0.5704172165802889, ic: -0.027339263661371883
train 16, step: 2000, loss: 1.9688332473175434, grad_norm: 0.7826530615199621, ic: -0.10233559283051497
Epoch 16: train loss: 1.6338861338371526
Eval step 0: eval loss: 0.8275847520416227
Eval: total loss: 1.0706204241352342, mse:4.621017303941718, ic :0.14804142666341633, sharpe5:11.99213686645031, irr5:410.0267333984375, ndcg5:0.8506491541930826 
train 17, step: 0, loss: 0.7738494752233026, grad_norm: 0.10859886866518431, ic: 0.04608039456636696
train 17, step: 500, loss: 1.818425806064717, grad_norm: 0.2860922899418826, ic: 0.07700512890416614
train 17, step: 1000, loss: 2.0393017965649802, grad_norm: 0.8161611727071518, ic: -0.016764126400962892
train 17, step: 1500, loss: 1.13161112002949, grad_norm: 0.13683991651571886, ic: 0.0167853061704453
train 17, step: 2000, loss: 1.6564448548463409, grad_norm: 1.4084802603120252, ic: 0.13138530096932652
Epoch 17: train loss: 1.63397159717785
Eval step 0: eval loss: 0.8397018705956928
Eval: total loss: 1.0746167247398097, mse:4.624083725067369, ic :0.1571207499314132, sharpe5:12.1535127389431, irr5:404.25225830078125, ndcg5:0.8460490474881863 
train 18, step: 0, loss: 1.0163695728249427, grad_norm: 0.05369804578026789, ic: 0.04203364411431944
train 18, step: 500, loss: 1.613221561728731, grad_norm: 0.39101126234335204, ic: 0.23840415970992218
train 18, step: 1000, loss: 1.7803280030299917, grad_norm: 0.2741331999722445, ic: -0.03148331417716102
train 18, step: 1500, loss: 0.7937161545412077, grad_norm: 0.029853836360420185, ic: 0.01576359133173431
train 18, step: 2000, loss: 3.2618570102725792, grad_norm: 0.7626526581636629, ic: 0.0291912870839076
Epoch 18: train loss: 1.633106279018159
Eval step 0: eval loss: 0.8344325235445205
Eval: total loss: 1.0724939672662006, mse:4.621968762561451, ic :0.16161736361692433, sharpe5:11.966751058101654, irr5:401.14239501953125, ndcg5:0.8493640942775965 
train 19, step: 0, loss: 2.381889003249609, grad_norm: 0.1744818554482905, ic: 0.05897258656044493
train 19, step: 500, loss: 9.212510893313041, grad_norm: 1.3450035230357409, ic: 0.005606650620422095
train 19, step: 1000, loss: 1.2657895496706786, grad_norm: 0.16023017150061572, ic: -0.004460566549625285
train 19, step: 1500, loss: 0.739471574053281, grad_norm: 0.030257006403708445, ic: 0.18826232800477266
train 19, step: 2000, loss: 1.3135887854356925, grad_norm: 0.20660591114782179, ic: 0.021000735737655728
Epoch 19: train loss: 1.633541229839885
Eval step 0: eval loss: 0.8366121667956401
Eval: total loss: 1.0732319958163494, mse:4.619122548442424, ic :0.15339057443039503, sharpe5:11.949363051652908, irr5:399.4303283691406, ndcg5:0.843210075136404 
