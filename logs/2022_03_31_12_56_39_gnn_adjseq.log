Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjSeqTimeDataset', dout=0.3, epochs=20, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=3, mask_adj=True, mask_type='soft', model_type='GNNModel', num_days=1, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
88146
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
    (1): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.8711773880428226, grad_norm: 0.00031938531930911685, ic: 0.07495166895605399
train 0, step: 500, loss: 0.9899332437170558, grad_norm: 0.14798799177718247, ic: 0.029254367701128138
train 0, step: 1000, loss: 1.0715773577418526, grad_norm: 0.025829468715807418, ic: -0.030988807568727367
train 0, step: 1500, loss: 0.9521592344781353, grad_norm: 0.10178789796892773, ic: 0.20669693590390875
train 0, step: 2000, loss: 1.0750157262394437, grad_norm: 0.3329296094020871, ic: 0.12176357295956633
Epoch 0: train loss: 1.6477469119203252
Eval step 0: eval loss: 1.012512753422854
Eval: total loss: 1.0911634496985192, mse:4.887575098724627, ic :0.017743247460936615, sharpe5:6.795848623216152, irr5:205.46188354492188, ndcg5:0.8380884735107111 
train 1, step: 0, loss: 0.9410529252295827, grad_norm: 0.07914486921932562, ic: 0.16008040147156014
train 1, step: 500, loss: 1.686579378675076, grad_norm: 0.10667073079888367, ic: -0.07605205117628822
train 1, step: 1000, loss: 1.1090346435916225, grad_norm: 0.38953148142450655, ic: 0.07869437674919974
train 1, step: 1500, loss: 0.894404469256722, grad_norm: 0.13337375032640614, ic: -0.0886160455172228
train 1, step: 2000, loss: 4.609245198885848, grad_norm: 4.319107553925754, ic: 0.0702475430898945
Epoch 1: train loss: 1.6454217553495638
Eval step 0: eval loss: 0.9995309388781923
Eval: total loss: 1.089252328814204, mse:4.878126123724239, ic :0.050554555576074856, sharpe5:7.2573865887522695, irr5:210.44937133789062, ndcg5:0.8559859316372614 
train 2, step: 0, loss: 0.9635673493940113, grad_norm: 0.12037936065903883, ic: 0.0017790296864678024
train 2, step: 500, loss: 1.3055909341309733, grad_norm: 0.09638364883154352, ic: 0.22851150365306994
train 2, step: 1000, loss: 1.0701959480648382, grad_norm: 0.011142202270629755, ic: 0.0011594868345424754
train 2, step: 1500, loss: 0.9704859189176366, grad_norm: 0.3400670337763937, ic: -0.025089463319844175
train 2, step: 2000, loss: 1.7201164532424813, grad_norm: 0.0804888520867083, ic: 0.09524986460611932
Epoch 2: train loss: 1.6446517362438537
Eval step 0: eval loss: 0.9926840735461097
Eval: total loss: 1.089838004053084, mse:4.892092583106528, ic :0.05123061982764056, sharpe5:6.8617219439148895, irr5:205.50234985351562, ndcg5:0.8534750267963889 
train 3, step: 0, loss: 0.8451572976825568, grad_norm: 0.06513448063658583, ic: 0.2628712795843597
train 3, step: 500, loss: 1.2191110851234448, grad_norm: 0.005504345788289283, ic: 0.17876588805184135
train 3, step: 1000, loss: 0.9013317799707603, grad_norm: 0.025505276788797934, ic: 0.21978922455031583
train 3, step: 1500, loss: 4.378722724885408, grad_norm: 0.7606222392513716, ic: 0.0820780255584674
train 3, step: 2000, loss: 1.6671035533361824, grad_norm: 0.4135942981010549, ic: -0.038116565241383904
Epoch 3: train loss: 1.643240062874127
Eval step 0: eval loss: 0.9987151440725381
Eval: total loss: 1.0917262753571604, mse:4.850598407383242, ic :0.09409710246931617, sharpe5:7.100034684240818, irr5:211.58543395996094, ndcg5:0.8363132566737608 
train 4, step: 0, loss: 1.2920146692470509, grad_norm: 0.2565167253017946, ic: 0.029694408199987712
train 4, step: 500, loss: 0.841185232447121, grad_norm: 0.05539223838355627, ic: 0.4751339239779983
train 4, step: 1000, loss: 1.9343978302388252, grad_norm: 0.2079595737360752, ic: 0.1585949699343986
train 4, step: 1500, loss: 1.1683874783882122, grad_norm: 0.013077642259660483, ic: 0.20711020380081735
train 4, step: 2000, loss: 6.642553122646838, grad_norm: 0.9846645254471882, ic: -0.04247914013966033
Epoch 4: train loss: 1.6387740029561484
Eval step 0: eval loss: 1.0019997289897642
Eval: total loss: 1.0854813856116599, mse:4.711311036668514, ic :0.12641073624289093, sharpe5:8.231952567100524, irr5:237.18202209472656, ndcg5:0.8668935452404304 
train 5, step: 0, loss: 3.7285462724401595, grad_norm: 0.5189809158334171, ic: 0.30398506226074834
train 5, step: 500, loss: 3.4306776258680554, grad_norm: 0.7257175028783296, ic: -0.039335164471632586
train 5, step: 1000, loss: 1.049780090266613, grad_norm: 0.06104237815001223, ic: 0.06316175104222164
train 5, step: 1500, loss: 1.4936853580332996, grad_norm: 0.9932410913883922, ic: 0.46301186749608847
train 5, step: 2000, loss: 1.7822770060373752, grad_norm: 0.48243447191776795, ic: 0.017231311436519445
Epoch 5: train loss: 1.6383436028004672
Eval step 0: eval loss: 1.00103846545468
Eval: total loss: 1.0863677756991519, mse:4.715947738579766, ic :0.12505904065643764, sharpe5:7.809688018262386, irr5:221.28306579589844, ndcg5:0.8352665103353187 
train 6, step: 0, loss: 1.1031566954495615, grad_norm: 0.026478573843115392, ic: 0.43319335732300046
train 6, step: 500, loss: 1.8459327903735443, grad_norm: 0.11566338611282109, ic: 0.19609276263981051
train 6, step: 1000, loss: 0.8734505720964567, grad_norm: 0.19117930777361475, ic: 0.05659862285529379
train 6, step: 1500, loss: 0.8498633633140755, grad_norm: 0.00226122664706242, ic: 0.08033526320034708
train 6, step: 2000, loss: 1.924844046619451, grad_norm: 0.2636941638222915, ic: 0.04612694288487123
Epoch 6: train loss: 1.636634867787426
Eval step 0: eval loss: 0.9936956051087743
Eval: total loss: 1.0851620345753892, mse:4.737443160888723, ic :0.121944972123139, sharpe5:8.052771534919739, irr5:226.56475830078125, ndcg5:0.844399217728385 
train 7, step: 0, loss: 0.8248667985014746, grad_norm: 0.10392235621149132, ic: 0.5083546543566165
train 7, step: 500, loss: 1.1390846344885959, grad_norm: 0.027967228304786117, ic: 0.04423382415801393
train 7, step: 1000, loss: 1.098031252961524, grad_norm: 0.024387072697268634, ic: -0.07282378163036507
train 7, step: 1500, loss: 1.103707276815016, grad_norm: 0.06018335815549004, ic: 0.4367935569907236
train 7, step: 2000, loss: 0.7128920910165633, grad_norm: 0.02037603272776198, ic: -0.038287671930512274
Epoch 7: train loss: 1.6372200742914802
Eval step 0: eval loss: 1.0035772579729463
Eval: total loss: 1.0979939168958686, mse:4.827443661932238, ic :0.10222693128416509, sharpe5:8.254315196871756, irr5:237.17385864257812, ndcg5:0.8414079862750174 
train 8, step: 0, loss: 2.0232049109516215, grad_norm: 0.3360203436888819, ic: -0.10312363807576577
train 8, step: 500, loss: 0.9523277098429952, grad_norm: 0.08276640719748439, ic: 0.48982058141764157
train 8, step: 1000, loss: 0.7387236351120978, grad_norm: 0.033389229173598364, ic: 0.0842536169366758
train 8, step: 1500, loss: 0.8494943292856356, grad_norm: 0.034254190725098624, ic: 0.18369621336143238
train 8, step: 2000, loss: 1.4839054176588278, grad_norm: 0.7538745675749162, ic: -0.010349546867437515
Epoch 8: train loss: 1.6364255065534488
Eval step 0: eval loss: 0.9988915321386255
Eval: total loss: 1.0877949826424818, mse:4.721459788693062, ic :0.12343218768159472, sharpe5:8.087904161810874, irr5:228.23843383789062, ndcg5:0.8628504423951374 
train 9, step: 0, loss: 0.7831333824963562, grad_norm: 0.019541931696311684, ic: 0.5143329003049305
train 9, step: 500, loss: 1.76647820096369, grad_norm: 0.048421608993883744, ic: -0.13869728631613143
train 9, step: 1000, loss: 0.7825067570779176, grad_norm: 0.010762672802477944, ic: 0.11940839107231672
train 9, step: 1500, loss: 1.0677144352431926, grad_norm: 0.05840268683462675, ic: -0.08833666761756065
train 9, step: 2000, loss: 4.501867340686275, grad_norm: 1.8517556104460977, ic: 0.26235161503709475
Epoch 9: train loss: 1.6365363048439754
Eval step 0: eval loss: 0.9985557262868614
Eval: total loss: 1.093487723425046, mse:4.78783342840089, ic :0.12058165018173855, sharpe5:10.490328449606896, irr5:300.48004150390625, ndcg5:0.8418372014726112 
train 10, step: 0, loss: 0.7971001805647252, grad_norm: 0.0006866396624418505, ic: 0.09946088418144136
train 10, step: 500, loss: 0.7968975805953558, grad_norm: 0.08135620340417288, ic: -0.023010137678130538
train 10, step: 1000, loss: 1.014137174097324, grad_norm: 0.38970154184215966, ic: -0.0397796632910268
train 10, step: 1500, loss: 1.3247518896087398, grad_norm: 0.13551336873126368, ic: 0.0644940829744792
train 10, step: 2000, loss: 1.2660493773424837, grad_norm: 0.44296329245255023, ic: -0.17395391807280838
Epoch 10: train loss: 1.6328659084674764
Eval step 0: eval loss: 1.0074765013041402
Eval: total loss: 1.0834976758867447, mse:4.705180757497249, ic :0.15382770720902852, sharpe5:14.59634318947792, irr5:471.2148132324219, ndcg5:0.8408165357810782 
train 11, step: 0, loss: 1.5368439822186837, grad_norm: 0.1053195985752364, ic: 0.17407281199107322
train 11, step: 500, loss: 1.321388442794998, grad_norm: 0.257847230915331, ic: 0.1694597999034262
train 11, step: 1000, loss: 2.243894140336503, grad_norm: 0.7287363577641973, ic: 0.2646021914493915
train 11, step: 1500, loss: 1.394571185976531, grad_norm: 1.213653582976792, ic: 0.01602380175906172
train 11, step: 2000, loss: 3.6998656183226495, grad_norm: 3.2353672960302653, ic: 0.2141296838419109
Epoch 11: train loss: 1.6291633046849412
Eval step 0: eval loss: 1.0132566173808584
Eval: total loss: 1.0867593522780017, mse:4.688986178389451, ic :0.1582800078007734, sharpe5:15.073290284872055, irr5:484.365234375, ndcg5:0.8516645759542049 
train 12, step: 0, loss: 0.999736152098844, grad_norm: 0.06973349729072478, ic: 0.09032068880435584
train 12, step: 500, loss: 1.212115620457849, grad_norm: 0.13972995319904655, ic: 0.11100883947943127
train 12, step: 1000, loss: 1.1901448567708335, grad_norm: 2.642004124627375, ic: 0.5994483783507822
train 12, step: 1500, loss: 0.7696258730376435, grad_norm: 0.09319977058249133, ic: 0.5940911883076391
train 12, step: 2000, loss: 2.7656817768895348, grad_norm: 0.6634542499534751, ic: -0.03842731250470246
Epoch 12: train loss: 1.6294435759599908
Eval step 0: eval loss: 1.0035917855614795
Eval: total loss: 1.0837194870289286, mse:4.69301093970024, ic :0.15205918889177614, sharpe5:13.892946208119392, irr5:416.43878173828125, ndcg5:0.8429834460990688 
train 13, step: 0, loss: 1.351604651093392, grad_norm: 0.5718236907931692, ic: 0.12758190826266733
train 13, step: 500, loss: 1.4311529011486872, grad_norm: 0.12091084051599618, ic: -0.09343881647767215
train 13, step: 1000, loss: 2.2024202393780787, grad_norm: 0.9801651827619839, ic: 0.05041306112306104
train 13, step: 1500, loss: 1.5811693391747237, grad_norm: 1.0170002928440853, ic: -0.10231103327149954
train 13, step: 2000, loss: 0.7158484111507956, grad_norm: 0.02737146937473567, ic: -0.052559545353599246
Epoch 13: train loss: 1.6282824576102128
Eval step 0: eval loss: 1.0134574323657186
Eval: total loss: 1.0837902896933436, mse:4.683952633491329, ic :0.16188470074538153, sharpe5:15.187124876976013, irr5:490.95880126953125, ndcg5:0.836653219609116 
train 14, step: 0, loss: 1.9651134252727613, grad_norm: 0.7672735760042346, ic: 0.15461354189853638
train 14, step: 500, loss: 2.7757784875525613, grad_norm: 1.4686568147974244, ic: -0.14701814508977512
train 14, step: 1000, loss: 1.06071926659303, grad_norm: 0.1098417684310703, ic: 0.17095323653375236
train 14, step: 1500, loss: 3.035486825504658, grad_norm: 1.2657985739279891, ic: 0.14070486148479075
train 14, step: 2000, loss: 0.8043632216125954, grad_norm: 0.0329363861194442, ic: -0.05183969095446882
Epoch 14: train loss: 1.6286332714241856
Eval step 0: eval loss: 1.0043783966273367
Eval: total loss: 1.0832185514357235, mse:4.680926281372879, ic :0.1641401801908272, sharpe5:15.081096063852309, irr5:494.9373779296875, ndcg5:0.8524941543099633 
train 15, step: 0, loss: 1.243737778658045, grad_norm: 0.2757125251512679, ic: 0.5321746511809579
train 15, step: 500, loss: 1.8069481860632184, grad_norm: 0.9553268459210787, ic: -0.023583387622491587
train 15, step: 1000, loss: 1.7390598204077745, grad_norm: 0.5271575830775246, ic: 0.18670398483875528
train 15, step: 1500, loss: 1.45718297301999, grad_norm: 0.46343211843465565, ic: -0.09285261957853336
train 15, step: 2000, loss: 3.1143246379276825, grad_norm: 0.03325893564530889, ic: 0.19838273115311525
Epoch 15: train loss: 1.6268759234311725
Eval step 0: eval loss: 1.0119571053021326
Eval: total loss: 1.0853169244929848, mse:4.695249495536621, ic :0.16308012784391704, sharpe5:15.562551441192626, irr5:511.79656982421875, ndcg5:0.8396484085246296 
train 16, step: 0, loss: 1.7944408716548104, grad_norm: 0.5618420390697764, ic: 0.1834298260228408
train 16, step: 500, loss: 1.3494066850418613, grad_norm: 0.6964752996933339, ic: 0.33830052370871694
train 16, step: 1000, loss: 0.9140211943608181, grad_norm: 0.009304890596293658, ic: 0.01134178610793064
train 16, step: 1500, loss: 1.3482169637425647, grad_norm: 1.0014992974616406, ic: 0.198918182374893
train 16, step: 2000, loss: 1.2510605935294516, grad_norm: 0.1070751038462879, ic: 0.238892882017543
Epoch 16: train loss: 1.6280467112626686
Eval step 0: eval loss: 1.009987717116739
Eval: total loss: 1.0828680656133518, mse:4.680126721236479, ic :0.16638124983416147, sharpe5:15.569608047008513, irr5:501.176025390625, ndcg5:0.8483777414754661 
train 17, step: 0, loss: 1.5501114828646227, grad_norm: 0.4192331238315472, ic: 0.12992004958580866
train 17, step: 500, loss: 2.2254463224809884, grad_norm: 0.6425782392805627, ic: 0.16043341463677713
train 17, step: 1000, loss: 0.9580892798858462, grad_norm: 0.006031642402551821, ic: 0.2019929662390809
train 17, step: 1500, loss: 0.8514680227574067, grad_norm: 0.1448428913500281, ic: 0.29248799148983495
train 17, step: 2000, loss: 3.1580111346667326, grad_norm: 1.1023242520952974, ic: 0.04636831522917026
Epoch 17: train loss: 1.6269858621790934
Eval step 0: eval loss: 1.01333748333827
Eval: total loss: 1.0838640265345532, mse:4.683151397688367, ic :0.1642806358706571, sharpe5:14.711545590162277, irr5:468.4609375, ndcg5:0.8589231589215992 
train 18, step: 0, loss: 1.0659092281580553, grad_norm: 0.34393399381061784, ic: -0.004164907324257573
train 18, step: 500, loss: 0.8073837581830533, grad_norm: 0.09375299004679186, ic: -0.04185110860010498
train 18, step: 1000, loss: 1.170201924102066, grad_norm: 0.7892979440472028, ic: 0.061904769591109396
train 18, step: 1500, loss: 0.9413406084168633, grad_norm: 0.03814027867354736, ic: 0.11153773187487462
train 18, step: 2000, loss: 1.7590766246756055, grad_norm: 0.8927740254851277, ic: 0.472969871330423
Epoch 18: train loss: 1.6254647641604625
Eval step 0: eval loss: 1.0022747889514216
Eval: total loss: 1.082344863122227, mse:4.702502947815259, ic :0.1577341702849797, sharpe5:15.56540246129036, irr5:498.741943359375, ndcg5:0.8488694731410389 
train 19, step: 0, loss: 1.0891648815524193, grad_norm: 0.8670023465611942, ic: 0.03323643337271789
train 19, step: 500, loss: 2.2501694339645026, grad_norm: 1.0431193546567266, ic: 0.2202436791446275
train 19, step: 1000, loss: 1.3148527968353425, grad_norm: 0.14029350283604625, ic: 0.555434286809217
train 19, step: 1500, loss: 1.4965634264434293, grad_norm: 0.087012068984033, ic: 0.46283695553918597
train 19, step: 2000, loss: 1.1564757324590633, grad_norm: 0.2900361453781422, ic: 0.1934670876619619
Epoch 19: train loss: 1.625496841068526
Eval step 0: eval loss: 1.008808475472288
Eval: total loss: 1.0845734913230005, mse:4.688298360801763, ic :0.159552123274437, sharpe5:14.032872269153595, irr5:446.466064453125, ndcg5:0.851976295689667 
