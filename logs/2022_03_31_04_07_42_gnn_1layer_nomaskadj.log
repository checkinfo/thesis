Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjTimeDataset', dout=0.3, epochs=20, gnn_layers=1, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=3, mask_adj=False, mask_type='soft', model_type='GNNModel', num_days=1, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
28574
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.8711797440440289, grad_norm: 0.0003195648440609827, ic: 0.07367423384902017
train 0, step: 500, loss: 0.9900431069983058, grad_norm: 0.14040369580812978, ic: 0.007231903502955382
train 0, step: 1000, loss: 1.070192365035966, grad_norm: 0.02339154618185333, ic: -0.08445310176119442
train 0, step: 1500, loss: 0.9567729282693894, grad_norm: 0.10904300617513658, ic: 0.12349183481309285
train 0, step: 2000, loss: 1.0708917829972335, grad_norm: 0.3345373241039497, ic: 0.10911231943737187
Epoch 0: train loss: 1.6472448496336658
Eval step 0: eval loss: 1.0131581383293837
Eval: total loss: 1.091492150471026, mse:4.888604312657585, ic :0.011150847677344568, sharpe5:6.938621920645237, irr5:182.75645446777344, ndcg5:0.8559364144931354 
train 1, step: 0, loss: 0.9396688812299394, grad_norm: 0.07722486279529574, ic: 0.13967064892641784
train 1, step: 500, loss: 1.6826605326489, grad_norm: 0.10769114809790345, ic: -0.08134728383475805
train 1, step: 1000, loss: 1.1133910394111668, grad_norm: 0.42702874620376047, ic: 0.08213004346911053
train 1, step: 1500, loss: 0.8952871238069581, grad_norm: 0.13408444557449256, ic: -0.08075725807149235
train 1, step: 2000, loss: 4.696925469727326, grad_norm: 1.7914709737252226, ic: 0.06701596826377779
Epoch 1: train loss: 1.6457994957052813
Eval step 0: eval loss: 1.0011972404267047
Eval: total loss: 1.0890825184032726, mse:4.8772814990260365, ic :0.051096830223797136, sharpe5:6.213853166699409, irr5:150.88685607910156, ndcg5:0.8466116268093496 
train 2, step: 0, loss: 0.9624492224631653, grad_norm: 0.11846592598989658, ic: 0.006830652642409731
train 2, step: 500, loss: 1.3079907966382576, grad_norm: 0.09502650212281886, ic: 0.21186249441704116
train 2, step: 1000, loss: 1.0693661934309235, grad_norm: 0.01085745906883976, ic: -0.0036036762959022976
train 2, step: 1500, loss: 0.9696198127530364, grad_norm: 0.32862846922962535, ic: -0.02630746699596779
train 2, step: 2000, loss: 1.7186391271146617, grad_norm: 0.07672277203879918, ic: 0.1087859848987248
Epoch 2: train loss: 1.644789644380966
Eval step 0: eval loss: 0.9950077806164428
Eval: total loss: 1.0901904107436953, mse:4.896877508619552, ic :0.045922162720708565, sharpe5:4.775279883742332, irr5:97.03953552246094, ndcg5:0.8427121396105881 
train 3, step: 0, loss: 0.8454499987764291, grad_norm: 0.05967766687192071, ic: 0.258806665333627
train 3, step: 500, loss: 1.221830614247181, grad_norm: 0.004931723229572944, ic: 0.10804803231821966
train 3, step: 1000, loss: 0.9040917112116228, grad_norm: 0.029695405383532648, ic: 0.22503882465184022
train 3, step: 1500, loss: 4.377431334749809, grad_norm: 0.7370368092366314, ic: 0.0971467328408565
train 3, step: 2000, loss: 1.6677583923094328, grad_norm: 0.4076032283562973, ic: -0.03128160457612444
Epoch 3: train loss: 1.6435813507904005
Eval step 0: eval loss: 1.0007014382569772
Eval: total loss: 1.0906359796315699, mse:4.877204002946332, ic :0.054098872072165745, sharpe5:7.211877629756927, irr5:215.22274780273438, ndcg5:0.8652213979285629 
train 4, step: 0, loss: 1.2964673557600837, grad_norm: 0.2470335235536602, ic: 0.029732886630449068
train 4, step: 500, loss: 0.8625754368711442, grad_norm: 0.04296635123614062, ic: -0.0017124630028036297
train 4, step: 1000, loss: 1.9391807845876186, grad_norm: 0.1957477736689839, ic: -0.05221730700221419
train 4, step: 1500, loss: 1.1700646095685383, grad_norm: 0.012665196936748318, ic: 0.20362618017277587
train 4, step: 2000, loss: 6.633568453501506, grad_norm: 1.0219876212495995, ic: -0.021971733055989068
Epoch 4: train loss: 1.6442283859138616
Eval step 0: eval loss: 0.9984391198904028
Eval: total loss: 1.0891550881852767, mse:4.875985511531301, ic :0.05482872082231062, sharpe5:4.889942633807659, irr5:91.34246826171875, ndcg5:0.8505493593211992 
train 5, step: 0, loss: 3.758356881648936, grad_norm: 0.6145241852480037, ic: 0.14041915854119014
train 5, step: 500, loss: 3.440782335069444, grad_norm: 0.7220132997493743, ic: -0.05467236454767875
train 5, step: 1000, loss: 1.0385932554386053, grad_norm: 0.04634902198305408, ic: 0.07241809937145294
train 5, step: 1500, loss: 1.5379335765825726, grad_norm: 1.0525413723352424, ic: -0.049632024417754404
train 5, step: 2000, loss: 1.7893668375955836, grad_norm: 0.459354255710512, ic: 0.037429404736949626
Epoch 5: train loss: 1.6440430790891865
Eval step 0: eval loss: 1.0018827369051144
Eval: total loss: 1.0896906880900303, mse:4.872488012825829, ic :0.0616957304948049, sharpe5:8.060943387150765, irr5:238.4799346923828, ndcg5:0.8350823609335308 
train 6, step: 0, loss: 1.126925141630117, grad_norm: 0.02010973383871504, ic: 0.00939380454292096
train 6, step: 500, loss: 1.844889926052144, grad_norm: 0.11546556973034366, ic: 0.19311189294461564
train 6, step: 1000, loss: 0.875404754398376, grad_norm: 0.19160969074944756, ic: 0.06529867573425031
train 6, step: 1500, loss: 0.8488344849658613, grad_norm: 0.016376551316278895, ic: 0.17602870941547055
train 6, step: 2000, loss: 1.930446921850662, grad_norm: 0.23374621744547375, ic: 0.12478654913043558
Epoch 6: train loss: 1.6385052261727215
Eval step 0: eval loss: 1.001817041349888
Eval: total loss: 1.0833462852230842, mse:4.701541302881649, ic :0.1503155655133957, sharpe5:14.639728072285651, irr5:446.5317687988281, ndcg5:0.8327597918811659 
train 7, step: 0, loss: 0.7969149841689799, grad_norm: 0.23006474459664888, ic: 0.5644069930132436
train 7, step: 500, loss: 1.1328553072656808, grad_norm: 0.0747306740160658, ic: 0.11426618417768653
train 7, step: 1000, loss: 1.0966219377843063, grad_norm: 0.040300733759450716, ic: 0.02146826261487512
train 7, step: 1500, loss: 1.1054700319519397, grad_norm: 0.06763075620329483, ic: 0.43580229930015313
train 7, step: 2000, loss: 0.7093613367256305, grad_norm: 0.011906989346421985, ic: -0.041365000301348084
Epoch 7: train loss: 1.6314317097702609
Eval step 0: eval loss: 1.0077129924466823
Eval: total loss: 1.0963852233079745, mse:4.797405460401784, ic :0.12614841593294088, sharpe5:14.236781691312789, irr5:447.4125671386719, ndcg5:0.8394646493632948 
train 8, step: 0, loss: 2.0081734543211724, grad_norm: 0.36104763194135314, ic: -0.03354740145799201
train 8, step: 500, loss: 0.9109018982649673, grad_norm: 0.41686515171907385, ic: 0.5641425008151422
train 8, step: 1000, loss: 0.7329768445839875, grad_norm: 0.08789213310134208, ic: 0.1466337790238591
train 8, step: 1500, loss: 0.8481913380110466, grad_norm: 0.03571651476868628, ic: 0.1710882025022605
train 8, step: 2000, loss: 1.460553644963914, grad_norm: 0.8992579490997392, ic: 0.1381717629414532
Epoch 8: train loss: 1.6306593829817004
Eval step 0: eval loss: 1.0030345304066284
Eval: total loss: 1.0864455121757555, mse:4.707332553802016, ic :0.13455613241646128, sharpe5:11.424081044197083, irr5:336.6727600097656, ndcg5:0.8516071114160835 
train 9, step: 0, loss: 0.7786609037422839, grad_norm: 0.07120744920732398, ic: 0.5309505997052825
train 9, step: 500, loss: 1.7744351193805938, grad_norm: 0.07458731979336468, ic: -0.08307917771316813
train 9, step: 1000, loss: 0.7773120171200912, grad_norm: 0.02017941465998852, ic: 0.1826157308000708
train 9, step: 1500, loss: 1.0630374176820245, grad_norm: 0.06045273301133459, ic: -0.08156898142370335
train 9, step: 2000, loss: 4.490724571078431, grad_norm: 2.05036242172931, ic: 0.2485054490110542
Epoch 9: train loss: 1.6293674738429893
Eval step 0: eval loss: 1.0040068503365258
Eval: total loss: 1.0925698271965525, mse:4.7648594644170155, ic :0.14165706113781779, sharpe5:14.627528468370437, irr5:469.450439453125, ndcg5:0.8475730778133482 
train 10, step: 0, loss: 0.7967973449420481, grad_norm: 0.003591706304580652, ic: 0.11049381763196617
train 10, step: 500, loss: 0.7932906682312253, grad_norm: 0.10900785352646619, ic: -0.024066177292630844
train 10, step: 1000, loss: 1.022628614140396, grad_norm: 0.4083722586813359, ic: 0.0800868476621588
train 10, step: 1500, loss: 1.3307513973577236, grad_norm: 0.22897593449520703, ic: 0.08064554328681016
train 10, step: 2000, loss: 1.270137051129457, grad_norm: 0.6588819661835069, ic: -0.1839989710986261
Epoch 10: train loss: 1.6282306436140601
Eval step 0: eval loss: 1.0008280725464058
Eval: total loss: 1.0811324239793267, mse:4.695666627831122, ic :0.1601375333461182, sharpe5:14.355548340082168, irr5:469.9200744628906, ndcg5:0.8411092713279206 
train 11, step: 0, loss: 1.5349680906648089, grad_norm: 0.031180670038413776, ic: 0.17873686857650767
train 11, step: 500, loss: 1.334753853934152, grad_norm: 0.2179725652651721, ic: 0.17218404617347363
train 11, step: 1000, loss: 2.2539704405926884, grad_norm: 0.9680501384047031, ic: 0.2585400438970755
train 11, step: 1500, loss: 1.387926665743352, grad_norm: 1.7294885507664834, ic: 0.04983477280127459
train 11, step: 2000, loss: 3.7191210482226107, grad_norm: 2.839950347967906, ic: 0.20289838282212452
Epoch 11: train loss: 1.6269160518824106
Eval step 0: eval loss: 1.009982767451619
Eval: total loss: 1.0848889038460758, mse:4.688971118992673, ic :0.1557979714264126, sharpe5:14.16404473364353, irr5:447.8673095703125, ndcg5:0.8325614773349479 
train 12, step: 0, loss: 1.0020632370138616, grad_norm: 0.113519144050898, ic: 0.0800968931369645
train 12, step: 500, loss: 1.2085496059683867, grad_norm: 0.16396461320754116, ic: 0.11425170243508172
train 12, step: 1000, loss: 1.20229541421078, grad_norm: 1.1991950122642192, ic: 0.6118720494123581
train 12, step: 1500, loss: 0.7572684999577989, grad_norm: 0.07422615380617607, ic: 0.6136167251441063
train 12, step: 2000, loss: 2.7669203241798175, grad_norm: 0.7064006242061794, ic: -0.026710893849695147
Epoch 12: train loss: 1.6281764178420186
Eval step 0: eval loss: 1.000994047031332
Eval: total loss: 1.0858265296207992, mse:4.7185479454315, ic :0.12290807799538393, sharpe5:7.502472157180309, irr5:212.59274291992188, ndcg5:0.8531983320261224 
train 13, step: 0, loss: 1.3555126729313884, grad_norm: 0.5858209268980202, ic: -0.00537545942462152
train 13, step: 500, loss: 1.4417418040097032, grad_norm: 0.14592632405198713, ic: -0.09486048504400302
train 13, step: 1000, loss: 2.2028343563988093, grad_norm: 0.9573924246786897, ic: 0.04768949637860222
train 13, step: 1500, loss: 1.579818761715667, grad_norm: 1.0517796508077146, ic: -0.10386503804054903
train 13, step: 2000, loss: 0.7175560761108887, grad_norm: 0.03170554056936867, ic: -0.050453064030290894
Epoch 13: train loss: 1.6294155969096984
Eval step 0: eval loss: 1.0096443903452474
Eval: total loss: 1.081732725809214, mse:4.683189187233517, ic :0.16235365051820885, sharpe5:13.582525550723075, irr5:444.6009216308594, ndcg5:0.8521933914886859 
train 14, step: 0, loss: 1.965855583380361, grad_norm: 0.7325598311462943, ic: 0.20644650283023835
train 14, step: 500, loss: 2.759925025683295, grad_norm: 2.0376817610406626, ic: -0.1213085344354572
train 14, step: 1000, loss: 1.0601927838314138, grad_norm: 0.12342040270954735, ic: 0.16628408538508882
train 14, step: 1500, loss: 3.0845693268390915, grad_norm: 1.5007549664818642, ic: 0.0906462339394626
train 14, step: 2000, loss: 0.8031272364026717, grad_norm: 0.036068246065091514, ic: -0.03871401893413255
Epoch 14: train loss: 1.6266851300687095
Eval step 0: eval loss: 1.0067254378332344
Eval: total loss: 1.0827166891761648, mse:4.6819237728968215, ic :0.16598081549968607, sharpe5:15.051599262356758, irr5:493.2247619628906, ndcg5:0.8444279984912785 
train 15, step: 0, loss: 1.243520399051143, grad_norm: 0.26400556102580275, ic: 0.5286552138295216
train 15, step: 500, loss: 1.798899215756705, grad_norm: 0.95681418438449, ic: -0.029292043439162193
train 15, step: 1000, loss: 1.7513760822575268, grad_norm: 0.5425312564019137, ic: 0.1809708126950338
train 15, step: 1500, loss: 1.44883800923583, grad_norm: 0.47294169639476236, ic: -0.09214718939937815
train 15, step: 2000, loss: 3.108786480608476, grad_norm: 0.1145410385071356, ic: 0.19892825730362318
Epoch 15: train loss: 1.62592335219447
Eval step 0: eval loss: 1.0048402582362426
Eval: total loss: 1.0835066824309871, mse:4.691013235932468, ic :0.16193515571406655, sharpe5:14.754913446307182, irr5:490.2927551269531, ndcg5:0.8532748492169194 
train 16, step: 0, loss: 1.8045531895748645, grad_norm: 0.6660483619420818, ic: 0.13735267485584246
train 16, step: 500, loss: 1.333943304614486, grad_norm: 0.47881299672906996, ic: 0.3486904090999958
train 16, step: 1000, loss: 0.9145231793640526, grad_norm: 0.010465203142816782, ic: 0.0141128402617609
train 16, step: 1500, loss: 1.4025948434402036, grad_norm: 1.612170076831224, ic: 0.23105023656315393
train 16, step: 2000, loss: 1.252968389488328, grad_norm: 0.11069896300140912, ic: 0.2167626603010152
Epoch 16: train loss: 1.6255390278939912
Eval step 0: eval loss: 1.00500661840936
Eval: total loss: 1.0826523281653317, mse:4.6825806129797485, ic :0.16810098008217617, sharpe5:14.444063338637351, irr5:481.5535888671875, ndcg5:0.8516296247312591 
train 17, step: 0, loss: 1.5743731194323352, grad_norm: 0.6888617465235984, ic: 0.12120371812656171
train 17, step: 500, loss: 2.235746271981939, grad_norm: 1.3317912341585652, ic: 0.14754062698218648
train 17, step: 1000, loss: 0.9612161629201258, grad_norm: 0.004641984340511859, ic: 0.1683093656834157
train 17, step: 1500, loss: 0.859309098104883, grad_norm: 0.1783416639338303, ic: 0.2925069840269322
train 17, step: 2000, loss: 3.1650081586234178, grad_norm: 1.7157396392042537, ic: 0.03126377147198481
Epoch 17: train loss: 1.6263247225662376
Eval step 0: eval loss: 1.0165075188627237
Eval: total loss: 1.0836452712469082, mse:4.686195929449719, ic :0.16484250283138444, sharpe5:14.462757668495177, irr5:463.5570068359375, ndcg5:0.844568858343223 
train 18, step: 0, loss: 1.0788260352039258, grad_norm: 0.703990742942821, ic: 0.008462212961190181
train 18, step: 500, loss: 0.8059889721776186, grad_norm: 0.11286689784851527, ic: -0.04485185633519476
train 18, step: 1000, loss: 1.1718068849507202, grad_norm: 0.8812582226743099, ic: 0.06899364980699352
train 18, step: 1500, loss: 0.9344265535942414, grad_norm: 0.014973594108772102, ic: 0.15866366845618063
train 18, step: 2000, loss: 1.7623668631055363, grad_norm: 1.0571704727620506, ic: 0.48007137107019043
Epoch 18: train loss: 1.6253671816984796
Eval step 0: eval loss: 0.9972965829054765
Eval: total loss: 1.0829140324668125, mse:4.7253253321273565, ic :0.15474450825891645, sharpe5:14.209386736750602, irr5:460.35662841796875, ndcg5:0.8585963157317741 
train 19, step: 0, loss: 1.1258621529050588, grad_norm: 1.4939068618862705, ic: 0.08649829659981131
train 19, step: 500, loss: 2.267743109932612, grad_norm: 1.2373126876340288, ic: 0.20390402914413025
train 19, step: 1000, loss: 1.3094350747045527, grad_norm: 0.18333701914949396, ic: 0.5611212130028254
train 19, step: 1500, loss: 1.5195246054262248, grad_norm: 0.058988445054657744, ic: 0.42732423523416674
train 19, step: 2000, loss: 1.151899257872001, grad_norm: 0.34389265267796565, ic: 0.2184550960936562
Epoch 19: train loss: 1.6246413329482576
Eval step 0: eval loss: 1.009569052585242
Eval: total loss: 1.0823897459939054, mse:4.692223807965385, ic :0.15675036272780674, sharpe5:13.816262848377226, irr5:440.56268310546875, ndcg5:0.8485391036486709 
