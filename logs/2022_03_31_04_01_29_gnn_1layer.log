Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjTimeDataset', dout=0.3, epochs=20, gnn_layers=1, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=3, mask_adj=True, mask_type='soft', model_type='GNNModel', num_days=1, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
62059
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.8711773880428226, grad_norm: 0.00031938521259670445, ic: 0.07495167261675106
train 0, step: 500, loss: 0.9815937088196537, grad_norm: 0.13349799975050014, ic: 0.04931288220837236
train 0, step: 1000, loss: 1.0707420040002935, grad_norm: 0.02463470065036418, ic: -0.05030202985592551
train 0, step: 1500, loss: 0.9520413540377476, grad_norm: 0.10143492606967498, ic: 0.18051511562044892
train 0, step: 2000, loss: 1.073103907501274, grad_norm: 0.3318364219281656, ic: 0.1103838888629625
Epoch 0: train loss: 1.647785290149525
Eval step 0: eval loss: 1.012687341610716
Eval: total loss: 1.0911931102574473, mse:4.887669918030828, ic :0.0211818905392781, sharpe5:6.854215550422668, irr5:206.8430938720703, ndcg5:0.8375518836039056 
train 1, step: 0, loss: 0.9418173883848966, grad_norm: 0.07914335963172092, ic: 0.13418738804157815
train 1, step: 500, loss: 1.6876639335759673, grad_norm: 0.10908149709672343, ic: -0.07541430296578135
train 1, step: 1000, loss: 1.1071414456966244, grad_norm: 0.3981122328253629, ic: 0.08848308419978956
train 1, step: 1500, loss: 0.8945690336681548, grad_norm: 0.13624287789542278, ic: -0.09875011120809579
train 1, step: 2000, loss: 4.6811779222048475, grad_norm: 1.5625773643146512, ic: 0.05065722987225911
Epoch 1: train loss: 1.6454571064561316
Eval step 0: eval loss: 1.0003586900177723
Eval: total loss: 1.0890381527646509, mse:4.87691820502458, ic :0.052376158406693345, sharpe5:7.041538638174534, irr5:212.2032012939453, ndcg5:0.8604885439429487 
train 2, step: 0, loss: 0.962568136585076, grad_norm: 0.12169336394917762, ic: 0.00513335971739954
train 2, step: 500, loss: 1.3061325677902826, grad_norm: 0.09775762129486028, ic: 0.213812479127343
train 2, step: 1000, loss: 1.0701367682251073, grad_norm: 0.011551009378845768, ic: -0.0011883051155389297
train 2, step: 1500, loss: 0.9685643411036605, grad_norm: 0.26701614183004496, ic: -0.02944503742618134
train 2, step: 2000, loss: 1.7277787975798873, grad_norm: 0.09848840894654075, ic: 0.1071624969306893
Epoch 2: train loss: 1.644397722129775
Eval step 0: eval loss: 0.9906673742759347
Eval: total loss: 1.0903490971566572, mse:4.888588795944515, ic :0.059761058287663864, sharpe5:6.706093043088913, irr5:204.64419555664062, ndcg5:0.8575049971228775 
train 3, step: 0, loss: 0.8437190283623728, grad_norm: 0.06308439943517477, ic: 0.2640548036062404
train 3, step: 500, loss: 1.2185209520497182, grad_norm: 0.005422970474681466, ic: 0.2902362360376816
train 3, step: 1000, loss: 0.8996088039108188, grad_norm: 0.02166319964622339, ic: 0.21951810342739955
train 3, step: 1500, loss: 4.383109422746371, grad_norm: 0.7718839495313538, ic: 0.08287818073257713
train 3, step: 2000, loss: 1.6860923122611624, grad_norm: 0.43312290361319034, ic: -0.058221905920903406
Epoch 3: train loss: 1.6407079853624873
Eval step 0: eval loss: 0.9994084828775013
Eval: total loss: 1.0881726907955094, mse:4.723176515742874, ic :0.11970437392337795, sharpe5:7.600957212746143, irr5:217.78158569335938, ndcg5:0.8409922905253114 
train 4, step: 0, loss: 1.2924782391552512, grad_norm: 0.25886702449754456, ic: 0.03139063255263985
train 4, step: 500, loss: 0.8373276814730831, grad_norm: 0.05318785669029318, ic: 0.47766903759087603
train 4, step: 1000, loss: 1.9418157263647151, grad_norm: 0.22355966486279205, ic: 0.15977523432298668
train 4, step: 1500, loss: 1.1692669692854563, grad_norm: 0.012736199862708101, ic: 0.20841218725366734
train 4, step: 2000, loss: 6.6405146955007535, grad_norm: 1.03546439687466, ic: -0.01203834137472259
Epoch 4: train loss: 1.6377967017864976
Eval step 0: eval loss: 1.002331742240982
Eval: total loss: 1.0862327568511805, mse:4.71452991387682, ic :0.12483810989232766, sharpe5:7.741644271016121, irr5:225.3089599609375, ndcg5:0.8626221132998266 
train 5, step: 0, loss: 3.7395450673204786, grad_norm: 0.5520772229485323, ic: 0.3013751326581237
train 5, step: 500, loss: 3.436815049913194, grad_norm: 0.7109415235509448, ic: -0.061731813537220556
train 5, step: 1000, loss: 1.045424679084606, grad_norm: 0.055235129425210444, ic: 0.07227664099797217
train 5, step: 1500, loss: 1.4996929671653143, grad_norm: 1.0218999802133921, ic: 0.463998690571649
train 5, step: 2000, loss: 1.783143385221598, grad_norm: 0.44904787552592546, ic: 0.04834776205586851
Epoch 5: train loss: 1.6380986678507274
Eval step 0: eval loss: 1.0026162515633228
Eval: total loss: 1.0857273185004885, mse:4.712210665557152, ic :0.12961323233856073, sharpe5:10.58788460612297, irr5:309.03985595703125, ndcg5:0.8609853841266635 
train 6, step: 0, loss: 1.0998102556195175, grad_norm: 0.032272479807453, ic: 0.4370889825910271
train 6, step: 500, loss: 1.8331413208460166, grad_norm: 0.14440757324783032, ic: 0.1979550094735739
train 6, step: 1000, loss: 0.8763989065575787, grad_norm: 0.19868983144095717, ic: 0.04105855464281585
train 6, step: 1500, loss: 0.8420885444021358, grad_norm: 0.01823636488335067, ic: 0.20747465708739882
train 6, step: 2000, loss: 1.9263254326080608, grad_norm: 0.2540484841769264, ic: 0.11718093407315669
Epoch 6: train loss: 1.6343364323156708
Eval step 0: eval loss: 1.000317099974493
Eval: total loss: 1.081694219852614, mse:4.697176150829764, ic :0.15274680980884245, sharpe5:14.414873962998389, irr5:441.73406982421875, ndcg5:0.8483244716225623 
train 7, step: 0, loss: 0.7989568875780707, grad_norm: 0.09656180284241132, ic: 0.5651849829104294
train 7, step: 500, loss: 1.112749639444494, grad_norm: 0.10235827388762322, ic: 0.2380439262099577
train 7, step: 1000, loss: 1.1044862644522366, grad_norm: 0.0795483198232355, ic: 0.03585678090394513
train 7, step: 1500, loss: 1.1056359022890534, grad_norm: 0.07923357967130551, ic: 0.4352643300770146
train 7, step: 2000, loss: 0.7131666270516412, grad_norm: 0.04537883278843859, ic: -0.021946322180274078
Epoch 7: train loss: 1.6305623408949406
Eval step 0: eval loss: 1.0053699366854265
Eval: total loss: 1.0945255958576543, mse:4.799454393512146, ic :0.12844845020006848, sharpe5:14.165356732606886, irr5:450.9384765625, ndcg5:0.8545685909696722 
train 8, step: 0, loss: 2.007634424009835, grad_norm: 0.3773824747532584, ic: -0.016091801000171248
train 8, step: 500, loss: 0.9097889438837129, grad_norm: 0.06768899604019317, ic: 0.5606443380393406
train 8, step: 1000, loss: 0.735541135587348, grad_norm: 0.04111361787073905, ic: 0.17297200387701467
train 8, step: 1500, loss: 0.8489180971997945, grad_norm: 0.03772036261165454, ic: 0.16661698058495453
train 8, step: 2000, loss: 1.455048782096917, grad_norm: 0.7950735610135231, ic: 0.14630341689964305
Epoch 8: train loss: 1.6302869978114924
Eval step 0: eval loss: 1.004356733807267
Eval: total loss: 1.0874927180822576, mse:4.709053018953498, ic :0.13790125463546055, sharpe5:12.12538469195366, irr5:359.8822937011719, ndcg5:0.8392569003931941 
train 9, step: 0, loss: 0.7737596718535664, grad_norm: 0.06602451186854313, ic: 0.5365596277226294
train 9, step: 500, loss: 1.7680586589071399, grad_norm: 0.055485009276320496, ic: -0.10835466334637162
train 9, step: 1000, loss: 0.7775246274154798, grad_norm: 0.017431452133918703, ic: 0.17285535477581315
train 9, step: 1500, loss: 1.0649356254932911, grad_norm: 0.10603558489798946, ic: -0.08127366487087934
train 9, step: 2000, loss: 4.478458563112745, grad_norm: 2.231687364520724, ic: 0.2504172335479452
Epoch 9: train loss: 1.6287645558474606
Eval step 0: eval loss: 1.001118045784788
Eval: total loss: 1.0887029025441723, mse:4.736032508195647, ic :0.1435303840803692, sharpe5:14.659421299695968, irr5:456.5115051269531, ndcg5:0.8346961215610414 
train 10, step: 0, loss: 0.7963551982467724, grad_norm: 0.0006841168737211807, ic: 0.11054243691787473
train 10, step: 500, loss: 0.7963496634140316, grad_norm: 0.12047142334818255, ic: -0.027746798445561
train 10, step: 1000, loss: 1.0129917178335777, grad_norm: 0.4825756200660622, ic: -0.03885229334415598
train 10, step: 1500, loss: 1.3212388449568089, grad_norm: 0.18572601211965262, ic: 0.09471907367793034
train 10, step: 2000, loss: 1.2657273847434662, grad_norm: 0.6220811945711264, ic: -0.17161498861471
Epoch 10: train loss: 1.6288303239728077
Eval step 0: eval loss: 1.0037392470132305
Eval: total loss: 1.081428805503138, mse:4.69760219578838, ic :0.1601721465664713, sharpe5:14.93974449276924, irr5:488.1611022949219, ndcg5:0.8517731275476053 
train 11, step: 0, loss: 1.5322239707736198, grad_norm: 0.04226467284667838, ic: 0.1786194751610089
train 11, step: 500, loss: 1.3349698549741276, grad_norm: 0.2903464808500814, ic: 0.1594760808181417
train 11, step: 1000, loss: 2.2601947643556133, grad_norm: 0.8385804645541611, ic: 0.25224505218453525
train 11, step: 1500, loss: 1.405488265511684, grad_norm: 1.7780001240105214, ic: 0.029608715701284608
train 11, step: 2000, loss: 3.6790117976641414, grad_norm: 2.6967542194557685, ic: 0.1986105424953842
Epoch 11: train loss: 1.6264323449037938
Eval step 0: eval loss: 1.0136146645849788
Eval: total loss: 1.0892732491659363, mse:4.701669964467356, ic :0.15020893375152708, sharpe5:13.325629153847693, irr5:408.3966064453125, ndcg5:0.8460066480361765 
train 12, step: 0, loss: 1.008212002840909, grad_norm: 0.12572973944393728, ic: 0.09130628687594114
train 12, step: 500, loss: 1.2075632494549418, grad_norm: 0.1323055047250662, ic: 0.11205632970871819
train 12, step: 1000, loss: 1.188804907023836, grad_norm: 3.155639566331357, ic: 0.6108173589943404
train 12, step: 1500, loss: 0.7671972425462104, grad_norm: 0.1878248743718941, ic: 0.5999653681432082
train 12, step: 2000, loss: 2.763340135745432, grad_norm: 0.6776353616487124, ic: -0.01513737233973849
Epoch 12: train loss: 1.6270784182314084
Eval step 0: eval loss: 1.004358790810953
Eval: total loss: 1.0830449015426011, mse:4.684258476670286, ic :0.1609963311536022, sharpe5:14.400534675121307, irr5:460.23443603515625, ndcg5:0.849824775236749 
train 13, step: 0, loss: 1.3431498131923223, grad_norm: 0.5539875590047213, ic: 0.17207370067421068
train 13, step: 500, loss: 1.4318110960985064, grad_norm: 0.13912756564014617, ic: -0.0938586510316419
train 13, step: 1000, loss: 2.2089929940861044, grad_norm: 1.0514296633448077, ic: 0.04916230042815072
train 13, step: 1500, loss: 1.5788960565990036, grad_norm: 1.297250873900254, ic: -0.09874676410601449
train 13, step: 2000, loss: 0.7185079117981885, grad_norm: 0.03077429901923432, ic: -0.05418044746143567
Epoch 13: train loss: 1.6269844200875598
Eval step 0: eval loss: 1.0079933877616507
Eval: total loss: 1.0826892190794233, mse:4.682300269108326, ic :0.16211949253423422, sharpe5:14.782093675732613, irr5:474.01025390625, ndcg5:0.839351407784899 
train 14, step: 0, loss: 1.9869956998918359, grad_norm: 0.7808293159251902, ic: 0.20590087481956315
train 14, step: 500, loss: 2.7562681425602062, grad_norm: 1.9111599871473766, ic: -0.12534910724066983
train 14, step: 1000, loss: 1.0563264805897536, grad_norm: 0.13666351967945492, ic: 0.1799256135483504
train 14, step: 1500, loss: 3.0616212453901395, grad_norm: 1.3157723920436952, ic: 0.12654300748130795
train 14, step: 2000, loss: 0.8006804255128817, grad_norm: 0.029054697669212114, ic: -0.02966255256724542
Epoch 14: train loss: 1.6273726311838612
Eval step 0: eval loss: 1.0081351924532649
Eval: total loss: 1.0832896794739046, mse:4.68387827842016, ic :0.16389588616649098, sharpe5:15.086257034540175, irr5:487.25830078125, ndcg5:0.85365860733663 
train 15, step: 0, loss: 1.2447070426985638, grad_norm: 0.2711245028329417, ic: 0.5290433140941935
train 15, step: 500, loss: 1.8176933668582376, grad_norm: 0.9704629490460137, ic: -0.027958564130063007
train 15, step: 1000, loss: 1.7448455066215702, grad_norm: 0.5351464801638657, ic: 0.18495402097405186
train 15, step: 1500, loss: 1.4592466037765688, grad_norm: 0.4843911845340787, ic: -0.09379618598127307
train 15, step: 2000, loss: 3.106989028479782, grad_norm: 0.05827857746470509, ic: 0.2019513473484762
Epoch 15: train loss: 1.62569189071495
Eval step 0: eval loss: 1.0054310682637242
Eval: total loss: 1.08303371897688, mse:4.687320775894759, ic :0.16472269811274493, sharpe5:15.145563738942146, irr5:496.76348876953125, ndcg5:0.8510948562846037 
train 16, step: 0, loss: 1.8063571836890244, grad_norm: 0.7091190747192267, ic: 0.16319623163606412
train 16, step: 500, loss: 1.3371569672106212, grad_norm: 0.49662266516686365, ic: 0.34114968976790605
train 16, step: 1000, loss: 0.9150585004478503, grad_norm: 0.010373051306360433, ic: 0.009900915645894098
train 16, step: 1500, loss: 1.356520825353731, grad_norm: 1.3113367310657293, ic: 0.23635679251107392
train 16, step: 2000, loss: 1.2513938017440283, grad_norm: 0.13998031796931898, ic: 0.24867198621655603
Epoch 16: train loss: 1.6257588648380845
Eval step 0: eval loss: 1.0137326851714716
Eval: total loss: 1.084118653168629, mse:4.6836401564470735, ic :0.16722687272281, sharpe5:15.104024357199668, irr5:487.9548034667969, ndcg5:0.8504600826174848 
train 17, step: 0, loss: 1.5498834455257298, grad_norm: 0.6984839805559379, ic: 0.12942551621203602
train 17, step: 500, loss: 2.232718928231939, grad_norm: 0.9773014758681962, ic: 0.1694955713546458
train 17, step: 1000, loss: 0.9568101242384173, grad_norm: 0.007253838167841577, ic: 0.18802023250253752
train 17, step: 1500, loss: 0.8548130350733816, grad_norm: 0.19594941890980472, ic: 0.2850756918509429
train 17, step: 2000, loss: 3.193751274784909, grad_norm: 1.8134100219260332, ic: 0.04007228639392449
Epoch 17: train loss: 1.625629101230108
Eval step 0: eval loss: 1.0166887280312005
Eval: total loss: 1.0855653575301738, mse:4.690208385744594, ic :0.15981030319895695, sharpe5:13.53871878504753, irr5:430.55767822265625, ndcg5:0.8530107418091258 
train 18, step: 0, loss: 1.0740871444962876, grad_norm: 0.2981336391192459, ic: -0.034028290159210134
train 18, step: 500, loss: 0.8082334737061512, grad_norm: 0.10247838070011743, ic: -0.0307144855177443
train 18, step: 1000, loss: 1.1823329737549753, grad_norm: 0.8838286749645472, ic: 0.05757066494440455
train 18, step: 1500, loss: 0.9381310804834906, grad_norm: 0.008905954648480426, ic: 0.12912375116157726
train 18, step: 2000, loss: 1.7571556012110725, grad_norm: 0.9078242674027055, ic: 0.477443579049361
Epoch 18: train loss: 1.6248180732896744
Eval step 0: eval loss: 0.9980947003357029
Eval: total loss: 1.0813631591253732, mse:4.694429753528643, ic :0.16103265146610993, sharpe5:15.185729636549949, irr5:491.19677734375, ndcg5:0.8433932839686276 
train 19, step: 0, loss: 1.092917774010264, grad_norm: 1.0235888396438029, ic: 0.06220210147460783
train 19, step: 500, loss: 2.2561520842231397, grad_norm: 1.1275976105124896, ic: 0.21524539389956182
train 19, step: 1000, loss: 1.3087291427070114, grad_norm: 0.11410035228992813, ic: 0.5580040017126952
train 19, step: 1500, loss: 1.4936216837881513, grad_norm: 0.11720486911654984, ic: 0.46566880097835955
train 19, step: 2000, loss: 1.151776722706826, grad_norm: 0.36246109333077514, ic: 0.17865895834146395
Epoch 19: train loss: 1.6250757184681057
Eval step 0: eval loss: 1.0130726441136781
Eval: total loss: 1.0844606678559718, mse:4.690541845477949, ic :0.15929922841565106, sharpe5:13.754270660877227, irr5:445.5155029296875, ndcg5:0.8418721885040841 
