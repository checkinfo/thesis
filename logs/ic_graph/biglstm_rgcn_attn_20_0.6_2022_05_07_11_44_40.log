Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_20_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
10542
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884420124975567, grad_norm: 5.086476252558565, ic: 0.011593250879900624
train 0, step: 500, loss: 0.8626645511492087, grad_norm: 0.021921475676595956, ic: 0.05302943291444101
train 0, step: 1000, loss: 1.9454896671543527, grad_norm: 0.5895859608626444, ic: 0.0076611547162269596
train 0, step: 1500, loss: 0.9756959455286561, grad_norm: 0.12131366111537145, ic: 0.07348658647366768
train 0, step: 2000, loss: 0.9941306062702483, grad_norm: 0.1591742382265054, ic: 0.023675231127414945
Epoch 0: 2022-05-07 11:50:51.633382: train loss: 1.649376348655071
Eval step 0: eval loss: 0.8351332380095824
Eval: 2022-05-07 11:51:12.574478: total loss: 1.0789978298866592, mse:4.823398841111337, ic :0.007281768582926058, sharpe5:7.343696634471416, irr5:207.51962280273438, ndcg5:0.8438592912586075, pnl5:2.6152641773223877 
train 1, step: 0, loss: 2.7625842678931454, grad_norm: 0.9542639453990821, ic: 0.06150169267231652
train 1, step: 500, loss: 1.7644640644666458, grad_norm: 0.8585605859719033, ic: 0.10868341918192198
train 1, step: 1000, loss: 0.8746487637339129, grad_norm: 0.19418444411821206, ic: 0.05979902511332731
train 1, step: 1500, loss: 1.7121490829292385, grad_norm: 0.23704879494232722, ic: -0.022576160789780576
train 1, step: 2000, loss: 2.1933453125, grad_norm: 0.9752459745200681, ic: 0.04603520941310664
Epoch 1: 2022-05-07 11:55:46.783349: train loss: 1.6469011717922926
Eval step 0: eval loss: 0.8340426446012249
Eval: 2022-05-07 11:56:07.469394: total loss: 1.0788329630139428, mse:4.823916836994936, ic :0.008606985983428319, sharpe5:7.271002716422081, irr5:205.70974731445312, ndcg5:0.8583643512723652, pnl5:2.668545961380005 
train 2, step: 0, loss: 2.1429994673295454, grad_norm: 0.010787399569444165, ic: 0.12508309146601493
train 2, step: 500, loss: 3.3056281482296557, grad_norm: 0.3102468140934754, ic: 0.055613635854961596
train 2, step: 1000, loss: 2.072421276340996, grad_norm: 0.0002555868325477538, ic: 0.21538220908168687
train 2, step: 1500, loss: 1.4862360597566793, grad_norm: 0.07077698330566604, ic: -0.012076485078256703
train 2, step: 2000, loss: 3.2354477163461537, grad_norm: 0.879029537442584, ic: 0.23138773943023663
Epoch 2: 2022-05-07 12:00:47.287788: train loss: 1.64674496420089
Eval step 0: eval loss: 0.8358291288321588
Eval: 2022-05-07 12:01:07.426383: total loss: 1.0794727510690245, mse:4.822648662491245, ic :0.01406279929881589, sharpe5:7.44282728433609, irr5:210.55889892578125, ndcg5:0.8448174843819735, pnl5:2.8550262451171875 
train 3, step: 0, loss: 1.5242617227197663, grad_norm: 0.6098095318365993, ic: 0.00867166269695571
train 3, step: 500, loss: 1.4949695942638999, grad_norm: 0.3882556711197441, ic: 0.15205592816735475
train 3, step: 1000, loss: 3.6698285477835353, grad_norm: 0.795880458877371, ic: -0.05144249959704601
train 3, step: 1500, loss: 1.9865149265420419, grad_norm: 1.0219869750712705, ic: -0.061259940146190656
train 3, step: 2000, loss: 0.8990775443412162, grad_norm: 0.0007786162549417473, ic: 0.010316892176835804
Epoch 3: 2022-05-07 12:05:45.079515: train loss: 1.6461187331691727
Eval step 0: eval loss: 0.8317844724504412
Eval: 2022-05-07 12:06:06.127745: total loss: 1.0782367220315898, mse:4.824784560407963, ic :0.02599832335964263, sharpe5:7.982478849291801, irr5:225.09034729003906, ndcg5:0.8566504310584816, pnl5:2.648660898208618 
train 4, step: 0, loss: 1.4408884725765307, grad_norm: 0.05362386419640803, ic: 0.10848213907013199
train 4, step: 500, loss: 1.6335080124261812, grad_norm: 0.6260628908971929, ic: 0.06622249192145421
train 4, step: 1000, loss: 2.9466325713367, grad_norm: 0.7503613054525471, ic: 0.04271439148028601
train 4, step: 1500, loss: 2.1340461827531647, grad_norm: 0.5412133673772295, ic: -0.00855616707535339
train 4, step: 2000, loss: 1.1016443231799184, grad_norm: 0.4774766746355902, ic: 0.12843457479609455
Epoch 4: 2022-05-07 12:10:42.006774: train loss: 1.6458373035217635
Eval step 0: eval loss: 0.8567932579112881
Eval: 2022-05-07 12:11:02.467389: total loss: 1.0899532686923543, mse:4.829902533086936, ic :0.06678531746666856, sharpe5:9.864359271526336, irr5:353.2846374511719, ndcg5:0.8309387297387143, pnl5:2.3076393604278564 
train 5, step: 0, loss: 1.349584305526426, grad_norm: 0.1792480117710181, ic: 0.34723696890604366
train 5, step: 500, loss: 0.8928260409954897, grad_norm: 0.012763873221716508, ic: 0.5526271548675161
train 5, step: 1000, loss: 0.9844268214200191, grad_norm: 0.18074589241273614, ic: 0.01569319377591747
train 5, step: 1500, loss: 1.527878111390936, grad_norm: 0.17882940834484445, ic: 0.01087117849364648
train 5, step: 2000, loss: 1.1046528473254422, grad_norm: 0.026035015249635106, ic: 0.14938207493071762
Epoch 5: 2022-05-07 12:15:43.705843: train loss: 1.6405085254421026
Eval step 0: eval loss: 0.8362698811248682
Eval: 2022-05-07 12:16:04.590638: total loss: 1.077246035899884, mse:4.722488002217394, ic :0.13658946250497714, sharpe5:11.293674779534339, irr5:392.6899719238281, ndcg5:0.852279865063198, pnl5:2.554581880569458 
train 6, step: 0, loss: 1.339881714451256, grad_norm: 0.527534664979049, ic: 0.15041470411692093
train 6, step: 500, loss: 1.0077437069295558, grad_norm: 0.05255392351990341, ic: 0.0900505571749214
train 6, step: 1000, loss: 1.1250230216254753, grad_norm: 0.1071492034973537, ic: 0.07976014641137175
train 6, step: 1500, loss: 1.5744452358277377, grad_norm: 0.8550910854599358, ic: 0.10955522246150397
train 6, step: 2000, loss: 0.8090443057571056, grad_norm: 0.14531309602501868, ic: 0.30501245191435833
Epoch 6: 2022-05-07 12:20:42.029824: train loss: 1.6344487878222964
Eval step 0: eval loss: 0.8238308005136986
Eval: 2022-05-07 12:21:02.411061: total loss: 1.0728197622105262, mse:4.7054027477723785, ic :0.1522144507798953, sharpe5:16.771813412904738, irr5:533.0671997070312, ndcg5:0.8358070471051622, pnl5:6.670742511749268 
train 7, step: 0, loss: 0.9806325912475586, grad_norm: 0.06801976546339457, ic: 0.18833370797128182
train 7, step: 500, loss: 0.6520762342087766, grad_norm: 0.0020299986581542593, ic: 0.040276682323255084
train 7, step: 1000, loss: 1.0214691278322543, grad_norm: 0.21494930903340598, ic: 0.10908004206003012
train 7, step: 1500, loss: 2.241558438839764, grad_norm: 0.6898358306259462, ic: 0.4402418863348724
train 7, step: 2000, loss: 0.9123304425610865, grad_norm: 0.11664391630378598, ic: -0.03976544520882931
Epoch 7: 2022-05-07 12:25:48.311794: train loss: 1.6300497429335938
Eval step 0: eval loss: 0.8282461698992359
Eval: 2022-05-07 12:26:08.358476: total loss: 1.0728080492247327, mse:4.687382710214082, ic :0.16147662845146182, sharpe5:17.13064276456833, irr5:544.0726318359375, ndcg5:0.8584008490330872, pnl5:11.53791618347168 
train 8, step: 0, loss: 3.6036741394927536, grad_norm: 1.2209621418816679, ic: 0.1639060967478338
train 8, step: 500, loss: 2.7445391159289514, grad_norm: 0.9505894925901336, ic: 0.06492679999182868
train 8, step: 1000, loss: 3.0670877207880434, grad_norm: 0.9412836943562316, ic: 0.10314395091298198
train 8, step: 1500, loss: 0.7113735514122226, grad_norm: 0.01581223744704119, ic: 0.4878877266719544
train 8, step: 2000, loss: 1.0900479830228365, grad_norm: 0.4144304516559602, ic: 0.5065674327454411
Epoch 8: 2022-05-07 12:30:45.922179: train loss: 1.6281728945686078
Eval step 0: eval loss: 0.8238607714123419
Eval: 2022-05-07 12:31:06.896513: total loss: 1.071282238605539, mse:4.6941738313530506, ic :0.16176959966998417, sharpe5:17.196802700757978, irr5:568.3593139648438, ndcg5:0.8671897283319059, pnl5:8.05024242401123 
train 9, step: 0, loss: 5.454294694477672, grad_norm: 0.8283597803433486, ic: 0.12474631763722503
train 9, step: 500, loss: 1.3563505265293918, grad_norm: 1.0896046808712814, ic: 0.3334341609516314
train 9, step: 1000, loss: 0.9259228635891318, grad_norm: 0.17702498774314973, ic: 0.12105003189073169
train 9, step: 1500, loss: 1.0914806427766015, grad_norm: 0.01635196037414337, ic: 0.39731354922315104
train 9, step: 2000, loss: 1.060774425947642, grad_norm: 0.2339401789616214, ic: 0.31078891770197736
Epoch 9: 2022-05-07 12:35:47.740392: train loss: 1.6273826870898693
Eval step 0: eval loss: 0.823661201243908
Eval: 2022-05-07 12:36:07.277303: total loss: 1.0713922287474429, mse:4.697009527253931, ic :0.16076573681978912, sharpe5:17.964761949777603, irr5:589.6864624023438, ndcg5:0.8587212468394162, pnl5:11.930720329284668 
train 10, step: 0, loss: 7.104443074662901, grad_norm: 1.1341400990650432, ic: 0.25665022578884955
train 10, step: 500, loss: 1.1256820223582222, grad_norm: 0.07930434816912374, ic: 0.06610666228263888
train 10, step: 1000, loss: 2.3613979597033166, grad_norm: 0.8927416631075991, ic: 0.1100627427610302
train 10, step: 1500, loss: 1.1094035359172079, grad_norm: 0.28934237267596474, ic: -0.004083287748581362
train 10, step: 2000, loss: 2.7385161147653876, grad_norm: 0.3959900769541793, ic: 0.44406558650826783
Epoch 10: 2022-05-07 12:40:50.261195: train loss: 1.627165316874745
Eval step 0: eval loss: 0.8233820731320798
Eval: 2022-05-07 12:41:10.794734: total loss: 1.069754561645881, mse:4.685611958067127, ic :0.1668693877253295, sharpe5:17.630628826618192, irr5:589.2931518554688, ndcg5:0.8475987370647824, pnl5:4.591379165649414 
train 11, step: 0, loss: 1.255475320838167, grad_norm: 0.14097368264106164, ic: 0.20688973684345258
train 11, step: 500, loss: 0.6649739611570741, grad_norm: 0.015198150204275222, ic: 0.5360842926579609
train 11, step: 1000, loss: 0.9447265021189716, grad_norm: 0.19417189904625545, ic: 0.016965898816048847
train 11, step: 1500, loss: 1.052317836828399, grad_norm: 0.06528918090793662, ic: 0.18575294764276848
train 11, step: 2000, loss: 0.7857398444442387, grad_norm: 0.0072169268341274505, ic: 0.13224483370285944
Epoch 11: 2022-05-07 12:45:51.930705: train loss: 1.62563789295639
Eval step 0: eval loss: 0.8268025501761722
Eval: 2022-05-07 12:46:12.694126: total loss: 1.0707409059723547, mse:4.670756853696655, ic :0.17107720093128229, sharpe5:17.473985420465468, irr5:571.9767456054688, ndcg5:0.8466754531272233, pnl5:5.5888237953186035 
train 12, step: 0, loss: 0.9595643679300944, grad_norm: 0.18598232488524635, ic: 0.4024846423548908
train 12, step: 500, loss: 0.9220948465333232, grad_norm: 0.12157108618945504, ic: 0.19395104013987285
train 12, step: 1000, loss: 2.952563632066083, grad_norm: 0.5182928181051198, ic: 0.3384000157193084
train 12, step: 1500, loss: 0.9308033323614814, grad_norm: 0.14640952701541915, ic: -0.1093296304283129
train 12, step: 2000, loss: 0.8746371081948641, grad_norm: 0.00903260685151723, ic: 0.17111748170807392
Epoch 12: 2022-05-07 12:50:58.258125: train loss: 1.6242441191827726
Eval step 0: eval loss: 0.8236938090671101
Eval: 2022-05-07 12:51:18.331963: total loss: 1.0682687978068635, mse:4.638288696883393, ic :0.17853189223164603, sharpe5:16.90976343512535, irr5:571.1171264648438, ndcg5:0.8487741325150785, pnl5:5.8458476066589355 
train 13, step: 0, loss: 2.0562273647868268, grad_norm: 1.4862017597223787, ic: 0.4240160564255133
train 13, step: 500, loss: 0.8260414779267565, grad_norm: 0.08140802300743519, ic: 0.5659991425962826
train 13, step: 1000, loss: 0.9651232828229865, grad_norm: 0.5687171165359285, ic: 0.5683847668800399
train 13, step: 1500, loss: 2.421050715688427, grad_norm: 0.5004729046368314, ic: -0.08876646393922312
train 13, step: 2000, loss: 1.4659124995489319, grad_norm: 0.07150961325927298, ic: 0.20214387585715665
Epoch 13: 2022-05-07 12:55:54.913161: train loss: 1.6229760012338836
Eval step 0: eval loss: 0.820737044853958
Eval: 2022-05-07 12:56:15.979451: total loss: 1.0677173901906176, mse:4.612352331237365, ic :0.1804252934610141, sharpe5:17.404527987241742, irr5:588.737060546875, ndcg5:0.8517293783133808, pnl5:5.839321136474609 
train 14, step: 0, loss: 4.503253430918487, grad_norm: 2.282850463203383, ic: 0.19717963586933468
train 14, step: 500, loss: 0.8252895716862576, grad_norm: 0.016140418973255567, ic: 0.13461657874902827
train 14, step: 1000, loss: 1.8240133530277145, grad_norm: 1.885497543865404, ic: 0.44775115008963245
train 14, step: 1500, loss: 1.124596899682349, grad_norm: 0.1309064256738191, ic: -0.03802921483185951
train 14, step: 2000, loss: 1.1424232713031104, grad_norm: 0.6772519964409125, ic: 0.0841679364466836
Epoch 14: 2022-05-07 13:00:55.579981: train loss: 1.6206586722595013
Eval step 0: eval loss: 0.8302611661535167
Eval: 2022-05-07 13:01:16.448159: total loss: 1.0689459201724396, mse:4.600825787280011, ic :0.17550409883609483, sharpe5:16.47004419207573, irr5:556.2088012695312, ndcg5:0.8278426081581456, pnl5:4.784112453460693 
train 15, step: 0, loss: 3.4158229723978604, grad_norm: 1.2934843726095764, ic: 0.0788223602966723
train 15, step: 500, loss: 1.2628749171991895, grad_norm: 0.046250584229819244, ic: -0.04973493663787556
train 15, step: 1000, loss: 1.3123197726117886, grad_norm: 0.2010503478440565, ic: 0.05570120685596967
train 15, step: 1500, loss: 0.8511710099347933, grad_norm: 0.2651966903564739, ic: 0.07643298481218114
train 15, step: 2000, loss: 1.467676528218096, grad_norm: 1.1807297330488356, ic: 0.05895898556523836
Epoch 15: 2022-05-07 13:05:53.716375: train loss: 1.6197843786762856
Eval step 0: eval loss: 0.8406788189870916
Eval: 2022-05-07 13:06:13.907571: total loss: 1.0729357774570658, mse:4.597803747817898, ic :0.18312080322565813, sharpe5:16.745671809911727, irr5:542.940185546875, ndcg5:0.8482087291633338, pnl5:5.3880839347839355 
train 16, step: 0, loss: 0.6844337445256828, grad_norm: 1.2576286481378085, ic: -0.05009223367910883
train 16, step: 500, loss: 1.5853310903586555, grad_norm: 0.8190152604337815, ic: 0.184510024777212
train 16, step: 1000, loss: 0.8798682935310133, grad_norm: 0.012479469211280344, ic: -0.1068554749417551
train 16, step: 1500, loss: 0.842387699160264, grad_norm: 0.35894714724011517, ic: 0.14598189468070777
train 16, step: 2000, loss: 3.326001722491742, grad_norm: 3.6776793933634746, ic: -0.007248297170771377
Epoch 16: 2022-05-07 13:10:48.496231: train loss: 1.619568805916908
Eval step 0: eval loss: 0.8269302802291886
Eval: 2022-05-07 13:11:09.182927: total loss: 1.0689211617262115, mse:4.601352859888895, ic :0.17684867956492628, sharpe5:16.277945598363875, irr5:536.45654296875, ndcg5:0.8609973251050534, pnl5:6.743928909301758 
train 17, step: 0, loss: 1.2732237960046418, grad_norm: 0.47640926613090623, ic: -0.11130427986305354
train 17, step: 500, loss: 1.7572352959857724, grad_norm: 1.231839247732662, ic: 0.20123643644108496
train 17, step: 1000, loss: 1.2749476508649258, grad_norm: 0.2298068996483036, ic: 0.15588644060258694
train 17, step: 1500, loss: 4.520852945903747, grad_norm: 2.192637396442148, ic: 0.19965558697113645
train 17, step: 2000, loss: 1.2973097723063345, grad_norm: 1.7334208645961584, ic: 0.09794169802576694
Epoch 17: 2022-05-07 13:15:49.377323: train loss: 1.6196509245494364
Eval step 0: eval loss: 0.831440193007442
Eval: 2022-05-07 13:16:10.123071: total loss: 1.0680643610282998, mse:4.588230816735934, ic :0.18842922343161533, sharpe5:17.909934203624726, irr5:599.0200805664062, ndcg5:0.8507289008282081, pnl5:6.093695163726807 
train 18, step: 0, loss: 1.41576188010179, grad_norm: 2.15065944834806, ic: 0.14033612133989404
train 18, step: 500, loss: 1.5080126992534302, grad_norm: 3.110018688253688, ic: -0.053807542232127414
train 18, step: 1000, loss: 0.6472481672731164, grad_norm: 0.01704078426300089, ic: 0.5804808675505218
train 18, step: 1500, loss: 1.4212871429668306, grad_norm: 0.08452434886112604, ic: 0.21411067627524416
train 18, step: 2000, loss: 0.9112101755324443, grad_norm: 0.011527671349680147, ic: -0.006815072814682585
Epoch 18: 2022-05-07 13:20:48.745268: train loss: 1.6195151109018184
Eval step 0: eval loss: 0.8212695106691253
Eval: 2022-05-07 13:21:09.317735: total loss: 1.0648581936816255, mse:4.586816268152173, ic :0.1915138761353352, sharpe5:17.64632930159569, irr5:601.06396484375, ndcg5:0.842737762629698, pnl5:5.550509929656982 
train 19, step: 0, loss: 1.4956033373635913, grad_norm: 0.8330295684094804, ic: -0.015454181258608
train 19, step: 500, loss: 0.8574871487087673, grad_norm: 0.02680390100824486, ic: 0.23208545367835803
train 19, step: 1000, loss: 0.9561403313638953, grad_norm: 0.04183230540137429, ic: 0.20203823288522732
train 19, step: 1500, loss: 3.954359256961525, grad_norm: 1.4564556860870674, ic: 0.15902308142110363
train 19, step: 2000, loss: 1.0062975135216345, grad_norm: 0.40216002986118055, ic: 0.21517100502448264
Epoch 19: 2022-05-07 13:25:46.723391: train loss: 1.619517219267555
Eval step 0: eval loss: 0.8270100311182823
Eval: 2022-05-07 13:26:06.794321: total loss: 1.0665267365656153, mse:4.588267924345312, ic :0.18987179945717206, sharpe5:17.707358536720275, irr5:595.3235473632812, ndcg5:0.8305712118947864, pnl5:6.593080043792725 
train 20, step: 0, loss: 2.291235255064229, grad_norm: 4.020410784693295, ic: 0.0652848274952139
train 20, step: 500, loss: 3.215802911931818, grad_norm: 1.1866882136818049, ic: 0.1000753401714638
train 20, step: 1000, loss: 0.9811477661132812, grad_norm: 0.12280750061199909, ic: 0.060884900218894325
train 20, step: 1500, loss: 1.7433538042567955, grad_norm: 3.981647363878598, ic: 0.25408307592664864
train 20, step: 2000, loss: 1.0081537193808625, grad_norm: 0.1462521125998396, ic: 0.1518535504654312
Epoch 20: 2022-05-07 13:30:50.375438: train loss: 1.6185439201786622
Eval step 0: eval loss: 0.826097076383858
Eval: 2022-05-07 13:31:10.780868: total loss: 1.065914782168158, mse:4.578333541710239, ic :0.19602832495957218, sharpe5:18.445369777679442, irr5:621.5864868164062, ndcg5:0.8555940635654731, pnl5:10.641053199768066 
train 21, step: 0, loss: 1.0225978126787758, grad_norm: 0.4031274562323744, ic: 0.07970607226206186
train 21, step: 500, loss: 0.7621690193108752, grad_norm: 0.04189905650111431, ic: 0.21771614463932315
train 21, step: 1000, loss: 0.9447564242178933, grad_norm: 1.8414705287154836, ic: 0.15118331687498834
train 21, step: 1500, loss: 0.9865331574255309, grad_norm: 0.3985541208471865, ic: 0.3148533906997282
train 21, step: 2000, loss: 0.9403326260555094, grad_norm: 0.3393961231996308, ic: 0.06617616193934156
Epoch 21: 2022-05-07 13:35:48.512654: train loss: 1.6185835329619875
Eval step 0: eval loss: 0.8261803646107745
Eval: 2022-05-07 13:36:09.400998: total loss: 1.0663878577857366, mse:4.588219318844259, ic :0.18811775712308007, sharpe5:17.949606328010557, irr5:595.0789794921875, ndcg5:0.8559819436830547, pnl5:7.14140510559082 
train 22, step: 0, loss: 1.0455173988126765, grad_norm: 0.05230021516213683, ic: 0.18147627806088396
train 22, step: 500, loss: 3.272799558561992, grad_norm: 3.094472843906904, ic: -0.21768504339217482
train 22, step: 1000, loss: 1.1892904116239162, grad_norm: 0.1916680125818455, ic: 0.4677630180440225
train 22, step: 1500, loss: 0.9766008793081276, grad_norm: 0.4602201172385113, ic: 0.07825213605647365
train 22, step: 2000, loss: 1.7649884905133928, grad_norm: 2.62233283773413, ic: 0.15673337788492564
Epoch 22: 2022-05-07 13:40:52.538368: train loss: 1.6173040704989035
Eval step 0: eval loss: 0.8267654402866503
Eval: 2022-05-07 13:41:13.717346: total loss: 1.0678163784334602, mse:4.620448506890516, ic :0.17987324370344215, sharpe5:17.474982426166534, irr5:577.393798828125, ndcg5:0.8397086225422159, pnl5:8.329828262329102 
train 23, step: 0, loss: 0.9772643867074928, grad_norm: 0.07751645810236506, ic: 0.19274407686316963
train 23, step: 500, loss: 1.428614066945889, grad_norm: 0.22867915708901157, ic: 0.03093424214320653
train 23, step: 1000, loss: 1.6512606811523438, grad_norm: 0.13270973166677075, ic: 0.2571304939900578
train 23, step: 1500, loss: 1.1301368836339216, grad_norm: 4.751201482813496, ic: 0.08956310954025386
train 23, step: 2000, loss: 1.9011089509959582, grad_norm: 5.542608976914069, ic: 0.4492089779552799
Epoch 23: 2022-05-07 13:45:50.783334: train loss: 1.6171613140250802
Eval step 0: eval loss: 0.8282580682173669
Eval: 2022-05-07 13:46:11.472770: total loss: 1.0664181410238525, mse:4.584278598619436, ic :0.18814383356236225, sharpe5:17.36828011393547, irr5:585.6013793945312, ndcg5:0.8307404235669978, pnl5:5.802595615386963 
train 24, step: 0, loss: 2.193796046312781, grad_norm: 0.0928644052600196, ic: 0.14709279767020428
train 24, step: 500, loss: 1.2306861966500486, grad_norm: 0.5035968677169367, ic: 0.08542537670091543
train 24, step: 1000, loss: 0.9096946742066172, grad_norm: 0.053971207414825834, ic: 0.5286023022637568
train 24, step: 1500, loss: 2.625679487840066, grad_norm: 8.96596779217483, ic: 0.07388961584226662
train 24, step: 2000, loss: 0.9505289912515281, grad_norm: 0.4354701252917918, ic: 0.08980354862850742
Epoch 24: 2022-05-07 13:50:44.183123: train loss: 1.61456171540171
Eval step 0: eval loss: 0.8221425256437697
Eval: 2022-05-07 13:51:04.581051: total loss: 1.0654194374147377, mse:4.599782025620835, ic :0.18995298694836174, sharpe5:18.00120468378067, irr5:611.05078125, ndcg5:0.8541192202991204, pnl5:4.997178077697754 
train 25, step: 0, loss: 0.8352007891680744, grad_norm: 0.08477527484911554, ic: 0.6129742349647532
train 25, step: 500, loss: 0.867479030459309, grad_norm: 0.008245064923679778, ic: 0.2324945780903063
train 25, step: 1000, loss: 2.096795900668793, grad_norm: 0.19737958178379467, ic: 0.24022775798292115
train 25, step: 1500, loss: 1.131850969162297, grad_norm: 0.4261119521342591, ic: 0.5367119956183608
train 25, step: 2000, loss: 1.0154514246562716, grad_norm: 0.5557591545219771, ic: 0.6113859126954679
Epoch 25: 2022-05-07 13:55:51.228661: train loss: 1.6166195721689935
Eval step 0: eval loss: 0.8255594653500394
Eval: 2022-05-07 13:56:11.675868: total loss: 1.0671352757607915, mse:4.60189821602711, ic :0.18728531684145358, sharpe5:18.211907744407654, irr5:604.9752197265625, ndcg5:0.8425103510802938, pnl5:4.318149089813232 
train 26, step: 0, loss: 6.701128194888178, grad_norm: 1.9485341539931709, ic: 0.11018259587921886
train 26, step: 500, loss: 3.938299422155981, grad_norm: 7.067823688091826, ic: 0.36579997111802454
train 26, step: 1000, loss: 1.2737970995940444, grad_norm: 1.8686336574218019, ic: 0.03172198266991383
train 26, step: 1500, loss: 0.8321820063946131, grad_norm: 0.33633358422078297, ic: 0.2977437433126839
train 26, step: 2000, loss: 0.9578832450764654, grad_norm: 1.0086636640143782, ic: 0.12051165784023235
Epoch 26: 2022-05-07 14:00:49.002797: train loss: 1.616639360091738
Eval step 0: eval loss: 0.825003974681408
Eval: 2022-05-07 14:01:10.043512: total loss: 1.0651565351436039, mse:4.586263526704841, ic :0.19135752747906043, sharpe5:18.237336119413374, irr5:606.941162109375, ndcg5:0.8471350260148962, pnl5:5.441300392150879 
train 27, step: 0, loss: 0.8284184474571078, grad_norm: 0.0617038460131902, ic: 0.12776887679358434
train 27, step: 500, loss: 0.918491531176048, grad_norm: 3.9474914389074667, ic: 0.2747806473028402
train 27, step: 1000, loss: 0.7487776447245988, grad_norm: 1.0054351417824028, ic: 0.2034320997660523
train 27, step: 1500, loss: 0.6371362331253401, grad_norm: 0.05140887701312746, ic: 0.5168814572238978
train 27, step: 2000, loss: 1.3796345009076025, grad_norm: 0.11295946329807723, ic: 0.04571769256234594
Epoch 27: 2022-05-07 14:05:51.498408: train loss: 1.6164797257382892
Eval step 0: eval loss: 0.8297630446728463
Eval: 2022-05-07 14:06:11.610879: total loss: 1.0672941700281744, mse:4.5945399082235365, ic :0.18654139378331233, sharpe5:18.117595167160033, irr5:605.651611328125, ndcg5:0.8476112143125882, pnl5:6.906723976135254 
train 28, step: 0, loss: 1.5380394659447394, grad_norm: 2.058380584460984, ic: 0.24879041590352866
train 28, step: 500, loss: 1.3811636978584405, grad_norm: 4.52710989902413, ic: 0.09173531863315237
train 28, step: 1000, loss: 0.9110592891207232, grad_norm: 0.46984052166446516, ic: 0.5786911110895062
train 28, step: 1500, loss: 1.038875458309052, grad_norm: 0.24025948987093548, ic: 0.0039805279693556
train 28, step: 2000, loss: 1.0444701025091065, grad_norm: 0.838582394510207, ic: 0.1188349645813527
Epoch 28: 2022-05-07 14:10:49.021081: train loss: 1.6123388419482445
Eval step 0: eval loss: 0.8239575015229846
Eval: 2022-05-07 14:11:09.612957: total loss: 1.073282991726347, mse:4.653822009883776, ic :0.17733228543912963, sharpe5:17.290294214487076, irr5:589.70751953125, ndcg5:0.8420145169432581, pnl5:4.866269111633301 
train 29, step: 0, loss: 0.9070529431624337, grad_norm: 0.23441331415566855, ic: 0.09550469284053663
train 29, step: 500, loss: 1.1026576205134322, grad_norm: 0.6402036390479516, ic: 0.6136093285217498
train 29, step: 1000, loss: 1.062630970296911, grad_norm: 0.5551300853639065, ic: 0.07470509598692485
train 29, step: 1500, loss: 2.358918165434719, grad_norm: 1.3795978743624138, ic: -0.04897207652809763
train 29, step: 2000, loss: 4.304256486304012, grad_norm: 44.89411301749018, ic: 0.20641348379786062
Epoch 29: 2022-05-07 14:15:47.595625: train loss: 1.615933209615815
Eval step 0: eval loss: 0.8295491965226554
Eval: 2022-05-07 14:16:08.963702: total loss: 1.065693391760758, mse:4.582337590191754, ic :0.19130304888841057, sharpe5:18.540062617063523, irr5:609.2427978515625, ndcg5:0.8397266436580033, pnl5:3.8789315223693848 
train 30, step: 0, loss: 1.0046435761436756, grad_norm: 0.16422638209441492, ic: 0.5218967972438884
train 30, step: 500, loss: 1.4277598104875924, grad_norm: 3.2018813688982277, ic: 0.02650563125920414
train 30, step: 1000, loss: 0.9771169026692709, grad_norm: 0.25320645686120385, ic: -0.0067681799664677834
train 30, step: 1500, loss: 1.5060754008770756, grad_norm: 6.786956491348905, ic: 0.17219468179601002
train 30, step: 2000, loss: 1.8707988012265684, grad_norm: 2.5903970303483224, ic: 0.007552290967372498
Epoch 30: 2022-05-07 14:20:54.102800: train loss: 1.6147884324388586
Eval step 0: eval loss: 0.8404392447436446
Eval: 2022-05-07 14:21:14.743007: total loss: 1.0763514297959438, mse:4.699177074795099, ic :0.18642994038676877, sharpe5:17.753135665655137, irr5:593.7625122070312, ndcg5:0.8551158683967902, pnl5:4.599351406097412 
train 31, step: 0, loss: 1.0723554143121174, grad_norm: 0.532034209701492, ic: 0.3307152476264056
train 31, step: 500, loss: 1.4964543346514918, grad_norm: 1.949845171758022, ic: 0.016107300344560427
train 31, step: 1000, loss: 4.380154969262295, grad_norm: 4.6822915238554845, ic: 0.47132533252207187
train 31, step: 1500, loss: 0.7703674560221788, grad_norm: 0.06585613119362975, ic: 0.7071414485209144
train 31, step: 2000, loss: 1.2505135115881458, grad_norm: 9.74204091361188, ic: 0.185339895063451
Epoch 31: 2022-05-07 14:25:53.491859: train loss: 1.6108173517673294
Eval step 0: eval loss: 0.8338165122415041
Eval: 2022-05-07 14:26:14.283641: total loss: 1.0704224266565279, mse:4.602723624663848, ic :0.17656237193712565, sharpe5:17.397656782865525, irr5:562.3101196289062, ndcg5:0.8481904021780902, pnl5:8.2255859375 
train 32, step: 0, loss: 1.134136720272311, grad_norm: 0.1458746632648247, ic: 0.1659853496331758
train 32, step: 500, loss: 1.4874167615034448, grad_norm: 2.7409217159329504, ic: 0.08639253238595054
train 32, step: 1000, loss: 1.050178141175448, grad_norm: 0.321486387214858, ic: 0.5026495403417031
train 32, step: 1500, loss: 0.9880814308614607, grad_norm: 5.414962946028217, ic: 0.03522579008617621
train 32, step: 2000, loss: 0.9437426802995836, grad_norm: 0.37028889453437064, ic: 0.5552702224589579
Epoch 32: 2022-05-07 14:30:54.881599: train loss: 1.6121426989215213
Eval step 0: eval loss: 0.8228239455388896
Eval: 2022-05-07 14:31:15.228210: total loss: 1.0643261213368407, mse:4.595064524541155, ic :0.19330592036433838, sharpe5:18.304875215291975, irr5:620.6685791015625, ndcg5:0.8417471377347048, pnl5:5.519183158874512 
train 33, step: 0, loss: 1.2834487596461033, grad_norm: 2.279339690797679, ic: 0.19272217886067936
train 33, step: 500, loss: 0.986773322610294, grad_norm: 0.02309549485570693, ic: 0.1401687001524276
train 33, step: 1000, loss: 1.0112441411483455, grad_norm: 7.232563247244798, ic: 0.24628036495368255
train 33, step: 1500, loss: 0.8986434121997332, grad_norm: 0.36286404499378166, ic: 0.562301962116494
train 33, step: 2000, loss: 0.8161290285137679, grad_norm: 0.11732687613630269, ic: 0.2203018565305308
Epoch 33: 2022-05-07 14:35:54.132480: train loss: 1.6148040268167652
Eval step 0: eval loss: 0.8271373752798998
Eval: 2022-05-07 14:36:14.255929: total loss: 1.0683183328953516, mse:4.586510090549576, ic :0.19414104302087618, sharpe5:18.78536764025688, irr5:628.2315063476562, ndcg5:0.8543574909597675, pnl5:4.675476551055908 
train 34, step: 0, loss: 1.0149633975429388, grad_norm: 1.0258671638187624, ic: 0.6009406234204703
train 34, step: 500, loss: 0.7898684627054664, grad_norm: 0.79482414882436, ic: 0.27944084831186133
train 34, step: 1000, loss: 3.1865328131000386, grad_norm: 3.1446085958706007, ic: 0.33167253629151544
train 34, step: 1500, loss: 0.8137262319353397, grad_norm: 1.0325805122654974, ic: 0.6827224582835405
train 34, step: 2000, loss: 6.421382558336561, grad_norm: 35.29402701567443, ic: 0.44100599371973676
Epoch 34: 2022-05-07 14:40:52.768671: train loss: 1.61411280607731
Eval step 0: eval loss: 0.8227980908151672
Eval: 2022-05-07 14:41:13.314585: total loss: 1.0642977399054832, mse:4.588815439594123, ic :0.19528097341746725, sharpe5:17.716191591024398, irr5:608.9776611328125, ndcg5:0.8550664363876856, pnl5:8.885960578918457 
train 35, step: 0, loss: 1.1967634133731617, grad_norm: 0.6333542738260799, ic: 0.551477670107272
train 35, step: 500, loss: 1.170381809587081, grad_norm: 1.571981360096642, ic: 0.13458320686879768
train 35, step: 1000, loss: 1.7941843992585589, grad_norm: 9.619801156579367, ic: 0.0788351832496633
train 35, step: 1500, loss: 1.6047528489191731, grad_norm: 2.2255092833584644, ic: 0.08454722328450812
train 35, step: 2000, loss: 0.7760678051627256, grad_norm: 0.06771378733145364, ic: 0.5674537771118996
Epoch 35: 2022-05-07 14:45:58.596209: train loss: 1.6137327987217756
Eval step 0: eval loss: 0.8313355521231229
Eval: 2022-05-07 14:46:19.294765: total loss: 1.0664238238625945, mse:4.587220763044623, ic :0.1902066205981885, sharpe5:17.379360687732696, irr5:594.1123046875, ndcg5:0.8494993675658774, pnl5:6.830562114715576 
train 36, step: 0, loss: 1.8344341379881282, grad_norm: 7.3065944372529135, ic: 0.09427794803924686
train 36, step: 500, loss: 0.8382026786593775, grad_norm: 0.016736491126605118, ic: 0.18215312567006883
train 36, step: 1000, loss: 1.8170593039772727, grad_norm: 14.560005436848963, ic: 0.19359405448412714
train 36, step: 1500, loss: 0.7643296586917235, grad_norm: 0.16066876713134692, ic: 0.39778225084236474
train 36, step: 2000, loss: 1.116736062847498, grad_norm: 2.5246491867246896, ic: 0.7723861105609784
Epoch 36: 2022-05-07 14:51:00.663383: train loss: 1.6122995040546926
Eval step 0: eval loss: 0.8273323790667808
Eval: 2022-05-07 14:51:21.534292: total loss: 1.065451072807614, mse:4.585094425942327, ic :0.19173073825988005, sharpe5:18.934649852514266, irr5:635.570556640625, ndcg5:0.8578871309673682, pnl5:4.860944747924805 
train 37, step: 0, loss: 2.03846654263974, grad_norm: 5.5399432018840375, ic: 0.2142118145337406
train 37, step: 500, loss: 2.3308969718597066, grad_norm: 2.373906218600305, ic: -0.06432720848162096
train 37, step: 1000, loss: 1.079300739630898, grad_norm: 0.36333514953958584, ic: 0.0314978806614233
train 37, step: 1500, loss: 2.0340131582981753, grad_norm: 6.287063033785629, ic: 0.5762953632923282
train 37, step: 2000, loss: 1.3123548133824752, grad_norm: 0.29374406431407873, ic: 0.21509762061949828
Epoch 37: 2022-05-07 14:56:01.024354: train loss: 1.6121146595188958
Eval step 0: eval loss: 0.8253433661666886
Eval: 2022-05-07 14:56:20.892344: total loss: 1.067023250650608, mse:4.6044508920745155, ic :0.1895739444295679, sharpe5:18.382836520671844, irr5:617.1494750976562, ndcg5:0.8446488018160857, pnl5:4.018910884857178 
train 38, step: 0, loss: 1.3407890505907012, grad_norm: 1.3436995902263908, ic: -0.07383250529215332
train 38, step: 500, loss: 0.9069195029176311, grad_norm: 0.14161982595861317, ic: 0.2716290987229417
train 38, step: 1000, loss: 0.9027155578372036, grad_norm: 0.48561408367344117, ic: 0.17508216864833295
train 38, step: 1500, loss: 0.9434433504769363, grad_norm: 0.05355283534174879, ic: 0.21594969357561294
train 38, step: 2000, loss: 2.3237319439199395, grad_norm: 21.611669622702085, ic: 0.01911574001862757
Epoch 38: 2022-05-07 15:01:06.090056: train loss: 1.6119133014978322
Eval step 0: eval loss: 0.8248361762381453
Eval: 2022-05-07 15:01:26.680531: total loss: 1.0636522917984343, mse:4.583370400146529, ic :0.197736481543225, sharpe5:18.94738012075424, irr5:633.8637084960938, ndcg5:0.8585558492629679, pnl5:5.699650764465332 
train 39, step: 0, loss: 0.9672803033017966, grad_norm: 0.04911366582063568, ic: 0.10294090420812396
train 39, step: 500, loss: 0.8903763556278824, grad_norm: 0.20935125142053862, ic: 0.21974385815336475
train 39, step: 1000, loss: 0.9355626799909559, grad_norm: 0.6956028855554774, ic: 0.19098537033398708
train 39, step: 1500, loss: 2.090510010152536, grad_norm: 2.7473563927895146, ic: 0.23799212339929338
train 39, step: 2000, loss: 0.6206950930434192, grad_norm: 0.48559342003923067, ic: 0.07939328565160053
Epoch 39: 2022-05-07 15:06:04.186743: train loss: 1.6090233204861097
Eval step 0: eval loss: 0.8325344523840884
Eval: 2022-05-07 15:06:25.273346: total loss: 1.0701061476897378, mse:4.6133800150704545, ic :0.1865727694857341, sharpe5:18.10423302054405, irr5:618.4459228515625, ndcg5:0.8617241293784684, pnl5:6.727828502655029 
