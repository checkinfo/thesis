Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
62321
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884480826084832, grad_norm: 5.085948305476681, ic: 0.011130630924413503
train 0, step: 500, loss: 0.862724804393133, grad_norm: 0.021999868216960768, ic: 0.053318795910707734
train 0, step: 1000, loss: 1.9454576984843637, grad_norm: 0.5893065141710764, ic: 0.0074470211734530625
train 0, step: 1500, loss: 0.9756544512722332, grad_norm: 0.12128412872436199, ic: 0.07457761006704972
train 0, step: 2000, loss: 0.9941954643628509, grad_norm: 0.15928357232385112, ic: 0.02347514060855667
Epoch 0: 2022-05-07 01:45:16.774397: train loss: 1.6493760603521332
Eval step 0: eval loss: 0.8351378043911354
Eval: 2022-05-07 01:45:35.096882: total loss: 1.0789976390426368, mse:4.823398023239925, ic :0.007311218247281824, sharpe5:7.418565708696842, irr5:210.12246704101562, ndcg5:0.8596186178964386, pnl5:2.802626609802246 
train 1, step: 0, loss: 2.7625927340599796, grad_norm: 0.9542719522016005, ic: 0.06137217517047375
train 1, step: 500, loss: 1.764610794672357, grad_norm: 0.8589329072480175, ic: 0.10880291249635635
train 1, step: 1000, loss: 0.8747078602050247, grad_norm: 0.1943195766527357, ic: 0.059223384103523284
train 1, step: 1500, loss: 1.7121757419630028, grad_norm: 0.23707924537259065, ic: -0.022396537622981237
train 1, step: 2000, loss: 2.1932146484375, grad_norm: 0.9749958483326866, ic: 0.04432483514008413
Epoch 1: 2022-05-07 01:50:13.658377: train loss: 1.6468985497492168
Eval step 0: eval loss: 0.8340371134911748
Eval: 2022-05-07 01:50:31.832279: total loss: 1.078829270459381, mse:4.823952132024403, ic :0.008520343772508142, sharpe5:7.239140627682208, irr5:204.5401611328125, ndcg5:0.853921077605818, pnl5:2.8232343196868896 
train 2, step: 0, loss: 2.142983664772727, grad_norm: 0.01083744270838227, ic: 0.1282126164016844
train 2, step: 500, loss: 3.3057366551741003, grad_norm: 0.31008713312026936, ic: 0.05638129256982749
train 2, step: 1000, loss: 2.072412109375, grad_norm: 0.0002591609552361496, ic: 0.21205770665292606
train 2, step: 1500, loss: 1.486303431387166, grad_norm: 0.07087620184576354, ic: -0.012502208667789465
train 2, step: 2000, loss: 3.236073467548077, grad_norm: 0.8787356593347753, ic: 0.23226040529339598
Epoch 2: 2022-05-07 01:55:23.406022: train loss: 1.6467417398309125
Eval step 0: eval loss: 0.8360088899087855
Eval: 2022-05-07 01:55:42.263217: total loss: 1.0794301117485803, mse:4.822653751046995, ic :0.013334000946831483, sharpe5:7.391687229573726, irr5:208.50274658203125, ndcg5:0.8466924141714632, pnl5:3.177694320678711 
train 3, step: 0, loss: 1.5238598831300814, grad_norm: 0.6088801018100696, ic: 0.00838232542219694
train 3, step: 500, loss: 1.4951083930843774, grad_norm: 0.38856328765648007, ic: 0.1369138578773552
train 3, step: 1000, loss: 3.669656229760363, grad_norm: 0.7956834113123451, ic: -0.052919841816344036
train 3, step: 1500, loss: 1.9856650294726883, grad_norm: 1.0245315790520682, ic: -0.07485369816708329
train 3, step: 2000, loss: 0.8994238941089527, grad_norm: 0.0012114070188590032, ic: 0.004862321692966514
Epoch 3: 2022-05-07 02:00:27.575209: train loss: 1.645985028513989
Eval step 0: eval loss: 0.8335793176616833
Eval: 2022-05-07 02:00:46.050009: total loss: 1.0781513980691853, mse:4.821442796717746, ic :0.028573190358048315, sharpe5:8.181934853196143, irr5:227.1360321044922, ndcg5:0.84580173123028, pnl5:2.8928134441375732 
train 4, step: 0, loss: 1.4386450693558674, grad_norm: 0.051751127886283894, ic: 0.0909321016649823
train 4, step: 500, loss: 1.6419391301673227, grad_norm: 0.6511354387935022, ic: 0.06066935789803808
train 4, step: 1000, loss: 2.9519024360470656, grad_norm: 0.7552786987310436, ic: 0.08025476760108474
train 4, step: 1500, loss: 2.140168240704114, grad_norm: 0.48748991158778093, ic: -0.023037498649895384
train 4, step: 2000, loss: 1.0921257814018759, grad_norm: 0.4601316374094486, ic: 0.2744597828752098
Epoch 4: 2022-05-07 02:05:32.342244: train loss: 1.6452439085014994
Eval step 0: eval loss: 0.8639335994097405
Eval: 2022-05-07 02:05:50.331317: total loss: 1.0921652472551606, mse:4.771365804639747, ic :0.1324899281150243, sharpe5:11.211290325522421, irr5:381.0420227050781, ndcg5:0.8430825717806621, pnl5:3.019530773162842 
train 5, step: 0, loss: 1.363596912358431, grad_norm: 0.21995736816649597, ic: 0.37025599407321425
train 5, step: 500, loss: 0.8870763453484338, grad_norm: 0.010809545941272323, ic: 0.06946598287288332
train 5, step: 1000, loss: 0.9835408060943487, grad_norm: 0.16951866766264037, ic: -0.0007934339025915584
train 5, step: 1500, loss: 1.5278244602734974, grad_norm: 0.1699204474135284, ic: -0.005886072176135966
train 5, step: 2000, loss: 1.1024017688016603, grad_norm: 0.03016583145622289, ic: 0.1715853706539191
Epoch 5: 2022-05-07 02:10:29.259417: train loss: 1.6381970939607156
Eval step 0: eval loss: 0.835105518144099
Eval: 2022-05-07 02:10:47.577614: total loss: 1.0758838177624515, mse:4.712460526032192, ic :0.13795609373706572, sharpe5:11.847681713104247, irr5:402.0922546386719, ndcg5:0.8616935538048073, pnl5:2.8347280025482178 
train 6, step: 0, loss: 1.3382693934098384, grad_norm: 0.4943818634752269, ic: 0.10515571282918308
train 6, step: 500, loss: 1.0076835129929174, grad_norm: 0.049168406692591926, ic: 0.03839411334970764
train 6, step: 1000, loss: 1.119320992009268, grad_norm: 0.09658218930681373, ic: 0.15919803558569628
train 6, step: 1500, loss: 1.5730954004713327, grad_norm: 0.8030663610693781, ic: 0.1323297656790203
train 6, step: 2000, loss: 0.8100398776052091, grad_norm: 0.05339137164704628, ic: 0.011699046771872945
Epoch 6: 2022-05-07 02:15:36.876765: train loss: 1.6361922387729249
Eval step 0: eval loss: 0.828890029698202
Eval: 2022-05-07 02:15:55.145549: total loss: 1.072904435869226, mse:4.69490763223242, ic :0.1462676913998034, sharpe5:12.35467330098152, irr5:402.70306396484375, ndcg5:0.8421262285364137, pnl5:3.9729652404785156 
train 7, step: 0, loss: 0.9855764389038086, grad_norm: 0.04636518630636895, ic: 0.10530432817099143
train 7, step: 500, loss: 0.6511918737533244, grad_norm: 0.0022580845626858823, ic: 0.03837557483777839
train 7, step: 1000, loss: 1.0177008461062453, grad_norm: 0.19312587882387036, ic: 0.10461250167795481
train 7, step: 1500, loss: 2.244584421891747, grad_norm: 0.6912782056526607, ic: 0.44175449444142506
train 7, step: 2000, loss: 0.910570184710723, grad_norm: 0.043278327091113636, ic: -0.06422493834778258
Epoch 7: 2022-05-07 02:20:35.947116: train loss: 1.6336690550667745
Eval step 0: eval loss: 0.832402541840918
Eval: 2022-05-07 02:20:54.490302: total loss: 1.07281146561977, mse:4.659738562984321, ic :0.1531741567542551, sharpe5:13.156270614266395, irr5:427.2236022949219, ndcg5:0.8406662938416632, pnl5:3.7801215648651123 
train 8, step: 0, loss: 3.6103887143342392, grad_norm: 1.2739902674509622, ic: 0.15844708280726097
train 8, step: 500, loss: 2.7486811582803004, grad_norm: 0.9384987373003937, ic: 0.05288011711055293
train 8, step: 1000, loss: 3.0521102241847826, grad_norm: 1.004994452421108, ic: 0.11763078863598211
train 8, step: 1500, loss: 0.7001798843778899, grad_norm: 0.02066461158426181, ic: 0.5874223772799271
train 8, step: 2000, loss: 1.0520826770413307, grad_norm: 0.5320354328142313, ic: 0.6713673977591323
Epoch 8: 2022-05-07 02:25:46.684701: train loss: 1.6275001131228433
Eval step 0: eval loss: 0.8277984072461143
Eval: 2022-05-07 02:26:05.086287: total loss: 1.0674814558500922, mse:4.601104357075525, ic :0.17532712128248354, sharpe5:15.904236835241317, irr5:499.3105163574219, ndcg5:0.8383580972923811, pnl5:5.187407493591309 
train 9, step: 0, loss: 5.459377647777114, grad_norm: 0.8953625874503623, ic: 0.15110470169614504
train 9, step: 500, loss: 1.3704559472188995, grad_norm: 1.1788556711087679, ic: 0.3189382799969402
train 9, step: 1000, loss: 0.9522811313372331, grad_norm: 0.05430535224922815, ic: 0.09579329990501227
train 9, step: 1500, loss: 1.0775639979231866, grad_norm: 0.026229732621349578, ic: 0.49430135571614137
train 9, step: 2000, loss: 1.0605828605687964, grad_norm: 0.5387961673037618, ic: 0.2684015862233098
Epoch 9: 2022-05-07 02:30:44.997559: train loss: 1.6240563861838972
Eval step 0: eval loss: 0.8290403343980505
Eval: 2022-05-07 02:31:02.803526: total loss: 1.0699022759728836, mse:4.613717357658594, ic :0.17417788142199753, sharpe5:16.111912933588027, irr5:521.4888916015625, ndcg5:0.8431354037595519, pnl5:5.8078532218933105 
train 10, step: 0, loss: 7.089475047831632, grad_norm: 1.2261010061375839, ic: 0.26032494201524825
train 10, step: 500, loss: 1.1295788823341837, grad_norm: 0.1748724321458685, ic: 0.06582713033970973
train 10, step: 1000, loss: 2.3844396673097203, grad_norm: 0.7232506265308989, ic: -0.011341119701572228
train 10, step: 1500, loss: 1.091153826032366, grad_norm: 0.25713931986644106, ic: 0.015751551182476897
train 10, step: 2000, loss: 2.656156870369491, grad_norm: 1.0536748529117055, ic: 0.5381607317716757
Epoch 10: 2022-05-07 02:35:44.696680: train loss: 1.6240047136407365
Eval step 0: eval loss: 0.831314906933285
Eval: 2022-05-07 02:36:02.754814: total loss: 1.068708252878934, mse:4.596817728220007, ic :0.17883880854363915, sharpe5:16.868948632478713, irr5:556.507080078125, ndcg5:0.8412871863805954, pnl5:5.257995128631592 
train 11, step: 0, loss: 1.2545419974743814, grad_norm: 0.014211416565659728, ic: 0.2007525101146908
train 11, step: 500, loss: 0.6433296918372657, grad_norm: 0.03534758477204281, ic: 0.6443458235506618
train 11, step: 1000, loss: 0.9397946803508862, grad_norm: 0.1259003320411729, ic: 0.0376291378091258
train 11, step: 1500, loss: 1.0521637765984786, grad_norm: 0.06244930949961574, ic: 0.1910713788620639
train 11, step: 2000, loss: 0.7876136623099328, grad_norm: 0.0366409353827324, ic: 0.12484458248659293
Epoch 11: 2022-05-07 02:40:44.955823: train loss: 1.6233154042345341
Eval step 0: eval loss: 0.8305341200029636
Eval: 2022-05-07 02:41:03.147936: total loss: 1.0692624413761078, mse:4.5966648082525134, ic :0.17512121595019647, sharpe5:16.076836116313935, irr5:527.0816650390625, ndcg5:0.8567676838261143, pnl5:9.357930183410645 
train 12, step: 0, loss: 0.9550294876098633, grad_norm: 0.08527134631085904, ic: 0.3983303197788314
train 12, step: 500, loss: 0.9344094516213658, grad_norm: 0.09809407184503165, ic: 0.16984832016485646
train 12, step: 1000, loss: 2.96229718445213, grad_norm: 0.38134016084546485, ic: 0.2699368804157679
train 12, step: 1500, loss: 0.9448480192069688, grad_norm: 0.12052882640709414, ic: -0.10663811114327817
train 12, step: 2000, loss: 0.8752201691134819, grad_norm: 0.002784246424777167, ic: 0.17857503797126212
Epoch 12: 2022-05-07 02:45:45.612012: train loss: 1.623399659775955
Eval step 0: eval loss: 0.8316604083657139
Eval: 2022-05-07 02:46:03.977775: total loss: 1.0679398236080713, mse:4.589941977142, ic :0.1847359840935729, sharpe5:17.254443629980088, irr5:555.1510009765625, ndcg5:0.8473920562964051, pnl5:4.770752429962158 
train 13, step: 0, loss: 2.0571804671000526, grad_norm: 1.4042811298690814, ic: 0.4341225015092283
train 13, step: 500, loss: 0.80764283392309, grad_norm: 0.07683223944467288, ic: 0.5966207967338658
train 13, step: 1000, loss: 0.9419381160864093, grad_norm: 0.5406752222310675, ic: 0.5923453116142868
train 13, step: 1500, loss: 2.381865668911007, grad_norm: 0.2606992104315682, ic: -0.11935828561743943
train 13, step: 2000, loss: 1.4845796721636835, grad_norm: 0.08329986368650319, ic: 0.15578571491070325
Epoch 13: 2022-05-07 02:50:49.932168: train loss: 1.6225734677806185
Eval step 0: eval loss: 0.8275790279858732
Eval: 2022-05-07 02:51:08.455966: total loss: 1.0679976414791827, mse:4.608809111162889, ic :0.1777697964491607, sharpe5:16.921305620670317, irr5:551.0711059570312, ndcg5:0.8453076593405131, pnl5:5.6311421394348145 
train 14, step: 0, loss: 4.55738561573713, grad_norm: 1.2465155182984153, ic: 0.13822827390926268
train 14, step: 500, loss: 0.8269834372610856, grad_norm: 0.007874094818740282, ic: 0.11981717391613861
train 14, step: 1000, loss: 1.848231101995539, grad_norm: 1.3979348286114381, ic: 0.43192098827984965
train 14, step: 1500, loss: 1.125579881892661, grad_norm: 0.07165788767526736, ic: -0.057756014904002215
train 14, step: 2000, loss: 1.1329912154691124, grad_norm: 0.19398584734655236, ic: 0.09316990042326247
Epoch 14: 2022-05-07 02:55:58.973939: train loss: 1.621937570885049
Eval step 0: eval loss: 0.8341613062063685
Eval: 2022-05-07 02:56:17.596669: total loss: 1.06864244246618, mse:4.593620459738002, ic :0.181214675637824, sharpe5:17.014901186227796, irr5:552.8516845703125, ndcg5:0.8514337193314346, pnl5:5.523097991943359 
train 15, step: 0, loss: 3.391844753161479, grad_norm: 0.857575466496303, ic: 0.08105281479357075
train 15, step: 500, loss: 1.2628025679616064, grad_norm: 0.00906413159875712, ic: -0.0994750509996353
train 15, step: 1000, loss: 1.3170058831935976, grad_norm: 0.321345124906812, ic: 0.052790423685919706
train 15, step: 1500, loss: 0.8498613973302165, grad_norm: 0.31382232952660866, ic: 0.06776521508349492
train 15, step: 2000, loss: 1.4642896176152376, grad_norm: 0.7675039445481926, ic: 0.04939840027697874
Epoch 15: 2022-05-07 03:01:03.962257: train loss: 1.621440008907366
Eval step 0: eval loss: 0.8409827084636788
Eval: 2022-05-07 03:01:20.543337: total loss: 1.0714195863955793, mse:4.58982981938263, ic :0.18396491481189917, sharpe5:17.657216906547546, irr5:568.9231567382812, ndcg5:0.849759232397895, pnl5:7.072976112365723 
train 16, step: 0, loss: 0.6935493910208828, grad_norm: 0.9498338428496778, ic: 0.019428469983178737
train 16, step: 500, loss: 1.593400784558629, grad_norm: 0.4174099229281975, ic: 0.1778793570147192
train 16, step: 1000, loss: 0.8791248668323863, grad_norm: 0.009223864065883169, ic: -0.11965935719530985
train 16, step: 1500, loss: 0.8608869788588456, grad_norm: 0.2152625517117139, ic: 0.13814228593730685
train 16, step: 2000, loss: 3.3367358458918246, grad_norm: 1.0033150157913575, ic: 0.04394001871856726
Epoch 16: 2022-05-07 03:05:59.476961: train loss: 1.6209288150249062
Eval step 0: eval loss: 0.8355458202301764
Eval: 2022-05-07 03:06:17.768907: total loss: 1.0704820593457032, mse:4.616054324122467, ic :0.16172118507594338, sharpe5:15.775989173054695, irr5:496.304443359375, ndcg5:0.8307912599619014, pnl5:6.194182872772217 
train 17, step: 0, loss: 1.2746341128564322, grad_norm: 0.2603957648329073, ic: -0.12136062463822464
train 17, step: 500, loss: 1.7394224254742547, grad_norm: 1.314754498236561, ic: 0.2472247189506427
train 17, step: 1000, loss: 1.2800069022153369, grad_norm: 0.09722540112560457, ic: 0.15367649691231072
train 17, step: 1500, loss: 4.5341151095242465, grad_norm: 1.23096832698062, ic: 0.21804409098688882
train 17, step: 2000, loss: 1.2763578647076372, grad_norm: 0.6751022432132439, ic: 0.08881120444025048
Epoch 17: 2022-05-07 03:10:57.896014: train loss: 1.6209280048277463
Eval step 0: eval loss: 0.8356673760208113
Eval: 2022-05-07 03:11:16.035049: total loss: 1.0678212318016636, mse:4.583964802872441, ic :0.19017866655771562, sharpe5:18.04135166287422, irr5:599.9523315429688, ndcg5:0.8495995840189131, pnl5:8.870807647705078 
train 18, step: 0, loss: 1.4204274439417781, grad_norm: 0.7019687679494748, ic: 0.13922112840382259
train 18, step: 500, loss: 1.4985449100522095, grad_norm: 0.9961910315383398, ic: -0.03467148914645574
train 18, step: 1000, loss: 0.6511558888056507, grad_norm: 0.01883453488087392, ic: 0.5745353840479589
train 18, step: 1500, loss: 1.4273515680986386, grad_norm: 0.036137512419623874, ic: 0.15445026489973623
train 18, step: 2000, loss: 0.9098841187300956, grad_norm: 0.0063909248827116825, ic: -0.008150140758721247
Epoch 18: 2022-05-07 03:16:13.601779: train loss: 1.6211114508456956
Eval step 0: eval loss: 0.8263884243899828
Eval: 2022-05-07 03:16:32.439820: total loss: 1.0655098500823328, mse:4.591176981374609, ic :0.19003324488565437, sharpe5:17.231533309221266, irr5:578.4561767578125, ndcg5:0.8595944588210067, pnl5:4.187305450439453 
train 19, step: 0, loss: 1.4882379441034226, grad_norm: 0.9591036814973755, ic: -0.02170970224808541
train 19, step: 500, loss: 0.8640607904504846, grad_norm: 0.03248297599344538, ic: 0.22296158481859957
train 19, step: 1000, loss: 0.9570075218299166, grad_norm: 0.014690067060380327, ic: 0.20262915649834545
train 19, step: 1500, loss: 3.962459304176915, grad_norm: 1.4621635220302678, ic: 0.13999300474753495
train 19, step: 2000, loss: 1.0104206730769232, grad_norm: 0.42967266220038913, ic: 0.2041148312551352
Epoch 19: 2022-05-07 03:21:17.935769: train loss: 1.62067445612772
Eval step 0: eval loss: 0.832698520543829
Eval: 2022-05-07 03:21:36.149258: total loss: 1.0678343822228789, mse:4.5914274961409065, ic :0.18690574105930716, sharpe5:17.042650862932206, irr5:576.7094116210938, ndcg5:0.8268854168414476, pnl5:4.235037326812744 
train 20, step: 0, loss: 2.316790313117589, grad_norm: 3.0750104723528446, ic: 0.05277986224896946
train 20, step: 500, loss: 3.1991644176136362, grad_norm: 0.8248195828845124, ic: 0.13122604565759938
train 20, step: 1000, loss: 0.9697672843933106, grad_norm: 0.3133123376431532, ic: 0.16094649489833598
train 20, step: 1500, loss: 1.7199022839299387, grad_norm: 2.3045926594015302, ic: 0.2705338843342914
train 20, step: 2000, loss: 1.0359764908110727, grad_norm: 0.04244070964389612, ic: 0.010899034055059358
Epoch 20: 2022-05-07 03:26:23.396928: train loss: 1.6194400842839363
Eval step 0: eval loss: 0.8328166676271074
Eval: 2022-05-07 03:26:41.980413: total loss: 1.0668377539151714, mse:4.58542517705218, ic :0.1886335173239514, sharpe5:16.962105288505555, irr5:571.40869140625, ndcg5:0.8692170291758762, pnl5:6.82288932800293 
train 21, step: 0, loss: 1.0151818782477593, grad_norm: 0.7208497910471221, ic: 0.0765056923734567
train 21, step: 500, loss: 0.7681501945563122, grad_norm: 0.015588511231313225, ic: 0.20265100655863377
train 21, step: 1000, loss: 0.9322442841111568, grad_norm: 1.5683689472680136, ic: 0.14532121553703636
train 21, step: 1500, loss: 0.9962074713710403, grad_norm: 0.8064096035972992, ic: 0.309008596607696
train 21, step: 2000, loss: 0.9487035673190061, grad_norm: 0.2703820418312588, ic: 0.04467480293996974
Epoch 21: 2022-05-07 03:31:27.702539: train loss: 1.6195431663890654
Eval step 0: eval loss: 0.8309120363128951
Eval: 2022-05-07 03:31:46.187249: total loss: 1.0668338131928266, mse:4.5948719189939515, ic :0.18401878185825848, sharpe5:17.689018929004668, irr5:577.4839477539062, ndcg5:0.8446815560953079, pnl5:5.850618839263916 
train 22, step: 0, loss: 1.039647851286635, grad_norm: 0.13434873489682406, ic: 0.22210672921637936
train 22, step: 500, loss: 3.241748245363313, grad_norm: 1.6490020713985039, ic: -0.20534334887499572
train 22, step: 1000, loss: 1.19168849349711, grad_norm: 0.11067887837716192, ic: 0.47291751814001504
train 22, step: 1500, loss: 0.9685513720100308, grad_norm: 0.6782921699252826, ic: 0.13441976890326013
train 22, step: 2000, loss: 1.7660857393087444, grad_norm: 2.4551238695753916, ic: 0.16091879550745292
Epoch 22: 2022-05-07 03:36:21.110396: train loss: 1.6190713131805905
Eval step 0: eval loss: 0.8302237990030624
Eval: 2022-05-07 03:36:39.761266: total loss: 1.0682971698373913, mse:4.600048036012322, ic :0.17955187871274095, sharpe5:17.36339913725853, irr5:558.4960327148438, ndcg5:0.8552047737941205, pnl5:4.791746616363525 
train 23, step: 0, loss: 0.9783599237887248, grad_norm: 0.06600337680701408, ic: 0.20066421708432366
train 23, step: 500, loss: 1.4242272968382066, grad_norm: 0.19635814917176725, ic: 0.05935421662995324
train 23, step: 1000, loss: 1.657677001953125, grad_norm: 0.19628809427627425, ic: 0.2610797738985373
train 23, step: 1500, loss: 1.120647813938987, grad_norm: 1.998635732041517, ic: 0.09176256935917503
train 23, step: 2000, loss: 1.931523347286374, grad_norm: 2.750873097577169, ic: 0.44897586131885076
Epoch 23: 2022-05-07 03:41:23.443806: train loss: 1.6181796663774433
Eval step 0: eval loss: 0.837763538099315
Eval: 2022-05-07 03:41:41.274843: total loss: 1.0685194604813475, mse:4.591756124989089, ic :0.1822285249994499, sharpe5:17.037539080381393, irr5:559.0985107421875, ndcg5:0.8423729979360125, pnl5:5.700708866119385 
train 24, step: 0, loss: 2.181783924932065, grad_norm: 0.14801784774191973, ic: 0.19250217619158444
train 24, step: 500, loss: 1.2330381163363329, grad_norm: 1.072297901678819, ic: 0.056701507590923156
train 24, step: 1000, loss: 0.9037545234427752, grad_norm: 0.33389888788373506, ic: 0.53819248865847
train 24, step: 1500, loss: 2.6083809270017517, grad_norm: 9.371721484568633, ic: 0.0625332110653528
train 24, step: 2000, loss: 0.9317864726594335, grad_norm: 0.1390331783695974, ic: 0.11393651526920434
Epoch 24: 2022-05-07 03:46:19.626323: train loss: 1.6151291835722759
Eval step 0: eval loss: 0.8313758134590687
Eval: 2022-05-07 03:46:38.059692: total loss: 1.067984764617393, mse:4.613474558901919, ic :0.18645117298786, sharpe5:17.185591589212418, irr5:582.3674926757812, ndcg5:0.8417890919718816, pnl5:6.917150497436523 
train 25, step: 0, loss: 0.832951561180321, grad_norm: 0.07793706738725421, ic: 0.62268493209001
train 25, step: 500, loss: 0.8660951366597432, grad_norm: 0.006330441402064283, ic: 0.2710971824626866
train 25, step: 1000, loss: 2.10978165312024, grad_norm: 1.4685746434137603, ic: 0.23308996017607864
train 25, step: 1500, loss: 1.128489080075482, grad_norm: 1.258038793752693, ic: 0.543118424785414
train 25, step: 2000, loss: 1.013503297677394, grad_norm: 0.5416121377654111, ic: 0.6003957554459639
Epoch 25: 2022-05-07 03:51:24.447391: train loss: 1.61651391419233
Eval step 0: eval loss: 0.8301037867780229
Eval: 2022-05-07 03:51:43.171272: total loss: 1.0661263600388091, mse:4.593319120115747, ic :0.19035859567078794, sharpe5:17.212459663152693, irr5:582.3402099609375, ndcg5:0.8430241414990803, pnl5:7.271714210510254 
train 26, step: 0, loss: 6.7282964444388975, grad_norm: 1.337017608381141, ic: 0.10971326336517384
train 26, step: 500, loss: 3.9035134870992962, grad_norm: 2.98390080404048, ic: 0.36207193318342584
train 26, step: 1000, loss: 1.2608711566056712, grad_norm: 1.0698664445392267, ic: 0.007721141691664425
train 26, step: 1500, loss: 0.8320808548558387, grad_norm: 0.2097793424289215, ic: 0.3052362788060882
train 26, step: 2000, loss: 0.9601327579477281, grad_norm: 0.5740670571635036, ic: 0.12024556790358684
Epoch 26: 2022-05-07 03:56:29.316693: train loss: 1.6179559443813687
Eval step 0: eval loss: 0.8372978958113804
Eval: 2022-05-07 03:56:47.880332: total loss: 1.0693277218339199, mse:4.60207526947269, ic :0.17792743113795934, sharpe5:16.934401016235352, irr5:548.122802734375, ndcg5:0.8456627055994547, pnl5:6.2728400230407715 
train 27, step: 0, loss: 0.8208550666360294, grad_norm: 0.10630359878142427, ic: 0.1379628940773748
train 27, step: 500, loss: 0.9077246654992815, grad_norm: 6.167502717905137, ic: 0.31015688133905417
train 27, step: 1000, loss: 0.753796972052035, grad_norm: 1.312607775373127, ic: 0.19228186324331814
train 27, step: 1500, loss: 0.6355159788577938, grad_norm: 0.03734876967322638, ic: 0.5155545845070002
train 27, step: 2000, loss: 1.3861590988634207, grad_norm: 0.08540424639644159, ic: 0.020558680147180146
Epoch 27: 2022-05-07 04:01:22.823253: train loss: 1.615004135202264
Eval step 0: eval loss: 0.8378497848269559
Eval: 2022-05-07 04:01:41.192862: total loss: 1.0702060734890264, mse:4.607115225700231, ic :0.17826519855658018, sharpe5:16.790726251602173, irr5:542.8908081054688, ndcg5:0.8370113980794702, pnl5:7.1494646072387695 
train 28, step: 0, loss: 1.5493272464708012, grad_norm: 1.4750981973241921, ic: 0.22111158194041527
train 28, step: 500, loss: 1.3707392070465345, grad_norm: 3.4261350227902225, ic: 0.19530767929047355
train 28, step: 1000, loss: 0.9089451404767954, grad_norm: 0.5080684109114824, ic: 0.5817501974527252
train 28, step: 1500, loss: 1.0383463921061578, grad_norm: 0.0851555407802069, ic: 0.015734803312824935
train 28, step: 2000, loss: 1.0409948430909701, grad_norm: 0.29197102997541163, ic: 0.11182831015848825
Epoch 28: 2022-05-07 04:06:25.824974: train loss: 1.6136215639047076
Eval step 0: eval loss: 0.8266207953273181
Eval: 2022-05-07 04:06:43.705540: total loss: 1.069775449504651, mse:4.633838493222416, ic :0.18302622865449586, sharpe5:17.12757796525955, irr5:589.9761352539062, ndcg5:0.8538085879083372, pnl5:4.3285627365112305 
train 29, step: 0, loss: 0.9114332220787529, grad_norm: 0.0856081062048867, ic: 0.046301702590575394
train 29, step: 500, loss: 1.104072001844172, grad_norm: 0.39510782135266354, ic: 0.6147131821634062
train 29, step: 1000, loss: 1.066130163115167, grad_norm: 1.3129102299465956, ic: 0.1012044458708315
train 29, step: 1500, loss: 2.3598202088212332, grad_norm: 0.39260193507682745, ic: -0.0767716741844056
train 29, step: 2000, loss: 4.320838080512153, grad_norm: 6.735641684980599, ic: 0.20178089190524948
Epoch 29: 2022-05-07 04:11:25.212395: train loss: 1.6151124199221178
Eval step 0: eval loss: 0.836872321913692
Eval: 2022-05-07 04:11:43.061236: total loss: 1.068452117433473, mse:4.599641238690609, ic :0.18133036190739787, sharpe5:16.4722141456604, irr5:561.7708129882812, ndcg5:0.854493957915972, pnl5:8.453238487243652 
train 30, step: 0, loss: 0.9996571609679066, grad_norm: 0.10699232913433626, ic: 0.5213000869095888
train 30, step: 500, loss: 1.4284227354004309, grad_norm: 2.970077743097751, ic: 0.05735080811575569
train 30, step: 1000, loss: 0.9811103589607008, grad_norm: 0.2915702659759055, ic: -0.05934827533587866
train 30, step: 1500, loss: 1.5048166447676754, grad_norm: 3.6911894251557946, ic: 0.15760626275905043
train 30, step: 2000, loss: 1.8540728334807766, grad_norm: 2.9562177199777997, ic: 0.040253913513808845
Epoch 30: 2022-05-07 04:16:23.762721: train loss: 1.6160950873039732
Eval step 0: eval loss: 0.8461243897770679
Eval: 2022-05-07 04:16:40.522496: total loss: 1.0743300528091597, mse:4.6798895136969705, ic :0.18687758184331155, sharpe5:17.320056821107862, irr5:586.4190063476562, ndcg5:0.8456392521862299, pnl5:3.854865789413452 
train 31, step: 0, loss: 1.0590566379694935, grad_norm: 1.2437386063922216, ic: 0.3675595632322186
train 31, step: 500, loss: 1.5110403806584363, grad_norm: 1.7994197289718743, ic: 0.017508266186975017
train 31, step: 1000, loss: 4.428784884855582, grad_norm: 3.026107416453533, ic: 0.467459693114761
train 31, step: 1500, loss: 0.7691613818294358, grad_norm: 0.09616588044140037, ic: 0.7134284458656337
train 31, step: 2000, loss: 1.241224294737842, grad_norm: 2.1767890850576834, ic: 0.1171368955945899
Epoch 31: 2022-05-07 04:21:27.930752: train loss: 1.6142026574712844
Eval step 0: eval loss: 0.8413753529619994
Eval: 2022-05-07 04:21:46.361173: total loss: 1.0727191922169412, mse:4.623335997029878, ic :0.16664453349234182, sharpe5:15.394229636192321, irr5:501.92791748046875, ndcg5:0.8457377840996917, pnl5:4.198669910430908 
train 32, step: 0, loss: 1.126991563352494, grad_norm: 0.29583689982129846, ic: 0.19796936756739775
train 32, step: 500, loss: 1.5075049212598426, grad_norm: 3.4293613124211597, ic: 0.05665735263685932
train 32, step: 1000, loss: 1.069872277334093, grad_norm: 0.4127640982964692, ic: 0.5136606069863803
train 32, step: 1500, loss: 0.9661793537852784, grad_norm: 3.2410146193254774, ic: 0.08492405129324399
train 32, step: 2000, loss: 0.937855894075802, grad_norm: 0.1455948040602465, ic: 0.5639777639872952
Epoch 32: 2022-05-07 04:26:28.660346: train loss: 1.6123584626855565
Eval step 0: eval loss: 0.8268542596236169
Eval: 2022-05-07 04:26:46.911819: total loss: 1.065108887500686, mse:4.597479455034635, ic :0.19133410885101812, sharpe5:18.06998861789703, irr5:607.6707763671875, ndcg5:0.8346970724750257, pnl5:6.934167861938477 
train 33, step: 0, loss: 1.2631976551250899, grad_norm: 1.017383170226741, ic: 0.19993044969795912
train 33, step: 500, loss: 0.9953624471805768, grad_norm: 0.026492319103516713, ic: 0.12652845394451215
train 33, step: 1000, loss: 1.0337187510466908, grad_norm: 6.74876578349225, ic: 0.21989855264544067
train 33, step: 1500, loss: 0.9080344999953663, grad_norm: 1.418800456276178, ic: 0.5611701814884655
train 33, step: 2000, loss: 0.8060546488089343, grad_norm: 0.3182819795158969, ic: 0.24925468271863338
Epoch 33: 2022-05-07 04:31:38.736996: train loss: 1.6121075150783315
Eval step 0: eval loss: 0.8293941324955545
Eval: 2022-05-07 04:31:57.294848: total loss: 1.065778948147795, mse:4.582414741056332, ic :0.19263026461548738, sharpe5:17.028774132728575, irr5:579.07275390625, ndcg5:0.8492809163845745, pnl5:4.631117343902588 
train 34, step: 0, loss: 0.9975728254033657, grad_norm: 1.1015532969099084, ic: 0.6070519082990143
train 34, step: 500, loss: 0.8005261902415425, grad_norm: 1.1519100995739493, ic: 0.24659437991229857
train 34, step: 1000, loss: 3.139487552203341, grad_norm: 1.9531304557826357, ic: 0.3311061158755416
train 34, step: 1500, loss: 0.8105449108173792, grad_norm: 1.5576619096775262, ic: 0.6825879460331579
train 34, step: 2000, loss: 6.072258817050736, grad_norm: 25.57871556758049, ic: 0.42615244597795043
Epoch 34: 2022-05-07 04:36:41.544300: train loss: 1.6133571818586672
Eval step 0: eval loss: 0.8274212627189804
Eval: 2022-05-07 04:37:00.280389: total loss: 1.0667999689853005, mse:4.59232611708759, ic :0.1908240368093696, sharpe5:16.48085233926773, irr5:571.305419921875, ndcg5:0.83562987346903, pnl5:6.225816249847412 
train 35, step: 0, loss: 1.1969265567555145, grad_norm: 0.7586173353043187, ic: 0.5585272301271706
train 35, step: 500, loss: 1.1801529242577167, grad_norm: 0.7369984844872981, ic: 0.12395322782189627
train 35, step: 1000, loss: 1.681579330671112, grad_norm: 9.137346958885264, ic: 0.09685381798120632
train 35, step: 1500, loss: 1.6278654399671053, grad_norm: 1.307420104851392, ic: 0.061950851059402526
train 35, step: 2000, loss: 0.7942835353912516, grad_norm: 0.34543874709363964, ic: 0.5618730332807262
Epoch 35: 2022-05-07 04:41:49.012839: train loss: 1.6106829803788134
Eval step 0: eval loss: 0.8350602402199683
Eval: 2022-05-07 04:42:07.200146: total loss: 1.0684731817432511, mse:4.599630517612036, ic :0.18530007494111017, sharpe5:17.10525222659111, irr5:562.61083984375, ndcg5:0.8430875876774423, pnl5:5.6897969245910645 
train 36, step: 0, loss: 1.8471135449372058, grad_norm: 3.4524395978813067, ic: 0.0389707635273578
train 36, step: 500, loss: 0.8335389963184595, grad_norm: 0.034426545369132824, ic: 0.20808829832399015
train 36, step: 1000, loss: 1.771425071022727, grad_norm: 9.639818206390691, ic: 0.2723920705950524
train 36, step: 1500, loss: 0.7646098096623213, grad_norm: 0.09185971795457361, ic: 0.37966740914561786
train 36, step: 2000, loss: 1.120705723478455, grad_norm: 0.40821818396620957, ic: 0.7562979143542992
Epoch 36: 2022-05-07 04:46:46.104043: train loss: 1.6090171189567188
Eval step 0: eval loss: 0.8336447262537868
Eval: 2022-05-07 04:47:04.611363: total loss: 1.067046158717095, mse:4.59406824753373, ic :0.18681094011212981, sharpe5:16.80912450671196, irr5:581.88134765625, ndcg5:0.850612238070104, pnl5:6.119388103485107 
train 37, step: 0, loss: 2.006656851191616, grad_norm: 5.181579235661632, ic: 0.20903295928633717
train 37, step: 500, loss: 2.332795533620514, grad_norm: 3.3948106737614285, ic: -0.08508878573368002
train 37, step: 1000, loss: 1.0767708927891932, grad_norm: 0.11931043160104715, ic: 0.06916079962171207
train 37, step: 1500, loss: 2.0229704900289445, grad_norm: 3.205864528657451, ic: 0.6041182635647822
train 37, step: 2000, loss: 1.3130074434502181, grad_norm: 0.20508642538773592, ic: 0.17467367274619278
Epoch 37: 2022-05-07 04:51:45.482699: train loss: 1.6051270096914256
Eval step 0: eval loss: 0.8276974323300842
Eval: 2022-05-07 04:52:04.379597: total loss: 1.0685639887497629, mse:4.610200005966351, ic :0.18817215913927426, sharpe5:17.484664921760558, irr5:584.1304931640625, ndcg5:0.854100752873515, pnl5:5.515871047973633 
train 38, step: 0, loss: 1.328088155606898, grad_norm: 0.6409415366138098, ic: -0.057401874746953054
train 38, step: 500, loss: 0.9088045850212191, grad_norm: 0.1760551415992444, ic: 0.2632572838920794
train 38, step: 1000, loss: 0.8996199898097826, grad_norm: 0.1702527281490666, ic: 0.17275850124125414
train 38, step: 1500, loss: 0.9546008736415623, grad_norm: 0.31845575718698, ic: 0.2053773732687959
train 38, step: 2000, loss: 2.3187336993721677, grad_norm: 8.287990113981696, ic: 0.015484276571349067
Epoch 38: 2022-05-07 04:56:50.787234: train loss: 1.6089935990506392
Eval step 0: eval loss: 0.830448966634286
Eval: 2022-05-07 04:57:08.934470: total loss: 1.0655954642454566, mse:4.589755687924836, ic :0.18993766659175684, sharpe5:16.658928530216215, irr5:580.6575927734375, ndcg5:0.8530703189175719, pnl5:4.8233184814453125 
train 39, step: 0, loss: 0.9709291603951644, grad_norm: 0.004578118941970047, ic: 0.04814856782492754
train 39, step: 500, loss: 0.8841869873234531, grad_norm: 0.1259478844219522, ic: 0.2614105083361398
train 39, step: 1000, loss: 0.9374626258449162, grad_norm: 0.06555531190333681, ic: 0.18257879462411108
train 39, step: 1500, loss: 2.0814621044225436, grad_norm: 0.2976890231876198, ic: 0.21158580129892948
train 39, step: 2000, loss: 0.6150074939215541, grad_norm: 0.18492581994551474, ic: 0.16490192964352993
Epoch 39: 2022-05-07 05:01:50.543954: train loss: 1.6092495598356062
Eval step 0: eval loss: 0.8284396301205216
Eval: 2022-05-07 05:02:08.666576: total loss: 1.0658163951395836, mse:4.5918462489659735, ic :0.1920267564962249, sharpe5:17.593626104593277, irr5:617.0949096679688, ndcg5:0.8465746800614773, pnl5:5.882721424102783 
