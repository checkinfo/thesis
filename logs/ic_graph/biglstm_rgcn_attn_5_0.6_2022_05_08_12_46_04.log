Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_5_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
70337
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884399127736513, grad_norm: 5.086404284258346, ic: 0.011542870473782274
train 0, step: 500, loss: 0.8626886524467785, grad_norm: 0.021938865091281283, ic: 0.05220191178500487
train 0, step: 1000, loss: 1.9455110986649597, grad_norm: 0.5899416496553376, ic: 0.007068917101041815
train 0, step: 1500, loss: 0.9765374104496047, grad_norm: 0.12498372380354228, ic: 0.07271249287057543
train 0, step: 2000, loss: 0.9940911465364021, grad_norm: 0.1590948533030086, ic: 0.023980702654198945
Epoch 0: 2022-05-08 00:54:10.090661: train loss: 1.6493782331235152
Eval step 0: eval loss: 0.8351388334348656
Eval: 2022-05-08 00:54:33.934238: total loss: 1.0789976957115712, mse:4.823395699220923, ic :0.0073780805228510285, sharpe5:7.427913346588611, irr5:209.4457244873047, ndcg5:0.8410341898940465, pnl5:2.6162242889404297 
train 1, step: 0, loss: 2.762532683341734, grad_norm: 0.954248934693724, ic: 0.06182658932009444
train 1, step: 500, loss: 1.76443287469517, grad_norm: 0.8585142982527794, ic: 0.10841298482860139
train 1, step: 1000, loss: 0.8746184132838171, grad_norm: 0.1940786738080956, ic: 0.05946638884034518
train 1, step: 1500, loss: 1.712128176634339, grad_norm: 0.23695969877779777, ic: -0.0223389356051521
train 1, step: 2000, loss: 2.19330078125, grad_norm: 0.9744410227608145, ic: 0.0448207135120952
Epoch 1: 2022-05-08 01:00:38.951762: train loss: 1.646896994363626
Eval step 0: eval loss: 0.8340386570567703
Eval: 2022-05-08 01:01:04.072412: total loss: 1.078832242933154, mse:4.823935151549596, ic :0.008779772163684342, sharpe5:7.272789097130298, irr5:205.6402130126953, ndcg5:0.8581801241571084, pnl5:2.505286693572998 
train 2, step: 0, loss: 2.1429595170454543, grad_norm: 0.01085113571262704, ic: 0.12881121034042697
train 2, step: 500, loss: 3.305895594923709, grad_norm: 0.31012105642320004, ic: 0.059884691226518474
train 2, step: 1000, loss: 2.07235467552682, grad_norm: 0.0002697824173753952, ic: 0.20982490124942182
train 2, step: 1500, loss: 1.4863407047650286, grad_norm: 0.07087192517646028, ic: -0.01343025851316653
train 2, step: 2000, loss: 3.2362064302884614, grad_norm: 0.8786354274586823, ic: 0.23639790450076326
Epoch 2: 2022-05-08 01:07:15.521611: train loss: 1.6467376042104085
Eval step 0: eval loss: 0.8360620786065924
Eval: 2022-05-08 01:07:35.479136: total loss: 1.0794736675593581, mse:4.822474691627983, ic :0.014931707235837512, sharpe5:7.3048526680469505, irr5:207.8995361328125, ndcg5:0.8539587886053965, pnl5:2.574263572692871 
train 3, step: 0, loss: 1.5243596767022358, grad_norm: 0.60999461945097, ic: 0.009931892561557971
train 3, step: 500, loss: 1.4951571447361982, grad_norm: 0.38858718286990646, ic: 0.15128894701030934
train 3, step: 1000, loss: 3.6691620452828153, grad_norm: 0.7946852197269845, ic: -0.05584532342007456
train 3, step: 1500, loss: 1.9893211452153234, grad_norm: 1.0195096131685055, ic: -0.0636089447973209
train 3, step: 2000, loss: 0.8993835119298986, grad_norm: 0.0009098935204094509, ic: 0.0054783959964043285
Epoch 3: 2022-05-08 01:12:21.763382: train loss: 1.6459965805068764
Eval step 0: eval loss: 0.8339845036304662
Eval: 2022-05-08 01:12:41.765033: total loss: 1.0785401974232058, mse:4.8223441065540635, ic :0.027712865876484468, sharpe5:8.143165141940116, irr5:229.9503173828125, ndcg5:0.8428406299869163, pnl5:2.188133478164673 
train 4, step: 0, loss: 1.4358143335459184, grad_norm: 0.05251987224238596, ic: 0.1101059633026999
train 4, step: 500, loss: 1.6479098102239174, grad_norm: 0.6590563189820369, ic: 0.05185855358268905
train 4, step: 1000, loss: 2.9450464016053735, grad_norm: 0.7273481317100569, ic: 0.07179297706318391
train 4, step: 1500, loss: 2.1472104100738396, grad_norm: 0.49991758743486864, ic: 0.012898888040730935
train 4, step: 2000, loss: 1.0623732784858573, grad_norm: 0.4924188541853621, ic: 0.24389848504742884
Epoch 4: 2022-05-08 01:17:20.487160: train loss: 1.6436963250428978
Eval step 0: eval loss: 0.8577572789408258
Eval: 2022-05-08 01:17:40.192339: total loss: 1.0876223014949968, mse:4.74750487731253, ic :0.14006940733417508, sharpe5:11.221748480200766, irr5:378.50177001953125, ndcg5:0.8412014696332077, pnl5:2.702850580215454 
train 5, step: 0, loss: 1.3512529821203858, grad_norm: 0.24656678846521216, ic: 0.3502986634400176
train 5, step: 500, loss: 0.8879286110353886, grad_norm: 0.010165860823373962, ic: 0.03428841799501373
train 5, step: 1000, loss: 0.9831128584470786, grad_norm: 0.188691930110265, ic: 0.009788440167600112
train 5, step: 1500, loss: 1.5250896553168072, grad_norm: 0.17336650456524244, ic: 0.002346175964157024
train 5, step: 2000, loss: 1.1391400472194024, grad_norm: 0.15102759702850174, ic: 0.1798059566998343
Epoch 5: 2022-05-08 01:22:19.617124: train loss: 1.6382289927123102
Eval step 0: eval loss: 0.8317330845791622
Eval: 2022-05-08 01:22:39.072460: total loss: 1.074845148145592, mse:4.716547720002554, ic :0.13658358183306152, sharpe5:11.743351782560348, irr5:395.1980285644531, ndcg5:0.8508270743200805, pnl5:2.7518467903137207 
train 6, step: 0, loss: 1.314736203523724, grad_norm: 0.45631323736904006, ic: 0.10217183612844292
train 6, step: 500, loss: 1.0061090302868971, grad_norm: 0.04779062222786257, ic: 0.03369430870168988
train 6, step: 1000, loss: 1.116791490910171, grad_norm: 0.0926071217316442, ic: 0.07886456811205717
train 6, step: 1500, loss: 1.5744407969072831, grad_norm: 0.8478952601260574, ic: 0.1290513471534406
train 6, step: 2000, loss: 0.8111597676578762, grad_norm: 0.06771634676713546, ic: 0.04894764253798716
Epoch 6: 2022-05-08 01:27:19.766459: train loss: 1.6337022572412012
Eval step 0: eval loss: 0.8241261360642781
Eval: 2022-05-08 01:27:39.749034: total loss: 1.0711364504876155, mse:4.697316432193562, ic :0.15797443800291483, sharpe5:15.421291624903677, irr5:498.3377990722656, ndcg5:0.8599003241358738, pnl5:8.089652061462402 
train 7, step: 0, loss: 0.9828117370605469, grad_norm: 0.04702291350344937, ic: 0.12380891468499428
train 7, step: 500, loss: 0.6536666267186313, grad_norm: 0.0035696894614861427, ic: 0.01770032275954434
train 7, step: 1000, loss: 1.016691000154703, grad_norm: 0.1972187229820549, ic: 0.08329156483953601
train 7, step: 1500, loss: 2.2473181165427385, grad_norm: 0.6823146210924782, ic: 0.4469073000899913
train 7, step: 2000, loss: 0.9110857789466378, grad_norm: 0.5243416489006371, ic: -0.06049029321559693
Epoch 7: 2022-05-08 01:32:23.295128: train loss: 1.6322030074564486
Eval step 0: eval loss: 0.8298382291803872
Eval: 2022-05-08 01:32:41.598121: total loss: 1.0732523487735621, mse:4.698335952176471, ic :0.1545761909578549, sharpe5:15.207894567847251, irr5:493.4475402832031, ndcg5:0.8381724502009728, pnl5:5.062942028045654 
train 8, step: 0, loss: 3.6165364583333335, grad_norm: 1.2421837543499625, ic: 0.16175561410979056
train 8, step: 500, loss: 2.7449791627089666, grad_norm: 0.9266169932782427, ic: 0.06497701912989823
train 8, step: 1000, loss: 3.0662640115489133, grad_norm: 0.9255569863382331, ic: 0.1036885659286895
train 8, step: 1500, loss: 0.7213616424071363, grad_norm: 0.606670018890005, ic: 0.4513216489032918
train 8, step: 2000, loss: 1.0858095230594758, grad_norm: 0.43604428648945526, ic: 0.5169705366757098
Epoch 8: 2022-05-08 01:37:23.425552: train loss: 1.6285263150931415
Eval step 0: eval loss: 0.8257372326544388
Eval: 2022-05-08 01:37:42.985242: total loss: 1.0707501640400703, mse:4.68326835025882, ic :0.16406698867508013, sharpe5:16.631602628231047, irr5:532.6094360351562, ndcg5:0.8455787234676939, pnl5:3.7317941188812256 
train 9, step: 0, loss: 5.4573365231259965, grad_norm: 0.8241400818805162, ic: 0.1488605207157895
train 9, step: 500, loss: 1.3356931090599369, grad_norm: 1.0573119107446822, ic: 0.3441057052315607
train 9, step: 1000, loss: 0.9249396864416564, grad_norm: 0.16430921538529317, ic: 0.14757013859231685
train 9, step: 1500, loss: 1.0891676890130246, grad_norm: 0.017629989416644377, ic: 0.40590445293588545
train 9, step: 2000, loss: 1.0620603322821585, grad_norm: 0.31945819324968394, ic: 0.27630760635400414
Epoch 9: 2022-05-08 01:42:20.471847: train loss: 1.6271170796914258
Eval step 0: eval loss: 0.8244838573910036
Eval: 2022-05-08 01:42:39.639531: total loss: 1.0707033952486629, mse:4.683515443343119, ic :0.163930310030938, sharpe5:17.13715262532234, irr5:555.4212646484375, ndcg5:0.8527667820019358, pnl5:4.6844282150268555 
train 10, step: 0, loss: 7.066135061725583, grad_norm: 1.477780575595159, ic: 0.24617999300556673
train 10, step: 500, loss: 1.1280494582147764, grad_norm: 0.08771585540685177, ic: 0.06280118628542504
train 10, step: 1000, loss: 2.3847394135832056, grad_norm: 0.9475246499631982, ic: 0.12373247989896112
train 10, step: 1500, loss: 1.1169970623858565, grad_norm: 0.2706731164573343, ic: 0.0029027407305984958
train 10, step: 2000, loss: 2.7343748144828077, grad_norm: 0.33390278443118804, ic: 0.47643633093782545
Epoch 10: 2022-05-08 01:47:23.491904: train loss: 1.626810682903208
Eval step 0: eval loss: 0.828120883825079
Eval: 2022-05-08 01:47:42.965086: total loss: 1.0712889746833967, mse:4.661399804671458, ic :0.16310774836160405, sharpe5:15.404132532477378, irr5:515.0774536132812, ndcg5:0.8521186260728338, pnl5:5.602421760559082 
train 11, step: 0, loss: 1.2614641300210268, grad_norm: 0.028431976342337437, ic: 0.19136703605290617
train 11, step: 500, loss: 0.6475034630356523, grad_norm: 0.06078984331379663, ic: 0.6283735644817158
train 11, step: 1000, loss: 0.9367000520081925, grad_norm: 0.12552900009092283, ic: 0.028741053465076227
train 11, step: 1500, loss: 1.053779133579187, grad_norm: 0.05293567620393905, ic: 0.1786383531419071
train 11, step: 2000, loss: 0.7883881241977686, grad_norm: 0.0017419218940499892, ic: 0.11976080518962418
Epoch 11: 2022-05-08 01:52:19.457924: train loss: 1.6243064606172055
Eval step 0: eval loss: 0.8307418582060062
Eval: 2022-05-08 01:52:38.523536: total loss: 1.0709763548813065, mse:4.65861584636641, ic :0.17013735910882172, sharpe5:16.87123777270317, irr5:544.130859375, ndcg5:0.8414750597408389, pnl5:5.807531833648682 
train 12, step: 0, loss: 0.9541764259338379, grad_norm: 0.09595761347230747, ic: 0.4034014228068281
train 12, step: 500, loss: 0.925422037360514, grad_norm: 0.20101861143401092, ic: 0.1952077131894776
train 12, step: 1000, loss: 2.9506810668167796, grad_norm: 0.3365119691300844, ic: 0.2513227340086167
train 12, step: 1500, loss: 0.9430668896967822, grad_norm: 0.13886976996792133, ic: -0.14861420345979887
train 12, step: 2000, loss: 0.8717776756632364, grad_norm: 0.004880207394453238, ic: 0.24703847884201013
Epoch 12: 2022-05-08 01:57:16.686609: train loss: 1.6219710161161325
Eval step 0: eval loss: 0.8268164422665305
Eval: 2022-05-08 01:57:35.972895: total loss: 1.0670152435265536, mse:4.593059916861982, ic :0.18291724094948056, sharpe5:17.24954941034317, irr5:564.671630859375, ndcg5:0.8562187526806638, pnl5:4.177005290985107 
train 13, step: 0, loss: 2.044108502590417, grad_norm: 0.7697170113831863, ic: 0.44601109623582036
train 13, step: 500, loss: 0.8094895429207879, grad_norm: 0.03903120045922494, ic: 0.5933714832717897
train 13, step: 1000, loss: 0.9394695102768456, grad_norm: 0.8203821488763854, ic: 0.6080916751854268
train 13, step: 1500, loss: 2.3953679223994926, grad_norm: 0.33885498033272726, ic: -0.10757049488841766
train 13, step: 2000, loss: 1.4781617244635294, grad_norm: 0.13814636515411338, ic: 0.13183122902143604
Epoch 13: 2022-05-08 02:02:22.981645: train loss: 1.6226270971138133
Eval step 0: eval loss: 0.824783244801271
Eval: 2022-05-08 02:02:43.073645: total loss: 1.06687406062306, mse:4.60824569261261, ic :0.18135933355240294, sharpe5:17.653331043720243, irr5:572.9630126953125, ndcg5:0.8521609630851165, pnl5:7.61135721206665 
train 14, step: 0, loss: 4.528391916926677, grad_norm: 1.3399533154124346, ic: 0.1466452809184511
train 14, step: 500, loss: 0.8263415415352637, grad_norm: 0.006382803504765732, ic: 0.08700744062978535
train 14, step: 1000, loss: 1.8307069215072134, grad_norm: 0.19241824921427808, ic: 0.44190378761779364
train 14, step: 1500, loss: 1.1255210505175628, grad_norm: 0.10024947808352484, ic: -0.06425733125114265
train 14, step: 2000, loss: 1.1446388757975638, grad_norm: 0.3235005787233298, ic: 0.08538230158882676
Epoch 14: 2022-05-08 02:07:25.635234: train loss: 1.6208349789485454
Eval step 0: eval loss: 0.8325173445320732
Eval: 2022-05-08 02:07:45.640300: total loss: 1.0676997294189787, mse:4.598504742265504, ic :0.17937126053332358, sharpe5:16.885504224300384, irr5:538.2211303710938, ndcg5:0.8479582267457086, pnl5:4.4018073081970215 
train 15, step: 0, loss: 3.4064916707198445, grad_norm: 1.0938063723866256, ic: 0.08933660487471295
train 15, step: 500, loss: 1.2675738631935787, grad_norm: 0.032300309785595636, ic: -0.0784091675899605
train 15, step: 1000, loss: 1.3194635654852642, grad_norm: 0.16048965108597488, ic: 0.040467707576229285
train 15, step: 1500, loss: 0.8590159018208662, grad_norm: 0.3692945532658085, ic: 0.06474637493373035
train 15, step: 2000, loss: 1.4664650639090178, grad_norm: 0.6365502723911495, ic: 0.06201189596940625
Epoch 15: 2022-05-08 02:12:30.225752: train loss: 1.620264010179996
Eval step 0: eval loss: 0.839215518802687
Eval: 2022-05-08 02:12:49.776175: total loss: 1.070406094749502, mse:4.588656852334619, ic :0.1868009265894554, sharpe5:17.56234244942665, irr5:575.7919311523438, ndcg5:0.8297270801933203, pnl5:5.385782718658447 
train 16, step: 0, loss: 0.6984040780569576, grad_norm: 0.3934687206963832, ic: -0.01164851616929672
train 16, step: 500, loss: 1.6232282033731473, grad_norm: 0.9920537910294369, ic: 0.18361458829759475
train 16, step: 1000, loss: 0.8800396543560606, grad_norm: 0.004141954603511761, ic: -0.13292053946718052
train 16, step: 1500, loss: 0.8467465423992809, grad_norm: 0.26805875512315247, ic: 0.12121681474637383
train 16, step: 2000, loss: 3.3416515195989884, grad_norm: 2.42114008469512, ic: -0.03436762511260242
Epoch 16: 2022-05-08 02:17:35.049913: train loss: 1.6185874378825655
Eval step 0: eval loss: 0.8300234570518308
Eval: 2022-05-08 02:17:54.494500: total loss: 1.0687828951927936, mse:4.603686076163199, ic :0.175578710599861, sharpe5:16.417906658649443, irr5:536.0767211914062, ndcg5:0.853380367993143, pnl5:5.986837387084961 
train 17, step: 0, loss: 1.2785557754476127, grad_norm: 0.3204783346043755, ic: -0.12948874992067885
train 17, step: 500, loss: 1.7524964536754744, grad_norm: 1.251518751167395, ic: 0.24284846053950182
train 17, step: 1000, loss: 1.2839078448104455, grad_norm: 0.09367243019214323, ic: 0.1286545130486318
train 17, step: 1500, loss: 4.515873266440117, grad_norm: 3.0032956014536554, ic: 0.23080048222046826
train 17, step: 2000, loss: 1.2866085662477627, grad_norm: 1.008309398932079, ic: 0.13248377655906257
Epoch 17: 2022-05-08 02:22:33.642864: train loss: 1.6194997058722693
Eval step 0: eval loss: 0.8346707471680715
Eval: 2022-05-08 02:22:53.030645: total loss: 1.0674057215071224, mse:4.59095478764815, ic :0.18833855128735594, sharpe5:17.71671941757202, irr5:596.8460693359375, ndcg5:0.8593972097497666, pnl5:7.247015953063965 
train 18, step: 0, loss: 1.4233317761605035, grad_norm: 1.5291269511094996, ic: 0.15201586615768223
train 18, step: 500, loss: 1.5286290843800443, grad_norm: 1.1500178021092242, ic: -0.021613997407902447
train 18, step: 1000, loss: 0.6518701840753425, grad_norm: 0.03517258006853369, ic: 0.5783883051635024
train 18, step: 1500, loss: 1.4185134175368552, grad_norm: 0.07368929850870384, ic: 0.19744815580502106
train 18, step: 2000, loss: 0.911618372437301, grad_norm: 0.017288309991676926, ic: -0.011618567981241116
Epoch 18: 2022-05-08 02:27:36.759014: train loss: 1.618003925251
Eval step 0: eval loss: 0.8224466080660563
Eval: 2022-05-08 02:27:56.213321: total loss: 1.0640803907511989, mse:4.593297401100077, ic :0.19075048656165675, sharpe5:17.78810653924942, irr5:599.14794921875, ndcg5:0.8475644621761822, pnl5:5.1686177253723145 
train 19, step: 0, loss: 1.4947593083457342, grad_norm: 1.2373508391873402, ic: -0.0318058591450557
train 19, step: 500, loss: 0.8595260337547019, grad_norm: 0.03279471734096382, ic: 0.2294187650746738
train 19, step: 1000, loss: 0.954523293647969, grad_norm: 0.019573738611073702, ic: 0.2046270300652799
train 19, step: 1500, loss: 3.9507680049850666, grad_norm: 1.4870753764473166, ic: 0.15045329779087677
train 19, step: 2000, loss: 1.0097654371995193, grad_norm: 0.311901139010416, ic: 0.21863510346750725
Epoch 19: 2022-05-08 02:32:32.789184: train loss: 1.6184538953891558
Eval step 0: eval loss: 0.8305548938232679
Eval: 2022-05-08 02:32:52.063268: total loss: 1.0663336216584376, mse:4.5923339639261815, ic :0.18932909244782292, sharpe5:17.16263208031654, irr5:574.7196044921875, ndcg5:0.8458703431061796, pnl5:5.0099945068359375 
train 20, step: 0, loss: 2.316187391921937, grad_norm: 2.7556098924153214, ic: 0.04620023100655163
train 20, step: 500, loss: 3.211534090909091, grad_norm: 0.5448238689822974, ic: 0.07985693643824075
train 20, step: 1000, loss: 0.9764178276062012, grad_norm: 0.1372677476129086, ic: 0.07318098352266339
train 20, step: 1500, loss: 1.7064532386581164, grad_norm: 2.548236194056603, ic: 0.2796991203144091
train 20, step: 2000, loss: 1.0218718934798126, grad_norm: 0.0914592913200745, ic: 0.0933526249737116
Epoch 20: 2022-05-08 02:37:29.536930: train loss: 1.6177962625439626
Eval step 0: eval loss: 0.8304842113820469
Eval: 2022-05-08 02:37:48.550202: total loss: 1.065670825159446, mse:4.591185182421216, ic :0.18635017152690586, sharpe5:16.73116320133209, irr5:563.754150390625, ndcg5:0.8499600793335791, pnl5:7.89708948135376 
train 21, step: 0, loss: 1.0145303710192601, grad_norm: 0.6948114928321535, ic: 0.053482818693042
train 21, step: 500, loss: 0.7618500025926438, grad_norm: 0.032829477932170184, ic: 0.2216589583873509
train 21, step: 1000, loss: 0.9341009708873013, grad_norm: 1.241627750690193, ic: 0.14980465595801554
train 21, step: 1500, loss: 0.9863228519843412, grad_norm: 0.6384897278611368, ic: 0.31475777953500117
train 21, step: 2000, loss: 0.9456988532149778, grad_norm: 0.19591210974883644, ic: 0.052493825204651144
Epoch 21: 2022-05-08 02:42:30.724449: train loss: 1.6171611497991902
Eval step 0: eval loss: 0.8278903780295047
Eval: 2022-05-08 02:42:50.189458: total loss: 1.0661004205704776, mse:4.593235543798919, ic :0.1874709390149338, sharpe5:18.25510438799858, irr5:596.0352783203125, ndcg5:0.8566149705351235, pnl5:6.670212268829346 
train 22, step: 0, loss: 1.0408013122903426, grad_norm: 0.023962868340375205, ic: 0.21157148087790711
train 22, step: 500, loss: 3.248851348132622, grad_norm: 1.964320622503795, ic: -0.21005579068390517
train 22, step: 1000, loss: 1.1904452108923411, grad_norm: 0.28398489214971173, ic: 0.46263182440179146
train 22, step: 1500, loss: 0.9755540887024177, grad_norm: 0.24545421417625174, ic: 0.10210657102575875
train 22, step: 2000, loss: 1.792970687624008, grad_norm: 3.394904104378914, ic: 0.12866287135080234
Epoch 22: 2022-05-08 02:47:28.779051: train loss: 1.6167586125223417
Eval step 0: eval loss: 0.8262835262447312
Eval: 2022-05-08 02:47:48.755442: total loss: 1.0660962189558119, mse:4.596847981894347, ic :0.18694879110893914, sharpe5:17.913596827983856, irr5:595.385009765625, ndcg5:0.8472520370287495, pnl5:6.341948509216309 
train 23, step: 0, loss: 0.978964787463977, grad_norm: 0.07126578864669328, ic: 0.1775972969696731
train 23, step: 500, loss: 1.4186036306048864, grad_norm: 0.20079027514870507, ic: 0.06663793708225117
train 23, step: 1000, loss: 1.6512746175130208, grad_norm: 0.1278610444270113, ic: 0.26086438882993374
train 23, step: 1500, loss: 1.1184519535746833, grad_norm: 1.0814150260888493, ic: 0.08668665925195378
train 23, step: 2000, loss: 1.9135550483545034, grad_norm: 2.1364026166828234, ic: 0.4411154735170494
Epoch 23: 2022-05-08 02:52:34.780198: train loss: 1.6152850788321804
Eval step 0: eval loss: 0.8331898246097865
Eval: 2022-05-08 02:52:54.417865: total loss: 1.0661943914701604, mse:4.580817314276149, ic :0.19150320191084497, sharpe5:17.417944997549057, irr5:591.407470703125, ndcg5:0.8482265142439884, pnl5:6.610021114349365 
train 24, step: 0, loss: 2.2058599606446774, grad_norm: 0.13862856534011764, ic: 0.1108638313978873
train 24, step: 500, loss: 1.2275536919382297, grad_norm: 0.3334683773983168, ic: 0.06338329225072344
train 24, step: 1000, loss: 0.9011352456638252, grad_norm: 0.13073095095002565, ic: 0.5310123418651114
train 24, step: 1500, loss: 2.6049283397954452, grad_norm: 4.553742646288963, ic: 0.057032268042530865
train 24, step: 2000, loss: 0.9345324080264363, grad_norm: 0.13014388855400372, ic: 0.0761146988351152
Epoch 24: 2022-05-08 02:57:38.147920: train loss: 1.6139052885715344
Eval step 0: eval loss: 0.8247714751136064
Eval: 2022-05-08 02:57:57.971386: total loss: 1.0660916570261303, mse:4.611927155280413, ic :0.18465599496645158, sharpe5:17.961528775691985, irr5:590.0851440429688, ndcg5:0.8597544673933932, pnl5:10.825201988220215 
train 25, step: 0, loss: 0.8380780194256757, grad_norm: 0.08338702360720915, ic: 0.6163450109171178
train 25, step: 500, loss: 0.8635968441689482, grad_norm: 0.011225623348175777, ic: 0.27147170255890657
train 25, step: 1000, loss: 2.1342271396610815, grad_norm: 0.5480659784450788, ic: 0.22047948007559673
train 25, step: 1500, loss: 1.1405909722809542, grad_norm: 0.46382685234346727, ic: 0.535235730765199
train 25, step: 2000, loss: 1.020097127645732, grad_norm: 0.45365183983459567, ic: 0.6062472687108518
Epoch 25: 2022-05-08 03:02:36.887436: train loss: 1.615639065498531
Eval step 0: eval loss: 0.8335075418614989
Eval: 2022-05-08 03:02:54.893398: total loss: 1.0717335469415639, mse:4.7004716093959065, ic :0.17705131180686456, sharpe5:16.24878838419914, irr5:576.3209838867188, ndcg5:0.8455816560688417, pnl5:3.1935324668884277 
train 26, step: 0, loss: 6.889824717951278, grad_norm: 5.16288055393913, ic: 0.09582855612336974
train 26, step: 500, loss: 3.9192026241204063, grad_norm: 3.546047878713028, ic: 0.3512086204209298
train 26, step: 1000, loss: 1.2682503740678965, grad_norm: 1.8436351869727905, ic: 0.021686803441058736
train 26, step: 1500, loss: 0.8279647109996384, grad_norm: 0.2649669560037308, ic: 0.3090398665315463
train 26, step: 2000, loss: 0.9649918478928993, grad_norm: 0.730104181830944, ic: 0.11704255299926666
Epoch 26: 2022-05-08 03:07:42.219492: train loss: 1.6135757625858154
Eval step 0: eval loss: 0.8265627829870258
Eval: 2022-05-08 03:08:01.727421: total loss: 1.064816757965792, mse:4.588508734691862, ic :0.1914308918506032, sharpe5:17.49145288467407, irr5:590.2808227539062, ndcg5:0.8437245731713636, pnl5:5.279072284698486 
train 27, step: 0, loss: 0.8254704733455882, grad_norm: 0.031396068672040596, ic: 0.12363931311790359
train 27, step: 500, loss: 0.9030705594468391, grad_norm: 2.4398674031327365, ic: 0.3033185975709756
train 27, step: 1000, loss: 0.7523666346432715, grad_norm: 0.6125812321423083, ic: 0.1871327128356961
train 27, step: 1500, loss: 0.643210327015608, grad_norm: 0.06752129671396187, ic: 0.5045610777093279
train 27, step: 2000, loss: 1.3809172124145785, grad_norm: 0.0704134641560989, ic: 0.03704156812783338
Epoch 27: 2022-05-08 03:12:18.568064: train loss: 1.6135562269538224
Eval step 0: eval loss: 0.8327617424180057
Eval: 2022-05-08 03:12:35.049853: total loss: 1.0671600120386417, mse:4.59280862098044, ic :0.1877711462963441, sharpe5:18.00446434378624, irr5:587.1742553710938, ndcg5:0.8439734510344266, pnl5:7.980647563934326 
train 28, step: 0, loss: 1.5502702514177122, grad_norm: 1.1144820288429311, ic: 0.15826661381103757
train 28, step: 500, loss: 1.3570774306525992, grad_norm: 2.102500251079722, ic: 0.16901770180520115
train 28, step: 1000, loss: 0.9229016807990346, grad_norm: 0.37147303762378464, ic: 0.5760529130502705
train 28, step: 1500, loss: 1.0404899733513016, grad_norm: 0.20431604547878213, ic: 0.0006260625109119448
train 28, step: 2000, loss: 1.0426729354390338, grad_norm: 0.2925153677605943, ic: 0.09285243530022888
Epoch 28: 2022-05-08 03:16:16.285619: train loss: 1.6101231788899717
Eval step 0: eval loss: 0.8221180858551764
Eval: 2022-05-08 03:16:32.545916: total loss: 1.0644436655796559, mse:4.594125198518418, ic :0.19428585636547577, sharpe5:17.826919763088224, irr5:602.1312255859375, ndcg5:0.8508133432957103, pnl5:9.751256942749023 
train 29, step: 0, loss: 0.9101329279994314, grad_norm: 0.07512312641554826, ic: 0.07771926236656815
train 29, step: 500, loss: 1.106305250614724, grad_norm: 0.19325283190950274, ic: 0.6128969840622799
train 29, step: 1000, loss: 1.0832438962970068, grad_norm: 0.4621976339485728, ic: 0.06400235823358744
train 29, step: 1500, loss: 2.3309792154566744, grad_norm: 0.4178471870238118, ic: -0.059292604208940995
train 29, step: 2000, loss: 4.3199997890142745, grad_norm: 5.599783138268505, ic: 0.21119748980192274
Epoch 29: 2022-05-08 03:20:13.438755: train loss: 1.6105391687575017
Eval step 0: eval loss: 0.8303632344285102
Eval: 2022-05-08 03:20:29.763593: total loss: 1.0660046518576596, mse:4.585322500403264, ic :0.19186839870484942, sharpe5:18.22143700003624, irr5:615.4127807617188, ndcg5:0.832805544377117, pnl5:6.119035243988037 
train 30, step: 0, loss: 1.0058943179143127, grad_norm: 0.09055029613072, ic: 0.5140828598923537
train 30, step: 500, loss: 1.4161563212994772, grad_norm: 2.4766179306778167, ic: 0.018200800897211544
train 30, step: 1000, loss: 0.9779116543856534, grad_norm: 0.07971315563388585, ic: -0.04628989998761439
train 30, step: 1500, loss: 1.4930958025073646, grad_norm: 2.7472756157661102, ic: 0.18855935128563772
train 30, step: 2000, loss: 1.8575627181761667, grad_norm: 1.5768511054343624, ic: 0.023399521419861895
Epoch 30: 2022-05-08 03:24:10.025683: train loss: 1.6100860128409857
Eval step 0: eval loss: 0.83719589185162
Eval: 2022-05-08 03:24:26.042619: total loss: 1.0704747006159288, mse:4.659528312042245, ic :0.1964208219066659, sharpe5:18.26394690155983, irr5:616.6688842773438, ndcg5:0.8543025707434303, pnl5:3.8654162883758545 
train 31, step: 0, loss: 1.0440782537920632, grad_norm: 0.23208216826542322, ic: 0.3602028264658753
train 31, step: 500, loss: 1.489745189525463, grad_norm: 1.9253313001795167, ic: 0.027018485940204267
train 31, step: 1000, loss: 4.597692080161983, grad_norm: 9.824131460099776, ic: 0.4646199541410383
train 31, step: 1500, loss: 0.7649649100678263, grad_norm: 0.1491118701270834, ic: 0.7162363035584188
train 31, step: 2000, loss: 1.2598630215259308, grad_norm: 3.6472224596019878, ic: 0.1931445848397268
Epoch 31: 2022-05-08 03:28:06.302185: train loss: 1.6088949992995236
Eval step 0: eval loss: 0.8357495708887644
Eval: 2022-05-08 03:28:22.576249: total loss: 1.067371732896187, mse:4.593267813432498, ic :0.18470331746253607, sharpe5:17.602332404851914, irr5:579.9739990234375, ndcg5:0.8423536588911209, pnl5:4.2210798263549805 
train 32, step: 0, loss: 1.1254620213854247, grad_norm: 0.05756404641299284, ic: 0.19394480038923556
train 32, step: 500, loss: 1.4764909879429133, grad_norm: 1.8025863610590482, ic: 0.04589348153555032
train 32, step: 1000, loss: 1.0408965724949784, grad_norm: 0.12467943480072281, ic: 0.5074752870639443
train 32, step: 1500, loss: 0.963589712647788, grad_norm: 1.8745409237870492, ic: 0.047916807045387734
train 32, step: 2000, loss: 0.9373255923234115, grad_norm: 0.08579178064719487, ic: 0.5635094110786539
Epoch 32: 2022-05-08 03:32:04.177651: train loss: 1.610050139623153
Eval step 0: eval loss: 0.8212807015196917
Eval: 2022-05-08 03:32:20.534941: total loss: 1.063188132348816, mse:4.592749928011457, ic :0.19512477084744273, sharpe5:18.486867538690568, irr5:623.6421508789062, ndcg5:0.8590194097296309, pnl5:6.206212997436523 
train 33, step: 0, loss: 1.2756876715487762, grad_norm: 0.8970536017648125, ic: 0.1929502906562902
train 33, step: 500, loss: 0.992661979564553, grad_norm: 0.0480368684533376, ic: 0.1303868927577466
train 33, step: 1000, loss: 1.0335364960142015, grad_norm: 3.8013656317381663, ic: 0.24161777108030535
train 33, step: 1500, loss: 0.891872347178974, grad_norm: 0.2918185079479918, ic: 0.5565281967633167
train 33, step: 2000, loss: 0.8151095189307647, grad_norm: 0.13913094870303516, ic: 0.2487590452069016
Epoch 33: 2022-05-08 03:36:00.314672: train loss: 1.6086414123426551
Eval step 0: eval loss: 0.8241904512974183
Eval: 2022-05-08 03:36:16.852569: total loss: 1.0645362680519481, mse:4.577349988458679, ic :0.19661430689427045, sharpe5:18.05066713929176, irr5:610.3582153320312, ndcg5:0.84904749634377, pnl5:5.795685291290283 
train 34, step: 0, loss: 0.9847968668676266, grad_norm: 0.4234955806605058, ic: 0.6116959753565874
train 34, step: 500, loss: 0.7934101816346522, grad_norm: 0.18684010946219348, ic: 0.21597711412756704
train 34, step: 1000, loss: 3.2280877226142475, grad_norm: 9.924701972361584, ic: 0.32522335571321587
train 34, step: 1500, loss: 0.8036938418364497, grad_norm: 1.432127991322014, ic: 0.688441381225144
train 34, step: 2000, loss: 5.495431487824361, grad_norm: 76.11949482531217, ic: 0.4408657954982494
Epoch 34: 2022-05-08 03:40:00.828450: train loss: 1.607034097015545
Eval step 0: eval loss: 0.8225003112857283
Eval: 2022-05-08 03:40:17.199832: total loss: 1.0645659995928438, mse:4.589981123527694, ic :0.19582165673087157, sharpe5:17.926410337686537, irr5:603.8934326171875, ndcg5:0.8385600050891472, pnl5:7.845794677734375 
train 35, step: 0, loss: 1.1853392118566175, grad_norm: 0.6832199529666452, ic: 0.5501987305113658
train 35, step: 500, loss: 1.1882318230018398, grad_norm: 0.5895171168166289, ic: 0.14671738005774349
train 35, step: 1000, loss: 1.6398653649980095, grad_norm: 3.919328215269828, ic: 0.07205292711008858
train 35, step: 1500, loss: 1.6232054746240603, grad_norm: 1.8493563997734563, ic: 0.033234238931320624
train 35, step: 2000, loss: 0.795635855771641, grad_norm: 2.384723726025724, ic: 0.5354452198130559
Epoch 35: 2022-05-08 03:43:56.343251: train loss: 1.6072601855842892
Eval step 0: eval loss: 0.8314282303740779
Eval: 2022-05-08 03:44:12.719869: total loss: 1.0662838503735124, mse:4.587576018338972, ic :0.19473254686262215, sharpe5:18.297300242185592, irr5:601.8871459960938, ndcg5:0.8452781052061326, pnl5:9.16696834564209 
train 36, step: 0, loss: 1.848548877183085, grad_norm: 3.3972424779479033, ic: 0.07310145452268926
train 36, step: 500, loss: 0.8310616134751774, grad_norm: 0.06507578865953279, ic: 0.24866510834092026
train 36, step: 1000, loss: 1.7996051136363636, grad_norm: 11.091374343758961, ic: 0.2222147870188356
train 36, step: 1500, loss: 0.7630315412554824, grad_norm: 0.04730563069648348, ic: 0.38500689684362943
train 36, step: 2000, loss: 1.1361468908235701, grad_norm: 1.7845244817389676, ic: 0.7721734424386114
Epoch 36: 2022-05-08 03:47:53.160940: train loss: 1.6020929826581345
Eval step 0: eval loss: 0.8278981601727147
Eval: 2022-05-08 03:48:09.352798: total loss: 1.065251419265569, mse:4.583877135696664, ic :0.1945722978597187, sharpe5:18.357489495277402, irr5:610.0352783203125, ndcg5:0.8402282785556993, pnl5:5.66342830657959 
train 37, step: 0, loss: 2.016509776093511, grad_norm: 3.377853275237188, ic: 0.21100531353374916
train 37, step: 500, loss: 2.338868382315457, grad_norm: 2.112400417051938, ic: -0.034790795784113315
train 37, step: 1000, loss: 1.072930230034722, grad_norm: 0.14770779200559675, ic: 0.07668470630383462
train 37, step: 1500, loss: 2.0242827937966053, grad_norm: 2.4431298303967726, ic: 0.5795859770679297
train 37, step: 2000, loss: 1.3211480327618874, grad_norm: 0.24834592224590907, ic: 0.1791917990960773
Epoch 37: 2022-05-08 03:51:54.483568: train loss: 1.6041091343992875
Eval step 0: eval loss: 0.8247274834941385
Eval: 2022-05-08 03:52:10.612940: total loss: 1.0656764870526618, mse:4.5913995972300885, ic :0.19623891250410075, sharpe5:18.65544095993042, irr5:617.3858642578125, ndcg5:0.8622786228892823, pnl5:5.937302112579346 
train 38, step: 0, loss: 1.3391561740782203, grad_norm: 0.5337986767890768, ic: -0.06684700075621212
train 38, step: 500, loss: 0.9088710455246913, grad_norm: 0.0859489121773033, ic: 0.2596486346958029
train 38, step: 1000, loss: 0.9081010298295454, grad_norm: 0.30582016773001097, ic: 0.19888344582571746
train 38, step: 1500, loss: 0.9486831977861617, grad_norm: 0.021862747205177377, ic: 0.20533401281962727
train 38, step: 2000, loss: 2.313378463699018, grad_norm: 9.125338579363461, ic: 0.04951510278846319
Epoch 38: 2022-05-08 03:55:51.108167: train loss: 1.6006043525035558
Eval step 0: eval loss: 0.82539726233206
Eval: 2022-05-08 03:56:07.296289: total loss: 1.063160351431311, mse:4.575327545198851, ic :0.19998039152004415, sharpe5:19.067425661087036, irr5:624.016357421875, ndcg5:0.8800315376019514, pnl5:9.588418960571289 
train 39, step: 0, loss: 0.9671680322116304, grad_norm: 0.011466055801166889, ic: 0.0742838696102521
train 39, step: 500, loss: 0.8860559485491929, grad_norm: 0.3069017598491931, ic: 0.24383928033799546
train 39, step: 1000, loss: 0.94156894843631, grad_norm: 0.11411868529740077, ic: 0.18272971782422717
train 39, step: 1500, loss: 2.0620465407091917, grad_norm: 0.24299561487256427, ic: 0.1809762814145831
train 39, step: 2000, loss: 0.6140010405867398, grad_norm: 0.1072805222571117, ic: 0.14760371698880495
Epoch 39: 2022-05-08 03:59:45.809340: train loss: 1.6006666842975468
Eval step 0: eval loss: 0.8284712732152265
Eval: 2022-05-08 04:00:01.765086: total loss: 1.0645240845743442, mse:4.58325276889423, ic :0.19473275189147127, sharpe5:19.15464947104454, irr5:626.5374145507812, ndcg5:0.8446788198140545, pnl5:5.977406978607178 
