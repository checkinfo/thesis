Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
45847
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884462882989641, grad_norm: 5.086887097223137, ic: 0.01135570097474664
train 0, step: 500, loss: 0.8626367702642238, grad_norm: 0.02187076102330048, ic: 0.053262468915923254
train 0, step: 1000, loss: 1.9455518185351135, grad_norm: 0.5895793130225642, ic: 0.007516800660549546
train 0, step: 1500, loss: 0.9753736413043478, grad_norm: 0.12012494934367368, ic: 0.07303705286936496
train 0, step: 2000, loss: 0.9941971341511429, grad_norm: 0.159249652636769, ic: 0.02314321835354223
Epoch 0: 2022-05-06 22:23:13.131971: train loss: 1.6493768363704882
Eval step 0: eval loss: 0.8351406985766266
Eval: 2022-05-06 22:23:32.926553: total loss: 1.0789988737651839, mse:4.823402029963331, ic :0.007249053877335529, sharpe5:7.370490453243256, irr5:207.50608825683594, ndcg5:0.8563264163786725, pnl5:2.6393158435821533 
train 1, step: 0, loss: 2.7625320926789314, grad_norm: 0.95414426175997, ic: 0.06195659577928151
train 1, step: 500, loss: 1.76438294033197, grad_norm: 0.858502153046054, ic: 0.10936084960270538
train 1, step: 1000, loss: 0.8746915485093784, grad_norm: 0.19424832191594957, ic: 0.05959455687880531
train 1, step: 1500, loss: 1.7122151692708334, grad_norm: 0.2371257039372675, ic: -0.022272892558437188
train 1, step: 2000, loss: 2.1932544921875, grad_norm: 0.9748457712265541, ic: 0.04466040577052269
Epoch 1: 2022-05-06 22:28:06.345306: train loss: 1.6468986954520755
Eval step 0: eval loss: 0.8340184620735642
Eval: 2022-05-06 22:28:25.767660: total loss: 1.0788345830872585, mse:4.824005999879685, ic :0.00836522784736281, sharpe5:7.199254251122475, irr5:203.69532775878906, ndcg5:0.8445084415077594, pnl5:2.546624183654785 
train 2, step: 0, loss: 2.1430390624999998, grad_norm: 0.010885725288705198, ic: 0.12701746484235332
train 2, step: 500, loss: 3.3056717038341157, grad_norm: 0.31026643134210374, ic: 0.056549981200861354
train 2, step: 1000, loss: 2.072408554837165, grad_norm: 0.00026024197791691084, ic: 0.2130516235721029
train 2, step: 1500, loss: 1.486340052480916, grad_norm: 0.07090732210809454, ic: -0.01332145646032705
train 2, step: 2000, loss: 3.2358875450721154, grad_norm: 0.8788634386204659, ic: 0.23558110882440458
Epoch 2: 2022-05-06 22:33:02.841906: train loss: 1.6467399411378656
Eval step 0: eval loss: 0.8359942260356296
Eval: 2022-05-06 22:33:22.860743: total loss: 1.079474063752636, mse:4.822818486435482, ic :0.013355825650598147, sharpe5:7.66972412019968, irr5:215.84185791015625, ndcg5:0.8597369232403825, pnl5:2.747175931930542 
train 3, step: 0, loss: 1.5239251857850609, grad_norm: 0.6091544408688574, ic: 0.009173650649818137
train 3, step: 500, loss: 1.4951305703063822, grad_norm: 0.3887385296798599, ic: 0.1448834938062211
train 3, step: 1000, loss: 3.668800261765976, grad_norm: 0.7939148885716468, ic: -0.0534256442664106
train 3, step: 1500, loss: 2.0233184265036828, grad_norm: 1.1048194788152406, ic: -0.049186658344386694
train 3, step: 2000, loss: 0.9004027660472973, grad_norm: 0.0005863465504235345, ic: -0.010818351129292438
Epoch 3: 2022-05-06 22:38:02.917633: train loss: 1.6462596219647478
Eval step 0: eval loss: 0.8334297204293993
Eval: 2022-05-06 22:38:22.739062: total loss: 1.0790089564713476, mse:4.826530209331495, ic :0.015931469650657976, sharpe5:9.420750381946563, irr5:259.8286437988281, ndcg5:0.8550280933558022, pnl5:2.118769645690918 
train 4, step: 0, loss: 1.435712890625, grad_norm: 0.05345655587073319, ic: 0.1317078210247313
train 4, step: 500, loss: 1.6478642501230314, grad_norm: 0.6584744753077774, ic: 0.03260490099436967
train 4, step: 1000, loss: 2.955025835735042, grad_norm: 0.7388849212840911, ic: 0.04476137012941336
train 4, step: 1500, loss: 2.14499192378692, grad_norm: 0.4941139768545969, ic: 0.0021316685318851607
train 4, step: 2000, loss: 1.080649843112121, grad_norm: 0.47112571234228284, ic: 0.26215030876404194
Epoch 4: 2022-05-06 22:42:54.578354: train loss: 1.6453462879109155
Eval step 0: eval loss: 0.849295516662276
Eval: 2022-05-06 22:43:14.396029: total loss: 1.0839755417442387, mse:4.8179137922029325, ic :0.055953141078624806, sharpe5:10.528188180327415, irr5:324.6694641113281, ndcg5:0.8608190429608654, pnl5:2.509449005126953 
train 5, step: 0, loss: 1.3478861354341443, grad_norm: 0.13287861543858065, ic: 0.11133442114486236
train 5, step: 500, loss: 0.8879487463446669, grad_norm: 0.009942434818184681, ic: 0.16464623647127938
train 5, step: 1000, loss: 0.9784439730004789, grad_norm: 0.16635714175147487, ic: 0.008665593252977756
train 5, step: 1500, loss: 1.5359580257585184, grad_norm: 0.1854824508162884, ic: 0.016611012660659986
train 5, step: 2000, loss: 1.101373659209143, grad_norm: 0.02881155252662918, ic: 0.17003371367182624
Epoch 5: 2022-05-06 22:47:56.877517: train loss: 1.6404434218666089
Eval step 0: eval loss: 0.8344349675233799
Eval: 2022-05-06 22:48:16.932161: total loss: 1.0766341483361346, mse:4.726539037360693, ic :0.13227068530259634, sharpe5:11.376605032682418, irr5:385.5628967285156, ndcg5:0.8491851246398475, pnl5:3.4780941009521484 
train 6, step: 0, loss: 1.3333762623666767, grad_norm: 0.48345738675099953, ic: 0.1089230560360494
train 6, step: 500, loss: 1.0063863523521248, grad_norm: 0.04826047363746129, ic: 0.042061879789435666
train 6, step: 1000, loss: 1.117669840185361, grad_norm: 0.09281496727556689, ic: 0.1295177194924777
train 6, step: 1500, loss: 1.5718424142884813, grad_norm: 0.8097637907785968, ic: 0.14715817105432955
train 6, step: 2000, loss: 0.8069743392370146, grad_norm: 0.04771202709807893, ic: 0.021840565568016303
Epoch 6: 2022-05-06 22:52:54.694693: train loss: 1.6364958325561203
Eval step 0: eval loss: 0.8287237105053016
Eval: 2022-05-06 22:53:14.900181: total loss: 1.0734127403508769, mse:4.717245265516412, ic :0.13815472729842831, sharpe5:11.942728328704833, irr5:402.1246032714844, ndcg5:0.8572002799146328, pnl5:3.740642786026001 
train 7, step: 0, loss: 0.9837916374206543, grad_norm: 0.04596854262977882, ic: 0.10386231495862122
train 7, step: 500, loss: 0.6513205299261493, grad_norm: 0.0019999142975035225, ic: 0.051599471166465856
train 7, step: 1000, loss: 1.010625881360672, grad_norm: 0.19233685010597656, ic: 0.08787156498937414
train 7, step: 1500, loss: 2.239401732482583, grad_norm: 0.686945358330293, ic: 0.4390834788512597
train 7, step: 2000, loss: 0.9091013968472335, grad_norm: 0.052155666375799936, ic: -0.06885071671404588
Epoch 7: 2022-05-06 22:57:59.543895: train loss: 1.633310964474189
Eval step 0: eval loss: 0.8278114632384417
Eval: 2022-05-06 22:58:17.920740: total loss: 1.0719550139416847, mse:4.673187293519583, ic :0.16918807356854024, sharpe5:15.853345085978507, irr5:508.947998046875, ndcg5:0.8627741510928358, pnl5:4.136504650115967 
train 8, step: 0, loss: 3.5944902060688406, grad_norm: 1.3387626198951053, ic: 0.17778160287925512
train 8, step: 500, loss: 2.744419828374335, grad_norm: 0.9576021469372864, ic: 0.03161100926379179
train 8, step: 1000, loss: 3.0590041893115942, grad_norm: 0.9657600425376432, ic: 0.10907248396317909
train 8, step: 1500, loss: 0.7121509782483045, grad_norm: 0.017773713516333896, ic: 0.4966628024231417
train 8, step: 2000, loss: 1.0848065321853675, grad_norm: 0.46974172175422546, ic: 0.5553296760205739
Epoch 8: 2022-05-06 23:02:59.076869: train loss: 1.6280437952302445
Eval step 0: eval loss: 0.8234738509697708
Eval: 2022-05-06 23:03:19.124499: total loss: 1.0698592447940998, mse:4.665294383576824, ic :0.17341576195968392, sharpe5:17.082688114643098, irr5:550.4716796875, ndcg5:0.8376849575495364, pnl5:11.714065551757812 
train 9, step: 0, loss: 5.45876009270335, grad_norm: 0.7877373954535001, ic: 0.13878267976441708
train 9, step: 500, loss: 1.3495941892194978, grad_norm: 1.1007853862561872, ic: 0.3291694699315096
train 9, step: 1000, loss: 0.9285098923055213, grad_norm: 0.24745591857084293, ic: 0.07614498958730807
train 9, step: 1500, loss: 1.086494007588135, grad_norm: 0.021819296128509073, ic: 0.46335811155492906
train 9, step: 2000, loss: 1.070455451846687, grad_norm: 0.3096852281031196, ic: 0.27661976862467086
Epoch 9: 2022-05-06 23:07:56.994842: train loss: 1.6261235545164825
Eval step 0: eval loss: 0.8238536967366965
Eval: 2022-05-06 23:08:17.154760: total loss: 1.068955704779257, mse:4.613651572612678, ic :0.18221041193635798, sharpe5:17.711104402542112, irr5:581.687744140625, ndcg5:0.8464237639623149, pnl5:4.593121528625488 
train 10, step: 0, loss: 7.095822704081632, grad_norm: 1.171805861115184, ic: 0.25499941996720743
train 10, step: 500, loss: 1.1300904470111361, grad_norm: 0.7486867566028343, ic: 0.05742168182508074
train 10, step: 1000, loss: 2.365245234015529, grad_norm: 0.7772731419910405, ic: 0.1275122514550389
train 10, step: 1500, loss: 1.1095930817839388, grad_norm: 0.27622090878960626, ic: 0.0028287035297427134
train 10, step: 2000, loss: 2.676613294607238, grad_norm: 2.5162557582002307, ic: 0.5311920462615024
Epoch 10: 2022-05-06 23:13:02.277116: train loss: 1.6242017083440323
Eval step 0: eval loss: 0.8250780015147523
Eval: 2022-05-06 23:13:22.834018: total loss: 1.0682533837470847, mse:4.594508932140844, ic :0.1814024939311378, sharpe5:17.659305510520934, irr5:565.569091796875, ndcg5:0.8634113247140299, pnl5:5.450929164886475 
train 11, step: 0, loss: 1.255788757371423, grad_norm: 0.8664962778723364, ic: 0.20302989225499973
train 11, step: 500, loss: 0.6393654139649115, grad_norm: 0.039370660388961955, ic: 0.6555722839809026
train 11, step: 1000, loss: 0.9407825139761954, grad_norm: 0.12938832184517207, ic: 0.043617584298980536
train 11, step: 1500, loss: 1.0524796603018778, grad_norm: 0.049676044658583725, ic: 0.18548553964878345
train 11, step: 2000, loss: 0.7886579130874802, grad_norm: 0.0529144781631604, ic: 0.13120316685493955
Epoch 11: 2022-05-06 23:18:00.598121: train loss: 1.6223720611521304
Eval step 0: eval loss: 0.8276585216140344
Eval: 2022-05-06 23:18:20.760228: total loss: 1.06809343300108, mse:4.587875935974312, ic :0.18450854764417682, sharpe5:18.656120134592054, irr5:599.7470092773438, ndcg5:0.8299840203693888, pnl5:5.376330852508545 
train 12, step: 0, loss: 0.9507482846577961, grad_norm: 0.3154822827235857, ic: 0.413134151949352
train 12, step: 500, loss: 0.9367983706362614, grad_norm: 0.5792702215804277, ic: 0.16867629298507034
train 12, step: 1000, loss: 2.9533263771397293, grad_norm: 3.384499027547403, ic: 0.37136760371655914
train 12, step: 1500, loss: 0.9444664681311882, grad_norm: 0.12430778024258157, ic: -0.13215623656472802
train 12, step: 2000, loss: 0.8750980988009819, grad_norm: 0.006172786056284795, ic: 0.19680466188067353
Epoch 12: 2022-05-06 23:23:00.416561: train loss: 1.6208010114993858
Eval step 0: eval loss: 0.8270988504552489
Eval: 2022-05-06 23:23:20.281300: total loss: 1.0694987658287232, mse:4.614672858642648, ic :0.1638652148401506, sharpe5:15.773307473659514, irr5:499.73992919921875, ndcg5:0.8532393172081116, pnl5:7.5195159912109375 
train 13, step: 0, loss: 2.010961063308474, grad_norm: 0.6428084759079177, ic: 0.4399462006119346
train 13, step: 500, loss: 0.8165840263418741, grad_norm: 0.04467062767086776, ic: 0.5915666071065607
train 13, step: 1000, loss: 0.9487445600880872, grad_norm: 0.44226142234119004, ic: 0.5904860223968139
train 13, step: 1500, loss: 2.393592042349727, grad_norm: 0.48392689705377584, ic: -0.07061228449000151
train 13, step: 2000, loss: 1.5131852852554848, grad_norm: 1.4498652823796185, ic: 0.12159228706676503
Epoch 13: 2022-05-06 23:27:58.533269: train loss: 1.6238525896063318
Eval step 0: eval loss: 0.8240085035028648
Eval: 2022-05-06 23:28:17.984243: total loss: 1.0677278409416204, mse:4.600526561150991, ic :0.182467825530076, sharpe5:18.208532681465147, irr5:593.2109985351562, ndcg5:0.8515770649059045, pnl5:7.323556423187256 
train 14, step: 0, loss: 4.528656243906006, grad_norm: 1.1464630489555228, ic: 0.16656551288774096
train 14, step: 500, loss: 0.8280055428134556, grad_norm: 0.15518700896736692, ic: 0.09338832401699879
train 14, step: 1000, loss: 1.8489422055927298, grad_norm: 4.528111773758165, ic: 0.4416555968500717
train 14, step: 1500, loss: 1.126654943828493, grad_norm: 0.0814938092032555, ic: -0.07135164346245247
train 14, step: 2000, loss: 1.1486747723997608, grad_norm: 0.1965394527538764, ic: 0.09943197290930458
Epoch 14: 2022-05-06 23:32:56.688989: train loss: 1.6220039308227145
Eval step 0: eval loss: 0.83430839514456
Eval: 2022-05-06 23:33:16.617210: total loss: 1.0711332098480697, mse:4.614067422169994, ic :0.16309837471619354, sharpe5:15.728409109711647, irr5:497.2646484375, ndcg5:0.8497667046367774, pnl5:4.641826152801514 
train 15, step: 0, loss: 3.415625379985409, grad_norm: 1.076791105683102, ic: 0.050016921905375225
train 15, step: 500, loss: 1.2556450670199502, grad_norm: 0.0034229791231446304, ic: -0.04146012664675952
train 15, step: 1000, loss: 1.3162226483104675, grad_norm: 0.15202430267095354, ic: 0.01750807619546693
train 15, step: 1500, loss: 0.8508971687376968, grad_norm: 0.5702209041800768, ic: 0.0712775010441182
train 15, step: 2000, loss: 1.4716606201565017, grad_norm: 0.7432272184671161, ic: 0.05231994990195847
Epoch 15: 2022-05-06 23:38:00.023504: train loss: 1.6208342741787312
Eval step 0: eval loss: 0.8378646416458113
Eval: 2022-05-06 23:38:19.491717: total loss: 1.0716824392586646, mse:4.5956227732048065, ic :0.1781389284002353, sharpe5:16.97322369933128, irr5:552.9496459960938, ndcg5:0.8446823016770181, pnl5:7.094492435455322 
train 16, step: 0, loss: 0.7067697605063589, grad_norm: 1.0063196744506995, ic: 0.031677810254133756
train 16, step: 500, loss: 1.6223445668177607, grad_norm: 0.8369021469284215, ic: 0.18747440157750675
train 16, step: 1000, loss: 0.8810250022194602, grad_norm: 0.0494786684744576, ic: -0.13496832230361902
train 16, step: 1500, loss: 0.8647893810948583, grad_norm: 0.20973902894376276, ic: 0.14870082653571773
train 16, step: 2000, loss: 3.3385303097259493, grad_norm: 1.1005890508515006, ic: 0.037777908796147286
Epoch 16: 2022-05-06 23:42:30.028101: train loss: 1.619010223300692
Eval step 0: eval loss: 0.8327943502412077
Eval: 2022-05-06 23:42:50.814722: total loss: 1.0716844382249646, mse:4.6437679068846975, ic :0.14845784188247818, sharpe5:16.153997168540954, irr5:507.21356201171875, ndcg5:0.8477521447022854, pnl5:5.882883548736572 
train 17, step: 0, loss: 1.2877696555868698, grad_norm: 0.22787725761678423, ic: -0.1312039194042179
train 17, step: 500, loss: 1.740405471925813, grad_norm: 1.8735964421148517, ic: 0.22645124892663174
train 17, step: 1000, loss: 1.2909325188063363, grad_norm: 0.08873750226072269, ic: 0.1389290004625296
train 17, step: 1500, loss: 4.522790643942873, grad_norm: 1.7142157067892208, ic: 0.23245092378917454
train 17, step: 2000, loss: 1.2676158046191328, grad_norm: 0.6885765688550216, ic: 0.05106010031681248
Epoch 17: 2022-05-06 23:47:34.980003: train loss: 1.6210517146329428
Eval step 0: eval loss: 0.8324419670788329
Eval: 2022-05-06 23:47:55.334098: total loss: 1.0674667963867024, mse:4.584260216848004, ic :0.1903738005689594, sharpe5:17.831492367982865, irr5:589.64013671875, ndcg5:0.8423588364274724, pnl5:7.896459102630615 
train 18, step: 0, loss: 1.4158007774082908, grad_norm: 0.637777453259952, ic: 0.1336115097064659
train 18, step: 500, loss: 1.5149838106587974, grad_norm: 0.9129665287268439, ic: -0.02035462537223451
train 18, step: 1000, loss: 0.658787055864726, grad_norm: 0.040814848548067106, ic: 0.574329398544322
train 18, step: 1500, loss: 1.429762481764435, grad_norm: 0.04043611213899066, ic: 0.22729450777505417
train 18, step: 2000, loss: 0.9113039636308221, grad_norm: 0.008684787164244125, ic: -0.01214961874130479
Epoch 18: 2022-05-06 23:52:36.546141: train loss: 1.61983297380715
Eval step 0: eval loss: 0.8263675219392123
Eval: 2022-05-06 23:52:57.006922: total loss: 1.0664699038657004, mse:4.589553595554625, ic :0.1894609967012697, sharpe5:18.15989128947258, irr5:607.6680908203125, ndcg5:0.8503122573083579, pnl5:10.131927490234375 
train 19, step: 0, loss: 1.485826667906746, grad_norm: 0.6913451125099725, ic: -0.03658723058133119
train 19, step: 500, loss: 0.8646035371003327, grad_norm: 0.045182406552815614, ic: 0.22818738717247575
train 19, step: 1000, loss: 0.9548839432956056, grad_norm: 0.020470040754942978, ic: 0.208456431390702
train 19, step: 1500, loss: 3.9653066392524594, grad_norm: 0.9512401397178811, ic: 0.13853101137141327
train 19, step: 2000, loss: 1.015357666015625, grad_norm: 0.10893446248188338, ic: 0.2505635853970332
Epoch 19: 2022-05-06 23:57:41.054976: train loss: 1.6216026405597783
Eval step 0: eval loss: 0.8356139300620719
Eval: 2022-05-06 23:58:00.885195: total loss: 1.0679937641235795, mse:4.586455056260311, ic :0.19046301138015354, sharpe5:17.77610274195671, irr5:604.8190307617188, ndcg5:0.8510962343838258, pnl5:9.645511627197266 
train 20, step: 0, loss: 2.287199697381423, grad_norm: 1.1843542206136473, ic: 0.06864139451551521
train 20, step: 500, loss: 3.191303977272727, grad_norm: 1.2670576313282846, ic: 0.11178323212855958
train 20, step: 1000, loss: 0.9764141082763672, grad_norm: 0.27504514419908266, ic: 0.16850891443291605
train 20, step: 1500, loss: 1.6030338865692, grad_norm: 2.4528599724134263, ic: 0.2983367873426829
train 20, step: 2000, loss: 1.0171159305541395, grad_norm: 0.2044100824969567, ic: 0.06366493086427227
Epoch 20: 2022-05-07 00:02:47.757564: train loss: 1.6196868117152323
Eval step 0: eval loss: 0.8273022795376712
Eval: 2022-05-07 00:03:07.750938: total loss: 1.06628317146903, mse:4.579327896918081, ic :0.19376222710953594, sharpe5:17.496407643556594, irr5:594.6769409179688, ndcg5:0.8662707875571821, pnl5:6.649781703948975 
train 21, step: 0, loss: 1.0202544537507152, grad_norm: 0.34095748637651996, ic: 0.09751703329122643
train 21, step: 500, loss: 0.7692663142111449, grad_norm: 0.015625979045515742, ic: 0.188547034632626
train 21, step: 1000, loss: 0.947270443564967, grad_norm: 0.8740932283717937, ic: 0.14452633882787236
train 21, step: 1500, loss: 0.9906800327067584, grad_norm: 0.5460015714279238, ic: 0.321874115948467
train 21, step: 2000, loss: 0.9408419958644795, grad_norm: 0.09481208102808387, ic: 0.06349791242663927
Epoch 21: 2022-05-07 00:07:47.956859: train loss: 1.6203327622044894
Eval step 0: eval loss: 0.8304160372349183
Eval: 2022-05-07 00:08:08.153066: total loss: 1.0675731384136262, mse:4.58909086081774, ic :0.18670367374413152, sharpe5:18.403705533742905, irr5:610.06201171875, ndcg5:0.8514838543290199, pnl5:7.753338813781738 
train 22, step: 0, loss: 1.047915873554467, grad_norm: 0.09312576866101094, ic: 0.1986766610060149
train 22, step: 500, loss: 3.2372963509908534, grad_norm: 0.6925967548462146, ic: -0.19708151566493037
train 22, step: 1000, loss: 1.2014654082369942, grad_norm: 0.17250565664545064, ic: 0.45811362103003656
train 22, step: 1500, loss: 0.9766485018004115, grad_norm: 1.039776168762153, ic: 0.08388971099673512
train 22, step: 2000, loss: 1.7752021218643708, grad_norm: 0.6395562914408874, ic: 0.13164724845276082
Epoch 22: 2022-05-07 00:12:47.972379: train loss: 1.618670113119008
Eval step 0: eval loss: 0.8291862656620456
Eval: 2022-05-07 00:13:08.530125: total loss: 1.0686380634914876, mse:4.598857565658901, ic :0.17899018890650634, sharpe5:17.111368582248687, irr5:560.00732421875, ndcg5:0.8501095926297441, pnl5:6.52833890914917 
train 23, step: 0, loss: 0.9862559381754323, grad_norm: 0.04942932755817672, ic: 0.2037276491588955
train 23, step: 500, loss: 1.4304341985442015, grad_norm: 0.1454159750469437, ic: -0.005167629304221416
train 23, step: 1000, loss: 1.6500109863281252, grad_norm: 0.155018936040891, ic: 0.2592943100146821
train 23, step: 1500, loss: 1.1110181244903587, grad_norm: 0.22125773094976242, ic: 0.09166037117847234
train 23, step: 2000, loss: 1.574596519558314, grad_norm: 0.5031678921826346, ic: 0.34908288774222296
Epoch 23: 2022-05-07 00:17:48.718114: train loss: 1.618191665491148
Eval step 0: eval loss: 0.8302896578017979
Eval: 2022-05-07 00:18:09.411948: total loss: 1.0679998528732233, mse:4.584616157725883, ic :0.18984597397925507, sharpe5:16.976776596307754, irr5:579.3101196289062, ndcg5:0.8342293371199431, pnl5:4.485177040100098 
train 24, step: 0, loss: 2.2003153696589206, grad_norm: 0.18148295542579768, ic: 0.12849159121837517
train 24, step: 500, loss: 1.2236111533317122, grad_norm: 0.1683217250663566, ic: 0.0951610753423578
train 24, step: 1000, loss: 0.8982841908971979, grad_norm: 0.02771388046264672, ic: 0.5389536820738681
train 24, step: 1500, loss: 2.603218550340066, grad_norm: 2.512171056149869, ic: 0.06780148848708606
train 24, step: 2000, loss: 0.9224116318383251, grad_norm: 0.6460816638914981, ic: 0.15159774396319847
Epoch 24: 2022-05-07 00:22:54.094583: train loss: 1.6141787394608753
Eval step 0: eval loss: 0.8242199076741965
Eval: 2022-05-07 00:23:14.711806: total loss: 1.0674011845380924, mse:4.6096332167842435, ic :0.18729899767180913, sharpe5:17.297070826292035, irr5:585.8662719726562, ndcg5:0.8435761342416424, pnl5:5.27065896987915 
train 25, step: 0, loss: 0.8335393132390203, grad_norm: 0.0693830919817191, ic: 0.6186677373336689
train 25, step: 500, loss: 0.8710834238104229, grad_norm: 0.0056613729395961, ic: 0.23840355076210856
train 25, step: 1000, loss: 2.155665773157845, grad_norm: 2.805630107576968, ic: 0.23518286615356954
train 25, step: 1500, loss: 1.1246811140212705, grad_norm: 0.7125580486908103, ic: 0.5469893546583313
train 25, step: 2000, loss: 1.010696273475451, grad_norm: 0.42959161825504477, ic: 0.6044021533559473
Epoch 25: 2022-05-07 00:28:00.932815: train loss: 1.6173284551251206
Eval step 0: eval loss: 0.8353720404702317
Eval: 2022-05-07 00:28:20.562189: total loss: 1.0733790182702652, mse:4.6189269788187, ic :0.16469124031453655, sharpe5:15.415799580216406, irr5:555.2507934570312, ndcg5:0.8524791623630126, pnl5:2.7307353019714355 
train 26, step: 0, loss: 6.7553430136781145, grad_norm: 2.8124391553794985, ic: 0.150166273097411
train 26, step: 500, loss: 3.8441134431196247, grad_norm: 1.456599526344839, ic: 0.373456751220287
train 26, step: 1000, loss: 1.2675975757640798, grad_norm: 0.8942695380894665, ic: -0.014896336478037033
train 26, step: 1500, loss: 0.8363557872096439, grad_norm: 0.18096702764210537, ic: 0.30275442119951523
train 26, step: 2000, loss: 0.9607559792363297, grad_norm: 0.5415956244010236, ic: 0.15391319334865258
Epoch 26: 2022-05-07 00:33:01.448692: train loss: 1.615914516814386
Eval step 0: eval loss: 0.8297884491899367
Eval: 2022-05-07 00:33:21.415398: total loss: 1.067092507874277, mse:4.588571744213478, ic :0.18556313024776816, sharpe5:17.512715402841568, irr5:590.1193237304688, ndcg5:0.8489544796466446, pnl5:6.735836029052734 
train 27, step: 0, loss: 0.8262407130821078, grad_norm: 0.008713857833412851, ic: 0.12198066118091813
train 27, step: 500, loss: 0.9039583135247633, grad_norm: 5.076498673836176, ic: 0.2942142355920153
train 27, step: 1000, loss: 0.7529825091085413, grad_norm: 0.6433716945509154, ic: 0.20116815825667084
train 27, step: 1500, loss: 0.6402676886391458, grad_norm: 0.03142356843335952, ic: 0.5116805540427193
train 27, step: 2000, loss: 1.3828931387030183, grad_norm: 0.017979171970693292, ic: 0.015697806795305774
Epoch 27: 2022-05-07 00:38:05.941125: train loss: 1.6160611396359583
Eval step 0: eval loss: 0.8374146922747628
Eval: 2022-05-07 00:38:26.499653: total loss: 1.0706467767704844, mse:4.602023688924531, ic :0.17342618151656322, sharpe5:17.04553593635559, irr5:567.6488647460938, ndcg5:0.8483329884354485, pnl5:5.8721466064453125 
train 28, step: 0, loss: 1.5249464587355213, grad_norm: 0.46640305288583217, ic: 0.24269499852869106
train 28, step: 500, loss: 1.3575842715029347, grad_norm: 1.865512984591514, ic: 0.20028016815456262
train 28, step: 1000, loss: 0.9042584178893547, grad_norm: 0.7026345008699845, ic: 0.5785775901710674
train 28, step: 1500, loss: 1.0316743532075563, grad_norm: 0.06850883730986018, ic: 0.04668155887136631
train 28, step: 2000, loss: 1.0361841119871549, grad_norm: 0.10653479494676235, ic: 0.12423541915045166
Epoch 28: 2022-05-07 00:43:10.561395: train loss: 1.6130578789641175
Eval step 0: eval loss: 0.8225055208196127
Eval: 2022-05-07 00:43:30.864816: total loss: 1.0717330407285983, mse:4.625128557867145, ic :0.17207311227278713, sharpe5:17.133130549192426, irr5:570.43017578125, ndcg5:0.8484613272648501, pnl5:7.327258110046387 
train 29, step: 0, loss: 0.9062918315248294, grad_norm: 0.04986168141567879, ic: 0.09936805567769846
train 29, step: 500, loss: 1.103154790030461, grad_norm: 0.4112798401244636, ic: 0.6193097060813763
train 29, step: 1000, loss: 1.0618888052810822, grad_norm: 0.31244908444060704, ic: 0.09904453274815748
train 29, step: 1500, loss: 2.351498463114754, grad_norm: 0.2669400112077328, ic: -0.10717475844140575
train 29, step: 2000, loss: 4.410089940200617, grad_norm: 2.799406645776538, ic: 0.2098000733429254
Epoch 29: 2022-05-07 00:48:02.589347: train loss: 1.614968356521925
Eval step 0: eval loss: 0.8330824181704425
Eval: 2022-05-07 00:48:23.221595: total loss: 1.0672517051571606, mse:4.587463946520869, ic :0.18573369067056783, sharpe5:18.22056296467781, irr5:614.9741821289062, ndcg5:0.8424025924604779, pnl5:4.708837032318115 
train 30, step: 0, loss: 1.011538671384573, grad_norm: 0.059531889246916866, ic: 0.510440045514396
train 30, step: 500, loss: 1.422318819216571, grad_norm: 2.311809905246207, ic: 0.03208539029399422
train 30, step: 1000, loss: 0.9737870649857955, grad_norm: 0.05619672957147934, ic: -0.010486374919991236
train 30, step: 1500, loss: 1.5121655783425951, grad_norm: 2.906557495354382, ic: 0.16970227799190207
train 30, step: 2000, loss: 1.826144229389824, grad_norm: 1.4543050936453408, ic: 0.07023186092861405
Epoch 30: 2022-05-07 00:53:07.801752: train loss: 1.617265294176377
Eval step 0: eval loss: 0.8337936160185062
Eval: 2022-05-07 00:53:27.761315: total loss: 1.0682261818223506, mse:4.614340390802911, ic :0.1924388571340742, sharpe5:18.150449059009553, irr5:624.9550170898438, ndcg5:0.8436402397987991, pnl5:7.054089546203613 
train 31, step: 0, loss: 1.0393104579155337, grad_norm: 0.6669164759860786, ic: 0.3599381421688258
train 31, step: 500, loss: 1.5274261349022633, grad_norm: 2.1663491600249927, ic: 0.05689676474925803
train 31, step: 1000, loss: 4.394462639051523, grad_norm: 2.2817587252471547, ic: 0.4835711044456766
train 31, step: 1500, loss: 0.7697435320614182, grad_norm: 0.052792186794791106, ic: 0.7085464459173318
train 31, step: 2000, loss: 1.2413313381577697, grad_norm: 1.3885237778617592, ic: 0.16512468160228194
Epoch 31: 2022-05-07 00:58:09.565158: train loss: 1.6114037085159205
Eval step 0: eval loss: 0.8407055098088447
Eval: 2022-05-07 00:58:29.590869: total loss: 1.0698235795078255, mse:4.594149887520027, ic :0.1815831725733991, sharpe5:16.654267008304593, irr5:549.1845703125, ndcg5:0.838338449769621, pnl5:5.7414445877075195 
train 32, step: 0, loss: 1.1229758972500974, grad_norm: 0.13710539931061255, ic: 0.19338613644004082
train 32, step: 500, loss: 1.48053238035187, grad_norm: 3.1031046190036005, ic: 0.1024536334512926
train 32, step: 1000, loss: 1.0425299607926453, grad_norm: 0.1823506988806915, ic: 0.5046877935549999
train 32, step: 1500, loss: 0.9834500217510489, grad_norm: 3.5781200097143646, ic: 0.08958119725578066
train 32, step: 2000, loss: 0.9444635051183683, grad_norm: 0.07727599444685022, ic: 0.5577210509183517
Epoch 32: 2022-05-07 01:03:14.204874: train loss: 1.613734733645262
Eval step 0: eval loss: 0.8263677148849117
Eval: 2022-05-07 01:03:34.088304: total loss: 1.0644459351567317, mse:4.581882565175687, ic :0.19304805296609145, sharpe5:18.54537871956825, irr5:638.73046875, ndcg5:0.849305816276477, pnl5:5.590368270874023 
train 33, step: 0, loss: 1.255566634747345, grad_norm: 0.7007846069499171, ic: 0.2105162861253685
train 33, step: 500, loss: 0.9980686965837471, grad_norm: 0.028313834799467162, ic: 0.11067655164946485
train 33, step: 1000, loss: 1.0680678716170955, grad_norm: 1.921794487357485, ic: 0.2098049590068481
train 33, step: 1500, loss: 0.8970743091080962, grad_norm: 0.49843786492412767, ic: 0.5544235861457967
train 33, step: 2000, loss: 0.8116250015476426, grad_norm: 0.12978606334835285, ic: 0.2577120991095311
Epoch 33: 2022-05-07 01:08:26.373189: train loss: 1.6124915732927516
Eval step 0: eval loss: 0.8311371396288856
Eval: 2022-05-07 01:08:46.635104: total loss: 1.0667721168974225, mse:4.5828392254612025, ic :0.190731160106735, sharpe5:17.595133910179136, irr5:599.8988647460938, ndcg5:0.8642960505441577, pnl5:8.279363632202148 
train 34, step: 0, loss: 0.9994386121020559, grad_norm: 2.122118145916984, ic: 0.6136793058360457
train 34, step: 500, loss: 0.8179769253512041, grad_norm: 0.7625744795726705, ic: 0.2143809267763573
train 34, step: 1000, loss: 3.07812987531202, grad_norm: 1.2820258953668426, ic: 0.2948408315800013
train 34, step: 1500, loss: 0.8159463402767431, grad_norm: 0.809474804377088, ic: 0.6806113277653635
train 34, step: 2000, loss: 6.220594576031178, grad_norm: 22.105971496905887, ic: 0.45859432342352574
Epoch 34: 2022-05-07 01:13:22.224239: train loss: 1.6144191167806634
Eval step 0: eval loss: 0.8271946158373946
Eval: 2022-05-07 01:13:42.237850: total loss: 1.0684981169947156, mse:4.605077985194686, ic :0.18827060650127414, sharpe5:17.774628989696502, irr5:617.9716796875, ndcg5:0.844320562223946, pnl5:5.058335781097412 
train 35, step: 0, loss: 1.1884814453124999, grad_norm: 0.6630481809774983, ic: 0.5521753735605914
train 35, step: 500, loss: 1.1836142114996424, grad_norm: 0.6024824486035727, ic: 0.10892492241486462
train 35, step: 1000, loss: 1.5242655565784236, grad_norm: 3.9785895675818495, ic: 0.11145854198228548
train 35, step: 1500, loss: 1.6325250381813912, grad_norm: 0.7618823570862139, ic: 0.03677616459657172
train 35, step: 2000, loss: 0.7903597418637198, grad_norm: 0.3250746829407224, ic: 0.5570996805849489
Epoch 35: 2022-05-07 01:18:25.816035: train loss: 1.6098788803305
Eval step 0: eval loss: 0.8356033823638369
Eval: 2022-05-07 01:18:46.045984: total loss: 1.066906904657005, mse:4.57965134273203, ic :0.18960154738335666, sharpe5:16.672179057598115, irr5:573.0426635742188, ndcg5:0.8465175182460565, pnl5:5.226485729217529 
train 36, step: 0, loss: 1.83080843891042, grad_norm: 2.717872044252286, ic: 0.12316075696238218
train 36, step: 500, loss: 0.8335829570220155, grad_norm: 0.44028019326376955, ic: 0.19511197241644643
train 36, step: 1000, loss: 1.6626274857954544, grad_norm: 3.999982077904339, ic: 0.22701102541423648
train 36, step: 1500, loss: 0.7599496426250811, grad_norm: 0.12731189531359619, ic: 0.4008187656834237
train 36, step: 2000, loss: 1.1654885604894758, grad_norm: 0.8213532573326319, ic: 0.7492295700778345
Epoch 36: 2022-05-07 01:23:22.703565: train loss: 1.6087965456747226
Eval step 0: eval loss: 0.83270038568559
Eval: 2022-05-07 01:23:42.865682: total loss: 1.0678411111696637, mse:4.583903813118612, ic :0.18962482056993088, sharpe5:18.256371928453444, irr5:626.0106201171875, ndcg5:0.8359011506953625, pnl5:6.781182765960693 
train 37, step: 0, loss: 2.0244675266797474, grad_norm: 1.082965081753586, ic: 0.20648446778081048
train 37, step: 500, loss: 2.3440690157269577, grad_norm: 2.2287572727790157, ic: -0.06752702626057724
train 37, step: 1000, loss: 1.0819885161006468, grad_norm: 0.1034340843842104, ic: 0.05362069663697634
train 37, step: 1500, loss: 2.009220237073195, grad_norm: 0.9600720729752712, ic: 0.6157568688478184
train 37, step: 2000, loss: 1.3200897520958783, grad_norm: 0.19144747016024127, ic: 0.16033709315607692
Epoch 37: 2022-05-07 01:28:19.617198: train loss: 1.6130457005115275
Eval step 0: eval loss: 0.8259749417561248
Eval: 2022-05-07 01:28:38.175617: total loss: 1.0665598775669014, mse:4.591322333854519, ic :0.19271952072124174, sharpe5:18.42186541557312, irr5:623.0731201171875, ndcg5:0.8305922352565519, pnl5:8.042513847351074 
train 38, step: 0, loss: 1.3399855450886051, grad_norm: 0.29594793112347356, ic: -0.06460099877667672
train 38, step: 500, loss: 0.9100151909722222, grad_norm: 0.14030799184876663, ic: 0.25917089981758823
train 38, step: 1000, loss: 0.9069413136116601, grad_norm: 0.24628729434656066, ic: 0.13106124874536726
train 38, step: 1500, loss: 0.9556106185044135, grad_norm: 0.017892521831865665, ic: 0.21176665508649656
train 38, step: 2000, loss: 2.3045241249291917, grad_norm: 4.22091190062457, ic: 0.00267482384537141
Epoch 38: 2022-05-07 01:33:26.201443: train loss: 1.6091141679884144
Eval step 0: eval loss: 0.8301670086521996
Eval: 2022-05-07 01:33:46.639625: total loss: 1.0648130585971443, mse:4.581033625078454, ic :0.19639762392884608, sharpe5:18.250563944578168, irr5:620.863525390625, ndcg5:0.8558778082333758, pnl5:3.9855613708496094 
train 39, step: 0, loss: 0.9709172146765099, grad_norm: 0.024447494389296668, ic: 0.07807025323985628
train 39, step: 500, loss: 0.8812149833661607, grad_norm: 0.16750260430575797, ic: 0.22315004837632088
train 39, step: 1000, loss: 0.9360368297196306, grad_norm: 0.07316179052523018, ic: 0.1926537443290431
train 39, step: 1500, loss: 2.039864178882726, grad_norm: 0.41082886914548516, ic: 0.21339729447338918
train 39, step: 2000, loss: 0.6208777646129542, grad_norm: 0.33580373195964924, ic: 0.12807871403546633
Epoch 39: 2022-05-07 01:38:25.678978: train loss: 1.6080164129663117
Eval step 0: eval loss: 0.8323856912498353
Eval: 2022-05-07 01:38:46.289996: total loss: 1.06663004788932, mse:4.582951352561151, ic :0.19204529349778013, sharpe5:18.257107858657836, irr5:625.2852172851562, ndcg5:0.842249837144473, pnl5:3.594494342803955 
