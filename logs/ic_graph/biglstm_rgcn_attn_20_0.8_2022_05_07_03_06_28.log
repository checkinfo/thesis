Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_20_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
26408
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884383475249218, grad_norm: 5.084703873324983, ic: 0.011452360135250014
train 0, step: 500, loss: 0.8626590317680859, grad_norm: 0.02191672296635542, ic: 0.05307699718914747
train 0, step: 1000, loss: 1.9454696644111193, grad_norm: 0.5896392966902586, ic: 0.007303890740542959
train 0, step: 1500, loss: 0.9750963052742094, grad_norm: 0.1190866437647026, ic: 0.07450739683542076
train 0, step: 2000, loss: 0.994204780023848, grad_norm: 0.15925666271610883, ic: 0.023755931074845364
Epoch 0: 2022-05-07 15:12:02.343542: train loss: 1.6493764789906789
Eval step 0: eval loss: 0.8351425637183877
Eval: 2022-05-07 15:12:21.015050: total loss: 1.078998374530148, mse:4.823395460141694, ic :0.007449784468967289, sharpe5:7.5066465273499485, irr5:211.4645538330078, ndcg5:0.8563065406838727, pnl5:2.538801908493042 
train 1, step: 0, loss: 2.7625066941784273, grad_norm: 0.9541743234032547, ic: 0.06217003245154974
train 1, step: 500, loss: 1.7641066880703273, grad_norm: 0.8582906442159195, ic: 0.10942385507195738
train 1, step: 1000, loss: 0.8746876711391018, grad_norm: 0.1942235974276194, ic: 0.05964590943588846
train 1, step: 1500, loss: 1.7121325262661637, grad_norm: 0.23700427014096886, ic: -0.02207019297372069
train 1, step: 2000, loss: 2.1932296875, grad_norm: 0.9746463211525401, ic: 0.045558231828264906
Epoch 1: 2022-05-07 15:16:58.233442: train loss: 1.6468990175887148
Eval step 0: eval loss: 0.8340281093585352
Eval: 2022-05-07 15:17:16.799605: total loss: 1.0788301419808175, mse:4.823948679002137, ic :0.00862434344965115, sharpe5:7.268213654458522, irr5:206.32208251953125, ndcg5:0.8445553939283821, pnl5:2.7105631828308105 
train 2, step: 0, loss: 2.1429772727272725, grad_norm: 0.010843788182818101, ic: 0.12789595680874238
train 2, step: 500, loss: 3.3057248111062205, grad_norm: 0.3101465454844736, ic: 0.057266444737984284
train 2, step: 1000, loss: 2.0724106127274906, grad_norm: 0.0002605601374840743, ic: 0.21232827280947475
train 2, step: 1500, loss: 1.4863175020873092, grad_norm: 0.07082517688035198, ic: -0.01395046556219529
train 2, step: 2000, loss: 3.235614483173077, grad_norm: 0.8787176479308598, ic: 0.23924407207253345
Epoch 2: 2022-05-07 15:21:48.801469: train loss: 1.6467431197189124
Eval step 0: eval loss: 0.835932483411815
Eval: 2022-05-07 15:22:07.504528: total loss: 1.0794378720733813, mse:4.82261063186365, ic :0.013813736448882105, sharpe5:7.230591824054717, irr5:205.48464965820312, ndcg5:0.8489586410182421, pnl5:2.6540985107421875 
train 3, step: 0, loss: 1.5241041229992378, grad_norm: 0.6093595203874278, ic: 0.011011546796995456
train 3, step: 500, loss: 1.4950557221821164, grad_norm: 0.388542991772285, ic: 0.1400538325913336
train 3, step: 1000, loss: 3.6693264923359243, grad_norm: 0.7948777521923046, ic: -0.04691536998818413
train 3, step: 1500, loss: 1.9851875367609453, grad_norm: 1.0157680736509314, ic: -0.06460773821311382
train 3, step: 2000, loss: 0.899307102512669, grad_norm: 0.0008573658051305023, ic: 0.0013061900816087662
Epoch 3: 2022-05-07 15:26:43.667029: train loss: 1.6460520511503378
Eval step 0: eval loss: 0.8317867877988342
Eval: 2022-05-07 15:27:01.911987: total loss: 1.0782733566296883, mse:4.8262077155099075, ic :0.02585467225642981, sharpe5:7.4329745221138, irr5:210.01695251464844, ndcg5:0.8501397702391532, pnl5:2.3733482360839844 
train 4, step: 0, loss: 1.4430590322066328, grad_norm: 0.054139742835876636, ic: 0.11375107529666273
train 4, step: 500, loss: 1.6487958292322835, grad_norm: 0.6589668326784703, ic: 0.05480401231788144
train 4, step: 1000, loss: 2.950949133896246, grad_norm: 0.7411279451646577, ic: 0.07091743264639229
train 4, step: 1500, loss: 2.1486622741956753, grad_norm: 0.5036435000228726, ic: 0.012498627427167652
train 4, step: 2000, loss: 1.083547162045587, grad_norm: 0.46234948761691286, ic: 0.256741161979226
Epoch 4: 2022-05-07 15:31:34.114825: train loss: 1.6447405123345005
Eval step 0: eval loss: 0.8590495649203108
Eval: 2022-05-07 15:31:51.907652: total loss: 1.0901237670381156, mse:4.7596273702058465, ic :0.13383201281054768, sharpe5:11.551151028871535, irr5:383.7363586425781, ndcg5:0.860847841656926, pnl5:3.1527373790740967 
train 5, step: 0, loss: 1.3554155797766358, grad_norm: 0.20937281347234343, ic: 0.3544925782523687
train 5, step: 500, loss: 0.8869859300654243, grad_norm: 0.013260002019178391, ic: 0.5541512741452645
train 5, step: 1000, loss: 0.9811754482459292, grad_norm: 0.17457889669363247, ic: 0.008752955516140117
train 5, step: 1500, loss: 1.529316409988754, grad_norm: 0.1764675864798457, ic: -0.0030396804933916707
train 5, step: 2000, loss: 1.1006891024914804, grad_norm: 0.029998446963464376, ic: 0.16709778876186926
Epoch 5: 2022-05-07 15:36:26.765718: train loss: 1.6387938918065175
Eval step 0: eval loss: 0.8341440054086537
Eval: 2022-05-07 15:36:43.813038: total loss: 1.0760893306770285, mse:4.718575552047018, ic :0.13605696836677875, sharpe5:11.417120922803878, irr5:388.41278076171875, ndcg5:0.8605126518693316, pnl5:2.4465291500091553 
train 6, step: 0, loss: 1.3370854865991328, grad_norm: 0.4983437202559313, ic: 0.11817105188051905
train 6, step: 500, loss: 1.0063930821090161, grad_norm: 0.04932983535237972, ic: 0.05155727430701071
train 6, step: 1000, loss: 1.1168577709125476, grad_norm: 0.09182638522918378, ic: 0.15485398264479894
train 6, step: 1500, loss: 1.5765956910188532, grad_norm: 0.8255620450393927, ic: 0.11408282712956863
train 6, step: 2000, loss: 0.81089353904999, grad_norm: 0.06096661903572785, ic: 0.05900409622625301
Epoch 6: 2022-05-07 15:41:19.320192: train loss: 1.6360530538538696
Eval step 0: eval loss: 0.8293628752922484
Eval: 2022-05-07 15:41:37.552728: total loss: 1.0731934791927475, mse:4.713769255885471, ic :0.1412136089654056, sharpe5:12.136114327311516, irr5:403.0845642089844, ndcg5:0.8346079338188355, pnl5:3.8386037349700928 
train 7, step: 0, loss: 0.9857873916625977, grad_norm: 0.046396224489266495, ic: 0.10736214361115073
train 7, step: 500, loss: 0.6512128371960486, grad_norm: 0.002778651995985782, ic: 0.050693857022175264
train 7, step: 1000, loss: 1.0120472149002762, grad_norm: 0.1992919223356593, ic: 0.07000016567077796
train 7, step: 1500, loss: 2.2413579975549305, grad_norm: 0.6834180463204075, ic: 0.44296211703992916
train 7, step: 2000, loss: 0.9156553302327343, grad_norm: 0.059808756367778326, ic: -0.04400213766193847
Epoch 7: 2022-05-07 15:46:09.717562: train loss: 1.631538043096461
Eval step 0: eval loss: 0.8319333622151607
Eval: 2022-05-07 15:46:28.070749: total loss: 1.0720522903340721, mse:4.68689404382174, ic :0.16089484143629007, sharpe5:15.596262341737747, irr5:488.4352111816406, ndcg5:0.8454700228214578, pnl5:8.165387153625488 
train 8, step: 0, loss: 3.6038581295289855, grad_norm: 1.3491527901119498, ic: 0.1709798341286208
train 8, step: 500, loss: 2.74585869971742, grad_norm: 0.9456139284007599, ic: 0.03757025585985973
train 8, step: 1000, loss: 3.065221637228261, grad_norm: 0.9689308543196897, ic: 0.10492145166531636
train 8, step: 1500, loss: 0.7147436553541153, grad_norm: 0.042379525585513035, ic: 0.48209475360443693
train 8, step: 2000, loss: 1.093814442826264, grad_norm: 0.4372800269695955, ic: 0.5248592924116047
Epoch 8: 2022-05-07 15:51:04.788856: train loss: 1.6290323506801465
Eval step 0: eval loss: 0.8278462577795705
Eval: 2022-05-07 15:51:23.279659: total loss: 1.070428931767957, mse:4.6853701698429555, ic :0.1659082828144593, sharpe5:16.00288931965828, irr5:523.4013061523438, ndcg5:0.85044190307542, pnl5:7.74791145324707 
train 9, step: 0, loss: 5.455570300039873, grad_norm: 0.7600327621475893, ic: 0.15047763607319867
train 9, step: 500, loss: 1.3426773999540755, grad_norm: 0.971296117205783, ic: 0.3355069820661768
train 9, step: 1000, loss: 0.9331345331101191, grad_norm: 0.02885749356250112, ic: 0.08463867144263654
train 9, step: 1500, loss: 1.0892893098766567, grad_norm: 0.02926061597329266, ic: 0.40720054277621615
train 9, step: 2000, loss: 1.0626604494168357, grad_norm: 0.243750204972161, ic: 0.26919312656694383
Epoch 9: 2022-05-07 15:55:59.808889: train loss: 1.6269788130568008
Eval step 0: eval loss: 0.8264281712040634
Eval: 2022-05-07 15:56:18.430862: total loss: 1.0705955327612489, mse:4.674812970360339, ic :0.16801696607676156, sharpe5:16.78755361676216, irr5:541.0560302734375, ndcg5:0.8360641160260139, pnl5:5.081196308135986 
train 10, step: 0, loss: 7.097009953534985, grad_norm: 1.1631126648883803, ic: 0.24288736928147403
train 10, step: 500, loss: 1.1236361469166993, grad_norm: 0.09976363587872851, ic: 0.06962683744300727
train 10, step: 1000, loss: 2.361329810019651, grad_norm: 0.7674162334375345, ic: 0.14964895837146763
train 10, step: 1500, loss: 1.1046102944906657, grad_norm: 0.2564173657217511, ic: 0.004868613706563638
train 10, step: 2000, loss: 2.7201324741166415, grad_norm: 0.2808554470024276, ic: 0.48127175596923644
Epoch 10: 2022-05-07 16:00:55.270206: train loss: 1.6273141255102688
Eval step 0: eval loss: 0.830467746682363
Eval: 2022-05-07 16:01:13.280380: total loss: 1.0701677263297673, mse:4.639504615840308, ic :0.17509605779209939, sharpe5:16.197290296554563, irr5:528.677734375, ndcg5:0.8549139620276663, pnl5:4.410027980804443 
train 11, step: 0, loss: 1.258006037618426, grad_norm: 0.036125871601091916, ic: 0.20508327779716085
train 11, step: 500, loss: 0.6507916950170237, grad_norm: 0.03594255553120261, ic: 0.6308121770126613
train 11, step: 1000, loss: 0.9384389249922712, grad_norm: 0.1272897057391925, ic: 0.030828152492587118
train 11, step: 1500, loss: 1.057739123963473, grad_norm: 0.059511139106852476, ic: 0.18816169746507838
train 11, step: 2000, loss: 0.7878004318164988, grad_norm: 0.0011886032025845564, ic: 0.12412747361060361
Epoch 11: 2022-05-07 16:05:49.498936: train loss: 1.6245289983094193
Eval step 0: eval loss: 0.8332006295689541
Eval: 2022-05-07 16:06:07.852344: total loss: 1.0710029163720174, mse:4.615160447641971, ic :0.1738871416294912, sharpe5:16.01839115023613, irr5:514.8025512695312, ndcg5:0.8529287921134512, pnl5:5.312891483306885 
train 12, step: 0, loss: 0.9536096254984537, grad_norm: 0.11077293650280282, ic: 0.40459404199542137
train 12, step: 500, loss: 0.9286435538876946, grad_norm: 0.11754718498999057, ic: 0.17975141868183137
train 12, step: 1000, loss: 2.966185745919586, grad_norm: 0.2806640137113169, ic: 0.36568716680087276
train 12, step: 1500, loss: 0.9472276930217061, grad_norm: 0.1175220385302827, ic: -0.13091827009312917
train 12, step: 2000, loss: 0.8745930374929192, grad_norm: 0.004242752272091969, ic: 0.20799749859012256
Epoch 12: 2022-05-07 16:10:41.849965: train loss: 1.6227779307075862
Eval step 0: eval loss: 0.830864250094672
Eval: 2022-05-07 16:11:00.003994: total loss: 1.0673366994782048, mse:4.589368764713696, ic :0.182516089594583, sharpe5:16.61715834259987, irr5:536.7420043945312, ndcg5:0.8525607572998224, pnl5:5.8379807472229 
train 13, step: 0, loss: 2.0540280367278245, grad_norm: 0.8620953242120788, ic: 0.43967961028509145
train 13, step: 500, loss: 0.8114701462312415, grad_norm: 0.03261853896296203, ic: 0.5918320949249486
train 13, step: 1000, loss: 0.9388160654362415, grad_norm: 0.44579731552393065, ic: 0.6057798162645366
train 13, step: 1500, loss: 2.3979023346018735, grad_norm: 0.5407117053602707, ic: -0.0517807085399426
train 13, step: 2000, loss: 1.478490628307833, grad_norm: 0.053011056063680735, ic: 0.18446113007290324
Epoch 13: 2022-05-07 16:15:39.215695: train loss: 1.6224865997712394
Eval step 0: eval loss: 0.82408336643424
Eval: 2022-05-07 16:15:57.582396: total loss: 1.0682189991592652, mse:4.6047739705750415, ic :0.1809457044349894, sharpe5:17.118005197048188, irr5:567.6033325195312, ndcg5:0.8530631936668986, pnl5:7.264198303222656 
train 14, step: 0, loss: 4.549388772425897, grad_norm: 1.496022293885554, ic: 0.1624523902669012
train 14, step: 500, loss: 0.826652690175841, grad_norm: 0.0035299667320475027, ic: 0.12528088211657273
train 14, step: 1000, loss: 1.8439806081529992, grad_norm: 0.6866881125745985, ic: 0.4521468787425823
train 14, step: 1500, loss: 1.1258107998307496, grad_norm: 0.06969737057711786, ic: -0.06469652385875352
train 14, step: 2000, loss: 1.1384128650213892, grad_norm: 0.21988894594754835, ic: 0.10216201258611375
Epoch 14: 2022-05-07 16:20:01.168023: train loss: 1.6218599901427415
Eval step 0: eval loss: 0.8361130805864725
Eval: 2022-05-07 16:20:19.677080: total loss: 1.0694530366073582, mse:4.595797601926542, ic :0.18044813616674396, sharpe5:17.09387841582298, irr5:546.5567016601562, ndcg5:0.8434744173762581, pnl5:4.140103340148926 
train 15, step: 0, loss: 3.402908408317121, grad_norm: 0.9108182471974886, ic: 0.09986191375738192
train 15, step: 500, loss: 1.258273180979842, grad_norm: 0.18860758454014154, ic: -0.02067977411068548
train 15, step: 1000, loss: 1.3186078823678862, grad_norm: 0.17115838785744553, ic: 0.047757027504130356
train 15, step: 1500, loss: 0.8533291553887795, grad_norm: 0.4725736690715672, ic: 0.07101913800215302
train 15, step: 2000, loss: 1.4602721086830717, grad_norm: 0.6341439692411512, ic: 0.040115619623766
Epoch 15: 2022-05-07 16:24:55.756116: train loss: 1.6212123170535546
Eval step 0: eval loss: 0.8411017559602212
Eval: 2022-05-07 16:25:13.476084: total loss: 1.0741734072364544, mse:4.714199286304915, ic :0.1528759795624314, sharpe5:17.089383376836775, irr5:562.5421142578125, ndcg5:0.8474560309349479, pnl5:6.215400695800781 
train 16, step: 0, loss: 0.693842582068364, grad_norm: 0.2596946826070714, ic: 0.0053833351458248205
train 16, step: 500, loss: 1.604762176788314, grad_norm: 0.45706617974113045, ic: 0.17332775927140054
train 16, step: 1000, loss: 0.8799604936079546, grad_norm: 0.009099882601275557, ic: -0.1306621434922665
train 16, step: 1500, loss: 0.858523874605989, grad_norm: 0.2158913628072548, ic: 0.17568946872049013
train 16, step: 2000, loss: 3.332891689461189, grad_norm: 0.9623891757744061, ic: 0.02668717360533735
Epoch 16: 2022-05-07 16:29:44.989458: train loss: 1.6203204313996185
Eval step 0: eval loss: 0.8314268154389488
Eval: 2022-05-07 16:30:02.899118: total loss: 1.0687320186106948, mse:4.601288127436041, ic :0.17921115548852778, sharpe5:16.55687260508537, irr5:540.7756958007812, ndcg5:0.853596775389955, pnl5:5.236485004425049 
train 17, step: 0, loss: 1.2869011107427055, grad_norm: 0.23491095482450952, ic: -0.13939706165381144
train 17, step: 500, loss: 1.7447204755250678, grad_norm: 0.48890091812355685, ic: 0.2274806894844738
train 17, step: 1000, loss: 1.2851833048200558, grad_norm: 0.09377694275599083, ic: 0.13761424150510768
train 17, step: 1500, loss: 4.522140200220425, grad_norm: 1.147378019464897, ic: 0.22519490099287556
train 17, step: 2000, loss: 1.2747933836142602, grad_norm: 0.6664921112116974, ic: 0.08706717345694931
Epoch 17: 2022-05-07 16:34:35.933943: train loss: 1.6207925345463068
Eval step 0: eval loss: 0.8359340269774104
Eval: 2022-05-07 16:34:54.429190: total loss: 1.0684837625636878, mse:4.584906613909071, ic :0.189909320175784, sharpe5:17.668146132230756, irr5:597.53515625, ndcg5:0.8325723639606395, pnl5:5.335821151733398 
train 18, step: 0, loss: 1.4220636279258458, grad_norm: 0.8804282709122335, ic: 0.1514902895842691
train 18, step: 500, loss: 1.5066152454096045, grad_norm: 0.9017271768041188, ic: -0.047854746155584965
train 18, step: 1000, loss: 0.6530266748715753, grad_norm: 0.01951673453405919, ic: 0.5738235328215355
train 18, step: 1500, loss: 1.4217200376868346, grad_norm: 0.03177001515045862, ic: 0.2669518294309141
train 18, step: 2000, loss: 0.9096189851214173, grad_norm: 0.005480289069323456, ic: 0.00446384040317927
Epoch 18: 2022-05-07 16:39:28.178403: train loss: 1.6199433147968636
Eval step 0: eval loss: 0.8203912218453635
Eval: 2022-05-07 16:39:46.480166: total loss: 1.0653828910968017, mse:4.588141905115288, ic :0.1944573932983997, sharpe5:17.82500520944595, irr5:608.7926025390625, ndcg5:0.8506636381979811, pnl5:4.69012451171875 
train 19, step: 0, loss: 1.4920677548363095, grad_norm: 0.6935681206264261, ic: -0.0388596119125239
train 19, step: 500, loss: 0.8667418868453414, grad_norm: 0.01686663696958464, ic: 0.21199132453716382
train 19, step: 1000, loss: 0.9580076271236713, grad_norm: 0.026453797710960385, ic: 0.19793247888483462
train 19, step: 1500, loss: 3.9751182443561137, grad_norm: 0.9364797328242951, ic: 0.14962683082041894
train 19, step: 2000, loss: 1.0052197265625, grad_norm: 0.12229652617930864, ic: 0.27769499886132215
Epoch 19: 2022-05-07 16:44:19.467194: train loss: 1.62082163633059
Eval step 0: eval loss: 0.8311885275001646
Eval: 2022-05-07 16:44:37.876444: total loss: 1.0675759827913032, mse:4.585967122298959, ic :0.19059219695350296, sharpe5:17.725128697156904, irr5:608.7903442382812, ndcg5:0.8520417853589964, pnl5:6.50998592376709 
train 20, step: 0, loss: 2.2864717144268774, grad_norm: 1.3968374699498054, ic: 0.0471576000673752
train 20, step: 500, loss: 3.2173267045454543, grad_norm: 0.5004796639165636, ic: 0.08587818336339316
train 20, step: 1000, loss: 0.9762500762939453, grad_norm: 0.1161638391570113, ic: 0.14230769839048923
train 20, step: 1500, loss: 1.7558930237844563, grad_norm: 1.1311637125944167, ic: 0.2685486087564666
train 20, step: 2000, loss: 1.0162469014452489, grad_norm: 0.055288208741962386, ic: 0.07779193781270692
Epoch 20: 2022-05-07 16:49:12.171231: train loss: 1.618886372870753
Eval step 0: eval loss: 0.82974960278912
Eval: 2022-05-07 16:49:30.264044: total loss: 1.066085414190792, mse:4.586154033330391, ic :0.18943041250791573, sharpe5:16.618982087373734, irr5:576.3618774414062, ndcg5:0.8477728794861231, pnl5:4.692569255828857 
train 21, step: 0, loss: 1.0195084374702041, grad_norm: 0.358396528679352, ic: 0.05922944648775945
train 21, step: 500, loss: 0.7636452733942892, grad_norm: 0.013486184764782456, ic: 0.21364632977502773
train 21, step: 1000, loss: 0.9400502255088404, grad_norm: 0.8441816448752109, ic: 0.15949399233783507
train 21, step: 1500, loss: 0.9860304632660637, grad_norm: 0.28331130416183214, ic: 0.3283328527636888
train 21, step: 2000, loss: 0.9452563990258167, grad_norm: 0.08896399008245442, ic: 0.06622414302300668
Epoch 21: 2022-05-07 16:54:04.392637: train loss: 1.6187498943699343
Eval step 0: eval loss: 0.8299086543606756
Eval: 2022-05-07 16:54:22.297583: total loss: 1.0673413607161033, mse:4.593142550782963, ic :0.18467966897509744, sharpe5:17.482958471775053, irr5:589.0535278320312, ndcg5:0.8460091931941935, pnl5:6.764069080352783 
train 22, step: 0, loss: 1.037880590406515, grad_norm: 0.018038737515958216, ic: 0.2169814808059978
train 22, step: 500, loss: 3.266863567073171, grad_norm: 1.123404985613559, ic: -0.2336204674063968
train 22, step: 1000, loss: 1.1939760770411851, grad_norm: 0.06120888485041961, ic: 0.46049573173779423
train 22, step: 1500, loss: 0.9721241640946502, grad_norm: 0.18831788635300917, ic: 0.11400641097235696
train 22, step: 2000, loss: 1.7487008231026786, grad_norm: 0.6049776033426402, ic: 0.1673058125596636
Epoch 22: 2022-05-07 16:58:51.622334: train loss: 1.6188617445839446
Eval step 0: eval loss: 0.833385535864232
Eval: 2022-05-07 16:59:09.505210: total loss: 1.0680530973951126, mse:4.595906999565496, ic :0.18298844011441937, sharpe5:17.295477887392043, irr5:588.9884643554688, ndcg5:0.8597503808716387, pnl5:5.44448184967041 
train 23, step: 0, loss: 0.9841630127656701, grad_norm: 0.07884630462705534, ic: 0.1842835526035301
train 23, step: 500, loss: 1.4267936478978611, grad_norm: 0.21495766511324138, ic: 0.01053259771793806
train 23, step: 1000, loss: 1.6466160074869793, grad_norm: 0.1567772255819439, ic: 0.2564148859692489
train 23, step: 1500, loss: 1.102319467095165, grad_norm: 0.44096446909044607, ic: 0.09956752872507998
train 23, step: 2000, loss: 1.9753720560286758, grad_norm: 1.445447441697668, ic: 0.44661810754833575
Epoch 23: 2022-05-07 17:03:45.958007: train loss: 1.6181928004582367
Eval step 0: eval loss: 0.8370493817505268
Eval: 2022-05-07 17:04:04.315375: total loss: 1.0671062372455111, mse:4.592672537507669, ic :0.17773712553795318, sharpe5:15.987675050497055, irr5:539.3493041992188, ndcg5:0.8514493847150019, pnl5:3.8415377140045166 
train 24, step: 0, loss: 2.1890340230275487, grad_norm: 0.033403168649694975, ic: 0.16060835115548983
train 24, step: 500, loss: 1.2251725133754865, grad_norm: 0.3027857571939872, ic: 0.07862864347115527
train 24, step: 1000, loss: 0.9038303537516881, grad_norm: 0.07482162871963474, ic: 0.5362093193298471
train 24, step: 1500, loss: 2.5887464254431163, grad_norm: 5.065343329150519, ic: 0.07182654931640348
train 24, step: 2000, loss: 0.9402058421836796, grad_norm: 0.08986028730828524, ic: 0.09270859918095471
Epoch 24: 2022-05-07 17:08:35.682143: train loss: 1.6144282217416868
Eval step 0: eval loss: 0.8297152584546232
Eval: 2022-05-07 17:08:53.997775: total loss: 1.0676746452700316, mse:4.613802657956368, ic :0.18533354217641773, sharpe5:17.43424140572548, irr5:596.9700317382812, ndcg5:0.8629323039078967, pnl5:7.129708290100098 
train 25, step: 0, loss: 0.8366132581556166, grad_norm: 0.08103252222462994, ic: 0.6162864701520985
train 25, step: 500, loss: 0.8652791832869147, grad_norm: 0.024374078173512115, ic: 0.25443298792886737
train 25, step: 1000, loss: 2.1118578711086253, grad_norm: 0.40872357657972047, ic: 0.23383024294938032
train 25, step: 1500, loss: 1.141922678450609, grad_norm: 0.5609697382480314, ic: 0.5331100041092388
train 25, step: 2000, loss: 1.0180258290954631, grad_norm: 0.5156961608989301, ic: 0.6054295113603582
Epoch 25: 2022-05-07 17:13:22.332098: train loss: 1.6170363665079885
Eval step 0: eval loss: 0.8273665947708113
Eval: 2022-05-07 17:13:39.002852: total loss: 1.065464764041408, mse:4.587309146281904, ic :0.19247126015141117, sharpe5:17.77024935364723, irr5:610.0560302734375, ndcg5:0.8354654706220903, pnl5:7.099794864654541 
train 26, step: 0, loss: 6.710588059105431, grad_norm: 0.5982993749996167, ic: 0.09836103807546595
train 26, step: 500, loss: 3.9046908595582486, grad_norm: 1.965270157461712, ic: 0.37336230706820717
train 26, step: 1000, loss: 1.274894390974539, grad_norm: 1.0359749415441921, ic: 0.021160084025112776
train 26, step: 1500, loss: 0.8323292321945047, grad_norm: 0.1836321785964573, ic: 0.3000050065716431
train 26, step: 2000, loss: 0.9640719506048387, grad_norm: 0.46544306133727836, ic: 0.1150443784025019
Epoch 26: 2022-05-07 17:18:13.371886: train loss: 1.615837341765676
Eval step 0: eval loss: 0.8323429859350302
Eval: 2022-05-07 17:18:31.230719: total loss: 1.0668568841345927, mse:4.591131879477265, ic :0.1864500438366145, sharpe5:17.490168317556382, irr5:590.0604248046875, ndcg5:0.8281889584256027, pnl5:5.241772174835205 
train 27, step: 0, loss: 0.8247151692708333, grad_norm: 0.0721763923029623, ic: 0.13022018193736362
train 27, step: 500, loss: 0.8852605091066599, grad_norm: 1.4100376874894567, ic: 0.28453390351712377
train 27, step: 1000, loss: 0.7556167449125097, grad_norm: 0.6568880292424703, ic: 0.1923177162314152
train 27, step: 1500, loss: 0.6373649655280876, grad_norm: 0.03965916233100655, ic: 0.5172543593143708
train 27, step: 2000, loss: 1.3821527456458809, grad_norm: 0.021181889034723695, ic: 0.027448732477966673
Epoch 27: 2022-05-07 17:23:02.456232: train loss: 1.6158160179269376
Eval step 0: eval loss: 0.8392026557560589
Eval: 2022-05-07 17:23:20.858137: total loss: 1.069821879647964, mse:4.604428204511801, ic :0.17965546538577198, sharpe5:17.48319873690605, irr5:576.6251220703125, ndcg5:0.8482346876702592, pnl5:7.02232027053833 
train 28, step: 0, loss: 1.5351568155767374, grad_norm: 0.43773520452051873, ic: 0.24314257435971723
train 28, step: 500, loss: 1.360076170783259, grad_norm: 0.6283454292116702, ic: 0.19957298192282308
train 28, step: 1000, loss: 0.9043420310912094, grad_norm: 0.3729689977421127, ic: 0.5830567245754495
train 28, step: 1500, loss: 1.0366838836077117, grad_norm: 0.08122875956182714, ic: 0.023768593662513566
train 28, step: 2000, loss: 1.038127407705857, grad_norm: 0.1468870636081411, ic: 0.12985049027917553
Epoch 28: 2022-05-07 17:27:56.305332: train loss: 1.6128412574641098
Eval step 0: eval loss: 0.8292128278533324
Eval: 2022-05-07 17:28:13.988134: total loss: 1.0714717050077767, mse:4.643805882462583, ic :0.17968227148904514, sharpe5:17.20439470052719, irr5:591.0133666992188, ndcg5:0.8447876111678811, pnl5:5.924093723297119 
train 29, step: 0, loss: 0.9077994322758719, grad_norm: 0.04509455857700925, ic: 0.09439150984719105
train 29, step: 500, loss: 1.0994499596300646, grad_norm: 0.213229341678207, ic: 0.6194014933958784
train 29, step: 1000, loss: 1.065534801000096, grad_norm: 0.7538151844421497, ic: 0.08669966991543429
train 29, step: 1500, loss: 2.35852860771614, grad_norm: 0.29494401869750936, ic: -0.07704607573458422
train 29, step: 2000, loss: 4.393547529055748, grad_norm: 3.477839654726215, ic: 0.19843293614877147
Epoch 29: 2022-05-07 17:32:46.621076: train loss: 1.6144977996675713
Eval step 0: eval loss: 0.8323004735659246
Eval: 2022-05-07 17:33:04.456494: total loss: 1.06737230443082, mse:4.59178468101623, ic :0.19003477025277793, sharpe5:17.897769598960874, irr5:608.77587890625, ndcg5:0.8475808486995876, pnl5:5.13319206237793 
train 30, step: 0, loss: 1.008705076898933, grad_norm: 0.044098455874639106, ic: 0.5079799367819217
train 30, step: 500, loss: 1.4193372593442373, grad_norm: 3.8993033523179585, ic: 0.02368444933709246
train 30, step: 1000, loss: 0.9727700898141572, grad_norm: 0.03715203707788935, ic: -0.012558931884443014
train 30, step: 1500, loss: 1.5070120796649036, grad_norm: 2.2018372151153898, ic: 0.18474259471824064
train 30, step: 2000, loss: 1.8595087449789593, grad_norm: 0.8961111609532384, ic: 0.019262159305398364
Epoch 30: 2022-05-07 17:37:37.490020: train loss: 1.614037763914926
Eval step 0: eval loss: 0.843625228447708
Eval: 2022-05-07 17:37:53.585883: total loss: 1.0736130113734397, mse:4.655878046632293, ic :0.18709075521276228, sharpe5:16.804268124103544, irr5:590.033935546875, ndcg5:0.8434684733027953, pnl5:4.800713539123535 
train 31, step: 0, loss: 1.059311068682019, grad_norm: 0.28307861481252117, ic: 0.3536334037172775
train 31, step: 500, loss: 1.5107015978652263, grad_norm: 1.4448754177121064, ic: 0.05806459070881378
train 31, step: 1000, loss: 4.265411924887783, grad_norm: 1.7810008139031155, ic: 0.46594520379412857
train 31, step: 1500, loss: 0.7618851947403462, grad_norm: 0.055145446955389435, ic: 0.7173996185958373
train 31, step: 2000, loss: 1.2352485076997055, grad_norm: 1.047873519385354, ic: 0.2039892526956411
Epoch 31: 2022-05-07 17:42:29.501697: train loss: 1.611684488127255
Eval step 0: eval loss: 0.8379643945724117
Eval: 2022-05-07 17:42:47.722948: total loss: 1.0683504896837903, mse:4.593833391842049, ic :0.18004607594042352, sharpe5:16.645827459096907, irr5:557.7171630859375, ndcg5:0.8382095212012047, pnl5:4.137938022613525 
train 32, step: 0, loss: 1.1331647247052805, grad_norm: 0.02064188045779311, ic: 0.15367731310785143
train 32, step: 500, loss: 1.4862980399544783, grad_norm: 1.5918813870922988, ic: 0.11493505368043938
train 32, step: 1000, loss: 1.0504957654125462, grad_norm: 0.2592637459417238, ic: 0.5181760734104585
train 32, step: 1500, loss: 0.9774031184448895, grad_norm: 1.2242831721083733, ic: 0.08494766312762807
train 32, step: 2000, loss: 0.9318065984538865, grad_norm: 0.07628182920694394, ic: 0.5693708686502387
Epoch 32: 2022-05-07 17:47:22.622720: train loss: 1.61113692914808
Eval step 0: eval loss: 0.8272726302151936
Eval: 2022-05-07 17:47:41.100634: total loss: 1.06466462645692, mse:4.589474928173271, ic :0.194208318549743, sharpe5:17.91536570906639, irr5:611.8245849609375, ndcg5:0.8262844651252814, pnl5:6.4877753257751465 
train 33, step: 0, loss: 1.2645693422651187, grad_norm: 0.9422194825727799, ic: 0.2058736002233138
train 33, step: 500, loss: 1.0007559220421123, grad_norm: 0.04873657707874092, ic: 0.10047479452509171
train 33, step: 1000, loss: 1.0587626597250135, grad_norm: 2.3555866101320624, ic: 0.21999951192887335
train 33, step: 1500, loss: 0.8853840856224051, grad_norm: 0.07742534943994074, ic: 0.5600941872681132
train 33, step: 2000, loss: 0.8086119348009113, grad_norm: 0.1312843348340495, ic: 0.2612277987147015
Epoch 33: 2022-05-07 17:52:17.418303: train loss: 1.6141048849099493
Eval step 0: eval loss: 0.8283680472660365
Eval: 2022-05-07 17:52:35.362696: total loss: 1.0662299596172893, mse:4.578214263679918, ic :0.19215869000237137, sharpe5:17.791920511722562, irr5:612.243408203125, ndcg5:0.8444936701280803, pnl5:3.2534778118133545 
train 34, step: 0, loss: 1.0005810411335443, grad_norm: 2.3151652382337877, ic: 0.6136405351877302
train 34, step: 500, loss: 0.8087867480170107, grad_norm: 0.455913831884903, ic: 0.24930683041732404
train 34, step: 1000, loss: 3.1223412673411097, grad_norm: 1.1870365958591353, ic: 0.3216314947412595
train 34, step: 1500, loss: 0.8094029932397199, grad_norm: 0.7208917656866399, ic: 0.6833683226636385
train 34, step: 2000, loss: 6.588092594161503, grad_norm: 26.962548388484485, ic: 0.4447172429466274
Epoch 34: 2022-05-07 17:57:09.882631: train loss: 1.6095716079483346
Eval step 0: eval loss: 0.8287732975500526
Eval: 2022-05-07 17:57:27.572146: total loss: 1.069101266663362, mse:4.617904393295935, ic :0.1847102494870659, sharpe5:17.190727965831755, irr5:596.0958862304688, ndcg5:0.842072633836409, pnl5:4.05186128616333 
train 35, step: 0, loss: 1.155413459329044, grad_norm: 0.7357916964500211, ic: 0.5551632734667546
train 35, step: 500, loss: 1.1802945577601187, grad_norm: 0.902851767857118, ic: 0.1372340995663841
train 35, step: 1000, loss: 1.7386295780254777, grad_norm: 2.169920634751149, ic: 0.05327301332614343
train 35, step: 1500, loss: 1.6163981731672934, grad_norm: 0.9281620287600054, ic: 0.044723882861718536
train 35, step: 2000, loss: 0.7954583805512616, grad_norm: 0.19264037165856268, ic: 0.5636020231981819
Epoch 35: 2022-05-07 18:01:58.747256: train loss: 1.6132241012363846
Eval step 0: eval loss: 0.8352382647853003
Eval: 2022-05-07 18:02:17.205246: total loss: 1.0667067912365693, mse:4.593490934225394, ic :0.18619040644116294, sharpe5:17.30095668911934, irr5:584.1339721679688, ndcg5:0.8406327291714732, pnl5:4.392937660217285 
train 36, step: 0, loss: 1.8249297089629122, grad_norm: 1.2877170595114786, ic: 0.10763063020293828
train 36, step: 500, loss: 0.8341255879383374, grad_norm: 0.09576635084626367, ic: 0.2207777880884552
train 36, step: 1000, loss: 1.6802688210227272, grad_norm: 2.3319405059688916, ic: 0.2534068813566424
train 36, step: 1500, loss: 0.7707951020853638, grad_norm: 0.09031655148287962, ic: 0.3581029803032211
train 36, step: 2000, loss: 1.1280944193990765, grad_norm: 0.6570408561275605, ic: 0.7617387542523869
Epoch 36: 2022-05-07 18:06:52.925413: train loss: 1.6062649323922449
Eval step 0: eval loss: 0.8354624676880268
Eval: 2022-05-07 18:07:11.092575: total loss: 1.0669951925379733, mse:4.586580711895584, ic :0.18851592664673592, sharpe5:17.26685795903206, irr5:593.6709594726562, ndcg5:0.8382066846823978, pnl5:5.265543460845947 
train 37, step: 0, loss: 2.028853458795942, grad_norm: 1.2391458024277244, ic: 0.19926777525421352
train 37, step: 500, loss: 2.3181423168586868, grad_norm: 0.8207102474758527, ic: -0.014159480895633014
train 37, step: 1000, loss: 1.076190269156678, grad_norm: 0.14468103512933822, ic: 0.05037806121552576
train 37, step: 1500, loss: 1.9911224029876375, grad_norm: 1.74366813376529, ic: 0.6015481888130021
train 37, step: 2000, loss: 1.32126168792826, grad_norm: 0.13721474510431236, ic: 0.222679818572928
Epoch 37: 2022-05-07 18:11:36.063139: train loss: 1.6133463626724336
Eval step 0: eval loss: 0.8267105150775487
Eval: 2022-05-07 18:11:53.516049: total loss: 1.0672363633235367, mse:4.602023759641875, ic :0.18855092463839956, sharpe5:18.253178483247755, irr5:619.9883422851562, ndcg5:0.8565233214008441, pnl5:5.835509300231934 
train 38, step: 0, loss: 1.3314334590260577, grad_norm: 0.34148483915056427, ic: -0.06253985999639766
train 38, step: 500, loss: 0.9116899655189044, grad_norm: 0.09316447967821048, ic: 0.2593115147921355
train 38, step: 1000, loss: 0.9066872336647727, grad_norm: 0.18012648805215908, ic: 0.17593438877017142
train 38, step: 1500, loss: 0.9551029654280563, grad_norm: 0.08086312463979496, ic: 0.20313272983105124
train 38, step: 2000, loss: 2.319216633131137, grad_norm: 3.3172103020185455, ic: 0.021066780477242728
Epoch 38: 2022-05-07 18:16:18.414732: train loss: 1.6132632227955372
Eval step 0: eval loss: 0.8300035836447905
Eval: 2022-05-07 18:16:35.812616: total loss: 1.0645812003950499, mse:4.580669266588773, ic :0.1953117216978775, sharpe5:17.485955164432525, irr5:613.4087524414062, ndcg5:0.8530903320105121, pnl5:5.61631441116333 
train 39, step: 0, loss: 0.9689777152618502, grad_norm: 0.0055248075735831945, ic: 0.08888603769979561
train 39, step: 500, loss: 0.8876174352060915, grad_norm: 0.10500363545396724, ic: 0.19642409113653728
train 39, step: 1000, loss: 0.9355013194378332, grad_norm: 0.034462389253837045, ic: 0.22266841317777805
train 39, step: 1500, loss: 2.047054139634509, grad_norm: 0.10721212803044237, ic: 0.2221331844336441
train 39, step: 2000, loss: 0.6177502190708926, grad_norm: 0.04198059269178542, ic: 0.12606630172016586
Epoch 39: 2022-05-07 18:21:09.000993: train loss: 1.6092095515971305
Eval step 0: eval loss: 0.8333049488771074
Eval: 2022-05-07 18:21:26.956947: total loss: 1.0670156370557646, mse:4.589947623642399, ic :0.19270372275029202, sharpe5:18.660507338047026, irr5:625.9322509765625, ndcg5:0.8509318326403154, pnl5:5.756006240844727 
