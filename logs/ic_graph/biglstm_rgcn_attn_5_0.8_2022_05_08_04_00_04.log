Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_5_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
83284
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884380421105356, grad_norm: 5.0864939476818956, ic: 0.011814629332908534
train 0, step: 500, loss: 0.8626272953266296, grad_norm: 0.02185010691808941, ic: 0.05286706417359363
train 0, step: 1000, loss: 1.9454082274140454, grad_norm: 0.589488111963393, ic: 0.006745813827655199
train 0, step: 1500, loss: 0.9754591387722332, grad_norm: 0.1206327389766402, ic: 0.07233222484567114
train 0, step: 2000, loss: 0.9941991554738121, grad_norm: 0.15928299240674126, ic: 0.023408103358699923
Epoch 0: 2022-05-08 04:04:19.667944: train loss: 1.6493767704573599
Eval step 0: eval loss: 0.8351443645449157
Eval: 2022-05-08 04:04:33.787973: total loss: 1.0789987886408354, mse:4.823390457724597, ic :0.007425799610312695, sharpe5:7.393947992026805, irr5:208.6643829345703, ndcg5:0.8516746297348995, pnl5:2.6898865699768066 
train 1, step: 0, loss: 2.7625878118699596, grad_norm: 0.9543202501035128, ic: 0.06211460804818942
train 1, step: 500, loss: 1.7646708695523916, grad_norm: 0.8586811871479458, ic: 0.10858908611575507
train 1, step: 1000, loss: 0.8746208867786487, grad_norm: 0.19409866394686154, ic: 0.05959657765507437
train 1, step: 1500, loss: 1.712121722341954, grad_norm: 0.23697375838737855, ic: -0.02191055882651359
train 1, step: 2000, loss: 2.1933283203125002, grad_norm: 0.9745766138507169, ic: 0.04554520483842067
Epoch 1: 2022-05-08 04:08:08.220502: train loss: 1.646898716332772
Eval step 0: eval loss: 0.8340357628712789
Eval: 2022-05-08 04:08:22.323965: total loss: 1.0788314416477376, mse:4.8239481345798065, ic :0.008741779567018758, sharpe5:7.265466213226318, irr5:205.76417541503906, ndcg5:0.8530867059247232, pnl5:2.5064680576324463 
train 2, step: 0, loss: 2.1429577414772725, grad_norm: 0.010847416229296738, ic: 0.12870795144073433
train 2, step: 500, loss: 3.3058447800518387, grad_norm: 0.3100636995425613, ic: 0.059743439256447234
train 2, step: 1000, loss: 2.072348314774904, grad_norm: 0.0002705484766689078, ic: 0.2119597837648825
train 2, step: 1500, loss: 1.4863323182550094, grad_norm: 0.0708882917792276, ic: -0.012472023117325228
train 2, step: 2000, loss: 3.2364115084134615, grad_norm: 0.878388542854009, ic: 0.23978781442126162
Epoch 2: 2022-05-08 04:11:59.695233: train loss: 1.6467342818131452
Eval step 0: eval loss: 0.8360691532822379
Eval: 2022-05-08 04:12:13.798676: total loss: 1.0794574478281587, mse:4.822526406919469, ic :0.014635399707057321, sharpe5:7.3948267570137975, irr5:209.68922424316406, ndcg5:0.8482266898382819, pnl5:2.6575987339019775 
train 3, step: 0, loss: 1.524061249523628, grad_norm: 0.60896243730884, ic: 0.012051681610493376
train 3, step: 500, loss: 1.4952554127716327, grad_norm: 0.38860633251153776, ic: 0.14747624304278492
train 3, step: 1000, loss: 3.6683063583945024, grad_norm: 0.79281198410758, ic: -0.05157591695113964
train 3, step: 1500, loss: 1.9932861328125, grad_norm: 1.008111794197251, ic: -0.050808945078299486
train 3, step: 2000, loss: 0.8987335040118244, grad_norm: 0.00059366582771696, ic: 0.00706255359562126
Epoch 3: 2022-05-08 04:15:48.297066: train loss: 1.6460967500712744
Eval step 0: eval loss: 0.833301990376383
Eval: 2022-05-08 04:16:02.529457: total loss: 1.0786589266909932, mse:4.8246864198060795, ic :0.024914287601659917, sharpe5:8.127091972231865, irr5:228.7324981689453, ndcg5:0.843865526191533, pnl5:2.3459877967834473 
train 4, step: 0, loss: 1.437349330357143, grad_norm: 0.05301736723078612, ic: 0.10604567767537568
train 4, step: 500, loss: 1.6460370401697835, grad_norm: 0.6558443758627842, ic: 0.04815174711591916
train 4, step: 1000, loss: 2.947438309832317, grad_norm: 0.7282867720905285, ic: 0.07119124728611426
train 4, step: 1500, loss: 2.149229051292194, grad_norm: 0.506112898992647, ic: -0.0040824289904246094
train 4, step: 2000, loss: 1.0788037906723853, grad_norm: 0.4741680557719265, ic: 0.2802194292038336
Epoch 4: 2022-05-08 04:19:36.474304: train loss: 1.644527493453474
Eval step 0: eval loss: 0.8732998268119402
Eval: 2022-05-08 04:19:50.645547: total loss: 1.0985119975395488, mse:4.817646816388456, ic :0.11772111269362708, sharpe5:11.913564493060111, irr5:382.2545471191406, ndcg5:0.8574311923330044, pnl5:2.958951234817505 
train 5, step: 0, loss: 1.3833371565645973, grad_norm: 0.36695244447745573, ic: 0.3518358365010263
train 5, step: 500, loss: 0.8904801806601903, grad_norm: 0.013008537805329162, ic: 0.5654071905766116
train 5, step: 1000, loss: 0.9778637414691093, grad_norm: 0.17634318196408436, ic: 0.02530499630192698
train 5, step: 1500, loss: 1.5292671519070635, grad_norm: 0.17484992726215967, ic: 0.012017851820775163
train 5, step: 2000, loss: 1.098215196082874, grad_norm: 0.028445468474328845, ic: 0.18180706442491645
Epoch 5: 2022-05-08 04:23:26.074833: train loss: 1.6387298555180956
Eval step 0: eval loss: 0.8358111848821127
Eval: 2022-05-08 04:23:40.226065: total loss: 1.0764797865156384, mse:4.725117640054846, ic :0.1359366678176243, sharpe5:11.516367448568344, irr5:390.291259765625, ndcg5:0.8457476502731892, pnl5:2.8247246742248535 
train 6, step: 0, loss: 1.3507553173594498, grad_norm: 0.604621168070316, ic: 0.10748766093316711
train 6, step: 500, loss: 1.0070714789911945, grad_norm: 0.048140260412691406, ic: 0.05000552587071139
train 6, step: 1000, loss: 1.12338885753327, grad_norm: 0.08405999366211529, ic: 0.023523464742998497
train 6, step: 1500, loss: 1.5756811725206612, grad_norm: 0.8000130098019468, ic: 0.03199367400013647
train 6, step: 2000, loss: 0.8042714933665682, grad_norm: 0.04112692306679274, ic: 0.14612413585013473
Epoch 6: 2022-05-08 04:27:13.094320: train loss: 1.6380566099521243
Eval step 0: eval loss: 0.8305999788016991
Eval: 2022-05-08 04:27:27.168490: total loss: 1.075549840708549, mse:4.72560340279927, ic :0.13834018304451082, sharpe5:11.187753801941872, irr5:383.4056701660156, ndcg5:0.8389253171643221, pnl5:2.9670932292938232 
train 7, step: 0, loss: 0.9939966201782227, grad_norm: 0.04863441679310896, ic: 0.03568574034846742
train 7, step: 500, loss: 0.653551791576629, grad_norm: 0.00334337267396324, ic: 0.030812876809886352
train 7, step: 1000, loss: 1.035289011924029, grad_norm: 0.2718326378434203, ic: 0.00664431981370165
train 7, step: 1500, loss: 2.2517490202974275, grad_norm: 1.3905778770472434, ic: 0.429816673686155
train 7, step: 2000, loss: 0.9038618392821083, grad_norm: 0.05473284148460861, ic: -0.012938249386951612
Epoch 7: 2022-05-08 04:31:02.764597: train loss: 1.6391320787525654
Eval step 0: eval loss: 0.8289774341000394
Eval: 2022-05-08 04:31:17.135433: total loss: 1.0742190928899242, mse:4.716367172428937, ic :0.1409369899801824, sharpe5:11.354546045064925, irr5:386.3629150390625, ndcg5:0.844227071847842, pnl5:2.9654905796051025 
train 8, step: 0, loss: 3.600472712862319, grad_norm: 1.8575054462981397, ic: 0.12925205924695007
train 8, step: 500, loss: 2.726215768367686, grad_norm: 0.9333539590926632, ic: 0.047117172491045606
train 8, step: 1000, loss: 3.0742272418478263, grad_norm: 1.2499084993979332, ic: 0.10584370698726184
train 8, step: 1500, loss: 0.727860569512947, grad_norm: 0.4243612425011848, ic: 0.39904265362077995
train 8, step: 2000, loss: 1.0953651598606158, grad_norm: 0.48098843618565656, ic: 0.5099844756266234
Epoch 8: 2022-05-08 04:34:53.074619: train loss: 1.634187067464031
Eval step 0: eval loss: 0.8237489272219112
Eval: 2022-05-08 04:35:07.271003: total loss: 1.0719219158983124, mse:4.6924811752268, ic :0.16046804501228315, sharpe5:15.84083711028099, irr5:506.194580078125, ndcg5:0.8401596770673099, pnl5:9.190675735473633 
train 9, step: 0, loss: 5.466589725378788, grad_norm: 0.9512745818499984, ic: 0.13164560381477414
train 9, step: 500, loss: 1.3955011374316475, grad_norm: 1.9656972349878485, ic: 0.31596563555623214
train 9, step: 1000, loss: 0.928864778164768, grad_norm: 1.4899860314405662, ic: 0.06917400877461988
train 9, step: 1500, loss: 1.0932101220199741, grad_norm: 0.06287961089435268, ic: 0.408583344514549
train 9, step: 2000, loss: 1.0682103980466109, grad_norm: 1.258995585054238, ic: 0.3035009774197337
Epoch 9: 2022-05-08 04:38:43.039830: train loss: 1.6285912840666568
Eval step 0: eval loss: 0.8228055513822115
Eval: 2022-05-08 04:38:57.308937: total loss: 1.0718469121573946, mse:4.693181154167124, ic :0.16257118401631432, sharpe5:17.50708903670311, irr5:586.521484375, ndcg5:0.8620693329472562, pnl5:8.168680191040039 
train 10, step: 0, loss: 7.088347587919096, grad_norm: 1.3602837248049529, ic: 0.2679517927268741
train 10, step: 500, loss: 1.1225492037259617, grad_norm: 0.08932505711516531, ic: 0.07293313815730731
train 10, step: 1000, loss: 2.403048088214149, grad_norm: 0.9677525013138797, ic: 0.16859852944412296
train 10, step: 1500, loss: 1.1043390051111, grad_norm: 0.38928590950244163, ic: 0.006760055819351959
train 10, step: 2000, loss: 2.761103946024886, grad_norm: 2.6379686965067215, ic: 0.4116525375063238
Epoch 10: 2022-05-08 04:42:32.819464: train loss: 1.628066630511903
Eval step 0: eval loss: 0.825107200630598
Eval: 2022-05-08 04:42:47.054891: total loss: 1.0704988030078872, mse:4.675097991877792, ic :0.1697819569786142, sharpe5:17.722900096178055, irr5:580.694580078125, ndcg5:0.8636661182821517, pnl5:5.454656600952148 
train 11, step: 0, loss: 1.2556078705046405, grad_norm: 0.3191591478930227, ic: 0.20855810878783457
train 11, step: 500, loss: 0.6567639490533917, grad_norm: 0.1296477522286359, ic: 0.5598830654322959
train 11, step: 1000, loss: 0.9338157509145714, grad_norm: 0.11904133383565693, ic: 0.051314431296343534
train 11, step: 1500, loss: 1.0552004094709428, grad_norm: 0.07595038378014324, ic: 0.18592137566184072
train 11, step: 2000, loss: 0.7911190858572769, grad_norm: 0.49417597269947494, ic: 0.0753235064604713
Epoch 11: 2022-05-08 04:46:24.512165: train loss: 1.6267370976707016
Eval step 0: eval loss: 0.8304654313339699
Eval: 2022-05-08 04:46:38.786551: total loss: 1.0714649512958518, mse:4.6270703289048765, ic :0.17686020899414448, sharpe5:17.400014029741286, irr5:579.0545043945312, ndcg5:0.8464386519064602, pnl5:4.113064765930176 
train 12, step: 0, loss: 0.9562902450561523, grad_norm: 0.08553196912871594, ic: 0.4106404560165213
train 12, step: 500, loss: 0.9361413304284398, grad_norm: 0.4273932487378377, ic: 0.16631860609364102
train 12, step: 1000, loss: 2.9781466927498013, grad_norm: 0.2652468192687564, ic: 0.35817647314236783
train 12, step: 1500, loss: 0.9301088424647753, grad_norm: 0.16644191065502978, ic: -0.08024784236835428
train 12, step: 2000, loss: 0.8759114706264162, grad_norm: 0.01639217676593871, ic: 0.20836137994268808
Epoch 12: 2022-05-08 04:50:15.503200: train loss: 1.6240240235178285
Eval step 0: eval loss: 0.8273330222191122
Eval: 2022-05-08 04:50:29.821106: total loss: 1.0679573151927957, mse:4.584512594583695, ic :0.18625461964758022, sharpe5:16.687777372598646, irr5:546.9287109375, ndcg5:0.8575088395762727, pnl5:7.938241481781006 
train 13, step: 0, loss: 2.0642918684414595, grad_norm: 0.9655208342085453, ic: 0.43197886541607416
train 13, step: 500, loss: 0.8096458362199437, grad_norm: 0.036428054321583705, ic: 0.5936451374154508
train 13, step: 1000, loss: 0.9444583355180368, grad_norm: 0.511672171984525, ic: 0.5848508758720605
train 13, step: 1500, loss: 2.375631983069867, grad_norm: 0.23624754955479288, ic: -0.1213880546282918
train 13, step: 2000, loss: 1.4642438293879907, grad_norm: 0.03702708373487462, ic: 0.19257983772578124
Epoch 13: 2022-05-08 04:54:06.448396: train loss: 1.6223734045193912
Eval step 0: eval loss: 0.8239964765542676
Eval: 2022-05-08 04:54:20.838210: total loss: 1.0675202722114374, mse:4.602461005679594, ic :0.18324292687929528, sharpe5:17.816075670719147, irr5:581.4878540039062, ndcg5:0.8263849533225136, pnl5:8.062569618225098 
train 14, step: 0, loss: 4.541410972845164, grad_norm: 1.3249878787496303, ic: 0.13696815349101066
train 14, step: 500, loss: 0.8289346024166189, grad_norm: 0.010868946849894243, ic: 0.08503121076985552
train 14, step: 1000, loss: 1.8315236896118072, grad_norm: 0.42100820791274784, ic: 0.451745434579313
train 14, step: 1500, loss: 1.124283675309066, grad_norm: 0.08531415525826216, ic: -0.04259586760572172
train 14, step: 2000, loss: 1.107684142075116, grad_norm: 0.4569166072314967, ic: 0.10278391384089396
Epoch 14: 2022-05-08 04:57:55.133626: train loss: 1.6226388625337966
Eval step 0: eval loss: 0.83158065747662
Eval: 2022-05-08 04:58:09.037309: total loss: 1.0700974931915739, mse:4.601475882688822, ic :0.17760382814729425, sharpe5:16.751122233867644, irr5:542.618896484375, ndcg5:0.8624584735234422, pnl5:6.099015235900879 
train 15, step: 0, loss: 3.3945479693579768, grad_norm: 0.7185106749034194, ic: 0.058289383878014474
train 15, step: 500, loss: 1.2625744598464776, grad_norm: 0.006100544691868116, ic: -0.10806578000676562
train 15, step: 1000, loss: 1.3080533655678355, grad_norm: 0.4906489343538457, ic: 0.09119771656927587
train 15, step: 1500, loss: 0.8553253414124016, grad_norm: 0.306938778952692, ic: 0.07351632536360357
train 15, step: 2000, loss: 1.4647859582578502, grad_norm: 0.5869426776657957, ic: 0.037661165546479136
Epoch 15: 2022-05-08 05:01:43.132782: train loss: 1.6217925645145268
Eval step 0: eval loss: 0.8348427261014884
Eval: 2022-05-08 05:01:57.084160: total loss: 1.0711550895653381, mse:4.589036889932142, ic :0.18534908500587702, sharpe5:17.94939822435379, irr5:574.1259155273438, ndcg5:0.8668248234125077, pnl5:5.336538314819336 
train 16, step: 0, loss: 0.6978311304619458, grad_norm: 0.46046683318556775, ic: 0.025496823762262918
train 16, step: 500, loss: 1.5740920915497618, grad_norm: 0.4243855995183449, ic: 0.20408706540297908
train 16, step: 1000, loss: 0.8807502515388258, grad_norm: 0.010254859253543774, ic: -0.13522033296333585
train 16, step: 1500, loss: 0.8628066283121553, grad_norm: 0.3543209645793658, ic: 0.1320801029857725
train 16, step: 2000, loss: 3.3558800190957885, grad_norm: 1.0011349637968998, ic: 0.01568710522065724
Epoch 16: 2022-05-08 05:05:29.432241: train loss: 1.6217060972531028
Eval step 0: eval loss: 0.8313718902298471
Eval: 2022-05-08 05:05:43.596424: total loss: 1.0695045943355312, mse:4.616842299767999, ic :0.16215872871251338, sharpe5:15.146028188467024, irr5:481.7384948730469, ndcg5:0.8517256911361194, pnl5:4.210542678833008 
train 17, step: 0, loss: 1.2819930423159813, grad_norm: 0.2231440169640037, ic: -0.13049767988932204
train 17, step: 500, loss: 1.7491134188685638, grad_norm: 2.724767928880768, ic: 0.23340608903813245
train 17, step: 1000, loss: 1.276520454997349, grad_norm: 0.08078294988014374, ic: 0.15491183129563738
train 17, step: 1500, loss: 4.529177405744856, grad_norm: 2.551998278284751, ic: 0.25326551990007784
train 17, step: 2000, loss: 1.2834554230683175, grad_norm: 0.5959684634359895, ic: 0.02989547529110187
Epoch 17: 2022-05-08 05:09:18.125943: train loss: 1.622499417694638
Eval step 0: eval loss: 0.8348326286098854
Eval: 2022-05-08 05:09:32.137592: total loss: 1.0698686130749249, mse:4.582792452855862, ic :0.19221602605394322, sharpe5:17.501150515079498, irr5:596.0578002929688, ndcg5:0.8571359134397849, pnl5:4.855249404907227 
train 18, step: 0, loss: 1.4187199002077597, grad_norm: 2.7584819936766993, ic: 0.134947616152011
train 18, step: 500, loss: 1.5058361235119047, grad_norm: 0.8546317724378927, ic: -0.046042994724238576
train 18, step: 1000, loss: 0.6550754494863014, grad_norm: 0.025124700538034374, ic: 0.5722231227398354
train 18, step: 1500, loss: 1.4366965953880018, grad_norm: 0.3342091699022301, ic: 0.10880832835137158
train 18, step: 2000, loss: 0.9108092678580315, grad_norm: 0.010323061143255641, ic: -0.011078333142662728
Epoch 18: 2022-05-08 05:13:09.652767: train loss: 1.6233360111415354
Eval step 0: eval loss: 0.8238620577170047
Eval: 2022-05-08 05:13:23.791934: total loss: 1.0664227857266253, mse:4.590628093912941, ic :0.18901581899489076, sharpe5:18.062345538139343, irr5:596.7171630859375, ndcg5:0.8531785150566789, pnl5:9.681957244873047 
train 19, step: 0, loss: 1.4849787636408731, grad_norm: 0.7780690578354048, ic: -0.028099626646395766
train 19, step: 500, loss: 0.870828699182581, grad_norm: 0.030491156436793544, ic: 0.2076255088876383
train 19, step: 1000, loss: 0.9613130724421033, grad_norm: 0.6142678040282656, ic: 0.18692834015513227
train 19, step: 1500, loss: 3.962049944000351, grad_norm: 1.0702972620562212, ic: 0.15212544194519914
train 19, step: 2000, loss: 1.0199491060697115, grad_norm: 0.13570759881028965, ic: 0.21638037985586636
Epoch 19: 2022-05-08 05:16:56.732597: train loss: 1.6215573869383415
Eval step 0: eval loss: 0.8321944820617097
Eval: 2022-05-08 05:17:10.632112: total loss: 1.0686598623875705, mse:4.587363559333775, ic :0.18729222173626575, sharpe5:18.2138279736042, irr5:594.4835205078125, ndcg5:0.8386238130341949, pnl5:12.719856262207031 
train 20, step: 0, loss: 2.2876061480978263, grad_norm: 1.0215603935961977, ic: 0.06926486178446822
train 20, step: 500, loss: 3.196327414772727, grad_norm: 0.4199985341822743, ic: 0.11172004542996694
train 20, step: 1000, loss: 0.9737337112426758, grad_norm: 0.1333636067726423, ic: 0.19073214042771
train 20, step: 1500, loss: 1.7963396104517613, grad_norm: 1.1157115831955176, ic: 0.2711188789797562
train 20, step: 2000, loss: 1.0214859680872757, grad_norm: 0.4940357618948464, ic: 0.05771728947966781
Epoch 20: 2022-05-08 05:20:44.730166: train loss: 1.6204741189804526
Eval step 0: eval loss: 0.8296875385891398
Eval: 2022-05-08 05:20:59.062054: total loss: 1.0669342939965774, mse:4.585099406740593, ic :0.19133559374781922, sharpe5:17.44203583359718, irr5:585.5866088867188, ndcg5:0.8488315721792676, pnl5:10.075164794921875 
train 21, step: 0, loss: 1.0240359331795863, grad_norm: 0.4593367034632629, ic: 0.06675934030238542
train 21, step: 500, loss: 0.7706203629485273, grad_norm: 0.014184223411087023, ic: 0.19377029417366617
train 21, step: 1000, loss: 0.9397914284153988, grad_norm: 0.8343991363543133, ic: 0.15426745524619567
train 21, step: 1500, loss: 0.9959011091612671, grad_norm: 0.3476439169905632, ic: 0.32546259687964174
train 21, step: 2000, loss: 0.9442179226796096, grad_norm: 0.15267539202443964, ic: 0.05058482642398507
Epoch 21: 2022-05-08 05:24:32.118485: train loss: 1.6210239566137281
Eval step 0: eval loss: 0.8294058378679859
Eval: 2022-05-08 05:24:46.278049: total loss: 1.067764972204373, mse:4.5902485661423915, ic :0.1854165517796255, sharpe5:18.2555943775177, irr5:601.1011352539062, ndcg5:0.842160679165942, pnl5:6.100764751434326 
train 22, step: 0, loss: 1.0412806279241702, grad_norm: 0.17370115265731467, ic: 0.2172044420700698
train 22, step: 500, loss: 3.2454706951854675, grad_norm: 0.9360047276201813, ic: -0.2074435872470959
train 22, step: 1000, loss: 1.1968301232839595, grad_norm: 0.10177550612802398, ic: 0.46250971830696475
train 22, step: 1500, loss: 0.9751556270897633, grad_norm: 0.9984895786950391, ic: 0.10133940740835128
train 22, step: 2000, loss: 1.8138552295918369, grad_norm: 1.6456354314111812, ic: 0.045574859104607505
Epoch 22: 2022-05-08 05:28:20.271428: train loss: 1.6209820660450869
Eval step 0: eval loss: 0.8336321204680913
Eval: 2022-05-08 05:28:34.331847: total loss: 1.0735144122495437, mse:4.706467039648843, ic :0.1429706074261721, sharpe5:12.835366262197494, irr5:418.9989013671875, ndcg5:0.8502005142741371, pnl5:3.1756556034088135 
train 23, step: 0, loss: 0.9897279415075649, grad_norm: 0.046072919381429024, ic: 0.19858913797573507
train 23, step: 500, loss: 1.4282321420906103, grad_norm: 0.15730228637539256, ic: -0.014000587727680982
train 23, step: 1000, loss: 1.6645064290364584, grad_norm: 0.11305565114521648, ic: 0.2555759131118565
train 23, step: 1500, loss: 1.11223170762783, grad_norm: 0.3552266240153111, ic: 0.09788389787238455
train 23, step: 2000, loss: 1.9414564313293878, grad_norm: 1.3684615260511461, ic: 0.44389466557322094
Epoch 23: 2022-05-08 05:32:09.957086: train loss: 1.621440707973953
Eval step 0: eval loss: 0.8352465614503753
Eval: 2022-05-08 05:32:23.978233: total loss: 1.0710256106550713, mse:4.627290956251495, ic :0.18208042239343594, sharpe5:17.073569390773773, irr5:561.2787475585938, ndcg5:0.8438645694211152, pnl5:5.574409008026123 
train 24, step: 0, loss: 2.2067827170399177, grad_norm: 0.25450352243828855, ic: 0.12248740882750446
train 24, step: 500, loss: 1.23086336484679, grad_norm: 0.158375162809897, ic: 0.11040562306040955
train 24, step: 1000, loss: 0.9098085020942354, grad_norm: 0.05946482285902774, ic: 0.5295590950812278
train 24, step: 1500, loss: 2.59771864372939, grad_norm: 2.6769689279093782, ic: 0.08211543876386575
train 24, step: 2000, loss: 0.9274430776283619, grad_norm: 0.20601017846140124, ic: 0.12491866212924607
Epoch 24: 2022-05-08 05:35:57.609592: train loss: 1.6176499408768283
Eval step 0: eval loss: 0.8241396422632375
Eval: 2022-05-08 05:36:11.718108: total loss: 1.0666120542795137, mse:4.598331414634598, ic :0.188153382772925, sharpe5:17.002244700193405, irr5:559.6340942382812, ndcg5:0.8405816254595525, pnl5:5.314634799957275 
train 25, step: 0, loss: 0.8475601608688768, grad_norm: 0.09664226154126086, ic: 0.6105727976432735
train 25, step: 500, loss: 0.8673287475217145, grad_norm: 0.017692801328106383, ic: 0.2767504705217867
train 25, step: 1000, loss: 2.09627210467441, grad_norm: 0.03783637812683663, ic: 0.24145032714615133
train 25, step: 1500, loss: 1.131759953273004, grad_norm: 1.290446954381029, ic: 0.5460086971831033
train 25, step: 2000, loss: 1.017886985762925, grad_norm: 1.0670895797113444, ic: 0.6051432448322558
Epoch 25: 2022-05-08 05:39:47.542309: train loss: 1.6186141212481935
Eval step 0: eval loss: 0.8281190186833179
Eval: 2022-05-08 05:40:01.811678: total loss: 1.0671795842074678, mse:4.582187399586479, ic :0.1911833599912269, sharpe5:17.410576236248016, irr5:585.1747436523438, ndcg5:0.8282403910076699, pnl5:10.082267761230469 
train 26, step: 0, loss: 6.69480705870607, grad_norm: 0.5350818997975741, ic: 0.10087709931714484
train 26, step: 500, loss: 3.817927977179437, grad_norm: 2.195182973532842, ic: 0.3768280059835569
train 26, step: 1000, loss: 1.2755088626496272, grad_norm: 2.5584335213448908, ic: 0.005062055467594905
train 26, step: 1500, loss: 0.8359214357940166, grad_norm: 0.17702634092740002, ic: 0.2999265838611316
train 26, step: 2000, loss: 0.960044782805124, grad_norm: 0.6028698162948449, ic: 0.17151080619042036
Epoch 26: 2022-05-08 05:43:36.004697: train loss: 1.6185813142468777
Eval step 0: eval loss: 0.8296649639423076
Eval: 2022-05-08 05:43:50.139170: total loss: 1.0682611094610168, mse:4.592434503529839, ic :0.18404913652150867, sharpe5:17.604869377613067, irr5:584.8707275390625, ndcg5:0.8428907074742408, pnl5:5.320356845855713 
train 27, step: 0, loss: 0.8242403875612745, grad_norm: 0.0455044570625665, ic: 0.1332403734005367
train 27, step: 500, loss: 0.8989556591763861, grad_norm: 3.030776006165377, ic: 0.29716154639466097
train 27, step: 1000, loss: 0.7566635946110064, grad_norm: 0.8934213783248582, ic: 0.18902248816443096
train 27, step: 1500, loss: 0.6578366484034956, grad_norm: 0.33186302477747115, ic: 0.31744689290975786
train 27, step: 2000, loss: 1.37923761018769, grad_norm: 0.03941965800008505, ic: 0.034927358852838396
Epoch 27: 2022-05-08 05:47:26.500371: train loss: 1.6215074438174824
Eval step 0: eval loss: 0.8326171617739067
Eval: 2022-05-08 05:47:40.766034: total loss: 1.0686103093530166, mse:4.588310494743106, ic :0.18862700580993624, sharpe5:17.75671504855156, irr5:600.001953125, ndcg5:0.8412602199271482, pnl5:6.566666603088379 
train 28, step: 0, loss: 1.5321503981660232, grad_norm: 0.27048999213964, ic: 0.22274060780794416
train 28, step: 500, loss: 1.3608100938024037, grad_norm: 2.1622482394141036, ic: 0.19805763862637232
train 28, step: 1000, loss: 0.9076457863577659, grad_norm: 0.9383671167737657, ic: 0.5784857870840018
train 28, step: 1500, loss: 1.0300372641681235, grad_norm: 0.05993765800098759, ic: 0.0451918595128394
train 28, step: 2000, loss: 1.0412528383219901, grad_norm: 0.10519251577924754, ic: 0.13722094006997268
Epoch 28: 2022-05-08 05:51:16.975604: train loss: 1.6178441209524201
Eval step 0: eval loss: 0.821906810314311
Eval: 2022-05-08 05:51:31.192043: total loss: 1.0706012243008676, mse:4.629283352132669, ic :0.17982884643944305, sharpe5:17.0082645714283, irr5:575.617919921875, ndcg5:0.8412419090197759, pnl5:6.9327545166015625 
train 29, step: 0, loss: 0.9099109988035443, grad_norm: 0.03189418126440374, ic: 0.09291368616608367
train 29, step: 500, loss: 1.0975481572041985, grad_norm: 0.25628929277597196, ic: 0.6197331257161927
train 29, step: 1000, loss: 1.0588060380492132, grad_norm: 0.2622619659719044, ic: 0.08895260297412781
train 29, step: 1500, loss: 2.367642238119633, grad_norm: 0.3847456305789591, ic: -0.08469304889521494
train 29, step: 2000, loss: 4.336244182822145, grad_norm: 3.524032180067567, ic: 0.2115377064127379
Epoch 29: 2022-05-08 05:55:04.266106: train loss: 1.6179057773949266
Eval step 0: eval loss: 0.8316464519601224
Eval: 2022-05-08 05:55:18.497405: total loss: 1.0685471610992257, mse:4.591591343637543, ic :0.18525938241950748, sharpe5:18.285444009304047, irr5:603.7567138671875, ndcg5:0.8515678379482559, pnl5:4.540609836578369 
train 30, step: 0, loss: 1.0111555253943032, grad_norm: 0.04459353263598645, ic: 0.5136665608429001
train 30, step: 500, loss: 1.423776452907096, grad_norm: 3.0794797796390814, ic: 0.012520130894909862
train 30, step: 1000, loss: 0.9753729617956913, grad_norm: 0.16307066963788536, ic: -0.04793241998319764
train 30, step: 1500, loss: 1.5294209721896759, grad_norm: 2.3186630629042866, ic: 0.1435991600437555
train 30, step: 2000, loss: 1.840072573462605, grad_norm: 0.4393517796947347, ic: 0.05257090543404673
Epoch 30: 2022-05-08 05:58:51.374654: train loss: 1.6184031719142173
Eval step 0: eval loss: 0.8412487805831796
Eval: 2022-05-08 05:59:05.674327: total loss: 1.0724975662626914, mse:4.635619258776447, ic :0.18721437981919678, sharpe5:18.204162504673004, irr5:606.84326171875, ndcg5:0.8305586301105614, pnl5:5.770901679992676 
train 31, step: 0, loss: 1.0528475659627803, grad_norm: 0.24148092425360163, ic: 0.3502977906199073
train 31, step: 500, loss: 1.48676858281893, grad_norm: 2.461643838796098, ic: 0.06240172379647136
train 31, step: 1000, loss: 4.358115989095433, grad_norm: 1.966519658648808, ic: 0.4704063646139732
train 31, step: 1500, loss: 0.7665332941494675, grad_norm: 0.032720304055428084, ic: 0.7127581559776193
train 31, step: 2000, loss: 1.295201634925912, grad_norm: 1.4871228780779453, ic: 0.0849936856497614
Epoch 31: 2022-05-08 06:02:40.781565: train loss: 1.614698305312959
Eval step 0: eval loss: 0.8350312983650553
Eval: 2022-05-08 06:02:55.072329: total loss: 1.0710438952981802, mse:4.652517799830193, ic :0.171640061134619, sharpe5:16.988619586229323, irr5:547.8775634765625, ndcg5:0.8415218038321882, pnl5:6.169707775115967 
train 32, step: 0, loss: 1.1299197285415044, grad_norm: 0.04512925995521512, ic: 0.1657358398608902
train 32, step: 500, loss: 1.4799456739050196, grad_norm: 2.023102607546436, ic: 0.07664507153527955
train 32, step: 1000, loss: 1.0559157476436958, grad_norm: 0.21022845611768642, ic: 0.5051795857131955
train 32, step: 1500, loss: 0.9876802096741515, grad_norm: 6.429693108507871, ic: 0.10312261333177652
train 32, step: 2000, loss: 0.9472525218325879, grad_norm: 0.25008583020560615, ic: 0.5513968498116403
Epoch 32: 2022-05-08 06:06:29.219652: train loss: 1.6147172525893707
Eval step 0: eval loss: 0.8223160481427818
Eval: 2022-05-08 06:06:43.269558: total loss: 1.065837463598374, mse:4.588881887011802, ic :0.1934179122428957, sharpe5:18.814791605472564, irr5:620.9609985351562, ndcg5:0.8390158521157737, pnl5:5.949645042419434 
train 33, step: 0, loss: 1.2606734625292475, grad_norm: 0.8565410326514373, ic: 0.2117905853207046
train 33, step: 500, loss: 0.9960060906464858, grad_norm: 0.013325451094800891, ic: 0.13543566073233512
train 33, step: 1000, loss: 1.075564794346195, grad_norm: 3.6197046129825394, ic: 0.1970780231363829
train 33, step: 1500, loss: 0.9236789038172821, grad_norm: 0.2856593627671338, ic: 0.5411806911940845
train 33, step: 2000, loss: 0.8110921772081765, grad_norm: 0.2853842087217411, ic: 0.2564201226697995
Epoch 33: 2022-05-08 06:10:16.422434: train loss: 1.6163415023404832
Eval step 0: eval loss: 0.8294767775701396
Eval: 2022-05-08 06:10:30.517562: total loss: 1.067803811518219, mse:4.5804758557762515, ic :0.1890628260026997, sharpe5:17.51016707897186, irr5:586.87841796875, ndcg5:0.8349487051807579, pnl5:4.619194984436035 
train 34, step: 0, loss: 1.020259605688324, grad_norm: 2.7754919523514023, ic: 0.6096770649400697
train 34, step: 500, loss: 0.8113265197940558, grad_norm: 1.5088844258701481, ic: 0.22483147928735706
train 34, step: 1000, loss: 3.118240567396313, grad_norm: 2.3083277589599227, ic: 0.3169468971295697
train 34, step: 1500, loss: 0.8060496242736889, grad_norm: 1.4826930715878124, ic: 0.6806100159611268
train 34, step: 2000, loss: 6.4896080436677, grad_norm: 7.47042252962251, ic: 0.4415351306129309
Epoch 34: 2022-05-08 06:14:06.390039: train loss: 1.6172133486270648
Eval step 0: eval loss: 0.8241878786880926
Eval: 2022-05-08 06:14:20.349353: total loss: 1.0679597783908588, mse:4.5965798321306846, ic :0.18653123962138365, sharpe5:17.522795187234877, irr5:580.2542114257812, ndcg5:0.8592928515592834, pnl5:9.118548393249512 
train 35, step: 0, loss: 1.2201068474264705, grad_norm: 0.6929157019587661, ic: 0.5501899351795606
train 35, step: 500, loss: 1.1900254481168235, grad_norm: 0.7940716107390244, ic: 0.12346337492051655
train 35, step: 1000, loss: 1.713801746408904, grad_norm: 7.423334975422991, ic: 0.08150788686105312
train 35, step: 1500, loss: 1.6365469337406016, grad_norm: 1.8419030988268899, ic: 0.006986588083025708
train 35, step: 2000, loss: 0.7852750564004011, grad_norm: 0.15678567684205322, ic: 0.566956912140942
Epoch 35: 2022-05-08 06:17:54.560343: train loss: 1.6167073955949858
Eval step 0: eval loss: 0.8329335927209562
Eval: 2022-05-08 06:18:08.881580: total loss: 1.0701155723607727, mse:4.653263831362505, ic :0.1797197798458382, sharpe5:17.75209136366844, irr5:589.3366088867188, ndcg5:0.8563956773955608, pnl5:6.513098239898682 
train 36, step: 0, loss: 1.8193865286499216, grad_norm: 3.655371237551172, ic: 0.127169575889524
train 36, step: 500, loss: 0.8365339034180458, grad_norm: 0.20652409270061606, ic: 0.22152001482036932
train 36, step: 1000, loss: 1.6668549360795453, grad_norm: 4.315234934169473, ic: 0.22254888804489698
train 36, step: 1500, loss: 0.7604744101283301, grad_norm: 0.23224543145835183, ic: 0.3993668299814396
train 36, step: 2000, loss: 1.1235830535767473, grad_norm: 2.711569671127638, ic: 0.7631238570484329
Epoch 36: 2022-05-08 06:21:39.236390: train loss: 1.6110913282455819
Eval step 0: eval loss: 0.8280853818163856
Eval: 2022-05-08 06:21:53.351011: total loss: 1.0690801766044522, mse:4.639460077783875, ic :0.18213621964007268, sharpe5:17.924843884706497, irr5:595.7265625, ndcg5:0.8342359949020628, pnl5:6.900304317474365 
train 37, step: 0, loss: 2.032327882728752, grad_norm: 1.1338093726528111, ic: 0.1953375900873892
train 37, step: 500, loss: 2.329971985560257, grad_norm: 1.3284351607723068, ic: -0.05229998643679998
train 37, step: 1000, loss: 1.0742973432148972, grad_norm: 0.11914313070172639, ic: 0.06718669143427718
train 37, step: 1500, loss: 2.0201822277889523, grad_norm: 4.089682698964183, ic: 0.574189287297202
train 37, step: 2000, loss: 1.3166189605611505, grad_norm: 0.22014383518822767, ic: 0.20648189201808992
Epoch 37: 2022-05-08 06:25:28.918937: train loss: 1.6112093855129015
Eval step 0: eval loss: 0.8241144306918466
Eval: 2022-05-08 06:25:42.910060: total loss: 1.0678266527162388, mse:4.599563868926816, ic :0.1890617526113863, sharpe5:18.97020909190178, irr5:623.888427734375, ndcg5:0.856650026504923, pnl5:12.397604942321777 
train 38, step: 0, loss: 1.3321304321289062, grad_norm: 0.35327639518763426, ic: -0.07658521372462414
train 38, step: 500, loss: 0.9066620249807099, grad_norm: 0.19917670705348875, ic: 0.2794900792610323
train 38, step: 1000, loss: 0.8989344660943676, grad_norm: 0.3576732863315224, ic: 0.19450338818544538
train 38, step: 1500, loss: 0.9575534551181664, grad_norm: 0.19922587502999628, ic: 0.20802404482434655
train 38, step: 2000, loss: 2.3159408338840635, grad_norm: 12.166486846085268, ic: -0.005017560369235131
Epoch 38: 2022-05-08 06:29:17.635631: train loss: 1.6149950009015326
Eval step 0: eval loss: 0.8254851812557625
Eval: 2022-05-08 06:29:31.959902: total loss: 1.0650993032563985, mse:4.582409001808649, ic :0.19273871918552943, sharpe5:19.20343464374542, irr5:618.1364135742188, ndcg5:0.8541324714247992, pnl5:5.140964984893799 
train 39, step: 0, loss: 0.972167782098146, grad_norm: 0.003036515720904128, ic: 0.06289204022884759
train 39, step: 500, loss: 0.8895091966516141, grad_norm: 0.046855182414591856, ic: 0.2328442875340395
train 39, step: 1000, loss: 0.9446183819854341, grad_norm: 0.038136249241938175, ic: 0.17315463066386255
train 39, step: 1500, loss: 2.075555719777635, grad_norm: 4.928361999549477, ic: 0.2461569906335969
train 39, step: 2000, loss: 0.6212833157459519, grad_norm: 0.28103873857918676, ic: 0.14199022860794627
Epoch 39: 2022-05-08 06:33:06.092254: train loss: 1.6142982451537713
Eval step 0: eval loss: 0.8303493423381519
Eval: 2022-05-08 06:33:20.246105: total loss: 1.0679523942309235, mse:4.59640783668308, ic :0.18421482047018234, sharpe5:17.677609173059462, irr5:587.967529296875, ndcg5:0.8351511659291893, pnl5:7.1056437492370605 
