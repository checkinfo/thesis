Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_60_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
56376
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884448757574277, grad_norm: 5.086148699794668, ic: 0.011244232252125952
train 0, step: 500, loss: 0.8626531444282216, grad_norm: 0.021927736791833095, ic: 0.05364667851450018
train 0, step: 1000, loss: 1.9453951899117594, grad_norm: 0.5891990501598344, ic: 0.007582847747125924
train 0, step: 1500, loss: 0.9750858834609684, grad_norm: 0.11896367387012924, ic: 0.0736854221103438
train 0, step: 2000, loss: 0.9941664627767278, grad_norm: 0.15925182472935806, ic: 0.024089278970536032
Epoch 0: 2022-05-07 21:50:52.511798: train loss: 1.649376414806499
Eval step 0: eval loss: 0.8351428209793202
Eval: 2022-05-07 21:51:12.974005: total loss: 1.0789984327471536, mse:4.823396879732328, ic :0.007316884100194523, sharpe5:7.395292152464389, irr5:208.5555419921875, ndcg5:0.8296027477298022, pnl5:2.6887781620025635 
train 1, step: 0, loss: 2.7626112414944557, grad_norm: 0.9542881563521848, ic: 0.0613853133693705
train 1, step: 500, loss: 1.7645436521593771, grad_norm: 0.8589965113147499, ic: 0.10880079628169345
train 1, step: 1000, loss: 0.8746630698932092, grad_norm: 0.19420435653799228, ic: 0.05954773357981828
train 1, step: 1500, loss: 1.7121370162086924, grad_norm: 0.23704735206739808, ic: -0.02256082578054662
train 1, step: 2000, loss: 2.19338046875, grad_norm: 0.9754867326812479, ic: 0.04591234208333543
Epoch 1: 2022-05-07 21:55:49.248132: train loss: 1.6469020600524853
Eval step 0: eval loss: 0.8340879225253556
Eval: 2022-05-07 21:56:09.091415: total loss: 1.0788289178362496, mse:4.8238877484676586, ic :0.008691644158021487, sharpe5:7.280500756502152, irr5:206.1155242919922, ndcg5:0.8420741896425923, pnl5:2.4249141216278076 
train 2, step: 0, loss: 2.142989346590909, grad_norm: 0.010763482379866615, ic: 0.12560039845206108
train 2, step: 500, loss: 3.305476467747457, grad_norm: 0.3102338117867481, ic: 0.052902897888598116
train 2, step: 1000, loss: 2.0724818905651343, grad_norm: 0.00024731624309096687, ic: 0.21244445638419923
train 2, step: 1500, loss: 1.4862322392354486, grad_norm: 0.07075565264341832, ic: -0.012632872027270646
train 2, step: 2000, loss: 3.2350893930288462, grad_norm: 0.8790216958375316, ic: 0.2312929077704856
Epoch 2: 2022-05-07 22:00:43.708006: train loss: 1.6467506472561648
Eval step 0: eval loss: 0.8360096616915832
Eval: 2022-05-07 22:01:03.800325: total loss: 1.0794216356648638, mse:4.822570579680207, ic :0.013553122902452964, sharpe5:7.455621402561664, irr5:211.31039428710938, ndcg5:0.8488064376394104, pnl5:2.5887606143951416 
train 3, step: 0, loss: 1.5239479126969004, grad_norm: 0.6086928935666533, ic: 0.00748560338608027
train 3, step: 500, loss: 1.4948564139584966, grad_norm: 0.3880127769243421, ic: 0.13875549907810505
train 3, step: 1000, loss: 3.6698901100136734, grad_norm: 0.7960168953462337, ic: -0.05119833199406977
train 3, step: 1500, loss: 1.9854148952153234, grad_norm: 1.0350640491536365, ic: -0.06869662848534122
train 3, step: 2000, loss: 0.8994267314189189, grad_norm: 0.0008766222729944897, ic: 0.0025297849999324704
Epoch 3: 2022-05-07 22:05:40.600945: train loss: 1.6460050372646526
Eval step 0: eval loss: 0.833323085772853
Eval: 2022-05-07 22:05:59.333803: total loss: 1.0785888882009327, mse:4.824210337778085, ic :0.02348819019498437, sharpe5:7.923478624820709, irr5:220.5514373779297, ndcg5:0.8524749691164505, pnl5:2.9052703380584717 
train 4, step: 0, loss: 1.4371157525510205, grad_norm: 0.05233122402206477, ic: 0.10229891711627545
train 4, step: 500, loss: 1.6487250861220473, grad_norm: 0.6616576982221789, ic: 0.04682133740420033
train 4, step: 1000, loss: 2.945305614936643, grad_norm: 0.7325681860623852, ic: 0.07378522965861148
train 4, step: 1500, loss: 2.1470404387526374, grad_norm: 0.5001250274607858, ic: 0.006139364254114706
train 4, step: 2000, loss: 1.0583395475918547, grad_norm: 0.48239513154229674, ic: 0.24230243131807266
Epoch 4: 2022-05-07 22:10:38.043868: train loss: 1.6436727374344964
Eval step 0: eval loss: 0.8522574904093124
Eval: 2022-05-07 22:10:57.742021: total loss: 1.0860018027899503, mse:4.74199553246999, ic :0.13912354932059817, sharpe5:11.859259843826294, irr5:386.8556213378906, ndcg5:0.8602328589107047, pnl5:2.992675542831421 
train 5, step: 0, loss: 1.3525506141201762, grad_norm: 0.2772810794212308, ic: 0.3332367653942119
train 5, step: 500, loss: 0.8831600276937946, grad_norm: 0.012944310014330725, ic: 0.8921029562420729
train 5, step: 1000, loss: 0.9838769905411878, grad_norm: 0.17413526814774988, ic: -0.00256600140422834
train 5, step: 1500, loss: 1.527809318320492, grad_norm: 0.17268493318771544, ic: 0.008726115026356746
train 5, step: 2000, loss: 1.1020611189910818, grad_norm: 0.02937336050091082, ic: 0.1723109169204594
Epoch 5: 2022-05-07 22:15:28.031167: train loss: 1.6379475369466803
Eval step 0: eval loss: 0.8330782376802884
Eval: 2022-05-07 22:15:47.984052: total loss: 1.0753838679912513, mse:4.715334120299462, ic :0.13664123547416976, sharpe5:11.818099778294563, irr5:388.61004638671875, ndcg5:0.8426440169318107, pnl5:4.343478679656982 
train 6, step: 0, loss: 1.3315900836074561, grad_norm: 0.4854023061183623, ic: 0.0990721923810277
train 6, step: 500, loss: 1.005808714885624, grad_norm: 0.048940200847749527, ic: 0.04973339389792331
train 6, step: 1000, loss: 1.116934076461502, grad_norm: 0.09340032318809688, ic: 0.1898673283710109
train 6, step: 1500, loss: 1.5741884846332643, grad_norm: 0.8214664899095439, ic: 0.14914678818552624
train 6, step: 2000, loss: 0.813035633584993, grad_norm: 0.06925757974319047, ic: 0.024262676425350287
Epoch 6: 2022-05-07 22:20:17.993404: train loss: 1.6354120781194301
Eval step 0: eval loss: 0.8299218389834694
Eval: 2022-05-07 22:20:37.763895: total loss: 1.0734039956574988, mse:4.726498400538591, ic :0.14066544488415397, sharpe5:14.146551729440688, irr5:440.9781494140625, ndcg5:0.8419962680731575, pnl5:4.979410648345947 
train 7, step: 0, loss: 0.9913290023803711, grad_norm: 0.055308680328710946, ic: 0.09307520219123694
train 7, step: 500, loss: 0.6525195739189542, grad_norm: 0.0032376667023465006, ic: 0.0327884789025487
train 7, step: 1000, loss: 1.0228590373310167, grad_norm: 0.19819636091566822, ic: 0.07601989575853142
train 7, step: 1500, loss: 2.242718172226688, grad_norm: 0.6859422747260691, ic: 0.4469680132447711
train 7, step: 2000, loss: 0.9166015230588651, grad_norm: 0.07613428624804502, ic: -0.037886924798149924
Epoch 7: 2022-05-07 22:25:16.094665: train loss: 1.6311572487900374
Eval step 0: eval loss: 0.8429128729254478
Eval: 2022-05-07 22:25:36.251965: total loss: 1.080192138130296, mse:4.741992657687421, ic :0.14687844142015405, sharpe5:12.915596843361854, irr5:437.0976867675781, ndcg5:0.8472576712125501, pnl5:2.744781732559204 
train 8, step: 0, loss: 3.525379302536232, grad_norm: 3.678808968432741, ic: 0.17119968232726399
train 8, step: 500, loss: 2.715541294642857, grad_norm: 1.1911833632982647, ic: 0.05212968001792948
train 8, step: 1000, loss: 3.0745053498641304, grad_norm: 1.0473856754147763, ic: 0.10676253162387059
train 8, step: 1500, loss: 0.7110426370077451, grad_norm: 0.035713136999216646, ic: 0.48624502436868605
train 8, step: 2000, loss: 1.0970561971735422, grad_norm: 0.4663359226305014, ic: 0.5123800670531237
Epoch 8: 2022-05-07 22:30:05.336203: train loss: 1.630644834548913
Eval step 0: eval loss: 0.8277446397112092
Eval: 2022-05-07 22:30:24.978171: total loss: 1.0700917354048831, mse:4.680269567020567, ic :0.16791832264765072, sharpe5:16.59864657640457, irr5:547.0431518554688, ndcg5:0.8501766439533858, pnl5:7.110591888427734 
train 9, step: 0, loss: 5.46722441312799, grad_norm: 0.9459983638052779, ic: -0.01713997146195515
train 9, step: 500, loss: 1.354092684659091, grad_norm: 1.3108484735005812, ic: 0.321718576686343
train 9, step: 1000, loss: 0.9310231576803674, grad_norm: 0.06508745899373342, ic: 0.06619722252312155
train 9, step: 1500, loss: 1.0929613970165224, grad_norm: 0.013923913997845535, ic: 0.3927319964427679
train 9, step: 2000, loss: 1.0658251160782202, grad_norm: 0.5278229372150747, ic: 0.24711769537601563
Epoch 9: 2022-05-07 22:34:51.931553: train loss: 1.6276714999955462
Eval step 0: eval loss: 0.8287416544553476
Eval: 2022-05-07 22:35:11.511717: total loss: 1.0723876632999871, mse:4.690834203557514, ic :0.15944330159684394, sharpe5:17.133457839488983, irr5:555.9739990234375, ndcg5:0.8668868241643194, pnl5:6.3372321128845215 
train 10, step: 0, loss: 7.084526039768586, grad_norm: 1.956092535324377, ic: 0.2701229839018457
train 10, step: 500, loss: 1.124739762190934, grad_norm: 0.09680739876044057, ic: 0.06968652451039066
train 10, step: 1000, loss: 2.384904919957822, grad_norm: 0.8637531333608548, ic: 0.10245162024999684
train 10, step: 1500, loss: 1.1177121447278309, grad_norm: 0.3058845188671606, ic: -0.002545718958783777
train 10, step: 2000, loss: 2.7298461543028116, grad_norm: 0.39444796907060437, ic: 0.4521833742542307
Epoch 10: 2022-05-07 22:39:42.801851: train loss: 1.6275365236606685
Eval step 0: eval loss: 0.8284824640657928
Eval: 2022-05-07 22:40:02.785923: total loss: 1.0704801392719046, mse:4.670654986172338, ic :0.1684817426299559, sharpe5:17.18988420009613, irr5:559.4712524414062, ndcg5:0.8396145233569096, pnl5:5.561473369598389 
train 11, step: 0, loss: 1.2519076200575214, grad_norm: 0.030385176292557357, ic: 0.2092069987581579
train 11, step: 500, loss: 0.6579910055950728, grad_norm: 0.0819871160021528, ic: 0.5992753653970748
train 11, step: 1000, loss: 0.9365726480381802, grad_norm: 0.1240718173018829, ic: 0.03389594398814651
train 11, step: 1500, loss: 1.0586861058285362, grad_norm: 1.4525319376374837, ic: 0.15563745753913508
train 11, step: 2000, loss: 0.7862786026362559, grad_norm: 0.003242623018077342, ic: 0.1223134278792159
Epoch 11: 2022-05-07 22:44:34.719199: train loss: 1.6256356130486
Eval step 0: eval loss: 0.8312138677020218
Eval: 2022-05-07 22:44:54.557890: total loss: 1.0704275615321273, mse:4.639471091633835, ic :0.17327833281800312, sharpe5:17.14622594475746, irr5:568.1111450195312, ndcg5:0.8458388862259745, pnl5:8.170339584350586 
train 12, step: 0, loss: 0.9587790171305338, grad_norm: 0.06699086489778103, ic: 0.4005362924745255
train 12, step: 500, loss: 0.9247996887157044, grad_norm: 0.12996930343672497, ic: 0.1801874875280479
train 12, step: 1000, loss: 2.949677291189789, grad_norm: 0.3952115540755513, ic: 0.39276859375130446
train 12, step: 1500, loss: 0.943980697085634, grad_norm: 0.1256459097501644, ic: -0.12623778083006076
train 12, step: 2000, loss: 0.8727752593348754, grad_norm: 0.006329830473220405, ic: 0.24127847035966182
Epoch 12: 2022-05-07 22:49:30.705079: train loss: 1.62209066577843
Eval step 0: eval loss: 0.8278977742813158
Eval: 2022-05-07 22:49:50.467989: total loss: 1.0679348961926847, mse:4.611538415360438, ic :0.17797836024768127, sharpe5:16.965724400281907, irr5:549.6879272460938, ndcg5:0.8540821926978464, pnl5:9.326850891113281 
train 13, step: 0, loss: 2.0413114553771776, grad_norm: 1.1156334079048609, ic: 0.44184162388140136
train 13, step: 500, loss: 0.8064648470807043, grad_norm: 0.05308141497013087, ic: 0.597872486989259
train 13, step: 1000, loss: 0.9427395199769295, grad_norm: 0.40561754269123756, ic: 0.5976492594228169
train 13, step: 1500, loss: 2.3888174826795474, grad_norm: 0.3150380834316284, ic: -0.08222366495709058
train 13, step: 2000, loss: 1.4731549622004907, grad_norm: 0.111300050284458, ic: 0.18254762561031895
Epoch 13: 2022-05-07 22:54:19.040538: train loss: 1.6215491232911035
Eval step 0: eval loss: 0.8225729231839436
Eval: 2022-05-07 22:54:38.780149: total loss: 1.0678414340177582, mse:4.608789458325794, ic :0.17892386735400123, sharpe5:17.29713704109192, irr5:567.830810546875, ndcg5:0.8576581990461026, pnl5:7.731696605682373 
train 14, step: 0, loss: 4.536322488055772, grad_norm: 1.3990993974826915, ic: 0.16840225947284349
train 14, step: 500, loss: 0.8257480819655486, grad_norm: 0.0066873834714320825, ic: 0.1265649058252802
train 14, step: 1000, loss: 1.8269474895596052, grad_norm: 0.30151261322374834, ic: 0.44249646280494787
train 14, step: 1500, loss: 1.1240157721742543, grad_norm: 0.10969476259019376, ic: -0.04346026120484536
train 14, step: 2000, loss: 1.143628113784259, grad_norm: 0.27438778651541373, ic: 0.10022894292142213
Epoch 14: 2022-05-07 22:59:14.530532: train loss: 1.6211683456331476
Eval step 0: eval loss: 0.834519477739726
Eval: 2022-05-07 22:59:34.602543: total loss: 1.0698403434090167, mse:4.6025439942182, ic :0.17729051254737518, sharpe5:16.639031928777694, irr5:558.101806640625, ndcg5:0.8490582942408469, pnl5:6.5913615226745605 
train 15, step: 0, loss: 3.3910008055690666, grad_norm: 0.8187628541508389, ic: 0.09725384329236715
train 15, step: 500, loss: 1.2587786109271093, grad_norm: 0.029903105918737627, ic: -0.021587649520849028
train 15, step: 1000, loss: 1.3143415745680895, grad_norm: 0.16015965858253253, ic: 0.07390813454036683
train 15, step: 1500, loss: 0.8494274229515256, grad_norm: 0.25277192135448867, ic: 0.06913131115141477
train 15, step: 2000, loss: 1.4618927345951591, grad_norm: 0.7111945964781997, ic: 0.05311513131275651
Epoch 15: 2022-05-07 23:04:07.860436: train loss: 1.620450019142906
Eval step 0: eval loss: 0.8399597746805848
Eval: 2022-05-07 23:04:27.998453: total loss: 1.07182310296512, mse:4.590806657371915, ic :0.18354092800772023, sharpe5:16.750057121515272, irr5:570.3627319335938, ndcg5:0.8387019503968756, pnl5:5.567775726318359 
train 16, step: 0, loss: 0.6897264542507917, grad_norm: 0.9758709787623533, ic: 0.018534869357934112
train 16, step: 500, loss: 1.5904072508602436, grad_norm: 0.4140078587108439, ic: 0.1628922725825392
train 16, step: 1000, loss: 0.8788819284150095, grad_norm: 0.009019629156214584, ic: -0.11194213371737116
train 16, step: 1500, loss: 0.8494698935554571, grad_norm: 0.286714103753247, ic: 0.16068543715426148
train 16, step: 2000, loss: 3.3225512957395744, grad_norm: 1.3630452769493604, ic: 0.022636652315704324
Epoch 16: 2022-05-07 23:09:02.015940: train loss: 1.6203512046990418
Eval step 0: eval loss: 0.8295374268349908
Eval: 2022-05-07 23:09:22.116482: total loss: 1.0680239002968586, mse:4.6006395033233725, ic :0.17637115119256658, sharpe5:16.01522986650467, irr5:533.160888671875, ndcg5:0.8538798614527068, pnl5:4.759111404418945 
train 17, step: 0, loss: 1.2729796553796418, grad_norm: 0.3238813027696657, ic: -0.08994436919983353
train 17, step: 500, loss: 1.7535751714939025, grad_norm: 1.3990324082026906, ic: 0.23148910408229822
train 17, step: 1000, loss: 1.2766409719230516, grad_norm: 0.10610420136570364, ic: 0.1452820142403247
train 17, step: 1500, loss: 4.5287371992101395, grad_norm: 1.4918679032160531, ic: 0.20645466448608088
train 17, step: 2000, loss: 1.2767659310983492, grad_norm: 1.1252108638670197, ic: 0.08751424971248059
Epoch 17: 2022-05-07 23:13:58.453332: train loss: 1.6196989254606706
Eval step 0: eval loss: 0.8372356386657007
Eval: 2022-05-07 23:14:18.439859: total loss: 1.0691673004900828, mse:4.590410487145156, ic :0.18735125760686536, sharpe5:17.38592541217804, irr5:585.459228515625, ndcg5:0.850549801078418, pnl5:8.668445587158203 
train 18, step: 0, loss: 1.42442704747738, grad_norm: 1.047931303486451, ic: 0.14812140586919956
train 18, step: 500, loss: 1.4752184457601896, grad_norm: 1.8923840574083013, ic: -0.027544516513905763
train 18, step: 1000, loss: 0.6463596960616438, grad_norm: 0.05338911963668231, ic: 0.5866294978802586
train 18, step: 1500, loss: 1.4163135525440214, grad_norm: 0.09066651971215313, ic: 0.2507064611905232
train 18, step: 2000, loss: 0.9107458029583002, grad_norm: 0.019941264145963873, ic: 0.010015984454718626
Epoch 18: 2022-05-07 23:18:53.056429: train loss: 1.6198524090161561
Eval step 0: eval loss: 0.8246401434075342
Eval: 2022-05-07 23:19:13.151075: total loss: 1.0651885017699232, mse:4.59088285287097, ic :0.18798481906962572, sharpe5:17.653244018554688, irr5:603.5435180664062, ndcg5:0.8599805205147407, pnl5:6.329213619232178 
train 19, step: 0, loss: 1.479253181578621, grad_norm: 0.9017116465667594, ic: -0.006844867892024131
train 19, step: 500, loss: 0.8598144672535083, grad_norm: 0.03700115063814051, ic: 0.22482858491311897
train 19, step: 1000, loss: 0.9535340328694476, grad_norm: 0.015440512938886063, ic: 0.2148687717171835
train 19, step: 1500, loss: 3.9516357936577653, grad_norm: 1.239720314070734, ic: 0.15153798961844056
train 19, step: 2000, loss: 1.0100226299579327, grad_norm: 0.2827832465757763, ic: 0.19219728835276892
Epoch 19: 2022-05-07 23:23:41.730735: train loss: 1.6193446754394152
Eval step 0: eval loss: 0.8283866986836472
Eval: 2022-05-07 23:24:01.398114: total loss: 1.0670431203308093, mse:4.589824728412454, ic :0.18808265711245764, sharpe5:16.925263373851774, irr5:586.3695678710938, ndcg5:0.835709163829666, pnl5:4.528741359710693 
train 20, step: 0, loss: 2.281442224555336, grad_norm: 1.8682688632090303, ic: 0.0723121788410704
train 20, step: 500, loss: 3.2112982954545455, grad_norm: 0.6544937639954873, ic: 0.0473887852196961
train 20, step: 1000, loss: 0.9705562591552734, grad_norm: 0.10401867858224166, ic: 0.11749471655205057
train 20, step: 1500, loss: 1.7905419248899312, grad_norm: 1.8907213385981496, ic: 0.26594961396452155
train 20, step: 2000, loss: 1.0038483014503468, grad_norm: 0.38101990793960094, ic: 0.1420643331642206
Epoch 20: 2022-05-07 23:28:33.836405: train loss: 1.6194506754951938
Eval step 0: eval loss: 0.8314473963135537
Eval: 2022-05-07 23:28:53.943038: total loss: 1.066499093704645, mse:4.584466941363282, ic :0.18860451256133307, sharpe5:16.914897919893264, irr5:577.4076538085938, ndcg5:0.8568278969770109, pnl5:5.2056403160095215 
train 21, step: 0, loss: 1.0174044704364513, grad_norm: 0.5835048516009873, ic: 0.06847364186068153
train 21, step: 500, loss: 0.7649062165116842, grad_norm: 0.02230187479745535, ic: 0.2225346575021782
train 21, step: 1000, loss: 0.9349342480040432, grad_norm: 0.7852057586337093, ic: 0.15265898072590145
train 21, step: 1500, loss: 0.9888495932044186, grad_norm: 0.35372813793288993, ic: 0.31190366390120733
train 21, step: 2000, loss: 0.9397374149968853, grad_norm: 0.11516074543134736, ic: 0.07632678110404623
Epoch 21: 2022-05-07 23:33:21.797293: train loss: 1.6182678401344373
Eval step 0: eval loss: 0.8288156169734588
Eval: 2022-05-07 23:33:41.404989: total loss: 1.0678055948080147, mse:4.594674713628945, ic :0.18234496595170685, sharpe5:16.92061131119728, irr5:568.3656005859375, ndcg5:0.862096156996166, pnl5:4.494062423706055 
train 22, step: 0, loss: 1.0439760024938207, grad_norm: 0.021349265661291875, ic: 0.2021084664619266
train 22, step: 500, loss: 3.254078140879065, grad_norm: 0.9473078885962053, ic: -0.23812810460086642
train 22, step: 1000, loss: 1.1948174449060693, grad_norm: 0.10464126247354885, ic: 0.45925835366373524
train 22, step: 1500, loss: 0.9747544528034979, grad_norm: 0.13002170431178334, ic: 0.08695131301248772
train 22, step: 2000, loss: 1.8351439045670352, grad_norm: 11.957845340188086, ic: 0.12746095223924261
Epoch 22: 2022-05-07 23:38:10.653681: train loss: 1.6179346211106371
Eval step 0: eval loss: 0.8292785580216017
Eval: 2022-05-07 23:38:30.519387: total loss: 1.0686366797840798, mse:4.618104646866086, ic :0.17812352919814523, sharpe5:16.376985912322997, irr5:550.91357421875, ndcg5:0.8541020699369883, pnl5:5.239255428314209 
train 23, step: 0, loss: 0.9760507193353747, grad_norm: 0.03689443584773286, ic: 0.1897514490468642
train 23, step: 500, loss: 1.4271871665583302, grad_norm: 0.20789370874838983, ic: 0.009568240804406089
train 23, step: 1000, loss: 1.6491995239257813, grad_norm: 0.11032916887545412, ic: 0.26020057174274414
train 23, step: 1500, loss: 1.1251188850549214, grad_norm: 1.242714009194863, ic: 0.08091252451047419
train 23, step: 2000, loss: 1.9248309998075441, grad_norm: 1.448125988228574, ic: 0.4405459100561725
Epoch 23: 2022-05-07 23:43:02.188538: train loss: 1.617260838677829
Eval step 0: eval loss: 0.8333281023610378
Eval: 2022-05-07 23:43:22.135183: total loss: 1.0691797810207662, mse:4.593155788703119, ic :0.17935862248854265, sharpe5:15.90256254673004, irr5:551.8143310546875, ndcg5:0.8426090104853924, pnl5:4.228662014007568 
train 24, step: 0, loss: 2.1916020749390928, grad_norm: 0.07258530437055809, ic: 0.16988794495856097
train 24, step: 500, loss: 1.2377384408438716, grad_norm: 0.2088891982652875, ic: 0.04852130993332565
train 24, step: 1000, loss: 0.909855896037306, grad_norm: 0.18489937298727238, ic: 0.5266305824064483
train 24, step: 1500, loss: 2.611163888602638, grad_norm: 3.2428477056715286, ic: 0.058886618653841954
train 24, step: 2000, loss: 0.9340064212764874, grad_norm: 0.11068027678952067, ic: 0.126055992265237
Epoch 24: 2022-05-07 23:47:50.596713: train loss: 1.6141121536067868
Eval step 0: eval loss: 0.8324011269057889
Eval: 2022-05-07 23:48:10.790123: total loss: 1.0714969224036894, mse:4.623293494822926, ic :0.17503096033793375, sharpe5:16.051975296735762, irr5:546.485595703125, ndcg5:0.8405433105272567, pnl5:3.902146339416504 
train 25, step: 0, loss: 0.8372438173036318, grad_norm: 0.1315868918961976, ic: 0.6192374777037948
train 25, step: 500, loss: 0.8682540478663142, grad_norm: 0.008447393487226403, ic: 0.23388814340260455
train 25, step: 1000, loss: 2.0911977728722393, grad_norm: 0.10165231214772397, ic: 0.25480119147437935
train 25, step: 1500, loss: 1.1314042314624917, grad_norm: 0.42922567192291106, ic: 0.5473708870121897
train 25, step: 2000, loss: 1.015149595007807, grad_norm: 0.6067257176387844, ic: 0.6001346715282317
Epoch 25: 2022-05-07 23:52:43.859694: train loss: 1.615816623897441
Eval step 0: eval loss: 0.8286612604139225
Eval: 2022-05-07 23:53:03.913728: total loss: 1.0695416270987808, mse:4.665966543430707, ic :0.17002673455130213, sharpe5:16.080103343725202, irr5:558.9547119140625, ndcg5:0.8331715730184791, pnl5:4.498120307922363 
train 26, step: 0, loss: 6.7631703337160545, grad_norm: 9.727717520258448, ic: 0.10341548340656043
train 26, step: 500, loss: 3.844264623240813, grad_norm: 2.634685241945334, ic: 0.37413256137726214
train 26, step: 1000, loss: 1.2746340573489012, grad_norm: 1.4762092120073158, ic: 0.017710504467464167
train 26, step: 1500, loss: 0.8318886139732465, grad_norm: 0.3353755478603472, ic: 0.3081071100202639
train 26, step: 2000, loss: 0.9599424972954367, grad_norm: 0.7872798140728849, ic: 0.14807244001424025
Epoch 26: 2022-05-07 23:57:34.171480: train loss: 1.6158952178850963
Eval step 0: eval loss: 0.8308043082973854
Eval: 2022-05-07 23:57:54.357775: total loss: 1.0671537777570306, mse:4.593222218751643, ic :0.18556739038549158, sharpe5:16.482704461812972, irr5:567.4722900390625, ndcg5:0.836612577381784, pnl5:3.499910831451416 
train 27, step: 0, loss: 0.8222053079044117, grad_norm: 0.022068943415623533, ic: 0.12669604511114077
train 27, step: 500, loss: 0.9225617796283384, grad_norm: 2.6014634395953813, ic: 0.30172813793297965
train 27, step: 1000, loss: 0.7526019008604022, grad_norm: 0.797831352986375, ic: 0.20173593164793419
train 27, step: 1500, loss: 0.6390241388397715, grad_norm: 0.07544167627185855, ic: 0.5078598562407004
train 27, step: 2000, loss: 1.3790176611735954, grad_norm: 0.07561357276258049, ic: 0.05747389014261621
Epoch 27: 2022-05-08 00:02:32.418183: train loss: 1.6146991521537073
Eval step 0: eval loss: 0.8381994667495389
Eval: 2022-05-08 00:02:52.238645: total loss: 1.0705972256423544, mse:4.603839570287115, ic :0.18088231969024432, sharpe5:16.391950457096097, irr5:561.13232421875, ndcg5:0.8402833939735539, pnl5:5.209749221801758 
train 28, step: 0, loss: 1.5439227836932914, grad_norm: 2.0658771586292572, ic: 0.22808318160764157
train 28, step: 500, loss: 1.3776895044368362, grad_norm: 1.810509644815578, ic: 0.18533784728908026
train 28, step: 1000, loss: 0.9147696973185552, grad_norm: 0.4469801445459173, ic: 0.5802010716359899
train 28, step: 1500, loss: 1.0364838476410743, grad_norm: 0.073736908293423, ic: 0.03374032789121525
train 28, step: 2000, loss: 1.0416673531561542, grad_norm: 0.13045546902872207, ic: 0.112174689474026
Epoch 28: 2022-05-08 00:07:25.516067: train loss: 1.6137117487836357
Eval step 0: eval loss: 0.8273897482547418
Eval: 2022-05-08 00:07:45.708612: total loss: 1.0714034330550224, mse:4.6491114720223, ic :0.17751854618222115, sharpe5:16.55583965420723, irr5:568.7548828125, ndcg5:0.8403302813453228, pnl5:4.204577922821045 
train 29, step: 0, loss: 0.9045898807690486, grad_norm: 0.07561387670584238, ic: 0.10271545046521109
train 29, step: 500, loss: 1.1108425675737668, grad_norm: 0.49754084117774994, ic: 0.613111164329934
train 29, step: 1000, loss: 1.0501951626055257, grad_norm: 0.8288042985798307, ic: 0.08402925105207895
train 29, step: 1500, loss: 2.3630470198453355, grad_norm: 0.4466979556547538, ic: -0.05965555361514715
train 29, step: 2000, loss: 4.28636602707851, grad_norm: 4.055389851907249, ic: 0.1890433299301867
Epoch 29: 2022-05-08 00:12:17.372784: train loss: 1.613282629969571
Eval step 0: eval loss: 0.836449191994863
Eval: 2022-05-08 00:12:37.334524: total loss: 1.0694284900656732, mse:4.596049535549259, ic :0.18501348847958937, sharpe5:17.558532260656357, irr5:604.59033203125, ndcg5:0.8479981616611388, pnl5:4.931685447692871 
train 30, step: 0, loss: 1.0090310575074546, grad_norm: 0.09717887338188713, ic: 0.5126844428852148
train 30, step: 500, loss: 1.4146897751743233, grad_norm: 2.422962143245614, ic: 0.033440549922820945
train 30, step: 1000, loss: 0.9762029474431818, grad_norm: 0.1745668183163025, ic: -0.02778431384309562
train 30, step: 1500, loss: 1.5053928009507231, grad_norm: 3.080495572296279, ic: 0.17854162407162327
train 30, step: 2000, loss: 1.8376909415048774, grad_norm: 0.7950506461475247, ic: 0.08192295332615204
Epoch 30: 2022-05-08 00:17:12.924042: train loss: 1.6152850975617745
Eval step 0: eval loss: 0.8424481953660102
Eval: 2022-05-08 00:17:32.760489: total loss: 1.0746892932834873, mse:4.662487745825535, ic :0.18421777610091936, sharpe5:16.88495747923851, irr5:583.935546875, ndcg5:0.8425870797312738, pnl5:4.40486478805542 
train 31, step: 0, loss: 1.050262845348615, grad_norm: 0.34658949750569085, ic: 0.3473316807846286
train 31, step: 500, loss: 1.5017891589506172, grad_norm: 1.4105839225958179, ic: 0.032562931104770744
train 31, step: 1000, loss: 4.343847580015613, grad_norm: 4.420450206293562, ic: 0.46070055098787377
train 31, step: 1500, loss: 0.7655613641446821, grad_norm: 0.045071066918951457, ic: 0.7135262208735295
train 31, step: 2000, loss: 1.2470979545616452, grad_norm: 3.9952724499865107, ic: 0.18726550970538816
Epoch 31: 2022-05-08 00:22:03.407370: train loss: 1.6094746117008634
Eval step 0: eval loss: 0.8370317593766464
Eval: 2022-05-08 00:22:23.499352: total loss: 1.0718924841847464, mse:4.6323251520269055, ic :0.1691836634212621, sharpe5:16.715786232948304, irr5:582.29541015625, ndcg5:0.845312840308465, pnl5:3.8824033737182617 
train 32, step: 0, loss: 1.135478732554316, grad_norm: 0.016642288722383318, ic: 0.1752632658434407
train 32, step: 500, loss: 1.4809255044291338, grad_norm: 0.7811757509154922, ic: 0.08494033712616018
train 32, step: 1000, loss: 1.057794483689354, grad_norm: 0.3879590569902537, ic: 0.5173560637046424
train 32, step: 1500, loss: 0.9903177105680302, grad_norm: 1.0838647069221783, ic: 0.07757045457123443
train 32, step: 2000, loss: 0.9372837977377005, grad_norm: 0.0922406002365067, ic: 0.564304359226594
Epoch 32: 2022-05-08 00:26:53.342666: train loss: 1.6159764468569147
Eval step 0: eval loss: 0.825858016662276
Eval: 2022-05-08 00:27:13.492216: total loss: 1.0646415960857838, mse:4.594177010377514, ic :0.19102856119209202, sharpe5:18.020144008398056, irr5:614.4137573242188, ndcg5:0.8528542091055903, pnl5:6.370388984680176 
train 33, step: 0, loss: 1.263581354897858, grad_norm: 0.337690842690828, ic: 0.20953384036276057
train 33, step: 500, loss: 0.9972982268191367, grad_norm: 0.029931983291079954, ic: 0.10711878037185324
train 33, step: 1000, loss: 1.025263844578979, grad_norm: 3.1805465301701963, ic: 0.22794719353751386
train 33, step: 1500, loss: 0.922250044020611, grad_norm: 0.7507162675205057, ic: 0.5542096904474452
train 33, step: 2000, loss: 0.8084775800750298, grad_norm: 0.1688251121547712, ic: 0.25633339595873816
Epoch 33: 2022-05-08 00:31:44.785705: train loss: 1.617441067769279
Eval step 0: eval loss: 0.8318373395720824
Eval: 2022-05-08 00:32:04.730423: total loss: 1.067504511049103, mse:4.58755525968206, ic :0.18909642182090566, sharpe5:16.85674240708351, irr5:603.1469116210938, ndcg5:0.8395584034064749, pnl5:4.188775062561035 
train 34, step: 0, loss: 1.0146626691533658, grad_norm: 0.8557549395781647, ic: 0.6086252851812368
train 34, step: 500, loss: 0.7902307539540329, grad_norm: 0.6946814662116181, ic: 0.23901703860066673
train 34, step: 1000, loss: 3.1822717903945854, grad_norm: 1.6402060301602264, ic: 0.31172169988997284
train 34, step: 1500, loss: 0.8027172065889452, grad_norm: 0.8418614651257548, ic: 0.6821721064409163
train 34, step: 2000, loss: 6.516367822908598, grad_norm: 10.496609337875135, ic: 0.4379453608448052
Epoch 34: 2022-05-08 00:36:33.392482: train loss: 1.6142424303581917
Eval step 0: eval loss: 0.8266930856493677
Eval: 2022-05-08 00:36:52.243127: total loss: 1.0663656698548944, mse:4.595583169428302, ic :0.18930535410999774, sharpe5:16.572669564485548, irr5:582.5333862304688, ndcg5:0.8493274710303909, pnl5:4.642651081085205 
train 35, step: 0, loss: 1.2009399414062498, grad_norm: 0.6113137608851902, ic: 0.5490975449488608
train 35, step: 500, loss: 1.165009917339534, grad_norm: 0.8084613938847036, ic: 0.1356171807134962
train 35, step: 1000, loss: 1.7331644046078822, grad_norm: 6.444504641995779, ic: 0.04735290243074382
train 35, step: 1500, loss: 1.5954286962523496, grad_norm: 1.4676711013942005, ic: 0.09030839565956957
train 35, step: 2000, loss: 0.7894267525902406, grad_norm: 0.44788512690732735, ic: 0.5656983898575703
Epoch 35: 2022-05-08 00:41:21.126417: train loss: 1.613218535335245
Eval step 0: eval loss: 0.8348841451116306
Eval: 2022-05-08 00:41:41.261120: total loss: 1.0679952863081479, mse:4.592392657761446, ic :0.18589813418227955, sharpe5:16.944098646640775, irr5:589.5228271484375, ndcg5:0.846238172518863, pnl5:4.771911144256592 
train 36, step: 0, loss: 1.8195509498871665, grad_norm: 1.4656851621216345, ic: 0.11118285696243295
train 36, step: 500, loss: 0.8297231687721631, grad_norm: 0.04231632637987956, ic: 0.20177270057924587
train 36, step: 1000, loss: 1.679194602272727, grad_norm: 5.780773214676181, ic: 0.27431182384639924
train 36, step: 1500, loss: 0.757675517914433, grad_norm: 0.1017552001551188, ic: 0.41036053098526903
train 36, step: 2000, loss: 1.1394013881304605, grad_norm: 3.1057233651128238, ic: 0.7622753216621391
Epoch 36: 2022-05-08 00:46:10.543597: train loss: 1.6104227142298648
Eval step 0: eval loss: 0.8354333328874143
Eval: 2022-05-08 00:46:30.845041: total loss: 1.0694834215464235, mse:4.617037269128847, ic :0.18002936581314394, sharpe5:16.714685175418854, irr5:574.5321655273438, ndcg5:0.8377568278802985, pnl5:4.897139549255371 
train 37, step: 0, loss: 2.0217146822358347, grad_norm: 4.295804653665867, ic: 0.20433415110146871
train 37, step: 500, loss: 2.3282902828048533, grad_norm: 2.792870171676853, ic: -0.02300152759664656
train 37, step: 1000, loss: 1.0752089687678368, grad_norm: 0.16822429388365906, ic: 0.05482241036938719
train 37, step: 1500, loss: 2.013753127452904, grad_norm: 4.765973336454588, ic: 0.6048958777810199
train 37, step: 2000, loss: 1.3110247133578177, grad_norm: 0.3305546604983389, ic: 0.18423529166542696
Epoch 37: 2022-05-08 00:52:31.283219: train loss: 1.6081630284253443
Eval step 0: eval loss: 0.8318968954779702
Eval: 2022-05-08 00:52:57.133469: total loss: 1.0688897886577287, mse:4.606097122781199, ic :0.1870360626902229, sharpe5:16.515022959709167, irr5:591.4723510742188, ndcg5:0.839135650923679, pnl5:3.2218916416168213 
train 38, step: 0, loss: 1.334050434391673, grad_norm: 0.7082379003007797, ic: -0.06203942836710296
train 38, step: 500, loss: 0.9098950798128858, grad_norm: 0.09589840702497596, ic: 0.26887854442448245
train 38, step: 1000, loss: 0.9014553869194664, grad_norm: 0.2227756174404284, ic: 0.1686057043042366
train 38, step: 1500, loss: 0.9461587429408694, grad_norm: 0.09147629275256267, ic: 0.22580577701433519
train 38, step: 2000, loss: 2.315669771407666, grad_norm: 12.191276745335832, ic: 0.03513843168745919
Epoch 38: 2022-05-08 00:59:31.052650: train loss: 1.6117716393206494
Eval step 0: eval loss: 0.8329968789103661
Eval: 2022-05-08 00:59:58.426223: total loss: 1.0658975995381066, mse:4.58934630599401, ic :0.19001309350871054, sharpe5:17.226714763641358, irr5:609.1049194335938, ndcg5:0.8495959584784731, pnl5:3.8511061668395996 
train 39, step: 0, loss: 0.9651448194404626, grad_norm: 0.01611994624995591, ic: 0.09008978330387288
train 39, step: 500, loss: 0.8906741659060339, grad_norm: 0.088217421031466, ic: 0.20784708401800428
train 39, step: 1000, loss: 0.9416750278167841, grad_norm: 0.20944179013739267, ic: 0.1999525927189592
train 39, step: 1500, loss: 2.0714244812301903, grad_norm: 0.2825166814343769, ic: 0.22080311839644287
train 39, step: 2000, loss: 0.6090330874185427, grad_norm: 0.18530524320992575, ic: 0.18097216367899005
Epoch 39: 2022-05-08 01:06:16.426297: train loss: 1.6120577343637117
Eval step 0: eval loss: 0.8485820034658192
Eval: 2022-05-08 01:06:40.962450: total loss: 1.0732806631017116, mse:4.622859375752884, ic :0.1791846319248391, sharpe5:16.61347869157791, irr5:577.6334838867188, ndcg5:0.8452611176852304, pnl5:3.961754560470581 
