Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
75490
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.8844556293979675, grad_norm: 5.086994370475185, ic: 0.011257345076406789
train 0, step: 500, loss: 0.8627069583941692, grad_norm: 0.021977299608398643, ic: 0.05341271780928214
train 0, step: 1000, loss: 1.9455353877103145, grad_norm: 0.58966087874617, ic: 0.007608000467325867
train 0, step: 1500, loss: 0.9755845865242094, grad_norm: 0.12088172112510018, ic: 0.07298275583989713
train 0, step: 2000, loss: 0.9942162048911086, grad_norm: 0.15928650706514452, ic: 0.023444351140945858
Epoch 0: 2022-05-07 05:08:37.781028: train loss: 1.6493774413327342
Eval step 0: eval loss: 0.8351398624785958
Eval: 2022-05-07 05:08:58.086225: total loss: 1.0789948164195897, mse:4.82338055305627, ic :0.007840169415244423, sharpe5:7.4076483070850365, irr5:208.67405700683594, ndcg5:0.8519263379015837, pnl5:2.5948469638824463 
train 1, step: 0, loss: 2.7625116163684478, grad_norm: 0.9541927010621742, ic: 0.062105921635004536
train 1, step: 500, loss: 1.7643434537739933, grad_norm: 0.8582103641027344, ic: 0.10891266320943294
train 1, step: 1000, loss: 0.8746923507239184, grad_norm: 0.19425290746756685, ic: 0.059650545201517524
train 1, step: 1500, loss: 1.7121958063936782, grad_norm: 0.23711502914110374, ic: -0.022490809145336774
train 1, step: 2000, loss: 2.1933421875000003, grad_norm: 0.9756621167163322, ic: 0.04470005118427651
Epoch 1: 2022-05-07 05:13:41.106943: train loss: 1.6469018170533503
Eval step 0: eval loss: 0.8340618105407007
Eval: 2022-05-07 05:14:01.004577: total loss: 1.0788105526197198, mse:4.8237986746750705, ic :0.00954614325467811, sharpe5:7.246018453538418, irr5:204.84262084960938, ndcg5:0.856948994535393, pnl5:2.580838918685913 
train 2, step: 0, loss: 2.143007279829545, grad_norm: 0.010804282461597862, ic: 0.12643176804664402
train 2, step: 500, loss: 3.3055047406836855, grad_norm: 0.3102165101436289, ic: 0.05593036648626084
train 2, step: 1000, loss: 2.072419405531609, grad_norm: 0.00025605669232343226, ic: 0.21513114861863425
train 2, step: 1500, loss: 1.4862716558325382, grad_norm: 0.07083555916468477, ic: -0.012465666190965945
train 2, step: 2000, loss: 3.2355626502403845, grad_norm: 0.8788674714641064, ic: 0.2330538260489152
Epoch 2: 2022-05-07 05:18:19.651709: train loss: 1.6467420586817496
Eval step 0: eval loss: 0.8360505661798603
Eval: 2022-05-07 05:18:39.259584: total loss: 1.0792999542209727, mse:4.821577252196957, ic :0.01814737764918775, sharpe5:7.566561934947967, irr5:214.0071563720703, ndcg5:0.8600276889881091, pnl5:2.722905158996582 
train 3, step: 0, loss: 1.5237352324695121, grad_norm: 0.6085468414195163, ic: 0.010208097402224116
train 3, step: 500, loss: 1.49508822328333, grad_norm: 0.38840849713156866, ic: 0.1520515611540721
train 3, step: 1000, loss: 3.669303160531808, grad_norm: 0.7948521738896284, ic: -0.05158213764666828
train 3, step: 1500, loss: 1.9921519377813013, grad_norm: 1.0259237392917382, ic: -0.06072354691008576
train 3, step: 2000, loss: 0.8992863835515202, grad_norm: 0.0006514906540754759, ic: 0.0012866348508374422
Epoch 3: 2022-05-07 05:23:16.379059: train loss: 1.6462015209337169
Eval step 0: eval loss: 0.8336249171619796
Eval: 2022-05-07 05:23:36.476404: total loss: 1.0781549898578555, mse:4.821748669475741, ic :0.0305250027283924, sharpe5:8.394129638671874, irr5:237.400146484375, ndcg5:0.855491010638865, pnl5:2.635789394378662 
train 4, step: 0, loss: 1.4381684470663265, grad_norm: 0.05240332142689459, ic: 0.10306746269738334
train 4, step: 500, loss: 1.6238988681102362, grad_norm: 0.6326415581687158, ic: 0.04070967444577639
train 4, step: 1000, loss: 2.9464368122379954, grad_norm: 0.7326859198217287, ic: 0.07068893013699581
train 4, step: 1500, loss: 2.148673193565401, grad_norm: 0.506685114392353, ic: 0.0044052949170227725
train 4, step: 2000, loss: 1.0795227335548698, grad_norm: 0.4724626490738822, ic: 0.26876371506065644
Epoch 4: 2022-05-07 05:28:16.180081: train loss: 1.6448508712417544
Eval step 0: eval loss: 0.8696043378309404
Eval: 2022-05-07 05:28:35.814591: total loss: 1.0938485121413775, mse:4.770927377603343, ic :0.14211908708314153, sharpe5:11.393444402217865, irr5:389.1352844238281, ndcg5:0.8514603617100475, pnl5:3.3276398181915283 
train 5, step: 0, loss: 1.3834662725461409, grad_norm: 0.32251724977280494, ic: 0.3467609327475778
train 5, step: 500, loss: 0.8914255721525575, grad_norm: 0.013511230877890948, ic: 0.2976014553757525
train 5, step: 1000, loss: 0.9817618534482759, grad_norm: 0.17389362128916452, ic: 0.012550702477644681
train 5, step: 1500, loss: 1.5328749558827048, grad_norm: 0.18115466727219098, ic: 0.00793274769013811
train 5, step: 2000, loss: 1.1011580535817864, grad_norm: 0.028296170958711432, ic: 0.1784197345721919
Epoch 5: 2022-05-07 05:33:10.941675: train loss: 1.6393177145087647
Eval step 0: eval loss: 0.8327163358634088
Eval: 2022-05-07 05:33:30.151068: total loss: 1.0770817232536798, mse:4.729287208285933, ic :0.13252853219891436, sharpe5:11.24998720049858, irr5:376.32275390625, ndcg5:0.8496809242174304, pnl5:3.3325531482696533 
train 6, step: 0, loss: 1.3110147138531698, grad_norm: 0.4364249475425115, ic: 0.11450795549066128
train 6, step: 500, loss: 1.0071389634977987, grad_norm: 0.046598500004405206, ic: 0.06715287978051676
train 6, step: 1000, loss: 1.120497786953422, grad_norm: 0.08748203187097686, ic: 0.06518070383518816
train 6, step: 1500, loss: 1.5738200542355372, grad_norm: 0.7975702818184498, ic: 0.0890049691779876
train 6, step: 2000, loss: 0.799933257152597, grad_norm: 0.035445305038165044, ic: 0.04136436655616544
Epoch 6: 2022-05-07 05:38:10.858158: train loss: 1.637789464577629
Eval step 0: eval loss: 0.8292412551863804
Eval: 2022-05-07 05:38:30.255884: total loss: 1.0731535579620535, mse:4.718493088282775, ic :0.14407556055601065, sharpe5:11.868746532797813, irr5:396.35906982421875, ndcg5:0.8555251814711942, pnl5:4.0530781745910645 
train 7, step: 0, loss: 0.9827960014343262, grad_norm: 0.04519003109561931, ic: 0.12750713419976942
train 7, step: 500, loss: 0.6524454134213525, grad_norm: 0.002738374343978408, ic: 0.03670397075179679
train 7, step: 1000, loss: 1.023434710883949, grad_norm: 0.19195840569916894, ic: 0.0975756705377751
train 7, step: 1500, loss: 2.2406831436729635, grad_norm: 0.6565293822563596, ic: 0.4328330767289393
train 7, step: 2000, loss: 0.9106121895193862, grad_norm: 0.04174683084361248, ic: -0.0541051771634492
Epoch 7: 2022-05-07 05:43:09.781551: train loss: 1.6360998338651587
Eval step 0: eval loss: 0.8337920724529109
Eval: 2022-05-07 05:43:29.357778: total loss: 1.0739205574011121, mse:4.6986562605672075, ic :0.1490278699095598, sharpe5:11.74780898451805, irr5:392.4526062011719, ndcg5:0.8410200798303716, pnl5:3.2064733505249023 
train 8, step: 0, loss: 3.620945850317029, grad_norm: 1.188606561576413, ic: 0.046785175457850374
train 8, step: 500, loss: 2.762329843631269, grad_norm: 0.9424817659746573, ic: 0.04878315181332192
train 8, step: 1000, loss: 3.0607726166213767, grad_norm: 0.9047182112475516, ic: 0.10158276623498211
train 8, step: 1500, loss: 0.7096195620761406, grad_norm: 0.0076776783329604486, ic: 0.5094910814091281
train 8, step: 2000, loss: 1.085557355478443, grad_norm: 0.4441719281487334, ic: 0.5703600091563681
Epoch 8: 2022-05-07 05:48:10.548244: train loss: 1.6304859032627477
Eval step 0: eval loss: 0.8240229744303214
Eval: 2022-05-07 05:48:29.748187: total loss: 1.069462066543346, mse:4.660172731193689, ic :0.17364746478545437, sharpe5:16.008689736127852, irr5:517.4317626953125, ndcg5:0.839934019116355, pnl5:4.164142608642578 
train 9, step: 0, loss: 5.460176653458931, grad_norm: 0.7959730875542045, ic: 0.15700943360259859
train 9, step: 500, loss: 1.351016646285458, grad_norm: 0.9565398197972474, ic: 0.3327768121491107
train 9, step: 1000, loss: 0.9333314692054084, grad_norm: 0.014672560582791417, ic: 0.00927663994169104
train 9, step: 1500, loss: 1.0808178279466587, grad_norm: 0.045328135776197216, ic: 0.48960165140328255
train 9, step: 2000, loss: 1.0613538266195486, grad_norm: 0.2950721316269229, ic: 0.263590137218369
Epoch 9: 2022-05-07 05:53:10.632041: train loss: 1.627457101803699
Eval step 0: eval loss: 0.8251112524902857
Eval: 2022-05-07 05:53:30.728739: total loss: 1.069540444750107, mse:4.6099223665393945, ic :0.17556948996589378, sharpe5:15.867049657702445, irr5:523.034423828125, ndcg5:0.8602876214810289, pnl5:5.4019951820373535 
train 10, step: 0, loss: 7.062502847120991, grad_norm: 1.2908744209423366, ic: 0.2507922841210962
train 10, step: 500, loss: 1.1333009728831438, grad_norm: 1.5793966411146616, ic: 0.06735166931569819
train 10, step: 1000, loss: 2.3577367864503453, grad_norm: 0.819673589023154, ic: -0.017252328791174346
train 10, step: 1500, loss: 1.1132527140827924, grad_norm: 0.26198241293052976, ic: -0.019361555202166078
train 10, step: 2000, loss: 2.6943654347335677, grad_norm: 0.6864447558152789, ic: 0.5395824452807965
Epoch 10: 2022-05-07 05:58:07.348549: train loss: 1.6252921127232507
Eval step 0: eval loss: 0.8248154667330743
Eval: 2022-05-07 05:58:26.948497: total loss: 1.0675896487431584, mse:4.592967420843693, ic :0.1816127751844162, sharpe5:16.07378645181656, irr5:535.5374755859375, ndcg5:0.8447036226547658, pnl5:5.289905548095703 
train 11, step: 0, loss: 1.2495803537980956, grad_norm: 0.41668796422709276, ic: 0.2076930640994633
train 11, step: 500, loss: 0.6426947737302654, grad_norm: 0.17189344547219304, ic: 0.6452778630785927
train 11, step: 1000, loss: 0.938906676692601, grad_norm: 0.37324027516860037, ic: 0.06739858702048919
train 11, step: 1500, loss: 1.0548795733535499, grad_norm: 0.05488105802747851, ic: 0.18672678492069716
train 11, step: 2000, loss: 0.7873761265952064, grad_norm: 0.002527278323041294, ic: 0.10727050374716948
Epoch 11: 2022-05-07 06:03:08.634312: train loss: 1.6235114759394016
Eval step 0: eval loss: 0.8306349662885273
Eval: 2022-05-07 06:03:28.547566: total loss: 1.0686931685254488, mse:4.5894412696597495, ic :0.1831098841147471, sharpe5:15.929846827983855, irr5:532.1427612304688, ndcg5:0.8535242057966551, pnl5:5.1016926765441895 
train 12, step: 0, loss: 0.963487943013509, grad_norm: 0.08724307958525357, ic: 0.4038261997139495
train 12, step: 500, loss: 0.9264984755707413, grad_norm: 0.06467596401572956, ic: 0.17113395647541849
train 12, step: 1000, loss: 2.9749860824293393, grad_norm: 0.585184383094532, ic: 0.20627845221179716
train 12, step: 1500, loss: 0.9570128418340632, grad_norm: 0.12037668548176715, ic: -0.13194669376976917
train 12, step: 2000, loss: 0.8726657279668618, grad_norm: 0.003711264495752703, ic: 0.2455838520831054
Epoch 12: 2022-05-07 06:08:02.667487: train loss: 1.6226545517650557
Eval step 0: eval loss: 0.8253424657534246
Eval: 2022-05-07 06:08:21.975684: total loss: 1.0672519096450759, mse:4.593520760935146, ic :0.1818056664435464, sharpe5:15.70970342874527, irr5:517.3881225585938, ndcg5:0.836555308735695, pnl5:4.420496940612793 
train 13, step: 0, loss: 2.0374834489671327, grad_norm: 0.8378025930111381, ic: 0.4443834527680168
train 13, step: 500, loss: 0.819502972320515, grad_norm: 1.359251644189651, ic: 0.5846742930253266
train 13, step: 1000, loss: 0.9302236262583892, grad_norm: 0.7122760897307228, ic: 0.6021023520269234
train 13, step: 1500, loss: 2.3719879793618266, grad_norm: 0.3641418564976194, ic: -0.08488469844791108
train 13, step: 2000, loss: 1.4995605844640107, grad_norm: 0.060486839437774054, ic: 0.06016172753594725
Epoch 13: 2022-05-07 06:13:01.926263: train loss: 1.6232180809741246
Eval step 0: eval loss: 0.8256749755087591
Eval: 2022-05-07 06:13:20.876179: total loss: 1.0677070840599514, mse:4.604477072478886, ic :0.18093950046052465, sharpe5:17.003345757722855, irr5:547.3223876953125, ndcg5:0.8513524053713237, pnl5:7.251495838165283 
train 14, step: 0, loss: 4.54076272425897, grad_norm: 1.1780063839397994, ic: 0.16347936893760656
train 14, step: 500, loss: 0.8262567082676319, grad_norm: 0.0031944989142405, ic: 0.11724009316243977
train 14, step: 1000, loss: 1.8350712364156228, grad_norm: 0.5683176525713078, ic: 0.44647513496300806
train 14, step: 1500, loss: 1.1255603353120094, grad_norm: 0.07326589900696451, ic: -0.060589216791406955
train 14, step: 2000, loss: 1.1470889204339472, grad_norm: 0.18271220101631575, ic: 0.08212705546974437
Epoch 14: 2022-05-07 06:18:00.443786: train loss: 1.622012951884838
Eval step 0: eval loss: 0.8317836363524104
Eval: 2022-05-07 06:18:20.010137: total loss: 1.068698818102852, mse:4.5913155095397, ic :0.18330987175151772, sharpe5:17.28657861828804, irr5:540.1582641601562, ndcg5:0.843020020904853, pnl5:10.171464920043945 
train 15, step: 0, loss: 3.370614208414397, grad_norm: 0.6852376109334224, ic: 0.08744067909089623
train 15, step: 500, loss: 1.2611818638624792, grad_norm: 0.03932375326871758, ic: -0.027911561312950015
train 15, step: 1000, loss: 1.318211501206809, grad_norm: 0.13649727894912297, ic: 0.02775497707556151
train 15, step: 1500, loss: 0.8566561000553642, grad_norm: 0.22733509543117594, ic: 0.056954628174156294
train 15, step: 2000, loss: 1.4765547354632145, grad_norm: 0.6357344700842361, ic: 0.05154676417412353
Epoch 15: 2022-05-07 06:22:57.428783: train loss: 1.6213493158584544
Eval step 0: eval loss: 0.838934203972932
Eval: 2022-05-07 06:23:17.327667: total loss: 1.0707089321697172, mse:4.588312328730149, ic :0.18739368689085154, sharpe5:17.36191781759262, irr5:541.9671630859375, ndcg5:0.8375868521415029, pnl5:8.570746421813965 
train 16, step: 0, loss: 0.7038337423606987, grad_norm: 0.3300253916951647, ic: -0.028771770625032
train 16, step: 500, loss: 1.5855179761944151, grad_norm: 0.34574419714546967, ic: 0.19116771319700557
train 16, step: 1000, loss: 0.8803995768229167, grad_norm: 0.09852028102042522, ic: -0.10967207843900953
train 16, step: 1500, loss: 0.858771093442179, grad_norm: 0.1939091841331313, ic: 0.1828448981427648
train 16, step: 2000, loss: 3.381760336563274, grad_norm: 1.3878318269294183, ic: -0.051909751118973516
Epoch 16: 2022-05-07 06:27:56.908454: train loss: 1.6210872901087063
Eval step 0: eval loss: 0.8302637387628424
Eval: 2022-05-07 06:28:16.740874: total loss: 1.069679595129376, mse:4.615871143036099, ic :0.1739636133097513, sharpe5:15.985225102901458, irr5:503.8861389160156, ndcg5:0.8425661085490038, pnl5:5.715722560882568 
train 17, step: 0, loss: 1.2862993720988063, grad_norm: 0.23234714860575917, ic: -0.1159120362316313
train 17, step: 500, loss: 1.7607711667936992, grad_norm: 0.5344825256955072, ic: 0.18713301282260525
train 17, step: 1000, loss: 1.2811580912811507, grad_norm: 0.07965572724601587, ic: 0.12903876486815813
train 17, step: 1500, loss: 4.50604593359662, grad_norm: 1.0866688784174583, ic: 0.2298369468129691
train 17, step: 2000, loss: 1.2404666679097056, grad_norm: 0.5935868149559237, ic: 0.04741698195552948
Epoch 17: 2022-05-07 06:32:52.439245: train loss: 1.6214600031304065
Eval step 0: eval loss: 0.8331397873584035
Eval: 2022-05-07 06:33:12.177026: total loss: 1.0681335740443172, mse:4.586215489869152, ic :0.18861793930997708, sharpe5:17.598560053110123, irr5:576.0253295898438, ndcg5:0.839814366566957, pnl5:5.655888557434082 
train 18, step: 0, loss: 1.407636472880606, grad_norm: 0.5389547466665777, ic: 0.13877029055310985
train 18, step: 500, loss: 1.5167996927650826, grad_norm: 0.7553091391803225, ic: -0.027032876507888215
train 18, step: 1000, loss: 0.656064319349315, grad_norm: 0.04839829021195129, ic: 0.5736562482376747
train 18, step: 1500, loss: 1.4268463909577191, grad_norm: 0.030118648031061915, ic: 0.21415012648884524
train 18, step: 2000, loss: 0.9119336559514332, grad_norm: 0.007099163838819773, ic: -0.02731270730384667
Epoch 18: 2022-05-07 06:37:45.359141: train loss: 1.6215076148655976
Eval step 0: eval loss: 0.8254898119525487
Eval: 2022-05-07 06:38:04.913186: total loss: 1.065695300008198, mse:4.59187048949559, ic :0.18928262757492956, sharpe5:17.649855712652204, irr5:583.0631713867188, ndcg5:0.8476688393764158, pnl5:6.882932186126709 
train 19, step: 0, loss: 1.484769015842014, grad_norm: 0.664895648875883, ic: -0.015483789461000895
train 19, step: 500, loss: 0.8688617988868995, grad_norm: 0.026869361980711284, ic: 0.2207962431530491
train 19, step: 1000, loss: 0.9578412518685935, grad_norm: 2.497593794899471, ic: 0.1979245820383767
train 19, step: 1500, loss: 3.956751251756852, grad_norm: 0.7763988278195273, ic: 0.13854438595387303
train 19, step: 2000, loss: 1.010868389423077, grad_norm: 0.11594165746410218, ic: 0.24543740149250287
Epoch 19: 2022-05-07 06:42:54.366483: train loss: 1.6215908471903757
Eval step 0: eval loss: 0.8341326859276211
Eval: 2022-05-07 06:43:13.938084: total loss: 1.066679485838425, mse:4.584733330227664, ic :0.1920226759419449, sharpe5:18.157831063270567, irr5:601.4532470703125, ndcg5:0.859098799747142, pnl5:12.346494674682617 
train 20, step: 0, loss: 2.281147325839921, grad_norm: 0.7408522265876891, ic: 0.0672433684351266
train 20, step: 500, loss: 3.1994538352272723, grad_norm: 0.4211257204738077, ic: 0.09937771485013662
train 20, step: 1000, loss: 0.9742927551269531, grad_norm: 0.06353427557992944, ic: 0.1458290992121986
train 20, step: 1500, loss: 1.7601593157781394, grad_norm: 0.8156820435674057, ic: 0.25255208192910794
train 20, step: 2000, loss: 1.0270641631831159, grad_norm: 0.09080924909889952, ic: 0.03534127792106053
Epoch 20: 2022-05-07 06:47:49.845815: train loss: 1.620307245563592
Eval step 0: eval loss: 0.8332079615055321
Eval: 2022-05-07 06:48:09.435577: total loss: 1.0678640236030665, mse:4.589298506921954, ic :0.19088008234940224, sharpe5:17.70652233839035, irr5:586.8167114257812, ndcg5:0.8549766713059089, pnl5:8.144647598266602 
train 21, step: 0, loss: 1.0121136395463863, grad_norm: 0.3529831818886132, ic: 0.07664953624956933
train 21, step: 500, loss: 0.7717702409862417, grad_norm: 0.02352053565303966, ic: 0.19311812257241162
train 21, step: 1000, loss: 0.9326806319387335, grad_norm: 0.551883830480631, ic: 0.14385695253464803
train 21, step: 1500, loss: 0.995395075425216, grad_norm: 0.2575506415918582, ic: 0.31220806319391037
train 21, step: 2000, loss: 0.9426712255805302, grad_norm: 0.08513007825962451, ic: 0.06573553201960568
Epoch 21: 2022-05-07 06:52:54.537986: train loss: 1.6211600397743104
Eval step 0: eval loss: 0.8330069120867359
Eval: 2022-05-07 06:53:14.347809: total loss: 1.0677315973181174, mse:4.59062815690665, ic :0.18655701646749437, sharpe5:17.531966882944108, irr5:584.309814453125, ndcg5:0.8393842499126388, pnl5:6.653989791870117 
train 22, step: 0, loss: 1.0451681708211953, grad_norm: 0.100123264367384, ic: 0.20009971463154536
train 22, step: 500, loss: 3.247531797827744, grad_norm: 0.7922974446751221, ic: -0.2240137024802647
train 22, step: 1000, loss: 1.1957139913746389, grad_norm: 0.1502578472355346, ic: 0.45867955638949254
train 22, step: 1500, loss: 0.974976892039609, grad_norm: 0.08820155472286326, ic: 0.11463722873039736
train 22, step: 2000, loss: 1.7738277928358843, grad_norm: 0.5643700307808839, ic: 0.0997745868017732
Epoch 22: 2022-05-07 06:57:53.007048: train loss: 1.6201823132844482
Eval step 0: eval loss: 0.8316014956121575
Eval: 2022-05-07 06:58:12.512234: total loss: 1.0699862997256764, mse:4.613868481043671, ic :0.17756540072566762, sharpe5:15.97854497551918, irr5:529.8530883789062, ndcg5:0.8401835744019736, pnl5:4.550826072692871 
train 23, step: 0, loss: 0.9893433672550432, grad_norm: 0.05876434290824388, ic: 0.19218203493548364
train 23, step: 500, loss: 1.4333917686666013, grad_norm: 0.13648939202194174, ic: 0.004366340704851886
train 23, step: 1000, loss: 1.6586138916015627, grad_norm: 0.12471062964121175, ic: 0.25964039932285815
train 23, step: 1500, loss: 1.105022813938987, grad_norm: 0.21848933177718163, ic: 0.0979919950782181
train 23, step: 2000, loss: 1.9086466753271747, grad_norm: 2.041769525735384, ic: 0.4406916163457185
Epoch 23: 2022-05-07 07:02:47.118957: train loss: 1.6204542369972215
Eval step 0: eval loss: 0.8391941661452844
Eval: 2022-05-07 07:03:06.836443: total loss: 1.0696718742443152, mse:4.600029779996742, ic :0.177478663737521, sharpe5:15.140899379253387, irr5:496.5530090332031, ndcg5:0.8478997744453028, pnl5:4.65796422958374 
train 24, step: 0, loss: 2.2115322953757497, grad_norm: 0.062166784229098, ic: 0.11063533484902278
train 24, step: 500, loss: 1.2362059596911479, grad_norm: 0.1622310309171631, ic: 0.007126803247661233
train 24, step: 1000, loss: 0.9111944658381163, grad_norm: 0.08022290331690556, ic: 0.5271826078061762
train 24, step: 1500, loss: 2.5817832852431986, grad_norm: 1.3453149074914235, ic: 0.07728713272480861
train 24, step: 2000, loss: 0.9264461203328749, grad_norm: 0.19464683112300454, ic: 0.1257686141579116
Epoch 24: 2022-05-07 07:07:46.116423: train loss: 1.6149453603816124
Eval step 0: eval loss: 0.8262531694546891
Eval: 2022-05-07 07:08:05.281065: total loss: 1.0681046627154074, mse:4.614221602794775, ic :0.18504371930820743, sharpe5:17.236132400035856, irr5:572.0925903320312, ndcg5:0.8522718820229568, pnl5:4.14691686630249 
train 25, step: 0, loss: 0.8660738558382601, grad_norm: 0.5657467237460139, ic: 0.6021486654280257
train 25, step: 500, loss: 0.8679608578408232, grad_norm: 0.0059575636221052, ic: 0.24949680759991566
train 25, step: 1000, loss: 2.091772330778751, grad_norm: 0.1632914499731856, ic: 0.2529125495533753
train 25, step: 1500, loss: 1.1300753334055735, grad_norm: 0.43420052298683276, ic: 0.542544181365591
train 25, step: 2000, loss: 1.0179745782009022, grad_norm: 0.44056428445717327, ic: 0.6034016948995989
Epoch 25: 2022-05-07 07:12:40.606809: train loss: 1.6194948341674171
Eval step 0: eval loss: 0.8261822297525355
Eval: 2022-05-07 07:13:00.298847: total loss: 1.066533377900217, mse:4.586067357688897, ic :0.19271488356569796, sharpe5:17.828039739131928, irr5:605.583251953125, ndcg5:0.8440654445453531, pnl5:5.743180751800537 
train 26, step: 0, loss: 6.703201440195687, grad_norm: 0.6888978560487918, ic: 0.09323940075377996
train 26, step: 500, loss: 3.8330429351544173, grad_norm: 0.5357799135035459, ic: 0.379727024914402
train 26, step: 1000, loss: 1.2728751525399824, grad_norm: 0.9649554261074147, ic: 0.008319755298743443
train 26, step: 1500, loss: 0.8417151417321945, grad_norm: 0.15873282172411124, ic: 0.294231728502782
train 26, step: 2000, loss: 0.9719003936983674, grad_norm: 0.24992238771627823, ic: 0.18000884002365786
Epoch 26: 2022-05-07 07:17:43.308768: train loss: 1.6190097845437161
Eval step 0: eval loss: 0.8313737553716082
Eval: 2022-05-07 07:18:01.135286: total loss: 1.0691726350652007, mse:4.587991475436176, ic :0.18773633144041574, sharpe5:17.517986100912093, irr5:592.855712890625, ndcg5:0.8282282057643846, pnl5:7.052130699157715 
train 27, step: 0, loss: 0.8168845741421569, grad_norm: 0.01967643922686144, ic: 0.18100151836337092
train 27, step: 500, loss: 0.8886276358603786, grad_norm: 0.8206090712770273, ic: 0.3094759403139439
train 27, step: 1000, loss: 0.7579261679476025, grad_norm: 0.872786308717785, ic: 0.19281333766664244
train 27, step: 1500, loss: 0.6419954683887038, grad_norm: 0.043460095397184095, ic: 0.5179549905953487
train 27, step: 2000, loss: 1.381750108259776, grad_norm: 0.018358913378356795, ic: 0.018805959472432784
Epoch 27: 2022-05-07 07:22:53.601504: train loss: 1.6194938932191192
Eval step 0: eval loss: 0.8367347516300052
Eval: 2022-05-07 07:23:12.808314: total loss: 1.0708549959623384, mse:4.598494358965796, ic :0.18031011225964996, sharpe5:16.291232070922852, irr5:544.6364135742188, ndcg5:0.8485706859088707, pnl5:5.165653228759766 
train 28, step: 0, loss: 1.5360019757480694, grad_norm: 0.18577243104033572, ic: 0.2018086629714219
train 28, step: 500, loss: 1.361182786953256, grad_norm: 0.5605615473120449, ic: 0.12828535485934048
train 28, step: 1000, loss: 0.9131208383617039, grad_norm: 0.4421893998861974, ic: 0.5763113962009497
train 28, step: 1500, loss: 1.027330850573038, grad_norm: 0.030661886184600712, ic: 0.05821246694279844
train 28, step: 2000, loss: 1.043447857254122, grad_norm: 0.13487276389528183, ic: 0.12786952447484146
Epoch 28: 2022-05-07 07:27:48.378575: train loss: 1.6163664360596188
Eval step 0: eval loss: 0.8229903290470231
Eval: 2022-05-07 07:28:08.286746: total loss: 1.0691962801421515, mse:4.6253399851755175, ic :0.181744539159684, sharpe5:17.111438580751418, irr5:582.38427734375, ndcg5:0.8400483305361662, pnl5:7.060971736907959 
train 29, step: 0, loss: 0.9050936174718063, grad_norm: 0.021005452479235395, ic: 0.11049136694047033
train 29, step: 500, loss: 1.1089155339382706, grad_norm: 0.7575720242352975, ic: 0.6160891659038732
train 29, step: 1000, loss: 1.0541403851688411, grad_norm: 0.3928511101259564, ic: 0.10332958284360186
train 29, step: 1500, loss: 2.3500111302205307, grad_norm: 0.26572502592166314, ic: -0.07490322704333782
train 29, step: 2000, loss: 4.388066044560185, grad_norm: 1.8067133244750508, ic: 0.22012837274330047
Epoch 29: 2022-05-07 07:32:50.490729: train loss: 1.6162932779880521
Eval step 0: eval loss: 0.8289656000971417
Eval: 2022-05-07 07:33:10.078332: total loss: 1.0681068356137864, mse:4.588483680272421, ic :0.18828378969397475, sharpe5:16.495631482601166, irr5:579.0767211914062, ndcg5:0.8566550309227724, pnl5:5.003278732299805 
train 30, step: 0, loss: 1.0106873209941933, grad_norm: 0.11864558623299903, ic: 0.5124684876557817
train 30, step: 500, loss: 1.4170426579163249, grad_norm: 2.39204203872956, ic: 0.0015344448084780443
train 30, step: 1000, loss: 0.974482773289536, grad_norm: 0.042907740038768655, ic: -0.05429594713248455
train 30, step: 1500, loss: 1.5141350491262722, grad_norm: 0.9327534280497117, ic: 0.15826353748284533
train 30, step: 2000, loss: 1.8390967581412585, grad_norm: 1.1250117872736398, ic: 0.08586940113490941
Epoch 30: 2022-05-07 07:37:50.607959: train loss: 1.6158456159777335
Eval step 0: eval loss: 0.8335964898289318
Eval: 2022-05-07 07:38:10.360687: total loss: 1.0743924691643316, mse:4.6938842480791925, ic :0.1874848271835859, sharpe5:17.986016900539397, irr5:607.9892578125, ndcg5:0.8473424616611763, pnl5:4.845694065093994 
train 31, step: 0, loss: 1.0672142708244816, grad_norm: 0.46862278612960157, ic: 0.3550412148417868
train 31, step: 500, loss: 1.4974764097865225, grad_norm: 1.3229411151409787, ic: 0.045826683556526956
train 31, step: 1000, loss: 4.334863357484387, grad_norm: 1.4840862957326153, ic: 0.4725668636612963
train 31, step: 1500, loss: 0.7662334010381991, grad_norm: 0.049142876384208785, ic: 0.7136931487018124
train 31, step: 2000, loss: 1.265984532318579, grad_norm: 1.0196042421495235, ic: 0.08547582886270148
Epoch 31: 2022-05-07 07:42:46.782538: train loss: 1.6131198672396834
Eval step 0: eval loss: 0.8344498886574683
Eval: 2022-05-07 07:43:07.000019: total loss: 1.0684539256364334, mse:4.598919546752576, ic :0.18438652042832582, sharpe5:17.008368623256683, irr5:567.005859375, ndcg5:0.8562645475445567, pnl5:5.013997554779053 
train 32, step: 0, loss: 1.1224257721161341, grad_norm: 0.02894387779394056, ic: 0.1860034976714945
train 32, step: 500, loss: 1.4667945681594488, grad_norm: 0.6876454951793042, ic: 0.09084919056987656
train 32, step: 1000, loss: 1.0759982001796198, grad_norm: 0.6376188500271753, ic: 0.4992511253367163
train 32, step: 1500, loss: 0.9773817026005912, grad_norm: 1.516030731739112, ic: 0.0862186570043935
train 32, step: 2000, loss: 0.9429663552831972, grad_norm: 0.06508668829208576, ic: 0.5581245905087105
Epoch 32: 2022-05-07 07:47:46.951759: train loss: 1.6164805440260086
Eval step 0: eval loss: 0.8257817387957718
Eval: 2022-05-07 07:48:06.748648: total loss: 1.0646664450554886, mse:4.585570697413545, ic :0.19307663557380092, sharpe5:18.010656373500822, irr5:596.5042114257812, ndcg5:0.852599301182445, pnl5:9.155008316040039 
train 33, step: 0, loss: 1.2635220334716972, grad_norm: 0.37598083421709455, ic: 0.1990761742542943
train 33, step: 500, loss: 0.9929083583007067, grad_norm: 0.018235041480549198, ic: 0.12676067932917495
train 33, step: 1000, loss: 1.0644726196158225, grad_norm: 1.7429967831538646, ic: 0.21434960865673808
train 33, step: 1500, loss: 0.9352297528358542, grad_norm: 0.4615459148947234, ic: 0.5468630137471786
train 33, step: 2000, loss: 0.8149151930529418, grad_norm: 0.04588884423985552, ic: 0.24614306087547552
Epoch 33: 2022-05-07 07:52:49.737241: train loss: 1.6166705332121574
Eval step 0: eval loss: 0.8273382960682296
Eval: 2022-05-07 07:53:09.438038: total loss: 1.0663953260395838, mse:4.5831906130471864, ic :0.19458881442801973, sharpe5:17.873538765907288, irr5:598.8743896484375, ndcg5:0.8544923315661539, pnl5:7.142632484436035 
train 34, step: 0, loss: 0.9878946234168979, grad_norm: 0.7548885956366131, ic: 0.6136042636034658
train 34, step: 500, loss: 0.8061389048165137, grad_norm: 0.6669802395530771, ic: 0.25331421772366297
train 34, step: 1000, loss: 3.157304942516321, grad_norm: 2.1081061339093763, ic: 0.31547389387086305
train 34, step: 1500, loss: 0.8108710378799165, grad_norm: 0.5978788336159928, ic: 0.6824184519254644
train 34, step: 2000, loss: 6.371996937935709, grad_norm: 14.384793181687439, ic: 0.4398062410287143
Epoch 34: 2022-05-07 07:57:50.416189: train loss: 1.614922056672394
Eval step 0: eval loss: 0.8236265353332455
Eval: 2022-05-07 07:58:09.798127: total loss: 1.0684257657109923, mse:4.607895946823832, ic :0.1877208294790145, sharpe5:16.869355380535126, irr5:579.5804443359375, ndcg5:0.8422780039353335, pnl5:9.845182418823242 
train 35, step: 0, loss: 1.2021755801930145, grad_norm: 0.9359277536416621, ic: 0.5560181074756897
train 35, step: 500, loss: 1.1813091487057952, grad_norm: 0.44206279131101633, ic: 0.12097388104927936
train 35, step: 1000, loss: 1.7159705030437233, grad_norm: 5.95013520839117, ic: 0.09453397889021822
train 35, step: 1500, loss: 1.615512842164004, grad_norm: 1.348329585516136, ic: 0.01616534572981109
train 35, step: 2000, loss: 0.7835821467924883, grad_norm: 0.18653763228722392, ic: 0.5633782933935126
Epoch 35: 2022-05-07 08:02:56.548947: train loss: 1.6161879106438344
Eval step 0: eval loss: 0.8358057824025289
Eval: 2022-05-07 08:03:16.342853: total loss: 1.0693793186137872, mse:4.6104113142889025, ic :0.17935456321404447, sharpe5:17.705875325202943, irr5:568.4859008789062, ndcg5:0.8450758548813452, pnl5:7.506429195404053 
train 36, step: 0, loss: 1.829759630715267, grad_norm: 2.640016620080922, ic: 0.11245346506521764
train 36, step: 500, loss: 0.8389781954910364, grad_norm: 0.009776485474612911, ic: 0.20067481725934877
train 36, step: 1000, loss: 1.6301750710227272, grad_norm: 1.1467315866165548, ic: 0.23535453124818093
train 36, step: 1500, loss: 0.7665886336754791, grad_norm: 0.07335437428207192, ic: 0.36655818014803687
train 36, step: 2000, loss: 1.1248840380510325, grad_norm: 0.5552185890482633, ic: 0.7669027691561259
Epoch 36: 2022-05-07 08:07:58.871816: train loss: 1.6142124301648808
Eval step 0: eval loss: 0.8298889095841017
Eval: 2022-05-07 08:08:18.618203: total loss: 1.0677533760048403, mse:4.601365691660433, ic :0.18747882320113587, sharpe5:17.546129282712936, irr5:587.5825805664062, ndcg5:0.8496219491607275, pnl5:6.397627353668213 
train 37, step: 0, loss: 2.0221771660844183, grad_norm: 1.3464280486447384, ic: 0.2248928178430824
train 37, step: 500, loss: 2.345923568451774, grad_norm: 1.3024634197973177, ic: -0.07578686257230223
train 37, step: 1000, loss: 1.080565477252188, grad_norm: 0.06896723000215432, ic: 0.06064501932122237
train 37, step: 1500, loss: 2.0280386117788463, grad_norm: 0.7644267027890869, ic: 0.6099595178585993
train 37, step: 2000, loss: 1.3114253959782496, grad_norm: 0.11050286421156982, ic: 0.24233778299497094
Epoch 37: 2022-05-07 08:13:05.037973: train loss: 1.6120672339399007
Eval step 0: eval loss: 0.82636604268885
Eval: 2022-05-07 08:13:24.963181: total loss: 1.067598495823338, mse:4.602978999010252, ic :0.18645588597156534, sharpe5:17.28077630996704, irr5:599.3056030273438, ndcg5:0.8461605938343749, pnl5:7.004148960113525 
train 38, step: 0, loss: 1.3353485479587461, grad_norm: 0.2868759792876047, ic: -0.06682749209208455
train 38, step: 500, loss: 0.9081235532407407, grad_norm: 0.14400458284870882, ic: 0.26866820360854565
train 38, step: 1000, loss: 0.9036570914649209, grad_norm: 0.19089676056248323, ic: 0.1515107910858056
train 38, step: 1500, loss: 0.9591826349836277, grad_norm: 0.12401219458200614, ic: 0.21243106283639057
train 38, step: 2000, loss: 2.3007967392843653, grad_norm: 3.4668919736895307, ic: -0.013324466342867769
Epoch 38: 2022-05-07 08:18:01.098839: train loss: 1.612021730310411
Eval step 0: eval loss: 0.8280794004997035
Eval: 2022-05-07 08:18:20.681410: total loss: 1.065339947420228, mse:4.583583740930463, ic :0.1905795806052649, sharpe5:17.983655869960785, irr5:599.211181640625, ndcg5:0.8337022358481393, pnl5:8.405238151550293 
train 39, step: 0, loss: 0.9713993364153287, grad_norm: 0.011449817669630043, ic: 0.0763933194352778
train 39, step: 500, loss: 0.8815227394191968, grad_norm: 0.20657016012723584, ic: 0.24302452245594294
train 39, step: 1000, loss: 0.9368657550099962, grad_norm: 0.01942562852683866, ic: 0.19222672118780393
train 39, step: 1500, loss: 2.0365255268175515, grad_norm: 0.13629903712808628, ic: 0.22398669210662514
train 39, step: 2000, loss: 0.6119355839010664, grad_norm: 0.06504533753365155, ic: 0.14971048203086634
Epoch 39: 2022-05-07 08:22:59.906804: train loss: 1.6107364955868588
Eval step 0: eval loss: 0.8330963102608008
Eval: 2022-05-07 08:23:19.548901: total loss: 1.0685883520226396, mse:4.606939905446938, ic :0.18368991387115088, sharpe5:17.646435245275498, irr5:575.8399047851562, ndcg5:0.8535806207842284, pnl5:5.81696081161499 
