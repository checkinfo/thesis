Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
90478
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.884443412822518, grad_norm: 5.086295958639521, ic: 0.011557789788346047
train 0, step: 500, loss: 0.8627821139671251, grad_norm: 0.02208816382260933, ic: 0.053650101939879985
train 0, step: 1000, loss: 1.9452658864644294, grad_norm: 0.588892389936859, ic: 0.00832026549051438
train 0, step: 1500, loss: 0.9759441390810276, grad_norm: 0.12262416602366133, ic: 0.07429915364542108
train 0, step: 2000, loss: 0.9941829848924586, grad_norm: 0.1592647977698532, ic: 0.02368709358906674
Epoch 0: 2022-05-07 08:29:04.383681: train loss: 1.6493776092148815
Eval step 0: eval loss: 0.8351406342613935
Eval: 2022-05-07 08:29:22.208159: total loss: 1.078994515969214, mse:4.823380609498995, ic :0.007798687416739883, sharpe5:7.411386132538318, irr5:208.76229858398438, ndcg5:0.8563630016466163, pnl5:2.695826530456543 
train 1, step: 0, loss: 2.762573045299899, grad_norm: 0.9545508518437024, ic: 0.061878053009003425
train 1, step: 500, loss: 1.7645855970244653, grad_norm: 0.8588077351020297, ic: 0.10857481635968366
train 1, step: 1000, loss: 0.8746882059487951, grad_norm: 0.1942769760337849, ic: 0.05925888217976044
train 1, step: 1500, loss: 1.7121618512033046, grad_norm: 0.23709800674329826, ic: -0.022248892988899656
train 1, step: 2000, loss: 2.1932923828125, grad_norm: 0.9752066426628367, ic: 0.045915544409677184
Epoch 1: 2022-05-07 08:33:56.710393: train loss: 1.6469010737231415
Eval step 0: eval loss: 0.8340784038708509
Eval: 2022-05-07 08:34:14.574104: total loss: 1.0788079844638763, mse:4.823792464273052, ic :0.009549620365586315, sharpe5:7.242457516193389, irr5:204.6986083984375, ndcg5:0.8548666570637176, pnl5:2.6712558269500732 
train 2, step: 0, loss: 2.1429765625, grad_norm: 0.010808769350036874, ic: 0.1275039381135723
train 2, step: 500, loss: 3.305558230022496, grad_norm: 0.3101426472922172, ic: 0.05722424316396181
train 2, step: 1000, loss: 2.072434372006705, grad_norm: 0.00025547358874147884, ic: 0.21276730940154837
train 2, step: 1500, loss: 1.4862531123270515, grad_norm: 0.07079492714271458, ic: -0.012263251046726091
train 2, step: 2000, loss: 3.23513671875, grad_norm: 0.8792729240052631, ic: 0.2288777185573968
Epoch 2: 2022-05-07 08:39:00.126633: train loss: 1.6467476830315633
Eval step 0: eval loss: 0.8362201011344178
Eval: 2022-05-07 08:39:17.627614: total loss: 1.0792965118243674, mse:4.821684746499763, ic :0.01702473068438398, sharpe5:7.2426396068930625, irr5:207.4210205078125, ndcg5:0.8439162005150678, pnl5:2.6308627128601074 
train 3, step: 0, loss: 1.5239229031694614, grad_norm: 0.608933724879408, ic: 0.009364392330300918
train 3, step: 500, loss: 1.4948037430562353, grad_norm: 0.38809288754839666, ic: 0.13612426270175695
train 3, step: 1000, loss: 3.66989826208981, grad_norm: 0.7961084257628647, ic: -0.050511773917086195
train 3, step: 1500, loss: 1.9860640056004502, grad_norm: 1.0300903296343484, ic: -0.062419431481197045
train 3, step: 2000, loss: 0.8993288112331081, grad_norm: 0.0009205515361976275, ic: 0.006297215424624571
Epoch 3: 2022-05-07 08:44:00.084367: train loss: 1.6460536512403654
Eval step 0: eval loss: 0.8345535648132902
Eval: 2022-05-07 08:44:17.921660: total loss: 1.0779685119882827, mse:4.820752780866508, ic :0.03253027851062886, sharpe5:8.249752051234244, irr5:232.7106475830078, ndcg5:0.866285219723068, pnl5:2.6201231479644775 
train 4, step: 0, loss: 1.4381891741071429, grad_norm: 0.05173301794247032, ic: 0.0875718011978319
train 4, step: 500, loss: 1.648106276144193, grad_norm: 0.6609970843448358, ic: 0.04980022422844767
train 4, step: 1000, loss: 2.9470185070503048, grad_norm: 0.7416977272647489, ic: 0.06728669018306427
train 4, step: 1500, loss: 2.148838426292194, grad_norm: 0.505185225511855, ic: 0.013771680643506813
train 4, step: 2000, loss: 1.079806836545004, grad_norm: 0.47439583432863164, ic: 0.27826950084424407
Epoch 4: 2022-05-07 08:48:55.465021: train loss: 1.6447892714847887
Eval step 0: eval loss: 0.8642728622645547
Eval: 2022-05-07 08:49:13.821611: total loss: 1.091155865244368, mse:4.8331305832337215, ic :0.07455307270291223, sharpe5:11.321114192605018, irr5:212.11163330078125, ndcg5:0.8374836825607069, pnl5:1.8460123538970947 
train 5, step: 0, loss: 1.3670169292680368, grad_norm: 0.20323886733462526, ic: 0.16727068788872912
train 5, step: 500, loss: 0.89013671875, grad_norm: 0.012351786972575302, ic: 0.10038818664160234
train 5, step: 1000, loss: 0.9806765969228928, grad_norm: 0.1751927318198771, ic: 0.014358730389057307
train 5, step: 1500, loss: 1.5283857406919985, grad_norm: 0.17629559713467788, ic: 0.006917090705118815
train 5, step: 2000, loss: 1.1025757402389065, grad_norm: 0.028020080806529284, ic: 0.15967697699271954
Epoch 5: 2022-05-07 08:53:57.084728: train loss: 1.6394255656198422
Eval step 0: eval loss: 0.8368914235379346
Eval: 2022-05-07 08:54:15.106091: total loss: 1.0752521077356045, mse:4.719144390016769, ic :0.14091703177967177, sharpe5:10.905576594471931, irr5:375.84716796875, ndcg5:0.8423081926942021, pnl5:2.860569953918457 
train 6, step: 0, loss: 1.3486565646182218, grad_norm: 0.6823760092686917, ic: 0.07141304932331874
train 6, step: 500, loss: 1.0077483803718414, grad_norm: 0.050668236229422514, ic: 0.030736403358825673
train 6, step: 1000, loss: 1.1226489164983364, grad_norm: 0.09954239201738893, ic: 0.08900080422241256
train 6, step: 1500, loss: 1.5888617397339875, grad_norm: 0.8310097072142353, ic: 0.15175992857016554
train 6, step: 2000, loss: 0.80840346105009, grad_norm: 0.05641025336106051, ic: 0.05053940301312983
Epoch 6: 2022-05-07 08:58:53.549759: train loss: 1.6395660454386136
Eval step 0: eval loss: 0.8299123846441978
Eval: 2022-05-07 08:59:11.567226: total loss: 1.073134137507937, mse:4.786181677885211, ic :0.09479130221365017, sharpe5:11.804998707175255, irr5:221.96022033691406, ndcg5:0.8640813399726764, pnl5:2.3943283557891846 
train 7, step: 0, loss: 0.9881093978881836, grad_norm: 0.05449869462979837, ic: 0.09008378006719536
train 7, step: 500, loss: 0.6507755267946191, grad_norm: 0.0020763557864961766, ic: 0.04769230076003794
train 7, step: 1000, loss: 1.0232586246905941, grad_norm: 0.19482843146049691, ic: 0.09195064135965852
train 7, step: 1500, loss: 2.2394896545083065, grad_norm: 0.666510488898783, ic: 0.43475404170603943
train 7, step: 2000, loss: 0.9139317526378231, grad_norm: 0.051149772907739796, ic: -0.0372080887431792
Epoch 7: 2022-05-07 09:03:56.451034: train loss: 1.6342371646865386
Eval step 0: eval loss: 0.8332498950375394
Eval: 2022-05-07 09:04:14.109355: total loss: 1.0724272808473798, mse:4.6972122920651875, ic :0.153358798109343, sharpe5:13.563291097283363, irr5:453.9534912109375, ndcg5:0.8379420574142277, pnl5:5.316181182861328 
train 8, step: 0, loss: 3.6298580446105073, grad_norm: 1.1871717356275053, ic: 0.026810951672218836
train 8, step: 500, loss: 2.7478567198779444, grad_norm: 0.9193122032095165, ic: 0.04792172944904147
train 8, step: 1000, loss: 3.0570998924365944, grad_norm: 0.954964495257205, ic: 0.10815636131911247
train 8, step: 1500, loss: 0.7132529224626233, grad_norm: 0.024690996999678534, ic: 0.4759092090038116
train 8, step: 2000, loss: 1.0903720145781637, grad_norm: 0.4315588101246112, ic: 0.515076454846987
Epoch 8: 2022-05-07 09:08:55.023106: train loss: 1.629382273889592
Eval step 0: eval loss: 0.8280958008841544
Eval: 2022-05-07 09:09:12.894031: total loss: 1.069101955891823, mse:4.67665689012849, ic :0.17185434024982832, sharpe5:16.31958524823189, irr5:528.8560791015625, ndcg5:0.8429647914235074, pnl5:6.0995378494262695 
train 9, step: 0, loss: 5.456109200558214, grad_norm: 0.7695846674419118, ic: 0.1661587559551422
train 9, step: 500, loss: 1.351316523731203, grad_norm: 0.9933838099309408, ic: 0.3116309953524271
train 9, step: 1000, loss: 0.9368249050697866, grad_norm: 0.7693255377582923, ic: 0.014658294427129327
train 9, step: 1500, loss: 1.0944780172760953, grad_norm: 0.04307929102650544, ic: 0.42124742041676005
train 9, step: 2000, loss: 1.0631023455987998, grad_norm: 0.2276790107877065, ic: 0.2723505690996117
Epoch 9: 2022-05-07 09:13:48.758923: train loss: 1.6277376136088528
Eval step 0: eval loss: 0.8295781383775684
Eval: 2022-05-07 09:14:06.654564: total loss: 1.0703123237594545, mse:4.670056900729874, ic :0.16967778391598065, sharpe5:16.760765000581742, irr5:538.11279296875, ndcg5:0.861503094352703, pnl5:8.13388442993164 
train 10, step: 0, loss: 7.104601801658164, grad_norm: 1.159834923489282, ic: 0.23691425053976425
train 10, step: 500, loss: 1.1241066062843408, grad_norm: 0.07625954722183673, ic: 0.06783567103166435
train 10, step: 1000, loss: 2.3819168184432518, grad_norm: 1.1509139496325056, ic: 0.14168814095062998
train 10, step: 1500, loss: 1.1101888681386973, grad_norm: 0.2648263408174246, ic: 0.002128960765559096
train 10, step: 2000, loss: 2.723793470388488, grad_norm: 3.29895656697133, ic: 0.5018720793476364
Epoch 10: 2022-05-07 09:18:52.205156: train loss: 1.6263555612104508
Eval step 0: eval loss: 0.8316636241273708
Eval: 2022-05-07 09:19:10.192272: total loss: 1.0685239459593265, mse:4.616359425961248, ic :0.18051406621479207, sharpe5:16.31141245007515, irr5:539.6070556640625, ndcg5:0.8578654514601, pnl5:6.669788360595703 
train 11, step: 0, loss: 1.2541745592251548, grad_norm: 0.02150751697132866, ic: 0.2086542408239419
train 11, step: 500, loss: 0.6435798470300572, grad_norm: 0.014730782077673532, ic: 0.6429230490890224
train 11, step: 1000, loss: 0.9440758965375103, grad_norm: 0.13293405236160977, ic: 0.021814618902291583
train 11, step: 1500, loss: 1.0521399514716967, grad_norm: 0.049827980029979464, ic: 0.18583136927813335
train 11, step: 2000, loss: 0.7861797700372729, grad_norm: 0.0017216104892526008, ic: 0.1379749407176336
Epoch 11: 2022-05-07 09:23:43.869504: train loss: 1.623581132016245
Eval step 0: eval loss: 0.8343990796232876
Eval: 2022-05-07 09:24:02.149594: total loss: 1.06802592732589, mse:4.591793676520681, ic :0.18185888064411362, sharpe5:15.773663141727447, irr5:514.2697143554688, ndcg5:0.8427740583143776, pnl5:7.043261528015137 
train 12, step: 0, loss: 0.9508749643961588, grad_norm: 0.07175051082259595, ic: 0.41495655856798996
train 12, step: 500, loss: 0.9322742708973178, grad_norm: 0.09085335929099686, ic: 0.16977647800073264
train 12, step: 1000, loss: 2.976063722258161, grad_norm: 1.1196026839355289, ic: 0.23447777694452404
train 12, step: 1500, loss: 0.9448299829231721, grad_norm: 0.1105016197536162, ic: -0.12961095958575708
train 12, step: 2000, loss: 0.8747482069309385, grad_norm: 0.003909886096852151, ic: 0.2076611108710881
Epoch 12: 2022-05-07 09:28:44.861137: train loss: 1.6222575836642028
Eval step 0: eval loss: 0.8306349662885273
Eval: 2022-05-07 09:29:02.644534: total loss: 1.067107208554998, mse:4.5920842800893995, ic :0.18408216775621813, sharpe5:15.987345868349074, irr5:529.537353515625, ndcg5:0.8523822335608199, pnl5:8.483388900756836 
train 13, step: 0, loss: 2.035308741214031, grad_norm: 0.6363054484223125, ic: 0.44672295179617605
train 13, step: 500, loss: 0.8091991055060539, grad_norm: 0.0418664085946449, ic: 0.5921004038266731
train 13, step: 1000, loss: 0.943377726510067, grad_norm: 0.46142555457302314, ic: 0.5964322067678985
train 13, step: 1500, loss: 2.398195074648712, grad_norm: 0.3197533781594246, ic: -0.12335554739490584
train 13, step: 2000, loss: 1.4617147467703522, grad_norm: 0.02814172558653493, ic: 0.1845580545966256
Epoch 13: 2022-05-07 09:33:40.637300: train loss: 1.6224569012097845
Eval step 0: eval loss: 0.8287727830281875
Eval: 2022-05-07 09:33:59.090693: total loss: 1.0674101772746836, mse:4.602341538208155, ic :0.18170857319287262, sharpe5:16.53855759143829, irr5:550.5912475585938, ndcg5:0.8402247706159708, pnl5:9.206223487854004 
train 14, step: 0, loss: 4.526033541341654, grad_norm: 1.513263073280898, ic: 0.1731817051490458
train 14, step: 500, loss: 0.8272607085901663, grad_norm: 0.0038547155660761173, ic: 0.12813348958946105
train 14, step: 1000, loss: 1.842943427593489, grad_norm: 1.0841672118879933, ic: 0.45412143851671755
train 14, step: 1500, loss: 1.119058797647665, grad_norm: 0.06025937801030217, ic: -0.045615360200692984
train 14, step: 2000, loss: 1.137611619281105, grad_norm: 0.16897329251428495, ic: 0.1269228159788722
Epoch 14: 2022-05-07 09:38:41.414657: train loss: 1.6226332443756073
Eval step 0: eval loss: 0.8335876786419915
Eval: 2022-05-07 09:38:59.429411: total loss: 1.0707433164787543, mse:4.597882979328583, ic :0.18243694071820907, sharpe5:16.39149641275406, irr5:537.09765625, ndcg5:0.8580996557653187, pnl5:7.172890663146973 
train 15, step: 0, loss: 3.3473150231031132, grad_norm: 0.627949471654155, ic: 0.1118775770513516
train 15, step: 500, loss: 1.2550535891716024, grad_norm: 0.057568967225366904, ic: -0.03066250441716258
train 15, step: 1000, loss: 1.3160461922002034, grad_norm: 0.1967575418455614, ic: 0.04567235090022269
train 15, step: 1500, loss: 0.8579116941437007, grad_norm: 0.4843481775295351, ic: 0.08302778009686852
train 15, step: 2000, loss: 1.4610601600241546, grad_norm: 0.6731695204093004, ic: 0.056092847297418244
Epoch 15: 2022-05-07 09:43:45.575387: train loss: 1.6216614411235322
Eval step 0: eval loss: 0.8455791252305057
Eval: 2022-05-07 09:44:03.262827: total loss: 1.0729247245438367, mse:4.592912415222145, ic :0.1839951466440992, sharpe5:16.339856436252592, irr5:540.1572875976562, ndcg5:0.8395906025048238, pnl5:4.864434242248535 
train 16, step: 0, loss: 0.6861549552621486, grad_norm: 0.3218362542167427, ic: -0.004222233550976378
train 16, step: 500, loss: 1.5844217343832716, grad_norm: 0.39594513866113645, ic: 0.1878757239653773
train 16, step: 1000, loss: 0.8877344304865057, grad_norm: 0.11194662839699238, ic: -0.0822351917446853
train 16, step: 1500, loss: 0.8548108785522558, grad_norm: 0.25937901604693875, ic: 0.18227949364772802
train 16, step: 2000, loss: 3.4071951125103217, grad_norm: 7.219584794309751, ic: 0.002944284967906406
Epoch 16: 2022-05-07 09:48:41.778460: train loss: 1.6220138115150553
Eval step 0: eval loss: 0.8310195070674723
Eval: 2022-05-07 09:48:59.679510: total loss: 1.0685800090057909, mse:4.595667299386819, ic :0.1827671112866181, sharpe5:16.118793597221373, irr5:532.7844848632812, ndcg5:0.8460374339502683, pnl5:8.462675094604492 
train 17, step: 0, loss: 1.28014806448939, grad_norm: 0.22693283885157275, ic: -0.11363731443131797
train 17, step: 500, loss: 1.7588807641006097, grad_norm: 1.1677702046351033, ic: 0.2341839867161741
train 17, step: 1000, loss: 1.2807777704135739, grad_norm: 0.0864967279496783, ic: 0.14343614188024284
train 17, step: 1500, loss: 4.5143513788115355, grad_norm: 1.1963091059560875, ic: 0.2236216167938897
train 17, step: 2000, loss: 1.265979654559467, grad_norm: 0.7758062166762294, ic: 0.08024933430945341
Epoch 17: 2022-05-07 09:53:43.544521: train loss: 1.62090425510178
Eval step 0: eval loss: 0.8402027576313883
Eval: 2022-05-07 09:54:01.490224: total loss: 1.0693718916498456, mse:4.588905008884082, ic :0.18989916513228644, sharpe5:16.47630811214447, irr5:566.2650756835938, ndcg5:0.8389409883425452, pnl5:7.0070366859436035 
train 18, step: 0, loss: 1.4142749465234068, grad_norm: 0.6919993875838126, ic: 0.13965133081478592
train 18, step: 500, loss: 1.5197745039157082, grad_norm: 0.8300719193041166, ic: -0.02729719145185596
train 18, step: 1000, loss: 0.6531637949486301, grad_norm: 0.02227359980733617, ic: 0.5739091728456666
train 18, step: 1500, loss: 1.433500672636415, grad_norm: 0.047191412272738466, ic: 0.20690388431334017
train 18, step: 2000, loss: 0.9113310795680732, grad_norm: 0.012292256073723948, ic: 0.01590238783496229
Epoch 18: 2022-05-07 09:58:42.813397: train loss: 1.6210735988930716
Eval step 0: eval loss: 0.827479789581138
Eval: 2022-05-07 09:59:00.587896: total loss: 1.066031151741541, mse:4.58605969812063, ic :0.1902953742670665, sharpe5:16.708563144206998, irr5:578.3847045898438, ndcg5:0.8462859611701058, pnl5:4.124877452850342 
train 19, step: 0, loss: 1.4850030808221726, grad_norm: 1.0659748255045451, ic: -0.02755450229655581
train 19, step: 500, loss: 0.8641360247576678, grad_norm: 0.013390072634429826, ic: 0.2174144849628138
train 19, step: 1000, loss: 0.9586660838434891, grad_norm: 0.2410726410789951, ic: 0.1921304673578932
train 19, step: 1500, loss: 3.9689651457088897, grad_norm: 0.9554456027459999, ic: 0.16213315233836562
train 19, step: 2000, loss: 1.0181757061298078, grad_norm: 0.1419736353223536, ic: 0.24604243087945016
Epoch 19: 2022-05-07 10:03:47.662986: train loss: 1.6216350046096355
Eval step 0: eval loss: 0.8356175960303609
Eval: 2022-05-07 10:04:05.137867: total loss: 1.0677821813135226, mse:4.585356142494711, ic :0.18996919126326883, sharpe5:17.071323763132096, irr5:580.3678588867188, ndcg5:0.8536034215920408, pnl5:4.296291351318359 
train 20, step: 0, loss: 2.3115477550642294, grad_norm: 1.3671538768347709, ic: 0.04802730408697142
train 20, step: 500, loss: 3.216517755681818, grad_norm: 0.911568377784056, ic: 0.1318322892939242
train 20, step: 1000, loss: 0.9750121116638184, grad_norm: 0.1772951036589305, ic: 0.13611537498155468
train 20, step: 1500, loss: 1.8364293330661372, grad_norm: 0.8463357242511557, ic: 0.26401651853870556
train 20, step: 2000, loss: 1.0272940655905893, grad_norm: 1.3707029713571248, ic: 0.02801610187325731
Epoch 20: 2022-05-07 10:08:52.517503: train loss: 1.620253705754728
Eval step 0: eval loss: 0.8345792909065463
Eval: 2022-05-07 10:09:10.381374: total loss: 1.0666057469015118, mse:4.583106779955137, ic :0.190137367895684, sharpe5:16.509670912027357, irr5:568.07958984375, ndcg5:0.8435565266477003, pnl5:5.241428375244141 
train 21, step: 0, loss: 1.0183822433972158, grad_norm: 0.5187525446149768, ic: 0.058378261937733
train 21, step: 500, loss: 0.7728537500432108, grad_norm: 0.015579540984847918, ic: 0.18845493240151448
train 21, step: 1000, loss: 0.9275834602222107, grad_norm: 0.8468183938338772, ic: 0.15911093408909102
train 21, step: 1500, loss: 0.9940580144213462, grad_norm: 0.35750335099553543, ic: 0.3187753216519748
train 21, step: 2000, loss: 0.9469096879758444, grad_norm: 0.11755283724317833, ic: 0.05035435163628235
Epoch 21: 2022-05-07 10:13:49.276347: train loss: 1.6209747131312626
Eval step 0: eval loss: 0.8326647550464303
Eval: 2022-05-07 10:14:07.640401: total loss: 1.0675605042732141, mse:4.594081484402236, ic :0.18426864856850153, sharpe5:16.555527498722075, irr5:566.1675415039062, ndcg5:0.8597744693915187, pnl5:7.083871841430664 
train 22, step: 0, loss: 1.0479467359639831, grad_norm: 0.04933164254893083, ic: 0.18465144170758208
train 22, step: 500, loss: 3.2283750952743904, grad_norm: 1.1965493711612132, ic: -0.1830318007010093
train 22, step: 1000, loss: 1.1912997030798411, grad_norm: 0.12653654659373753, ic: 0.4628099703161495
train 22, step: 1500, loss: 0.9763847696437757, grad_norm: 1.9389547447586872, ic: 0.07330618914730853
train 22, step: 2000, loss: 1.7645187550931831, grad_norm: 0.6928789633335665, ic: 0.20909284647819604
Epoch 22: 2022-05-07 10:18:44.967370: train loss: 1.6195219131221839
Eval step 0: eval loss: 0.8345608967498682
Eval: 2022-05-07 10:19:02.204049: total loss: 1.0692698399910778, mse:4.604000330479861, ic :0.17972088509094417, sharpe5:15.818784744143485, irr5:530.3811645507812, ndcg5:0.8621481354116965, pnl5:6.704092502593994 
train 23, step: 0, loss: 0.9864919876846182, grad_norm: 0.0592323363098477, ic: 0.15975720756189132
train 23, step: 500, loss: 1.4287412155366956, grad_norm: 0.1615936588991829, ic: 0.007083645636505352
train 23, step: 1000, loss: 1.6477593994140627, grad_norm: 0.12032376919641974, ic: 0.26332692412185255
train 23, step: 1500, loss: 1.1216595079563987, grad_norm: 0.5118820730816687, ic: 0.07793518851762164
train 23, step: 2000, loss: 1.9454053448566204, grad_norm: 1.332802576324396, ic: 0.4430160453068396
Epoch 23: 2022-05-07 10:23:48.267260: train loss: 1.6189717480393546
Eval step 0: eval loss: 0.8381431266053082
Eval: 2022-05-07 10:24:05.914552: total loss: 1.0686962645779952, mse:4.591215587333256, ic :0.18395705584458516, sharpe5:15.970538660287856, irr5:547.280517578125, ndcg5:0.8420612667583656, pnl5:5.743581295013428 
train 24, step: 0, loss: 2.196625991203617, grad_norm: 0.03688561477624168, ic: 0.12932008507963325
train 24, step: 500, loss: 1.235699439141537, grad_norm: 0.7357411895949837, ic: 0.021904366878603126
train 24, step: 1000, loss: 0.9042132968116983, grad_norm: 0.08097021019694374, ic: 0.5336909184227409
train 24, step: 1500, loss: 2.5975996931033594, grad_norm: 6.815824582263121, ic: 0.07104152320825227
train 24, step: 2000, loss: 0.9316207276385493, grad_norm: 0.05231905310672781, ic: 0.13730867671871977
Epoch 24: 2022-05-07 10:28:45.146028: train loss: 1.6154878516537428
Eval step 0: eval loss: 0.8346549899359522
Eval: 2022-05-07 10:29:01.822583: total loss: 1.0712177523039437, mse:4.62571674366688, ic :0.18154903499552832, sharpe5:15.648484062552452, irr5:550.619873046875, ndcg5:0.8389805862801906, pnl5:3.9240150451660156 
train 25, step: 0, loss: 0.8619463946368243, grad_norm: 0.453115825785573, ic: 0.6042305064228749
train 25, step: 500, loss: 0.8697494091944392, grad_norm: 0.016609461503027497, ic: 0.24467895598000047
train 25, step: 1000, loss: 2.1134616128379666, grad_norm: 0.5384303131225777, ic: 0.2259914309396499
train 25, step: 1500, loss: 1.1266501791906294, grad_norm: 1.3885997986686376, ic: 0.5416768482204152
train 25, step: 2000, loss: 1.0280166191826423, grad_norm: 0.6343012716474166, ic: 0.6054768544960052
Epoch 25: 2022-05-07 10:33:46.016011: train loss: 1.618003547727334
Eval step 0: eval loss: 0.832148303724315
Eval: 2022-05-07 10:34:04.135292: total loss: 1.0664385901655018, mse:4.591601746356512, ic :0.1929235757604847, sharpe5:18.065041426420212, irr5:606.5173950195312, ndcg5:0.8653225732959485, pnl5:4.843430519104004 
train 26, step: 0, loss: 6.795455396365814, grad_norm: 2.7380971376845302, ic: 0.11543779359782697
train 26, step: 500, loss: 3.862310414019742, grad_norm: 3.5124247081172735, ic: 0.3813255710856186
train 26, step: 1000, loss: 1.2588722312843408, grad_norm: 1.000920049506782, ic: -0.006067577391215552
train 26, step: 1500, loss: 0.8368483934381779, grad_norm: 0.15657785580163747, ic: 0.29969962411805506
train 26, step: 2000, loss: 0.9621507117918962, grad_norm: 0.3589940231096493, ic: 0.11745580104591333
Epoch 26: 2022-05-07 10:38:41.388968: train loss: 1.6163271002706305
Eval step 0: eval loss: 0.8317654351414318
Eval: 2022-05-07 10:38:59.068155: total loss: 1.0666853232875444, mse:4.594390669149204, ic :0.1868713981145562, sharpe5:16.52013095855713, irr5:570.0595703125, ndcg5:0.848837933297777, pnl5:5.92879581451416 
train 27, step: 0, loss: 0.8265185546875, grad_norm: 0.26181064446557323, ic: 0.12749819356094005
train 27, step: 500, loss: 0.8960994284567275, grad_norm: 4.275293115376047, ic: 0.2913462206563211
train 27, step: 1000, loss: 0.7553981890165797, grad_norm: 1.0502270876293183, ic: 0.19442141639752186
train 27, step: 1500, loss: 0.6395699352642138, grad_norm: 0.04718832809326301, ic: 0.5067761788275739
train 27, step: 2000, loss: 1.3852425982939447, grad_norm: 0.04700432317448722, ic: 0.014617035480016685
Epoch 27: 2022-05-07 10:43:45.980806: train loss: 1.61587015552608
Eval step 0: eval loss: 0.8386923786963251
Eval: 2022-05-07 10:44:03.989508: total loss: 1.070165476989836, mse:4.596770235555321, ic :0.18323604707626537, sharpe5:15.736434343457221, irr5:548.9718627929688, ndcg5:0.835142577828342, pnl5:4.350900173187256 
train 28, step: 0, loss: 1.5465497933759653, grad_norm: 0.5520187844703139, ic: 0.22152329535221144
train 28, step: 500, loss: 1.357630670503773, grad_norm: 1.9301779517087745, ic: 0.18561665180953973
train 28, step: 1000, loss: 0.9112340415396342, grad_norm: 0.32484371588965644, ic: 0.5810733235032446
train 28, step: 1500, loss: 1.0348757823123058, grad_norm: 0.07522532499465137, ic: 0.04010290463360739
train 28, step: 2000, loss: 1.0393322903685775, grad_norm: 0.21108327746596378, ic: 0.1200711213759731
Epoch 28: 2022-05-07 10:48:06.883178: train loss: 1.6133125579991008
Eval step 0: eval loss: 0.8273858893407534
Eval: 2022-05-07 10:48:25.163632: total loss: 1.0714002042554875, mse:4.63273543079386, ic :0.18021797999752628, sharpe5:16.583275283575055, irr5:575.9505615234375, ndcg5:0.8387520439917189, pnl5:5.543543338775635 
train 29, step: 0, loss: 0.9084243138149166, grad_norm: 0.03253625846947458, ic: 0.08530552487724302
train 29, step: 500, loss: 1.0992647396139166, grad_norm: 0.15342978813573166, ic: 0.6204294065017698
train 29, step: 1000, loss: 1.0616716456614543, grad_norm: 1.0136725915056837, ic: 0.09274085353217049
train 29, step: 1500, loss: 2.3654291538592895, grad_norm: 0.4136170800681095, ic: -0.06258366462615261
train 29, step: 2000, loss: 4.156531063126929, grad_norm: 4.373939534290358, ic: 0.202930241411438
Epoch 29: 2022-05-07 10:53:09.671102: train loss: 1.6133776223780638
Eval step 0: eval loss: 0.8386854326511459
Eval: 2022-05-07 10:53:27.426830: total loss: 1.0680228017528168, mse:4.590541419205823, ic :0.18452309128106914, sharpe5:16.006268166303634, irr5:546.70458984375, ndcg5:0.8441566928475348, pnl5:4.710546970367432 
train 30, step: 0, loss: 1.0071659028268205, grad_norm: 0.16095446812114822, ic: 0.5044334645370396
train 30, step: 500, loss: 1.4187135891996514, grad_norm: 2.5770360820749927, ic: 0.005876486456315607
train 30, step: 1000, loss: 0.9769494258996212, grad_norm: 0.08201983936110577, ic: -0.06436944292256097
train 30, step: 1500, loss: 1.51456553168519, grad_norm: 2.6230578734092074, ic: 0.1678946003157511
train 30, step: 2000, loss: 1.8506587874187068, grad_norm: 0.7479599382137619, ic: 0.051502646597183156
Epoch 30: 2022-05-07 10:58:10.991701: train loss: 1.6149342246666138
Eval step 0: eval loss: 0.8572789665519626
Eval: 2022-05-07 10:58:29.334106: total loss: 1.0838250248702765, mse:4.707644252856173, ic :0.16934736068117837, sharpe5:16.297340859174728, irr5:558.2783813476562, ndcg5:0.8591386709992201, pnl5:5.0160441398620605 
train 31, step: 0, loss: 1.0384638492628313, grad_norm: 0.22398083296761517, ic: 0.36335743122146524
train 31, step: 500, loss: 1.5198435088734568, grad_norm: 2.124814930136687, ic: 0.042077518891014666
train 31, step: 1000, loss: 4.33489918764637, grad_norm: 1.5245947300157097, ic: 0.4667420922474007
train 31, step: 1500, loss: 0.7667932827844125, grad_norm: 0.07572427552654738, ic: 0.7160163152190031
train 31, step: 2000, loss: 1.265939173365074, grad_norm: 3.748040784374508, ic: 0.10305985095572853
Epoch 31: 2022-05-07 11:03:20.540912: train loss: 1.611877760774685
Eval step 0: eval loss: 0.8393458214650289
Eval: 2022-05-07 11:03:38.529510: total loss: 1.069191848828772, mse:4.5900946822101885, ic :0.18271968891462195, sharpe5:15.860905870199202, irr5:541.66552734375, ndcg5:0.8486071499491046, pnl5:4.914128303527832 
train 32, step: 0, loss: 1.123240398784587, grad_norm: 0.08032740333971916, ic: 0.20183614754230725
train 32, step: 500, loss: 1.4817046398252953, grad_norm: 1.9039771449408907, ic: 0.0541324789288512
train 32, step: 1000, loss: 1.0558135948605532, grad_norm: 0.34087893882886894, ic: 0.5120489122876641
train 32, step: 1500, loss: 0.973421633652746, grad_norm: 3.1054402441134528, ic: 0.090238810345426
train 32, step: 2000, loss: 0.945820511306678, grad_norm: 0.08841168468725172, ic: 0.5568164063022245
Epoch 32: 2022-05-07 11:08:25.253896: train loss: 1.6111794311700907
Eval step 0: eval loss: 0.8286984346186775
Eval: 2022-05-07 11:08:43.350494: total loss: 1.064939343674351, mse:4.586455946105442, ic :0.19262004568739505, sharpe5:16.960082899332047, irr5:591.3897094726562, ndcg5:0.8481588439006692, pnl5:4.574181079864502 
train 33, step: 0, loss: 1.2619033055480562, grad_norm: 0.8401568294864117, ic: 0.20740554459968877
train 33, step: 500, loss: 0.9952427082338617, grad_norm: 0.02778260454791982, ic: 0.13271715728637973
train 33, step: 1000, loss: 1.0351028687700965, grad_norm: 4.6538665345718275, ic: 0.2000699062922218
train 33, step: 1500, loss: 0.9109286379559609, grad_norm: 0.18964541115028422, ic: 0.5515021492216571
train 33, step: 2000, loss: 0.8142834646766046, grad_norm: 0.07437656392406587, ic: 0.20968620048087447
Epoch 33: 2022-05-07 11:13:31.069685: train loss: 1.6154691599352218
Eval step 0: eval loss: 0.8326481617162802
Eval: 2022-05-07 11:13:49.427071: total loss: 1.0677193669182674, mse:4.585949789119972, ic :0.1920060601569193, sharpe5:16.73299640536308, irr5:571.293701171875, ndcg5:0.8516237295359322, pnl5:4.82818078994751 
train 34, step: 0, loss: 1.0107803080000868, grad_norm: 1.105358951272012, ic: 0.6056243335352673
train 34, step: 500, loss: 0.801711896143922, grad_norm: 2.5912842235278215, ic: 0.219327861648585
train 34, step: 1000, loss: 3.142581125192012, grad_norm: 1.1384936465491318, ic: 0.33060777626246296
train 34, step: 1500, loss: 0.8070862762123807, grad_norm: 1.002243634252127, ic: 0.6808761919305577
train 34, step: 2000, loss: 6.485701037228893, grad_norm: 18.347038753561435, ic: 0.44132124047966104
Epoch 34: 2022-05-07 11:18:45.488735: train loss: 1.611246856146535
Eval step 0: eval loss: 0.8254754696555584
Eval: 2022-05-07 11:19:03.812055: total loss: 1.0722689145666187, mse:4.6284874820430755, ic :0.1790439450120635, sharpe5:16.182206565141676, irr5:572.814208984375, ndcg5:0.8338283455458743, pnl5:3.714340925216675 
train 35, step: 0, loss: 1.1874704159007352, grad_norm: 0.8133405180460695, ic: 0.5538798450317487
train 35, step: 500, loss: 1.1928639072720768, grad_norm: 0.46243124834755683, ic: 0.09948474462837836
train 35, step: 1000, loss: 1.7321349708897293, grad_norm: 11.66617293293274, ic: 0.10359574532551614
train 35, step: 1500, loss: 1.6106753700657896, grad_norm: 0.9609178285172641, ic: 0.09304796320558498
train 35, step: 2000, loss: 0.7844660957866811, grad_norm: 0.3861594110105497, ic: 0.5669204337647288
Epoch 35: 2022-05-07 11:23:49.288198: train loss: 1.6131538157769956
Eval step 0: eval loss: 0.8364356214806704
Eval: 2022-05-07 11:24:06.667281: total loss: 1.0684028939015806, mse:4.5949641875105645, ic :0.18740544399346593, sharpe5:16.54894385576248, irr5:562.9785766601562, ndcg5:0.858688878304921, pnl5:4.300408363342285 
train 36, step: 0, loss: 1.835213126717033, grad_norm: 4.098392099843163, ic: 0.12654945100433565
train 36, step: 500, loss: 0.8318735878706167, grad_norm: 0.2315573133035411, ic: 0.23011673744999103
train 36, step: 1000, loss: 1.6981079545454545, grad_norm: 6.2478394010047555, ic: 0.23625753543896566
train 36, step: 1500, loss: 0.7595667749197936, grad_norm: 0.09891060563541694, ic: 0.40400210718988866
train 36, step: 2000, loss: 1.1110510186035543, grad_norm: 1.9871213139354322, ic: 0.7755887411190141
Epoch 36: 2022-05-07 11:28:59.914645: train loss: 1.6088504174249207
Eval step 0: eval loss: 0.8326335621583574
Eval: 2022-05-07 11:29:15.720085: total loss: 1.066775369995001, mse:4.587818437419062, ic :0.18689769216653085, sharpe5:16.177529908418656, irr5:559.6624145507812, ndcg5:0.8551709006341434, pnl5:5.428177833557129 
train 37, step: 0, loss: 2.028410042591884, grad_norm: 2.9539143146885065, ic: 0.17084628851661443
train 37, step: 500, loss: 2.337627765599511, grad_norm: 1.5100806298524339, ic: -0.0202969706956769
train 37, step: 1000, loss: 1.073145385927987, grad_norm: 0.1385143352941703, ic: 0.07267817013310227
train 37, step: 1500, loss: 2.0076800814977434, grad_norm: 2.294897610065992, ic: 0.601936066970337
train 37, step: 2000, loss: 1.316430582952658, grad_norm: 0.4215731860629712, ic: 0.1528116966639822
Epoch 37: 2022-05-07 11:34:07.762229: train loss: 1.605855523610298
Eval step 0: eval loss: 0.8300027475467597
Eval: 2022-05-07 11:34:25.548882: total loss: 1.069756177075692, mse:4.614352338967672, ic :0.18280814800535053, sharpe5:16.646909598112106, irr5:579.9290161132812, ndcg5:0.845302670044619, pnl5:5.6141581535339355 
train 38, step: 0, loss: 1.3277291088569456, grad_norm: 0.7738934416214358, ic: -0.07075547679382391
train 38, step: 500, loss: 0.9038141698013117, grad_norm: 0.107923165318114, ic: 0.26121370753338186
train 38, step: 1000, loss: 0.9061094020195158, grad_norm: 0.1533451911426298, ic: 0.1448138414558987
train 38, step: 1500, loss: 0.949934951446232, grad_norm: 0.04661821366377671, ic: 0.21802439269798218
train 38, step: 2000, loss: 2.345380247179475, grad_norm: 4.656650692583461, ic: 0.004616482158656944
Epoch 38: 2022-05-07 11:39:09.837120: train loss: 1.6071831954421416
Eval step 0: eval loss: 0.8316600867895482
Eval: 2022-05-07 11:39:27.796457: total loss: 1.0656720657742735, mse:4.58518144756738, ic :0.1913417516979201, sharpe5:16.731021312475203, irr5:579.2637939453125, ndcg5:0.8437051660940609, pnl5:7.396233081817627 
train 39, step: 0, loss: 0.969307155784117, grad_norm: 0.0033974347378268556, ic: 0.08954693465293057
train 39, step: 500, loss: 0.885513059365392, grad_norm: 0.13541402296597757, ic: 0.23544254327293543
train 39, step: 1000, loss: 0.9377474875642613, grad_norm: 0.06898099271496132, ic: 0.1947644370840918
train 39, step: 1500, loss: 2.067087212757528, grad_norm: 0.5186831185267765, ic: 0.2346785557768559
train 39, step: 2000, loss: 0.6207847655478623, grad_norm: 0.09045072498917382, ic: 0.12163711150043874
Epoch 39: 2022-05-07 11:44:18.800965: train loss: 1.60816564210655
Eval step 0: eval loss: 0.8329693519905821
Eval: 2022-05-07 11:44:37.123258: total loss: 1.0667231592488127, mse:4.586035067467883, ic :0.19080823041621767, sharpe5:16.829435423612594, irr5:580.206787109375, ndcg5:0.8415260610009421, pnl5:6.4810614585876465 
