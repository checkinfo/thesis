Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='SparseAdjSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=1, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_60_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
41229
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.8844483758062935, grad_norm: 5.086529890353829, ic: 0.011080768022936845
train 0, step: 500, loss: 0.8626277552750565, grad_norm: 0.021845479933168533, ic: 0.05270063239366424
train 0, step: 1000, loss: 1.945514313391551, grad_norm: 0.5896999431198008, ic: 0.007373496872161656
train 0, step: 1500, loss: 0.9754544103569663, grad_norm: 0.1203006587542964, ic: 0.07227164357549577
train 0, step: 2000, loss: 0.9941551257930615, grad_norm: 0.1592178456654952, ic: 0.023822025601038417
Epoch 0: 2022-05-07 18:27:57.315863: train loss: 1.6493776445188217
Eval step 0: eval loss: 0.8351404413156941
Eval: 2022-05-07 18:28:20.438751: total loss: 1.0789986860416314, mse:4.823401339027062, ic :0.007219014974401239, sharpe5:7.399318958222866, irr5:208.48025512695312, ndcg5:0.8538948244995641, pnl5:2.688011646270752 
train 1, step: 0, loss: 2.762567138671875, grad_norm: 0.9542544254041988, ic: 0.06177142284761713
train 1, step: 500, loss: 1.7645190690882633, grad_norm: 0.8586151958135821, ic: 0.10878067166805169
train 1, step: 1000, loss: 0.8746514377823795, grad_norm: 0.19417646068028088, ic: 0.05972345424770253
train 1, step: 1500, loss: 1.7121394014906608, grad_norm: 0.2369929064385693, ic: -0.02239826904510048
train 1, step: 2000, loss: 2.1932923828125, grad_norm: 0.9751932741073508, ic: 0.04472453770716356
Epoch 1: 2022-05-07 18:32:57.443947: train loss: 1.646899164843813
Eval step 0: eval loss: 0.8340456674171824
Eval: 2022-05-07 18:33:20.361031: total loss: 1.078835454405826, mse:4.823983660637721, ic :0.008371420022900877, sharpe5:7.270232732892036, irr5:205.40126037597656, ndcg5:0.842259951647128, pnl5:2.50300931930542 
train 2, step: 0, loss: 2.142989701704545, grad_norm: 0.010836622221109649, ic: 0.12783153556837193
train 2, step: 500, loss: 3.3057183159722223, grad_norm: 0.310095991226038, ic: 0.056428411817897824
train 2, step: 1000, loss: 2.0724164122365902, grad_norm: 0.0002603903501233552, ic: 0.21256613721999212
train 2, step: 1500, loss: 1.4862880561187977, grad_norm: 0.0708301854813364, ic: -0.013583024841706615
train 2, step: 2000, loss: 3.2359912109375, grad_norm: 0.8787293132714434, ic: 0.23790731581188806
Epoch 2: 2022-05-07 18:38:05.946424: train loss: 1.6467383661548958
Eval step 0: eval loss: 0.8360721760981954
Eval: 2022-05-07 18:38:29.751212: total loss: 1.0794914599135463, mse:4.822849445305407, ic :0.013213263021696835, sharpe5:7.470679121017456, irr5:211.8809814453125, ndcg5:0.853408557355922, pnl5:2.722460985183716 
train 3, step: 0, loss: 1.5240871522484756, grad_norm: 0.6091986782070183, ic: 0.008795108473792177
train 3, step: 500, loss: 1.4953060762529367, grad_norm: 0.38873673244712714, ic: 0.14665283826093262
train 3, step: 1000, loss: 3.6689843862442433, grad_norm: 0.7940821261409663, ic: -0.05432143346707964
train 3, step: 1500, loss: 1.9921475424509003, grad_norm: 1.0091720433149611, ic: -0.06516468449525306
train 3, step: 2000, loss: 0.8990694942989865, grad_norm: 0.000589189735040613, ic: 0.006139674353939967
Epoch 3: 2022-05-07 18:43:09.795826: train loss: 1.646125742061077
Eval step 0: eval loss: 0.8335926952301764
Eval: 2022-05-07 18:43:33.029261: total loss: 1.078857764415301, mse:4.826578411395052, ic :0.019702637536198227, sharpe5:8.105214602351188, irr5:228.22190856933594, ndcg5:0.8506750846133933, pnl5:2.41976261138916 
train 4, step: 0, loss: 1.4373474370216837, grad_norm: 0.05312883720581266, ic: 0.10963262251538772
train 4, step: 500, loss: 1.6439303180364173, grad_norm: 0.6511086400774289, ic: 0.049845735230893876
train 4, step: 1000, loss: 2.947179468666635, grad_norm: 0.7280677890464696, ic: 0.06344802664113904
train 4, step: 1500, loss: 2.1494486748417723, grad_norm: 0.5058537667549338, ic: 0.0038166664855377695
train 4, step: 2000, loss: 1.0814387438642108, grad_norm: 0.47198391300445086, ic: 0.2713955504718981
Epoch 4: 2022-05-07 18:48:15.000607: train loss: 1.6448669311632051
Eval step 0: eval loss: 0.8639624769494204
Eval: 2022-05-07 18:48:37.737918: total loss: 1.0926422631165906, mse:4.831298757822761, ic :0.07701028892263077, sharpe5:11.157966601252555, irr5:376.7618103027344, ndcg5:0.8478072254826007, pnl5:3.1327083110809326 
train 5, step: 0, loss: 1.3737773306417784, grad_norm: 0.21462562404142976, ic: 0.20583594626142415
train 5, step: 500, loss: 0.8838647635185368, grad_norm: 0.015010425416363764, ic: 0.7927989387298637
train 5, step: 1000, loss: 0.9812998570701629, grad_norm: 0.1712711113552002, ic: -0.0014952212188068083
train 5, step: 1500, loss: 1.527375903282925, grad_norm: 0.17092591656232065, ic: 0.0037571015095717863
train 5, step: 2000, loss: 1.101755872403386, grad_norm: 0.02944956532629954, ic: 0.1672328730737048
Epoch 5: 2022-05-07 18:53:17.567514: train loss: 1.6382128973058323
Eval step 0: eval loss: 0.8345711228719375
Eval: 2022-05-07 18:53:40.955060: total loss: 1.0762126653460855, mse:4.715407364955864, ic :0.13360293368628332, sharpe5:11.785486151576041, irr5:392.52764892578125, ndcg5:0.8391349455111335, pnl5:2.9030961990356445 
train 6, step: 0, loss: 1.3340924276689594, grad_norm: 0.48547375888633043, ic: 0.09258281663796389
train 6, step: 500, loss: 1.0070786760923143, grad_norm: 0.04905475675655347, ic: 0.03543510238074011
train 6, step: 1000, loss: 1.1187750638664449, grad_norm: 0.09371932328141237, ic: 0.10949450122407026
train 6, step: 1500, loss: 1.573373236537965, grad_norm: 0.8118473837047799, ic: 0.14843800025030573
train 6, step: 2000, loss: 0.8084911777077912, grad_norm: 0.05782198327846125, ic: 0.13331239737215606
Epoch 6: 2022-05-07 18:58:13.256475: train loss: 1.635706132527894
Eval step 0: eval loss: 0.8239721653961406
Eval: 2022-05-07 18:58:36.313761: total loss: 1.0718149905877958, mse:4.70528483804781, ic :0.1502927691233007, sharpe5:14.796226751804351, irr5:470.9250183105469, ndcg5:0.8450149853813869, pnl5:5.463319301605225 
train 7, step: 0, loss: 0.9824222564697266, grad_norm: 0.04548059031897716, ic: 0.10162412454031931
train 7, step: 500, loss: 0.6525891892453457, grad_norm: 0.0033522658933503747, ic: 0.05281679962711665
train 7, step: 1000, loss: 1.019677771563214, grad_norm: 0.20135918825639212, ic: 0.0781368664200218
train 7, step: 1500, loss: 2.2428432517751875, grad_norm: 0.6870263824196626, ic: 0.43990359635889703
train 7, step: 2000, loss: 0.9157318460344306, grad_norm: 0.0722004973901385, ic: -0.064883571440155
Epoch 7: 2022-05-07 19:03:18.612532: train loss: 1.6308504560601922
Eval step 0: eval loss: 0.827226902084431
Eval: 2022-05-07 19:03:41.923846: total loss: 1.072664789891684, mse:4.697406540741709, ic :0.15525539313814138, sharpe5:15.050828332901, irr5:476.82257080078125, ndcg5:0.8338334174397594, pnl5:5.175918102264404 
train 8, step: 0, loss: 3.622043421648551, grad_norm: 1.2416361024505747, ic: 0.16381053291834846
train 8, step: 500, loss: 2.8220526512632977, grad_norm: 1.5495228544686306, ic: 0.0832385941260262
train 8, step: 1000, loss: 3.064318670742754, grad_norm: 0.9614769776281724, ic: 0.10590488163004469
train 8, step: 1500, loss: 0.7155445630394575, grad_norm: 0.03772045756117384, ic: 0.46357788241351516
train 8, step: 2000, loss: 1.0803228714330024, grad_norm: 0.4356851654896885, ic: 0.5047401724334544
Epoch 8: 2022-05-07 19:08:21.518922: train loss: 1.6285496715790686
Eval step 0: eval loss: 0.8225208921603332
Eval: 2022-05-07 19:08:45.058913: total loss: 1.070731719420154, mse:4.687838787892066, ic :0.16525984793939386, sharpe5:16.8970199239254, irr5:533.0145874023438, ndcg5:0.8442195450812886, pnl5:6.68275260925293 
train 9, step: 0, loss: 5.461511444627193, grad_norm: 0.815593258520322, ic: 0.15044803658299366
train 9, step: 500, loss: 1.3529880442850737, grad_norm: 1.0744602342289766, ic: 0.32276513568655585
train 9, step: 1000, loss: 0.930247640374846, grad_norm: 0.05754396465859049, ic: 0.07921112332769754
train 9, step: 1500, loss: 1.090865437166329, grad_norm: 0.014562134734117715, ic: 0.39687037901288025
train 9, step: 2000, loss: 1.0647250801586798, grad_norm: 0.2692224381006135, ic: 0.2606146377563796
Epoch 9: 2022-05-07 19:13:26.170358: train loss: 1.6268591531742618
Eval step 0: eval loss: 0.8268641641695205
Eval: 2022-05-07 19:13:49.024817: total loss: 1.071723815892193, mse:4.6874443156969745, ic :0.16089270694407629, sharpe5:16.770519386529923, irr5:552.4630737304688, ndcg5:0.8568527816365054, pnl5:8.149969100952148 
train 10, step: 0, loss: 7.035176179846939, grad_norm: 1.7852358406477427, ic: 0.25109682275145806
train 10, step: 500, loss: 1.124248893893495, grad_norm: 0.06759614817803798, ic: 0.0634284994499496
train 10, step: 1000, loss: 2.3783698520777414, grad_norm: 1.375791409107378, ic: 0.14203151214910012
train 10, step: 1500, loss: 1.1061458835354099, grad_norm: 0.2643110700623192, ic: -0.002259237003409763
train 10, step: 2000, loss: 2.7070857920545213, grad_norm: 0.3612416881013974, ic: 0.49999088945977416
Epoch 10: 2022-05-07 19:18:32.212935: train loss: 1.6265417247340779
Eval step 0: eval loss: 0.8294079602706796
Eval: 2022-05-07 19:18:55.428309: total loss: 1.0692304314949892, mse:4.629971564663616, ic :0.17929673735193233, sharpe5:17.789831907749175, irr5:579.5482788085938, ndcg5:0.8491293696696299, pnl5:9.538066864013672 
train 11, step: 0, loss: 1.2510997657156324, grad_norm: 0.05626969071433817, ic: 0.2202350651818336
train 11, step: 500, loss: 0.6510437138787083, grad_norm: 0.03121849561483439, ic: 0.6328760269599936
train 11, step: 1000, loss: 0.9388305965967643, grad_norm: 0.1298276331637513, ic: 0.043240975024062225
train 11, step: 1500, loss: 1.0535905403003358, grad_norm: 0.054185882366145695, ic: 0.1814325686354725
train 11, step: 2000, loss: 0.7890200742989731, grad_norm: 0.0014200938471079505, ic: 0.12223045137166162
Epoch 11: 2022-05-07 19:23:38.921480: train loss: 1.62377406112311
Eval step 0: eval loss: 0.830320464798472
Eval: 2022-05-07 19:24:02.415044: total loss: 1.0689850126901081, mse:4.591152542860417, ic :0.18238000208855873, sharpe5:16.16447424173355, irr5:530.8649291992188, ndcg5:0.8486221602281104, pnl5:6.228967666625977 
train 12, step: 0, loss: 0.9580179055531819, grad_norm: 0.08662216737910458, ic: 0.40661225099880105
train 12, step: 500, loss: 0.933847288339476, grad_norm: 0.08122259758998458, ic: 0.1659801433907634
train 12, step: 1000, loss: 2.9453768396073845, grad_norm: 0.3608439886094359, ic: 0.3756343379137924
train 12, step: 1500, loss: 0.9332320946187167, grad_norm: 0.11628961670315191, ic: -0.11576589181992525
train 12, step: 2000, loss: 0.8734326319391994, grad_norm: 0.006041339636237604, ic: 0.23558117415468513
Epoch 12: 2022-05-07 19:28:48.236993: train loss: 1.6221042797725176
Eval step 0: eval loss: 0.8311713553329162
Eval: 2022-05-07 19:29:11.878732: total loss: 1.0670493740259455, mse:4.594374817685688, ic :0.17933312792942585, sharpe5:16.0487837433815, irr5:526.2736206054688, ndcg5:0.8603534696763139, pnl5:7.1827545166015625 
train 13, step: 0, loss: 2.0427889315354406, grad_norm: 0.7398286212918335, ic: 0.44522897545416446
train 13, step: 500, loss: 0.8115460862892224, grad_norm: 0.050339256628330636, ic: 0.595716258980952
train 13, step: 1000, loss: 0.9376902330641778, grad_norm: 0.3656036596017672, ic: 0.607500966192276
train 13, step: 1500, loss: 2.4030940869559916, grad_norm: 0.3980393607859737, ic: -0.044472734778298675
train 13, step: 2000, loss: 1.5125174225064952, grad_norm: 0.2666372100926385, ic: 0.18665984181888995
Epoch 13: 2022-05-07 19:33:53.803679: train loss: 1.6216598948281864
Eval step 0: eval loss: 0.821695663403912
Eval: 2022-05-07 19:34:17.127167: total loss: 1.065995745847379, mse:4.595594181229559, ic :0.18651768180339634, sharpe5:17.203547151088713, irr5:583.2744140625, ndcg5:0.8534267228416478, pnl5:7.841763496398926 
train 14, step: 0, loss: 4.5157042219188765, grad_norm: 1.8001422077813165, ic: 0.13681933213072345
train 14, step: 500, loss: 0.827209845959958, grad_norm: 0.005899293200823422, ic: 0.06429739273314466
train 14, step: 1000, loss: 1.8231936189018603, grad_norm: 0.22752752580327767, ic: 0.4443852072382193
train 14, step: 1500, loss: 1.1255089776295135, grad_norm: 0.06942559263492895, ic: -0.07139048435772055
train 14, step: 2000, loss: 1.1532138196055683, grad_norm: 0.22101524993546606, ic: 0.08680759225235488
Epoch 14: 2022-05-07 19:38:52.347538: train loss: 1.6207480571842332
Eval step 0: eval loss: 0.8331145757870125
Eval: 2022-05-07 19:39:16.042307: total loss: 1.0678800829193282, mse:4.592496545907065, ic :0.1828624613668178, sharpe5:16.492449388504028, irr5:552.166015625, ndcg5:0.8539247468198065, pnl5:7.439404487609863 
train 15, step: 0, loss: 3.44541281614786, grad_norm: 1.2794532556144604, ic: 0.08914332078674589
train 15, step: 500, loss: 1.2560611005039486, grad_norm: 0.028954502887529036, ic: 0.05322006846846377
train 15, step: 1000, loss: 1.3176232810912094, grad_norm: 0.14621917321650843, ic: 0.04160359822723182
train 15, step: 1500, loss: 0.8540509081262303, grad_norm: 0.21028741626053904, ic: 0.07494667842643708
train 15, step: 2000, loss: 1.4656531292459238, grad_norm: 0.6803254541546787, ic: 0.05686243032040559
Epoch 15: 2022-05-07 19:43:55.340576: train loss: 1.6202388248760282
Eval step 0: eval loss: 0.8405321802555321
Eval: 2022-05-07 19:44:18.307457: total loss: 1.0706895905351692, mse:4.586408471588241, ic :0.18609707546269255, sharpe5:16.56447028040886, irr5:565.2074584960938, ndcg5:0.8423919997210717, pnl5:5.835379600524902 
train 16, step: 0, loss: 0.7012177842624208, grad_norm: 0.4968392625344473, ic: -0.04319657900050092
train 16, step: 500, loss: 1.586856161494177, grad_norm: 0.4964515868157374, ic: 0.16815556751657468
train 16, step: 1000, loss: 0.8790886156486742, grad_norm: 0.009511174453358015, ic: -0.12167059387163934
train 16, step: 1500, loss: 0.8470103066206167, grad_norm: 0.2408255275517754, ic: 0.13984176969244566
train 16, step: 2000, loss: 3.333743056809971, grad_norm: 1.8256687825983777, ic: 0.007435908573835611
Epoch 16: 2022-05-07 19:48:56.761930: train loss: 1.618454263499908
Eval step 0: eval loss: 0.8317211219457982
Eval: 2022-05-07 19:49:19.887171: total loss: 1.0685456524723624, mse:4.600266634351271, ic :0.17448702720495204, sharpe5:15.916104419231415, irr5:518.9295654296875, ndcg5:0.8585653988818164, pnl5:7.6044087409973145 
train 17, step: 0, loss: 1.2782126833968832, grad_norm: 0.39360974759207545, ic: -0.13314867758164017
train 17, step: 500, loss: 1.7409636210619919, grad_norm: 1.7938611225239178, ic: 0.2400103433899033
train 17, step: 1000, loss: 1.2795982320387065, grad_norm: 0.09319891437941344, ic: 0.14383120658267487
train 17, step: 1500, loss: 4.549044461218773, grad_norm: 1.5277854253645993, ic: 0.21042913578658823
train 17, step: 2000, loss: 1.3032541206742243, grad_norm: 1.0275343234304968, ic: 0.06013608399644996
Epoch 17: 2022-05-07 19:53:59.584727: train loss: 1.620114563201792
Eval step 0: eval loss: 0.8308863745348721
Eval: 2022-05-07 19:54:21.386777: total loss: 1.0664846374312045, mse:4.582546069538617, ic :0.1917572252438353, sharpe5:18.307684614658356, irr5:609.4832153320312, ndcg5:0.8502986610409708, pnl5:6.818448543548584 
train 18, step: 0, loss: 1.4217202751585858, grad_norm: 1.240137591257676, ic: 0.1374375234720087
train 18, step: 500, loss: 1.5226242890561945, grad_norm: 1.5309825807401642, ic: 0.0072868405290159345
train 18, step: 1000, loss: 0.6538530741652396, grad_norm: 0.022833332284247825, ic: 0.5761185253884044
train 18, step: 1500, loss: 1.420098232109951, grad_norm: 0.052282270230444045, ic: 0.21447942016800567
train 18, step: 2000, loss: 0.9112335982596039, grad_norm: 0.009075939191255076, ic: -0.015367337785202034
Epoch 18: 2022-05-07 19:59:00.000859: train loss: 1.6185288380264249
Eval step 0: eval loss: 0.8256286685408982
Eval: 2022-05-07 19:59:23.483466: total loss: 1.0650430324002405, mse:4.595947251114786, ic :0.1868232820955729, sharpe5:17.66704129099846, irr5:580.9722290039062, ndcg5:0.8467118811227848, pnl5:12.324408531188965 
train 19, step: 0, loss: 1.4847619435143848, grad_norm: 1.2519252506866225, ic: 0.05861495457533226
train 19, step: 500, loss: 0.8619708308467158, grad_norm: 0.025817282612276244, ic: 0.21221157852702008
train 19, step: 1000, loss: 0.9553013180998482, grad_norm: 0.018518306576207438, ic: 0.19943873020878944
train 19, step: 1500, loss: 3.9653509036806045, grad_norm: 1.7555815694088734, ic: 0.13974177074084845
train 19, step: 2000, loss: 1.009089637169471, grad_norm: 0.16818956507951938, ic: 0.17495877535953705
Epoch 19: 2022-05-07 20:04:00.100071: train loss: 1.6198541682235386
Eval step 0: eval loss: 0.8290285647103859
Eval: 2022-05-07 20:04:23.306144: total loss: 1.066619277575407, mse:4.589278132117895, ic :0.19065745639980594, sharpe5:17.292464168071746, irr5:583.2272338867188, ndcg5:0.8366741617662408, pnl5:5.035469055175781 
train 20, step: 0, loss: 2.306830533596838, grad_norm: 3.5652395839408175, ic: 0.04649148107459791
train 20, step: 500, loss: 3.2095241477272727, grad_norm: 0.6278878683708684, ic: 0.10380348015761082
train 20, step: 1000, loss: 0.9775936126708985, grad_norm: 0.16738495410358878, ic: 0.09352074699761793
train 20, step: 1500, loss: 1.8440872355953293, grad_norm: 2.2900248076297425, ic: 0.26752005560319214
train 20, step: 2000, loss: 1.0465947162074327, grad_norm: 0.13161367699324622, ic: 0.009055630254718904
Epoch 20: 2022-05-07 20:09:07.043071: train loss: 1.6173601700058542
Eval step 0: eval loss: 0.8347460603060787
Eval: 2022-05-07 20:09:30.391725: total loss: 1.0668437361929333, mse:4.589422124915905, ic :0.18626974345754943, sharpe5:16.89301109075546, irr5:558.7070922851562, ndcg5:0.8390659886460237, pnl5:6.423463821411133 
train 21, step: 0, loss: 1.016647001334859, grad_norm: 0.4541107945575152, ic: 0.06544049244914109
train 21, step: 500, loss: 0.7658740018321349, grad_norm: 0.025906144547601737, ic: 0.21738964531774493
train 21, step: 1000, loss: 0.9379614445201138, grad_norm: 1.2858104170562399, ic: 0.15895655434448258
train 21, step: 1500, loss: 0.994251182561645, grad_norm: 0.31339228207792447, ic: 0.316286085133752
train 21, step: 2000, loss: 0.9442540165593853, grad_norm: 0.16558738773749843, ic: 0.06245959705246374
Epoch 21: 2022-05-07 20:14:06.437071: train loss: 1.618109624489531
Eval step 0: eval loss: 0.8247299917882309
Eval: 2022-05-07 20:14:29.518595: total loss: 1.0653228674457909, mse:4.595525076228591, ic :0.18731611328440848, sharpe5:17.842103762626646, irr5:595.6817016601562, ndcg5:0.8466257430777426, pnl5:7.552474021911621 
train 22, step: 0, loss: 1.045418345995542, grad_norm: 0.05746109262426252, ic: 0.2017346021729523
train 22, step: 500, loss: 3.261842209730691, grad_norm: 1.3368569349499009, ic: -0.2174464082071924
train 22, step: 1000, loss: 1.1904460576228324, grad_norm: 0.06428232521616199, ic: 0.46695413003155417
train 22, step: 1500, loss: 0.9768667213220165, grad_norm: 0.3015397773727517, ic: 0.08967136101743695
train 22, step: 2000, loss: 1.7433551952682116, grad_norm: 1.897036596389714, ic: 0.1815769194122654
Epoch 22: 2022-05-07 20:19:04.842630: train loss: 1.616436964158412
Eval step 0: eval loss: 0.8296410386755795
Eval: 2022-05-07 20:19:28.291845: total loss: 1.0676362502286432, mse:4.599810655859336, ic :0.18178675636479943, sharpe5:16.78736632347107, irr5:561.59228515625, ndcg5:0.8388813483408387, pnl5:5.312081336975098 
train 23, step: 0, loss: 0.971872326413905, grad_norm: 0.05551212633908263, ic: 0.21579855739525233
train 23, step: 500, loss: 1.4190157376802885, grad_norm: 0.19557847480768756, ic: 0.07267505125133607
train 23, step: 1000, loss: 1.6511069742838542, grad_norm: 0.09551492369696013, ic: 0.25955588052232653
train 23, step: 1500, loss: 1.1190223957334036, grad_norm: 1.1174443818290627, ic: 0.07873538127366751
train 23, step: 2000, loss: 1.9324683350173208, grad_norm: 1.5195833901058842, ic: 0.4454612924348055
Epoch 23: 2022-05-07 20:24:08.425894: train loss: 1.61662690831483
Eval step 0: eval loss: 0.831157527557791
Eval: 2022-05-07 20:24:33.307768: total loss: 1.066603141875837, mse:4.588280308767159, ic :0.18841495366576994, sharpe5:17.313936681747435, irr5:585.2537841796875, ndcg5:0.850568493976887, pnl5:8.937240600585938 
train 24, step: 0, loss: 2.193879134651424, grad_norm: 0.04690709192938211, ic: 0.14328384160625507
train 24, step: 500, loss: 1.2307083308000974, grad_norm: 0.5225460055444028, ic: 0.07247495584267974
train 24, step: 1000, loss: 0.9044695538012746, grad_norm: 0.1653161917062626, ic: 0.5341858758613098
train 24, step: 1500, loss: 2.620815594728978, grad_norm: 4.501890469066625, ic: 0.07080858533144885
train 24, step: 2000, loss: 0.940502710816524, grad_norm: 0.3904964388964271, ic: 0.09639761787149417
Epoch 24: 2022-05-07 20:29:09.927087: train loss: 1.612685697077227
Eval step 0: eval loss: 0.8269599295516662
Eval: 2022-05-07 20:29:33.489589: total loss: 1.0669429636051344, mse:4.617037338473416, ic :0.1849102702984841, sharpe5:17.083932952880858, irr5:581.0315551757812, ndcg5:0.866396282838628, pnl5:4.12284517288208 
train 25, step: 0, loss: 0.8265248891469595, grad_norm: 0.09151004656541786, ic: 0.6277830844096162
train 25, step: 500, loss: 0.8657443228084876, grad_norm: 0.01280001469850929, ic: 0.2750084000294145
train 25, step: 1000, loss: 2.148225899062262, grad_norm: 1.7061705724332579, ic: 0.22395555294319988
train 25, step: 1500, loss: 1.1357008917244589, grad_norm: 0.8217266211224754, ic: 0.5341126541091824
train 25, step: 2000, loss: 1.0259334609212352, grad_norm: 0.46859114405550173, ic: 0.5974992795253916
Epoch 25: 2022-05-07 20:34:16.174680: train loss: 1.6168114634960336
Eval step 0: eval loss: 0.8288911873723985
Eval: 2022-05-07 20:34:39.259783: total loss: 1.0673336850991635, mse:4.624512161088617, ic :0.19362869953405076, sharpe5:17.831857495307922, irr5:603.3289184570312, ndcg5:0.8343611147333531, pnl5:4.1185688972473145 
train 26, step: 0, loss: 6.711169940595048, grad_norm: 2.138995654731609, ic: 0.1385909970859171
train 26, step: 500, loss: 3.921707785623534, grad_norm: 5.891931767958842, ic: 0.3597005578090465
train 26, step: 1000, loss: 1.2621452295611755, grad_norm: 1.0469539069095877, ic: 0.04026380033422161
train 26, step: 1500, loss: 0.8315839236487708, grad_norm: 0.25863321187133376, ic: 0.30351003965500156
train 26, step: 2000, loss: 0.9623001542830448, grad_norm: 0.40344410290504307, ic: 0.13297974021371783
Epoch 26: 2022-05-07 20:39:13.854407: train loss: 1.6135261365002798
Eval step 0: eval loss: 0.8256651995933219
Eval: 2022-05-07 20:39:36.994092: total loss: 1.0655737823059093, mse:4.594260193916268, ic :0.19072243286541804, sharpe5:17.723280358314515, irr5:611.455078125, ndcg5:0.8514490228085573, pnl5:7.590433120727539 
train 27, step: 0, loss: 0.8290546492034313, grad_norm: 0.020622629451525855, ic: 0.12278300255947218
train 27, step: 500, loss: 0.9199250938926217, grad_norm: 2.400926309725781, ic: 0.2987478477151265
train 27, step: 1000, loss: 0.7608738874891242, grad_norm: 0.605437654673832, ic: 0.1957210399435157
train 27, step: 1500, loss: 0.6362066922691104, grad_norm: 0.06048372637222017, ic: 0.5130777372697627
train 27, step: 2000, loss: 1.3821128897351938, grad_norm: 0.08135753344840098, ic: 0.041399552018127464
Epoch 27: 2022-05-07 20:44:11.656284: train loss: 1.6131458780798609
Eval step 0: eval loss: 0.8380856287868809
Eval: 2022-05-07 20:44:34.721211: total loss: 1.0683731014956854, mse:4.601704920759492, ic :0.18405365025252568, sharpe5:17.155170618295667, irr5:579.36767578125, ndcg5:0.8421633667681382, pnl5:5.76418924331665 
train 28, step: 0, loss: 1.537740558638996, grad_norm: 0.572184409380379, ic: 0.21713009115049756
train 28, step: 500, loss: 1.370039400939771, grad_norm: 1.3808874312131858, ic: 0.17211419959037794
train 28, step: 1000, loss: 0.9303309329480014, grad_norm: 0.43806833008855156, ic: 0.5766880746729711
train 28, step: 1500, loss: 1.0392836872329059, grad_norm: 0.09525358952055168, ic: 0.017537249026119497
train 28, step: 2000, loss: 1.043930896220763, grad_norm: 0.16098407726771774, ic: 0.08854000461229844
Epoch 28: 2022-05-07 20:49:16.702658: train loss: 1.6097775617691485
Eval step 0: eval loss: 0.8277243160975368
Eval: 2022-05-07 20:49:39.967820: total loss: 1.0709093701697208, mse:4.636386369421773, ic :0.17953440154092398, sharpe5:17.49520253419876, irr5:573.2237548828125, ndcg5:0.8533327034328065, pnl5:10.019591331481934 
train 29, step: 0, loss: 0.9069950083514974, grad_norm: 0.04961540515699122, ic: 0.09329602790097118
train 29, step: 500, loss: 1.0995753988824868, grad_norm: 0.1362364054518988, ic: 0.6158539444497564
train 29, step: 1000, loss: 1.0821965086579048, grad_norm: 0.5200835088760198, ic: 0.0318332579386924
train 29, step: 1500, loss: 2.328588123902225, grad_norm: 0.3430445463531085, ic: -0.043854016644395394
train 29, step: 2000, loss: 4.032075104890046, grad_norm: 9.469144765727599, ic: 0.20127543877596435
Epoch 29: 2022-05-07 20:54:19.262540: train loss: 1.61072225473052
Eval step 0: eval loss: 0.836842029438883
Eval: 2022-05-07 20:54:43.167067: total loss: 1.067344631078921, mse:4.58971612689685, ic :0.18785033507785592, sharpe5:18.0552170419693, irr5:612.74853515625, ndcg5:0.8453127392632668, pnl5:7.603705883026123 
train 30, step: 0, loss: 1.0027727509023856, grad_norm: 0.03934745431447765, ic: 0.5185207162185305
train 30, step: 500, loss: 1.42153031910121, grad_norm: 1.4220875057102234, ic: 0.025181391279125824
train 30, step: 1000, loss: 0.9747277462121212, grad_norm: 0.08611101302071031, ic: -0.010787217890728035
train 30, step: 1500, loss: 1.522172467109668, grad_norm: 2.134913782954066, ic: 0.15053948240512696
train 30, step: 2000, loss: 1.840832640947781, grad_norm: 1.0567449624054746, ic: 0.05110317271927591
Epoch 30: 2022-05-07 20:59:29.184199: train loss: 1.6090319849196413
Eval step 0: eval loss: 0.8335485749802423
Eval: 2022-05-07 20:59:52.575046: total loss: 1.0669512628061777, mse:4.59921549803331, ic :0.1915054598271664, sharpe5:18.072029925584793, irr5:613.6052856445312, ndcg5:0.8579125987322234, pnl5:7.99223518371582 
train 31, step: 0, loss: 1.0374199032864548, grad_norm: 0.3259197234101626, ic: 0.34973444204113613
train 31, step: 500, loss: 1.5032560120884773, grad_norm: 1.2250175882560965, ic: 0.030095010115286588
train 31, step: 1000, loss: 4.360397303132318, grad_norm: 3.2166379291245644, ic: 0.4644918244566647
train 31, step: 1500, loss: 0.768930488515313, grad_norm: 0.07030210780141616, ic: 0.7095970032247585
train 31, step: 2000, loss: 1.2403070049807656, grad_norm: 2.73437808996486, ic: 0.19271439942405216
Epoch 31: 2022-05-07 21:04:34.489440: train loss: 1.610476780826016
Eval step 0: eval loss: 0.8380983632030427
Eval: 2022-05-07 21:04:58.279759: total loss: 1.0683032745028656, mse:4.600893888437431, ic :0.180322496129574, sharpe5:17.097081320285795, irr5:561.3986206054688, ndcg5:0.8401294911089519, pnl5:4.758907794952393 
train 32, step: 0, loss: 1.127289365439887, grad_norm: 0.08267838085693022, ic: 0.1781802836623005
train 32, step: 500, loss: 1.489074995386319, grad_norm: 1.824338634251599, ic: 0.03289077240267618
train 32, step: 1000, loss: 1.063100694578569, grad_norm: 0.4342816572040149, ic: 0.5099109774883659
train 32, step: 1500, loss: 0.9677769757699276, grad_norm: 1.6704366204035568, ic: 0.07469737823982837
train 32, step: 2000, loss: 0.9423040428844078, grad_norm: 0.05562235772725372, ic: 0.561556142979893
Epoch 32: 2022-05-07 21:09:36.790399: train loss: 1.6078642458733345
Eval step 0: eval loss: 0.8265510776145942
Eval: 2022-05-07 21:10:00.087586: total loss: 1.0644353635301242, mse:4.5872524275364555, ic :0.19681717626977904, sharpe5:18.361025365591047, irr5:627.97265625, ndcg5:0.8403364683478801, pnl5:7.367061138153076 
train 33, step: 0, loss: 1.2611540979065425, grad_norm: 0.43990084181990924, ic: 0.20073973041287563
train 33, step: 500, loss: 0.9934768385337089, grad_norm: 0.022366121917818414, ic: 0.1284277684958255
train 33, step: 1000, loss: 1.029048155102827, grad_norm: 3.572415743803857, ic: 0.2510506154454584
train 33, step: 1500, loss: 0.9018929007497406, grad_norm: 0.24258155428209177, ic: 0.5674514648645987
train 33, step: 2000, loss: 0.8138122074955428, grad_norm: 0.2893759229383682, ic: 0.2300203962403049
Epoch 33: 2022-05-07 21:14:45.392502: train loss: 1.6096711795728578
Eval step 0: eval loss: 0.8319659057231296
Eval: 2022-05-07 21:15:08.380956: total loss: 1.0650242263450367, mse:4.5797846480160835, ic :0.196689830843034, sharpe5:17.788413019180297, irr5:615.6663208007812, ndcg5:0.8613812011150072, pnl5:7.314604759216309 
train 34, step: 0, loss: 0.991106826178652, grad_norm: 0.5349232155343178, ic: 0.6076675470378078
train 34, step: 500, loss: 0.8169105833094419, grad_norm: 0.6295602730085111, ic: 0.19156058987380303
train 34, step: 1000, loss: 3.0805120277697773, grad_norm: 2.6298346625839137, ic: 0.3239895062981014
train 34, step: 1500, loss: 0.8050518195023838, grad_norm: 1.0035288310520938, ic: 0.6768803211163963
train 34, step: 2000, loss: 5.182850927091402, grad_norm: 88.9126408885724, ic: 0.42331230433007716
Epoch 34: 2022-05-07 21:19:50.138977: train loss: 1.60703570851287
Eval step 0: eval loss: 0.8262099496180189
Eval: 2022-05-07 21:20:13.806489: total loss: 1.0736143384758376, mse:4.641207474798167, ic :0.18108728147508327, sharpe5:17.88219587802887, irr5:600.6163330078125, ndcg5:0.8479236472246175, pnl5:9.533134460449219 
train 35, step: 0, loss: 1.1759191176470587, grad_norm: 0.9781707732448424, ic: 0.5546255290494069
train 35, step: 500, loss: 1.1824706632001227, grad_norm: 0.624590972116375, ic: 0.14187519839201507
train 35, step: 1000, loss: 1.5616696108678343, grad_norm: 3.92976024068682, ic: 0.09083819221283339
train 35, step: 1500, loss: 1.6158186604205829, grad_norm: 1.7053029018545252, ic: 0.04782506838629451
train 35, step: 2000, loss: 0.7924492983894552, grad_norm: 0.2401036683391866, ic: 0.5608957570677131
Epoch 35: 2022-05-07 21:24:50.600805: train loss: 1.6058390123667698
Eval step 0: eval loss: 0.8380676205216017
Eval: 2022-05-07 21:25:13.950094: total loss: 1.0675316622047137, mse:4.597171129295434, ic :0.19123584591450568, sharpe5:17.94061057448387, irr5:607.8704223632812, ndcg5:0.847014055227196, pnl5:7.884955406188965 
train 36, step: 0, loss: 1.8323255985086344, grad_norm: 1.5478802212221288, ic: 0.11030021260009859
train 36, step: 500, loss: 0.8221790503102837, grad_norm: 0.06472425706058772, ic: 0.24593305234227825
train 36, step: 1000, loss: 1.7235706676136362, grad_norm: 5.40062333248929, ic: 0.27929242629307294
train 36, step: 1500, loss: 0.7704145346003898, grad_norm: 0.19024365497933188, ic: 0.39314908810871074
train 36, step: 2000, loss: 1.1145618732625098, grad_norm: 0.6901521612173602, ic: 0.7714377403890091
Epoch 36: 2022-05-07 21:29:57.947114: train loss: 1.6035081837377552
Eval step 0: eval loss: 0.8417763584406283
Eval: 2022-05-07 21:30:21.172052: total loss: 1.0709479358270715, mse:4.621287591802029, ic :0.1765457280259077, sharpe5:17.168826001882554, irr5:574.5731201171875, ndcg5:0.848847405105789, pnl5:4.969939708709717 
train 37, step: 0, loss: 1.9719049474181662, grad_norm: 6.371025457441558, ic: 0.20690917561482652
train 37, step: 500, loss: 2.3324287252752858, grad_norm: 3.4809549519292755, ic: -0.07650967094627267
train 37, step: 1000, loss: 1.0765973559979072, grad_norm: 0.1586068588986329, ic: 0.06898325185375868
train 37, step: 1500, loss: 2.00941781084429, grad_norm: 2.9300073106902778, ic: 0.6047883503018252
train 37, step: 2000, loss: 1.3122367985620846, grad_norm: 0.1349389652232441, ic: 0.20401827370046496
Epoch 37: 2022-05-07 21:35:01.524352: train loss: 1.6074418303369358
Eval step 0: eval loss: 0.8277060505713251
Eval: 2022-05-07 21:35:25.064855: total loss: 1.0687471374107713, mse:4.60781269905981, ic :0.18883247430304803, sharpe5:17.93396639227867, irr5:605.4586791992188, ndcg5:0.8529088743027249, pnl5:12.19652271270752 
train 38, step: 0, loss: 1.3308855382407585, grad_norm: 0.3996952366035729, ic: -0.057569478241723634
train 38, step: 500, loss: 0.895662208839699, grad_norm: 0.0920838251469295, ic: 0.28125512056894886
train 38, step: 1000, loss: 0.9008177263463438, grad_norm: 0.21249441429699065, ic: 0.16535486068654676
train 38, step: 1500, loss: 0.9479437316106682, grad_norm: 0.03628783086987515, ic: 0.22496576879484048
train 38, step: 2000, loss: 2.339498191441654, grad_norm: 8.1476213113998, ic: -0.016920219708242316
Epoch 38: 2022-05-07 21:40:11.914551: train loss: 1.5997207544967234
Eval step 0: eval loss: 0.8271376325408324
Eval: 2022-05-07 21:40:33.201550: total loss: 1.0645532714177826, mse:4.587937853464008, ic :0.19602502754809975, sharpe5:18.583446553945542, irr5:627.1185913085938, ndcg5:0.8487236896153038, pnl5:8.466994285583496 
train 39, step: 0, loss: 0.9666815241542431, grad_norm: 0.029468728688988786, ic: 0.08097943523122052
train 39, step: 500, loss: 0.8935057092500961, grad_norm: 0.3704087083495853, ic: 0.2069408460159066
train 39, step: 1000, loss: 0.9382022994216489, grad_norm: 0.18445453320484584, ic: 0.1970675048096545
train 39, step: 1500, loss: 2.068427471275753, grad_norm: 0.6291188894150943, ic: 0.2165998188425843
train 39, step: 2000, loss: 0.614383064740077, grad_norm: 0.10932813787683271, ic: 0.16813780756182226
Epoch 39: 2022-05-07 21:44:56.321656: train loss: 1.605402459134888
Eval step 0: eval loss: 0.8301839235585154
Eval: 2022-05-07 21:45:18.669340: total loss: 1.0646055318724692, mse:4.578691635741407, ic :0.19537935390664385, sharpe5:17.999581475257873, irr5:615.7859497070312, ndcg5:0.8411870068721217, pnl5:8.715372085571289 
