Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0001, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_5_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
96732
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795790397160868, grad_norm: 4.816657405557322, ic: 0.022771143722705188
train 0, step: 500, loss: 0.8600123045403165, grad_norm: 0.035055915125698445, ic: 0.045913676801191905
train 0, step: 1000, loss: 1.9659930147563096, grad_norm: 0.8908162407792538, ic: 0.033381329883683496
train 0, step: 1500, loss: 0.954252292798913, grad_norm: 0.06425122487112228, ic: 0.03365166567466053
train 0, step: 2000, loss: 1.0008428036694563, grad_norm: 0.22375307443902973, ic: 0.06340292786923149
Epoch 0: 2022-05-04 21:17:57.312388: train loss: 1.6482328010840535
Eval step 0: eval loss: 0.8358459151080083
Eval: 2022-05-04 21:18:39.554602: total loss: 1.0793870466561113, mse:4.822891279172813, ic :0.007573864601360181, sharpe5:7.910417755544185, irr5:224.5764923095703, ndcg5:0.8500916347724747, pnl5:2.4314334392547607 
train 1, step: 0, loss: 2.786832944808468, grad_norm: 1.1577531970525978, ic: 0.062352490261973516
train 1, step: 500, loss: 1.761818157793817, grad_norm: 1.0871953135460775, ic: 0.09107680789294637
train 1, step: 1000, loss: 0.8790239749837417, grad_norm: 0.22877144597193955, ic: 0.08384653789784785
train 1, step: 1500, loss: 1.713977471713362, grad_norm: 0.2637806919504208, ic: -0.03780153559817778
train 1, step: 2000, loss: 2.177330859375, grad_norm: 1.1672655130176937, ic: -0.03885959238373114
Epoch 1: 2022-05-04 21:30:45.718478: train loss: 1.646458421163478
Eval step 0: eval loss: 0.8340010969606164
Eval: 2022-05-04 21:31:31.970247: total loss: 1.0793719713758996, mse:4.825518686307799, ic :0.0076439263160003375, sharpe5:7.775232201218604, irr5:221.5745086669922, ndcg5:0.8574858511670844, pnl5:2.479982852935791 
train 2, step: 0, loss: 2.1420846946022727, grad_norm: 0.01636802562047137, ic: 0.13357936917792884
train 2, step: 500, loss: 3.309969190140845, grad_norm: 0.408421302576961, ic: 0.07945657594144054
train 2, step: 1000, loss: 2.070692648467433, grad_norm: 0.0009092021785530545, ic: 0.19164878609721048
train 2, step: 1500, loss: 1.4905044206226146, grad_norm: 0.08071570971081266, ic: -0.0378537215362343
train 2, step: 2000, loss: 3.250232872596154, grad_norm: 1.1014102246401591, ic: 0.17110553600426487
Epoch 2: 2022-05-04 21:44:00.120779: train loss: 1.6463426143077542
Eval step 0: eval loss: 0.8354909593363079
Eval: 2022-05-04 21:44:45.077808: total loss: 1.0798379703576928, mse:4.8248264661315545, ic :0.00968100049093991, sharpe5:7.752195126414299, irr5:221.03480529785156, ndcg5:0.8384933182929217, pnl5:2.812927484512329 
train 3, step: 0, loss: 1.5227310800940042, grad_norm: 0.6970979597566125, ic: -0.001664349051480038
train 3, step: 500, loss: 1.5106161979370596, grad_norm: 0.44519863552263705, ic: 0.10238888305439574
train 3, step: 1000, loss: 3.6791058915335353, grad_norm: 0.9221078732170319, ic: -0.05715280078367399
train 3, step: 1500, loss: 1.971907445529869, grad_norm: 1.3512128797814462, ic: -0.055750134845435353
train 3, step: 2000, loss: 0.8979484269425676, grad_norm: 0.0013470271490456823, ic: 0.01888937230157557
Epoch 3: 2022-05-04 21:56:50.912200: train loss: 1.6462350031416382
Eval step 0: eval loss: 0.8341081175085616
Eval: 2022-05-04 21:57:33.383278: total loss: 1.0798041246109569, mse:4.827580193702405, ic :0.009195885168383041, sharpe5:7.606231221556663, irr5:216.3682403564453, ndcg5:0.8336532067332155, pnl5:2.2997612953186035 
train 4, step: 0, loss: 1.4308636599170919, grad_norm: 0.0567143327262499, ic: 0.14029222438683528
train 4, step: 500, loss: 1.6596364419291338, grad_norm: 0.7516720213644212, ic: 0.025287986318801404
train 4, step: 1000, loss: 2.9694018480254383, grad_norm: 0.8905698708677441, ic: 0.015887818282694855
train 4, step: 1500, loss: 2.1558694702663503, grad_norm: 0.6154992101426705, ic: -0.029770179749526393
train 4, step: 2000, loss: 1.0829864549414365, grad_norm: 0.4765159061916649, ic: 0.15018283418823003
Epoch 4: 2022-05-04 22:09:06.510543: train loss: 1.6457481656440762
Eval step 0: eval loss: 0.8429153812195402
Eval: 2022-05-04 22:09:51.960340: total loss: 1.08223495748861, mse:4.8251044223544355, ic :0.017408008403553644, sharpe5:8.16020126402378, irr5:232.02859497070312, ndcg5:0.8407843708538677, pnl5:2.5634334087371826 
train 5, step: 0, loss: 1.3374962313863255, grad_norm: 0.12328640649003067, ic: 0.05698795272332803
train 5, step: 500, loss: 0.8885610339945975, grad_norm: 0.007651773849735178, ic: 0.03748008148392085
train 5, step: 1000, loss: 0.9798107863386015, grad_norm: 0.18298059419370202, ic: -0.030972054638967222
train 5, step: 1500, loss: 1.535993263513352, grad_norm: 0.19348892650418842, ic: 0.026938212347177425
train 5, step: 2000, loss: 1.107150544699826, grad_norm: 0.033400700391508865, ic: 0.09730162880741254
Epoch 5: 2022-05-04 22:21:43.280130: train loss: 1.6452957007910638
Eval step 0: eval loss: 0.8396102857037012
Eval: 2022-05-04 22:22:33.150615: total loss: 1.0808462360801103, mse:4.829178086979645, ic :0.03688643456097207, sharpe5:9.737535227537155, irr5:274.3453063964844, ndcg5:0.8422436142120724, pnl5:3.185555934906006 
train 6, step: 0, loss: 1.3425501286508175, grad_norm: 0.5222635129619126, ic: 0.0397395402959745
train 6, step: 500, loss: 1.0093432392862272, grad_norm: 0.056924112297982, ic: 0.042334733554737054
train 6, step: 1000, loss: 1.1200028220057034, grad_norm: 0.1106182831506109, ic: 0.10196970057995296
train 6, step: 1500, loss: 1.563913190857438, grad_norm: 0.8545362434981087, ic: 0.17384010045685366
train 6, step: 2000, loss: 0.8108083146204463, grad_norm: 0.05859645435918497, ic: 0.026680560970138725
Epoch 6: 2022-05-04 22:34:31.706243: train loss: 1.6438314335961908
Eval step 0: eval loss: 0.8359236079096417
Eval: 2022-05-04 22:35:18.034446: total loss: 1.078244349012133, mse:4.824183171745406, ic :0.044144003416429205, sharpe5:11.208145122528075, irr5:336.939208984375, ndcg5:0.8437996181611981, pnl5:2.7933456897735596 
train 7, step: 0, loss: 0.9928040504455566, grad_norm: 0.06433964382679061, ic: 0.07628037111802499
train 7, step: 500, loss: 0.6476296653863507, grad_norm: 0.002162343461973365, ic: 0.07303419299906283
train 7, step: 1000, loss: 1.0356079008591965, grad_norm: 0.2503618525515342, ic: 0.08922427616598046
train 7, step: 1500, loss: 2.268066929595391, grad_norm: 0.7924861013283451, ic: 0.3024769906591187
train 7, step: 2000, loss: 0.9150568110107028, grad_norm: 0.05725013029921788, ic: -0.06598639330145513
Epoch 7: 2022-05-04 22:48:01.715560: train loss: 1.6417932105796282
Eval step 0: eval loss: 0.8350140618825737
Eval: 2022-05-04 22:48:45.085985: total loss: 1.0750547228672027, mse:4.745984034180881, ic :0.1277265009186278, sharpe5:12.577441644072533, irr5:418.7828674316406, ndcg5:0.8491356503217572, pnl5:2.7649619579315186 
train 8, step: 0, loss: 3.6328592051630437, grad_norm: 1.3854560945445205, ic: -0.025860859729405935
train 8, step: 500, loss: 2.752170551149316, grad_norm: 1.1798621783116072, ic: 0.030860575304923202
train 8, step: 1000, loss: 3.107848236526268, grad_norm: 1.3129391493090998, ic: 0.07236188250731236
train 8, step: 1500, loss: 0.7243560527608662, grad_norm: 0.004093799038444567, ic: 0.4053924652917145
train 8, step: 2000, loss: 1.0841817169568082, grad_norm: 0.43194644192238874, ic: 0.5157346929478486
Epoch 8: 2022-05-04 23:00:24.778194: train loss: 1.6358605881885284
Eval step 0: eval loss: 0.8283838044981559
Eval: 2022-05-04 23:01:07.684156: total loss: 1.0737969547496324, mse:4.7405336076075635, ic :0.13525534205396056, sharpe5:11.885911300778389, irr5:405.3528747558594, ndcg5:0.8478880483867931, pnl5:3.435990571975708 
train 9, step: 0, loss: 5.426952501993621, grad_norm: 1.0732906876228767, ic: 0.1446944125110887
train 9, step: 500, loss: 1.367074357591422, grad_norm: 1.2775813807540777, ic: 0.3045464392171716
train 9, step: 1000, loss: 0.9288376180213465, grad_norm: 0.023693592883698933, ic: 0.035800070636858326
train 9, step: 1500, loss: 1.0993738637932622, grad_norm: 0.026834203248150203, ic: 0.4151546164607422
train 9, step: 2000, loss: 1.0838326743682387, grad_norm: 0.28201128431854416, ic: 0.1801313294160691
Epoch 9: 2022-05-04 23:12:45.129560: train loss: 1.6339139718433542
Eval step 0: eval loss: 0.8279342410185062
Eval: 2022-05-04 23:13:32.441769: total loss: 1.0722123229759304, mse:4.72023279336367, ic :0.14260910424737602, sharpe5:11.771733337640761, irr5:403.03631591796875, ndcg5:0.850880374172626, pnl5:3.7351772785186768 
train 10, step: 0, loss: 7.106880922011662, grad_norm: 1.5746595785789257, ic: 0.1726643770938328
train 10, step: 500, loss: 1.1392095976010597, grad_norm: 0.08190255514420491, ic: -0.03821355431766274
train 10, step: 1000, loss: 2.372245367319306, grad_norm: 0.8903433451540492, ic: -0.043424447376752016
train 10, step: 1500, loss: 1.1051794275060878, grad_norm: 0.3364584939424554, ic: -0.014395911905220498
train 10, step: 2000, loss: 2.7910805560172873, grad_norm: 0.36089801254562287, ic: 0.43884156378535133
Epoch 10: 2022-05-04 23:25:02.051242: train loss: 1.634073859767559
Eval step 0: eval loss: 0.8299491086423209
Eval: 2022-05-04 23:25:51.021882: total loss: 1.0716176779362079, mse:4.71861760259921, ic :0.14242621611858813, sharpe5:11.395566113591194, irr5:394.3967590332031, ndcg5:0.8558476514513084, pnl5:3.6131908893585205 
train 11, step: 0, loss: 1.26934413216599, grad_norm: 0.0266236389597135, ic: 0.14234670094563107
train 11, step: 500, loss: 0.6956401668763011, grad_norm: 0.015134494989622896, ic: 0.45543575745851067
train 11, step: 1000, loss: 0.9435542044517725, grad_norm: 0.17867282813805047, ic: 0.059517811187172066
train 11, step: 1500, loss: 1.0540808962102521, grad_norm: 0.06604446990031788, ic: 0.17308121794304696
train 11, step: 2000, loss: 0.7851700865638576, grad_norm: 0.0034151799987480586, ic: 0.1318111003549273
Epoch 11: 2022-05-04 23:38:37.458986: train loss: 1.6334506112938147
Eval step 0: eval loss: 0.8314786535168598
Eval: 2022-05-04 23:39:22.366269: total loss: 1.0721473828031574, mse:4.704478630501281, ic :0.14868926375777441, sharpe5:11.950221952199936, irr5:404.6275939941406, ndcg5:0.8506706530538964, pnl5:2.9452481269836426 
train 12, step: 0, loss: 0.986548105875651, grad_norm: 0.09205424862479537, ic: 0.2941731090169858
train 12, step: 500, loss: 0.9415073254184583, grad_norm: 0.08454016910756888, ic: 0.06275170765746496
train 12, step: 1000, loss: 2.912377813059813, grad_norm: 0.8044497738534555, ic: 0.41148672394331115
train 12, step: 1500, loss: 0.9281116494311691, grad_norm: 0.1524530731718492, ic: -0.10031974656655261
train 12, step: 2000, loss: 0.8726381607286159, grad_norm: 0.008204591217995624, ic: 0.21888799822569877
Epoch 12: 2022-05-04 23:51:00.477484: train loss: 1.6324137704660946
Eval step 0: eval loss: 0.8298909676715621
Eval: 2022-05-04 23:51:50.151867: total loss: 1.0712313770238306, mse:4.703097119144849, ic :0.15134811977404936, sharpe5:12.39998125076294, irr5:403.51531982421875, ndcg5:0.8402774444909425, pnl5:4.069236755371094 
train 13, step: 0, loss: 2.0844546099524814, grad_norm: 0.8877093902849204, ic: 0.33221965858504166
train 13, step: 500, loss: 0.8298019263461375, grad_norm: 0.06476784867744856, ic: 0.5001654592131571
train 13, step: 1000, loss: 0.992006934249161, grad_norm: 0.5245370391453978, ic: 0.46692110693383315
train 13, step: 1500, loss: 2.420373182572209, grad_norm: 0.5170291202341912, ic: -0.02072473813309118
train 13, step: 2000, loss: 1.4954603751683986, grad_norm: 0.12126620276301653, ic: 0.12923734215989374
Epoch 13: 2022-05-05 00:04:09.858358: train loss: 1.6320056469251787
Eval step 0: eval loss: 0.8269921514834694
Eval: 2022-05-05 00:04:58.119562: total loss: 1.072190625629979, mse:4.730068064783929, ic :0.14182955319413254, sharpe5:11.735155336260796, irr5:394.2872009277344, ndcg5:0.8567625378601426, pnl5:3.503614664077759 
train 14, step: 0, loss: 4.478619603768526, grad_norm: 2.902746938714149, ic: 0.17999784647516248
train 14, step: 500, loss: 0.8256774342388188, grad_norm: 0.007082367839906267, ic: 0.12088851215931255
train 14, step: 1000, loss: 1.8720376862661352, grad_norm: 0.22812845563505385, ic: 0.35148259532112747
train 14, step: 1500, loss: 1.1315165808661205, grad_norm: 0.08984411270980264, ic: -0.07077570344067008
train 14, step: 2000, loss: 1.161302889582729, grad_norm: 0.34693658952877904, ic: 0.07711501437548443
Epoch 14: 2022-05-05 00:16:41.172793: train loss: 1.6309275334728857
Eval step 0: eval loss: 0.8319848144016727
Eval: 2022-05-05 00:17:26.797844: total loss: 1.071697779236672, mse:4.700160370394084, ic :0.15018606158278522, sharpe5:13.272935416102408, irr5:435.9472961425781, ndcg5:0.8428481618501318, pnl5:2.8085458278656006 
train 15, step: 0, loss: 3.49291593202821, grad_norm: 3.0781837524884255, ic: 0.047484081747756236
train 15, step: 500, loss: 1.2601520287822112, grad_norm: 0.017368650777667263, ic: -0.022117797325966835
train 15, step: 1000, loss: 1.3156102126206808, grad_norm: 0.2757747637345599, ic: -0.0654385249578851
train 15, step: 1500, loss: 0.854733444574311, grad_norm: 0.20708476460262598, ic: 0.01358365661642915
train 15, step: 2000, loss: 1.4601101345486112, grad_norm: 0.9024577125253707, ic: 0.05529245770159319
Epoch 15: 2022-05-05 00:29:33.098928: train loss: 1.6300996518500395
Eval step 0: eval loss: 0.8393235683943624
Eval: 2022-05-05 00:30:20.633142: total loss: 1.0765725843815526, mse:4.707368041991241, ic :0.1454460929155988, sharpe5:13.669949896931648, irr5:446.503173828125, ndcg5:0.8570039572938016, pnl5:3.2700564861297607 
train 16, step: 0, loss: 0.6786686526530334, grad_norm: 0.39383644033488396, ic: -0.056676052043010766
train 16, step: 500, loss: 1.5985283839002118, grad_norm: 0.6650899107689174, ic: 0.18745986077952426
train 16, step: 1000, loss: 0.8795322302616003, grad_norm: 0.00850703941967506, ic: -0.06616501445800207
train 16, step: 1500, loss: 0.8272933058141253, grad_norm: 0.24876582578220122, ic: 0.11604783612698388
train 16, step: 2000, loss: 3.342554497187242, grad_norm: 1.6838432301086077, ic: 0.0007529657178627176
Epoch 16: 2022-05-05 00:42:23.723311: train loss: 1.6304498063369643
Eval step 0: eval loss: 0.8271376325408324
Eval: 2022-05-05 00:43:11.650025: total loss: 1.0732106224529938, mse:4.7383599084698105, ic :0.13713280028194844, sharpe5:17.090876047611236, irr5:557.6221923828125, ndcg5:0.8557359587795208, pnl5:12.101554870605469 
train 17, step: 0, loss: 1.2833041487068964, grad_norm: 0.44365140357543154, ic: -0.11249832062740255
train 17, step: 500, loss: 1.7704932566903795, grad_norm: 0.821708524735318, ic: 0.2194955941458882
train 17, step: 1000, loss: 1.2704886540628315, grad_norm: 0.23820893462051354, ic: 0.13672952886278106
train 17, step: 1500, loss: 4.494894752250183, grad_norm: 1.7118430524162767, ic: 0.2107149065518049
train 17, step: 2000, loss: 1.309417263325378, grad_norm: 1.2114614697573587, ic: 0.09471672448678252
Epoch 17: 2022-05-05 00:55:22.676176: train loss: 1.6263109618211722
Eval step 0: eval loss: 0.8327189084727344
Eval: 2022-05-05 00:56:07.576931: total loss: 1.0715762793857344, mse:4.670131204261604, ic :0.17306469735858507, sharpe5:16.969054058790206, irr5:555.1748657226562, ndcg5:0.8487348043427372, pnl5:11.971017837524414 
train 18, step: 0, loss: 1.434507308467742, grad_norm: 0.9354041143339786, ic: 0.06130209300989362
train 18, step: 500, loss: 1.4633929950754136, grad_norm: 1.5920568830647432, ic: 0.022147974273561345
train 18, step: 1000, loss: 0.6675398651541096, grad_norm: 0.005508496563087663, ic: 0.5210361495194044
train 18, step: 1500, loss: 1.418524014959562, grad_norm: 0.039792742371447254, ic: 0.21083304515603848
train 18, step: 2000, loss: 0.9097381397417397, grad_norm: 0.006267459427657404, ic: -0.03206861008741984
Epoch 18: 2022-05-05 01:07:44.869166: train loss: 1.6235064999190327
Eval step 0: eval loss: 0.8201273364437894
Eval: 2022-05-05 01:08:30.823112: total loss: 1.066928519616515, mse:4.671276937814084, ic :0.17743898908594477, sharpe5:17.682815548181534, irr5:573.4818725585938, ndcg5:0.8518706660877957, pnl5:22.621654510498047 
train 19, step: 0, loss: 1.5258971199156746, grad_norm: 1.257149003543934, ic: 0.05396198972140142
train 19, step: 500, loss: 0.8537293186894169, grad_norm: 0.019208121350324237, ic: 0.2367646254707987
train 19, step: 1000, loss: 0.9625671433062832, grad_norm: 0.2533053374968419, ic: 0.19934865122215176
train 19, step: 1500, loss: 3.960505836041813, grad_norm: 1.7173789847127119, ic: 0.1313036981870565
train 19, step: 2000, loss: 1.0228814227764422, grad_norm: 0.24722264486735115, ic: 0.1861709771589036
Epoch 19: 2022-05-05 01:19:47.584404: train loss: 1.6233600576577083
Eval step 0: eval loss: 0.8257928653311051
Eval: 2022-05-05 01:20:31.742148: total loss: 1.0697889510926264, mse:4.662696975884341, ic :0.17612767070681865, sharpe5:17.293461173772812, irr5:573.43505859375, ndcg5:0.8460346594184568, pnl5:11.426722526550293 
train 20, step: 0, loss: 2.2852712759387352, grad_norm: 1.6734289329378145, ic: 0.06817178450719072
train 20, step: 500, loss: 3.2456718749999998, grad_norm: 1.2683606257766478, ic: 0.08984780622371744
train 20, step: 1000, loss: 0.9709295272827149, grad_norm: 0.12936697191964613, ic: 0.1824073875677712
train 20, step: 1500, loss: 1.754926368982102, grad_norm: 4.993277645969, ic: 0.2279713067300369
train 20, step: 2000, loss: 1.0128155905893148, grad_norm: 0.10007710215120484, ic: 0.1262609533091502
Epoch 20: 2022-05-05 01:32:11.689048: train loss: 1.6216110083531667
Eval step 0: eval loss: 0.8351644952128885
Eval: 2022-05-05 01:32:55.533420: total loss: 1.0715125671953505, mse:4.659613405536604, ic :0.18093471770984304, sharpe5:17.713081387281417, irr5:583.47705078125, ndcg5:0.848043770977538, pnl5:11.53993034362793 
train 21, step: 0, loss: 0.9953590934282037, grad_norm: 0.46354107730232774, ic: 0.05886045397034875
train 21, step: 500, loss: 0.763613675547912, grad_norm: 0.09275983190781582, ic: 0.2124627572878488
train 21, step: 1000, loss: 0.9393853973924068, grad_norm: 2.7361188815353867, ic: 0.1752869694891762
train 21, step: 1500, loss: 0.9957415125539957, grad_norm: 0.4926068364631129, ic: 0.30249803423716726
train 21, step: 2000, loss: 0.9435301112178156, grad_norm: 0.3179474503410446, ic: 0.05481832254235615
Epoch 21: 2022-05-05 01:44:04.736434: train loss: 1.6215314290751721
Eval step 0: eval loss: 0.8209959136673471
Eval: 2022-05-05 01:44:48.049176: total loss: 1.0700252002829123, mse:4.658878367048412, ic :0.17469665573492651, sharpe5:17.493910399675368, irr5:553.6544189453125, ndcg5:0.8648511037090915, pnl5:12.76937484741211 
train 22, step: 0, loss: 1.0401345807953744, grad_norm: 0.14600954140994393, ic: 0.2226033487778935
train 22, step: 500, loss: 3.2650960286458335, grad_norm: 2.425396925385142, ic: -0.2064641087356417
train 22, step: 1000, loss: 1.200578034682081, grad_norm: 0.024338133385174075, ic: 0.453691252832823
train 22, step: 1500, loss: 0.9768912358539095, grad_norm: 0.2685828615786391, ic: 0.10885024064863469
train 22, step: 2000, loss: 1.7652658475499576, grad_norm: 3.0096930080921114, ic: 0.18878781174775047
Epoch 22: 2022-05-05 01:57:19.834579: train loss: 1.6218280727520822
Eval step 0: eval loss: 0.8206124662473656
Eval: 2022-05-05 01:58:01.910328: total loss: 1.0680726641826372, mse:4.644582880996357, ic :0.18137855037755798, sharpe5:17.495170372724534, irr5:579.69091796875, ndcg5:0.8384094612698038, pnl5:5.20938777923584 
train 23, step: 0, loss: 0.9709314351810159, grad_norm: 0.08273262811512434, ic: 0.21310004524141773
train 23, step: 500, loss: 1.4275709119284734, grad_norm: 0.2494830972173294, ic: 0.07274356612001837
train 23, step: 1000, loss: 1.6438659667968751, grad_norm: 0.1265303306733893, ic: 0.24211163334498764
train 23, step: 1500, loss: 1.145306017063987, grad_norm: 1.750862711852264, ic: 0.08767765184114171
train 23, step: 2000, loss: 1.8213787122907044, grad_norm: 7.841746831628117, ic: 0.4254934371150534
Epoch 23: 2022-05-05 02:10:03.442366: train loss: 1.6196668416713578
Eval step 0: eval loss: 0.8210648595972734
Eval: 2022-05-05 02:10:47.879644: total loss: 1.066843418285885, mse:4.623651151530092, ic :0.18490364514435495, sharpe5:17.446965998411176, irr5:575.1446533203125, ndcg5:0.8592265321633158, pnl5:17.006601333618164 
train 24, step: 0, loss: 2.2035067671242503, grad_norm: 0.29417569649354, ic: 0.15112163466366402
train 24, step: 500, loss: 1.2112180242278698, grad_norm: 0.3071155460024354, ic: 0.1294102585905719
train 24, step: 1000, loss: 0.9229553346292624, grad_norm: 0.04492834029499407, ic: 0.4633304987842072
train 24, step: 1500, loss: 2.6089177143446003, grad_norm: 5.757669175644289, ic: 0.04217072332716036
train 24, step: 2000, loss: 0.9305712413088325, grad_norm: 0.26476386246422484, ic: 0.10076606319785983
Epoch 24: 2022-05-05 02:22:21.381476: train loss: 1.6150878299859606
Eval step 0: eval loss: 0.820631953763007
Eval: 2022-05-05 02:23:07.813073: total loss: 1.0700727032596375, mse:4.636737762488606, ic :0.18099406470323356, sharpe5:17.37584941148758, irr5:553.6663208007812, ndcg5:0.8419555080621443, pnl5:5.948734283447266 
train 25, step: 0, loss: 0.8393597577069257, grad_norm: 0.09841149073057628, ic: 0.611049989061337
train 25, step: 500, loss: 0.8719283273933157, grad_norm: 0.012504791093930781, ic: 0.22260570702898483
train 25, step: 1000, loss: 2.0817422975771134, grad_norm: 0.11627677408445618, ic: 0.26434917121010676
train 25, step: 1500, loss: 1.1302091317183272, grad_norm: 0.5975946560830483, ic: 0.5289576507516756
train 25, step: 2000, loss: 1.0074082532746356, grad_norm: 0.47451671178422794, ic: 0.5913409306180448
Epoch 25: 2022-05-05 02:35:20.091472: train loss: 1.616692329635122
Eval step 0: eval loss: 0.8213142740713909
Eval: 2022-05-05 02:36:04.243784: total loss: 1.0658945922606762, mse:4.5957955657488805, ic :0.1928052626932111, sharpe5:17.624423553943632, irr5:592.41796875, ndcg5:0.8472708053121337, pnl5:9.91657543182373 
train 26, step: 0, loss: 6.666099865215655, grad_norm: 10.106227424648235, ic: 0.16181992285172658
train 26, step: 500, loss: 3.9672523242034794, grad_norm: 9.553053788927617, ic: 0.3725332353377785
train 26, step: 1000, loss: 1.2991581172733517, grad_norm: 2.257815818811851, ic: 0.012373529891718362
train 26, step: 1500, loss: 0.8413345436212039, grad_norm: 0.39205602082317814, ic: 0.3180852301417194
train 26, step: 2000, loss: 0.9681343660872345, grad_norm: 1.0766854880473185, ic: 0.14783288433097605
Epoch 26: 2022-05-05 02:44:50.267404: train loss: 1.6163876820129865
Eval step 0: eval loss: 0.8195485636607612
Eval: 2022-05-05 02:45:22.843207: total loss: 1.0656335697107662, mse:4.596880878501857, ic :0.18897267172003926, sharpe5:17.583194435834884, irr5:583.5380859375, ndcg5:0.846816696353113, pnl5:5.914862155914307 
train 27, step: 0, loss: 0.830973881740196, grad_norm: 0.03843538638897663, ic: 0.09353805681717234
train 27, step: 500, loss: 0.8951500202047413, grad_norm: 3.673088114153683, ic: 0.2924353953506678
train 27, step: 1000, loss: 0.7501898084789492, grad_norm: 0.7781980382422955, ic: 0.19378978723624246
train 27, step: 1500, loss: 0.636791142078686, grad_norm: 0.08046711077312041, ic: 0.5205581122305167
train 27, step: 2000, loss: 1.3848303213387434, grad_norm: 0.12623958263958607, ic: 0.048888683010701595
Epoch 27: 2022-05-05 02:53:39.310469: train loss: 1.61567874514266
Eval step 0: eval loss: 0.8247490290972405
Eval: 2022-05-05 02:54:12.316059: total loss: 1.066398258148252, mse:4.593973537036265, ic :0.18672443018681656, sharpe5:16.90066741347313, irr5:555.6860961914062, ndcg5:0.8391880436575743, pnl5:10.58987808227539 
train 28, step: 0, loss: 1.5501334761100387, grad_norm: 2.5974079157021697, ic: 0.24420604431232953
train 28, step: 500, loss: 1.39825081225545, grad_norm: 4.241281858129182, ic: 0.17643264732560487
train 28, step: 1000, loss: 0.9194813963520495, grad_norm: 0.3071981964016748, ic: 0.5835863413754468
train 28, step: 1500, loss: 1.0289516256313131, grad_norm: 0.02660394201550544, ic: 0.022255468295816844
train 28, step: 2000, loss: 1.048027178992523, grad_norm: 1.1425903557200612, ic: 0.06964893142361561
Epoch 28: 2022-05-05 03:02:39.425706: train loss: 1.6136806713543954
Eval step 0: eval loss: 0.8197844076206862
Eval: 2022-05-05 03:03:12.285225: total loss: 1.0788162794637954, mse:4.694694188099129, ic :0.18052859443391484, sharpe5:17.41241889953613, irr5:583.0670776367188, ndcg5:0.847094255409031, pnl5:15.856081008911133 
train 29, step: 0, loss: 0.9103069175274829, grad_norm: 0.18629812031983495, ic: 0.10647087503367003
train 29, step: 500, loss: 1.1089959584189666, grad_norm: 0.09027600819609344, ic: 0.6117851062167824
train 29, step: 1000, loss: 1.0667122220956446, grad_norm: 4.006508111721268, ic: 0.08695201153727738
train 29, step: 1500, loss: 2.4535641100702574, grad_norm: 6.749151335493771, ic: 0.004740339698493724
train 29, step: 2000, loss: 3.823781708140432, grad_norm: 29.582374798281485, ic: 0.24113142985268232
Epoch 29: 2022-05-05 03:11:25.749788: train loss: 1.6135658473906143
Eval step 0: eval loss: 0.8221939135150487
Eval: 2022-05-05 03:11:57.281255: total loss: 1.0650074908299758, mse:4.584198257709473, ic :0.19446420073315893, sharpe5:17.421705998182297, irr5:575.4830322265625, ndcg5:0.8424757339913647, pnl5:6.301512241363525 
train 30, step: 0, loss: 1.0162101405563404, grad_norm: 0.2623551595546849, ic: 0.49981042684356247
train 30, step: 500, loss: 1.4524396437910174, grad_norm: 3.5810319783110574, ic: 0.015524977260337702
train 30, step: 1000, loss: 0.9813951896898674, grad_norm: 0.24410671198055103, ic: -0.020295985931008516
train 30, step: 1500, loss: 1.5300108588310122, grad_norm: 12.136810239835825, ic: 0.14873815465816723
train 30, step: 2000, loss: 1.8517463059487376, grad_norm: 1.4366883596404267, ic: 0.04706917210301073
Epoch 30: 2022-05-05 03:20:23.396890: train loss: 1.6157972334082433
Eval step 0: eval loss: 0.822507064385208
Eval: 2022-05-05 03:20:55.879161: total loss: 1.0656265996324277, mse:4.591047743990859, ic :0.19289044515341297, sharpe5:17.628124015331267, irr5:583.1101684570312, ndcg5:0.8490564467205001, pnl5:6.301496505737305 
train 31, step: 0, loss: 1.0275448216296312, grad_norm: 0.9480804402538123, ic: 0.38574666982902095
train 31, step: 500, loss: 1.4960004139338992, grad_norm: 2.8538255734761515, ic: 0.014213940027034997
train 31, step: 1000, loss: 4.506579027615144, grad_norm: 11.379254975841057, ic: 0.479148022928639
train 31, step: 1500, loss: 0.7725308312874501, grad_norm: 0.047358076258490034, ic: 0.711186286109039
train 31, step: 2000, loss: 1.2517784605634974, grad_norm: 8.223121999782318, ic: 0.21719437488813303
Epoch 31: 2022-05-05 03:29:16.283711: train loss: 1.6076700314797054
Eval step 0: eval loss: 0.8252868973919915
Eval: 2022-05-05 03:29:45.402086: total loss: 1.0649380239738262, mse:4.5841781008971125, ic :0.18903383379335315, sharpe5:16.761264449357984, irr5:539.4617919921875, ndcg5:0.8432881759392945, pnl5:4.992868900299072 
train 32, step: 0, loss: 1.1304714711308943, grad_norm: 0.1903332802024174, ic: 0.17488291054101063
train 32, step: 500, loss: 1.496601639394685, grad_norm: 4.885070449749301, ic: 0.0376312522648387
train 32, step: 1000, loss: 1.0413380777290635, grad_norm: 0.21673283964335283, ic: 0.5036673170646219
train 32, step: 1500, loss: 0.9614209394069413, grad_norm: 2.415415000727335, ic: 0.06238261576764692
train 32, step: 2000, loss: 0.9422461329088911, grad_norm: 0.10896962633089077, ic: 0.5572596350942609
Epoch 32: 2022-05-05 03:38:07.089324: train loss: 1.610319603066348
Eval step 0: eval loss: 0.8180970974792544
Eval: 2022-05-05 03:38:37.482869: total loss: 1.0648018944533388, mse:4.600585928955047, ic :0.19360030607113224, sharpe5:17.5080955016613, irr5:584.1884765625, ndcg5:0.8433907252233807, pnl5:7.152211666107178 
train 33, step: 0, loss: 1.2841928700391467, grad_norm: 1.9222849891643026, ic: 0.19394916261972797
train 33, step: 500, loss: 0.9836724196070473, grad_norm: 0.029843853713485014, ic: 0.19150356179531758
train 33, step: 1000, loss: 1.005503761806672, grad_norm: 10.538760623993888, ic: 0.25509394112052197
train 33, step: 1500, loss: 0.87691699624203, grad_norm: 0.03803171337630777, ic: 0.5587473439182145
train 33, step: 2000, loss: 0.8035520139473554, grad_norm: 0.17870530606822338, ic: 0.27658710361025063
Epoch 33: 2022-05-05 03:46:43.127528: train loss: 1.6114676102047716
Eval step 0: eval loss: 0.8214661866520679
Eval: 2022-05-05 03:47:15.010350: total loss: 1.06460077246867, mse:4.578025209935731, ic :0.19604875188105572, sharpe5:17.766698348522187, irr5:592.5689697265625, ndcg5:0.8444843607572174, pnl5:8.873859405517578 
train 34, step: 0, loss: 0.993047498481957, grad_norm: 0.6283727287253424, ic: 0.5979850652150152
train 34, step: 500, loss: 0.8045134471461678, grad_norm: 1.2831710439296697, ic: 0.30069103282769233
train 34, step: 1000, loss: 3.1261708249327955, grad_norm: 5.993638394993051, ic: 0.36496025329717874
train 34, step: 1500, loss: 0.8304374976720799, grad_norm: 1.0754726019767364, ic: 0.6907060737442234
train 34, step: 2000, loss: 4.933734825837529, grad_norm: 91.96548951301683, ic: 0.44821644654935483
Epoch 34: 2022-05-05 03:55:43.573668: train loss: 1.6087455178250885
Eval step 0: eval loss: 0.8195160201527923
Eval: 2022-05-05 03:56:16.747696: total loss: 1.0654361399661452, mse:4.605173041778524, ic :0.19526636021044522, sharpe5:17.493950128555298, irr5:577.982666015625, ndcg5:0.8527279187106513, pnl5:6.435751438140869 
train 35, step: 0, loss: 1.1534571748621323, grad_norm: 0.8824836023615359, ic: 0.5533150738207459
train 35, step: 500, loss: 1.1928826719644319, grad_norm: 2.6640365144919977, ic: 0.12128268890623592
train 35, step: 1000, loss: 1.8144875949608545, grad_norm: 13.244117979241617, ic: 0.08377159528404744
train 35, step: 1500, loss: 1.5978734066611844, grad_norm: 3.9726914132654034, ic: 0.0684142321826008
train 35, step: 2000, loss: 0.7795792033965574, grad_norm: 0.18680390530052787, ic: 0.5701921873795881
Epoch 35: 2022-05-05 04:04:41.998132: train loss: 1.6107609278482333
Eval step 0: eval loss: 0.8240793145745521
Eval: 2022-05-05 04:05:14.408751: total loss: 1.064668825430975, mse:4.583281371417682, ic :0.19496348083130274, sharpe5:17.710217124223707, irr5:585.5396118164062, ndcg5:0.8458122331971372, pnl5:7.129720211029053 
train 36, step: 0, loss: 1.842674363164737, grad_norm: 5.644214646712842, ic: 0.10975514487848965
train 36, step: 500, loss: 0.8330365745358058, grad_norm: 0.23317029546626833, ic: 0.17697717597226018
train 36, step: 1000, loss: 1.7634005681818181, grad_norm: 23.089396163248537, ic: 0.2591717991037647
train 36, step: 1500, loss: 0.7595889046255685, grad_norm: 0.08041743405364853, ic: 0.403106749627028
train 36, step: 2000, loss: 1.1219228391518565, grad_norm: 3.0666360167875064, ic: 0.7688676559682844
Epoch 36: 2022-05-05 04:13:17.890560: train loss: 1.6082813254620965
Eval step 0: eval loss: 0.8257630873781612
Eval: 2022-05-05 04:13:50.219992: total loss: 1.0663405654639402, mse:4.592232906521708, ic :0.18976284743153024, sharpe5:17.62976046681404, irr5:580.12939453125, ndcg5:0.848935996280599, pnl5:6.520668983459473 
train 37, step: 0, loss: 1.9794862054938744, grad_norm: 11.93622681647975, ic: 0.17008773088066806
train 37, step: 500, loss: 2.3588536621890293, grad_norm: 8.284562065428412, ic: -0.07485443527352353
train 37, step: 1000, loss: 1.0651861971734684, grad_norm: 0.6571588465834384, ic: 0.04381298177603506
train 37, step: 1500, loss: 1.996767340499902, grad_norm: 7.846208123343737, ic: 0.6135823859408912
train 37, step: 2000, loss: 1.3202310859958992, grad_norm: 0.9618219973553319, ic: 0.16278315089344397
Epoch 37: 2022-05-05 04:22:15.355280: train loss: 1.6058579893454576
Eval step 0: eval loss: 0.8224900851636591
Eval: 2022-05-05 04:22:46.941405: total loss: 1.0661289925680284, mse:4.604217239605861, ic :0.19396913528156962, sharpe5:17.886456327438353, irr5:584.89599609375, ndcg5:0.8606172988187465, pnl5:11.390460968017578 
train 38, step: 0, loss: 1.3243925513290777, grad_norm: 2.054261869218611, ic: -0.06954741826664006
train 38, step: 500, loss: 0.8945416485821759, grad_norm: 0.1443833025546145, ic: 0.27242507792498627
train 38, step: 1000, loss: 0.918182300673172, grad_norm: 0.2558574804478026, ic: 0.1564423849901449
train 38, step: 1500, loss: 0.9504961782816059, grad_norm: 0.057562076722512054, ic: 0.21331578870370632
train 38, step: 2000, loss: 2.317087594115842, grad_norm: 14.905671375881893, ic: 0.04244438879050764
Epoch 38: 2022-05-05 04:31:02.145685: train loss: 1.6057559580760017
Eval step 0: eval loss: 0.8207394245175842
Eval: 2022-05-05 04:31:32.636860: total loss: 1.062838876834358, mse:4.5884547309555845, ic :0.19801155883259558, sharpe5:17.53449628829956, irr5:578.7973022460938, ndcg5:0.8357398453141924, pnl5:10.503874778747559 
train 39, step: 0, loss: 0.9694941809418005, grad_norm: 0.015627758980404298, ic: 0.04852591307838593
train 39, step: 500, loss: 0.8932125903751922, grad_norm: 1.3038699047985016, ic: 0.2340051102581988
train 39, step: 1000, loss: 0.9378982857720869, grad_norm: 0.2965819970243575, ic: 0.1988787223229297
train 39, step: 1500, loss: 2.105661431507528, grad_norm: 1.7233652913126134, ic: 0.23668083806395326
train 39, step: 2000, loss: 0.6143062163679898, grad_norm: 0.13142600161746337, ic: 0.029608196487505938
Epoch 39: 2022-05-05 04:40:05.544375: train loss: 1.6056436632204665
Eval step 0: eval loss: 0.8226426408966675
Eval: 2022-05-05 04:40:38.309594: total loss: 1.0647481490298683, mse:4.593041856579773, ic :0.1942062010693912, sharpe5:18.013813873529433, irr5:602.7781372070312, ndcg5:0.853141724141259, pnl5:9.627293586730957 
