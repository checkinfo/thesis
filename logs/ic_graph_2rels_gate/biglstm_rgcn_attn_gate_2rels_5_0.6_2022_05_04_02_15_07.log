Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0001, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_5_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
66369
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795801468432369, grad_norm: 4.816428940788592, ic: 0.022851361137673748
train 0, step: 500, loss: 0.8600137763752826, grad_norm: 0.035061270464916416, ic: 0.04589665677298407
train 0, step: 1000, loss: 1.9660189111649597, grad_norm: 0.8909375376557374, ic: 0.03338447597843486
train 0, step: 1500, loss: 0.9542390725358202, grad_norm: 0.064212720634919, ic: 0.03365871634683094
train 0, step: 2000, loss: 1.0008472857327664, grad_norm: 0.22383950037738706, ic: 0.06353591285878385
Epoch 0: 2022-05-04 14:24:11.701889: train loss: 1.6482324497116745
Eval step 0: eval loss: 0.835844821749045
Eval: 2022-05-04 14:24:42.904154: total loss: 1.0793874911521533, mse:4.822893292699911, ic :0.007566025299596191, sharpe5:7.907161879241466, irr5:224.52513122558594, ndcg5:0.8798425323176345, pnl5:2.429109811782837 
train 1, step: 0, loss: 2.786838851436492, grad_norm: 1.157819225125021, ic: 0.06232527109542546
train 1, step: 500, loss: 1.7618075563443991, grad_norm: 1.0870898999758138, ic: 0.09102867038334916
train 1, step: 1000, loss: 0.8790259805200917, grad_norm: 0.22877327136891787, ic: 0.08383893544636062
train 1, step: 1500, loss: 1.7139979570761494, grad_norm: 0.26382136270496553, ic: -0.03794203775938672
train 1, step: 2000, loss: 2.1773246093750003, grad_norm: 1.167370124158492, ic: -0.038870006818105055
Epoch 1: 2022-05-04 14:32:59.027144: train loss: 1.6464582773778342
Eval step 0: eval loss: 0.8339996820254872
Eval: 2022-05-04 14:33:31.153121: total loss: 1.079374070884638, mse:4.825526626845832, ic :0.007642627696550285, sharpe5:7.750799885988235, irr5:221.07208251953125, ndcg5:0.8396205974007185, pnl5:2.4572105407714844 
train 2, step: 0, loss: 2.1420767045454543, grad_norm: 0.016368310445573114, ic: 0.13371117271817273
train 2, step: 500, loss: 3.3099615488067293, grad_norm: 0.4084577475647643, ic: 0.07951857342921165
train 2, step: 1000, loss: 2.0706874102011494, grad_norm: 0.0009098522891301068, ic: 0.19176002237788942
train 2, step: 1500, loss: 1.4905115025644085, grad_norm: 0.08071322598015801, ic: -0.03805867908665217
train 2, step: 2000, loss: 3.250233623798077, grad_norm: 1.1015106388700089, ic: 0.17155878231065078
Epoch 2: 2022-05-04 14:41:53.085225: train loss: 1.6463421413689354
Eval step 0: eval loss: 0.8354874219984851
Eval: 2022-05-04 14:42:24.893213: total loss: 1.0798418787488977, mse:4.824845522232391, ic :0.009670707444045551, sharpe5:7.774071550369262, irr5:221.65184020996094, ndcg5:0.8432228352919582, pnl5:2.8226406574249268 
train 3, step: 0, loss: 1.5227613495617378, grad_norm: 0.6971450311002471, ic: -0.0018186494747204993
train 3, step: 500, loss: 1.5106106536315584, grad_norm: 0.44517984722912385, ic: 0.10219810280452189
train 3, step: 1000, loss: 3.679093522866293, grad_norm: 0.9222060270295478, ic: -0.057361203534925354
train 3, step: 1500, loss: 1.9719591905559535, grad_norm: 1.351718629748613, ic: -0.05551496348886489
train 3, step: 2000, loss: 0.8979646589949325, grad_norm: 0.0013517250712545901, ic: 0.01867591939975213
Epoch 3: 2022-05-04 14:50:36.946424: train loss: 1.6462351745071282
Eval step 0: eval loss: 0.834103744072708
Eval: 2022-05-04 14:51:08.627774: total loss: 1.0798049166574635, mse:4.827586275843516, ic :0.009196115135817463, sharpe5:7.720776203870773, irr5:219.59242248535156, ndcg5:0.8336125726398538, pnl5:2.31331467628479 
train 4, step: 0, loss: 1.4308126395089287, grad_norm: 0.056719185195828366, ic: 0.14133074376760213
train 4, step: 500, loss: 1.659564545398622, grad_norm: 0.7515896538180866, ic: 0.025353455458521978
train 4, step: 1000, loss: 2.969196412621475, grad_norm: 0.8908844662440325, ic: 0.01610384049745592
train 4, step: 1500, loss: 2.155849485759494, grad_norm: 0.6155966467258257, ic: -0.029679437332420188
train 4, step: 2000, loss: 1.0829707927318235, grad_norm: 0.4765317974316428, ic: 0.15101302841281883
Epoch 4: 2022-05-04 14:59:18.952324: train loss: 1.6457474089326884
Eval step 0: eval loss: 0.8429351903113474
Eval: 2022-05-04 14:59:50.695053: total loss: 1.0822471497326749, mse:4.825114440906884, ic :0.017543395448281335, sharpe5:8.0953996771574, irr5:229.32620239257812, ndcg5:0.8547702429128966, pnl5:2.467103958129883 
train 5, step: 0, loss: 1.3375505485790686, grad_norm: 0.12347975136159886, ic: 0.05726005185977294
train 5, step: 500, loss: 0.8885812661082474, grad_norm: 0.0076654711764796825, ic: 0.0369175290090553
train 5, step: 1000, loss: 0.979618280052682, grad_norm: 0.18289430352830754, ic: -0.02904381446273575
train 5, step: 1500, loss: 1.5361482348595425, grad_norm: 0.1935904852536347, ic: 0.024893792634279384
train 5, step: 2000, loss: 1.1072591617876304, grad_norm: 0.03338222600414657, ic: 0.09514236636586196
Epoch 5: 2022-05-04 15:08:11.321812: train loss: 1.6452670988604812
Eval step 0: eval loss: 0.8396927378325869
Eval: 2022-05-04 15:08:42.782479: total loss: 1.0808353640868351, mse:4.828982532834642, ic :0.037245653511930536, sharpe5:9.910886519551276, irr5:287.2413635253906, ndcg5:0.8455521933876537, pnl5:3.1047122478485107 
train 6, step: 0, loss: 1.3422195458906498, grad_norm: 0.5215190533228704, ic: 0.04116512968834865
train 6, step: 500, loss: 1.0090761987940275, grad_norm: 0.056687851138343445, ic: 0.04439295358155922
train 6, step: 1000, loss: 1.119297134921578, grad_norm: 0.11033264135526545, ic: 0.10287555200119185
train 6, step: 1500, loss: 1.5639551588326446, grad_norm: 0.8546181595436824, ic: 0.17334036775549347
train 6, step: 2000, loss: 0.811206533586244, grad_norm: 0.05891220809103594, ic: 0.0243603297376193
Epoch 6: 2022-05-04 15:16:52.023529: train loss: 1.643788375394499
Eval step 0: eval loss: 0.8361343689286419
Eval: 2022-05-04 15:17:24.111810: total loss: 1.0782541773525298, mse:4.823984982387427, ic :0.04474524943377736, sharpe5:10.397387464642524, irr5:319.90045166015625, ndcg5:0.8463457273239893, pnl5:2.6563832759857178 
train 7, step: 0, loss: 0.9926839828491212, grad_norm: 0.06399971965841882, ic: 0.07544614710225853
train 7, step: 500, loss: 0.6470453326100636, grad_norm: 0.002300122132065167, ic: 0.0762227818592314
train 7, step: 1000, loss: 1.0339939323710015, grad_norm: 0.24952999813052798, ic: 0.09743772280015922
train 7, step: 1500, loss: 2.266526724109057, grad_norm: 0.79339579608195, ic: 0.31811273249005584
train 7, step: 2000, loss: 0.9151563998763127, grad_norm: 0.05695901156484351, ic: -0.06824381732968576
Epoch 7: 2022-05-04 15:25:39.925879: train loss: 1.6415742536165172
Eval step 0: eval loss: 0.8356959319843256
Eval: 2022-05-04 15:26:10.006904: total loss: 1.0755114441022564, mse:4.7490567222322415, ic :0.1253569401611647, sharpe5:11.515378010272979, irr5:404.3997802734375, ndcg5:0.8391269503929791, pnl5:2.7938215732574463 
train 8, step: 0, loss: 3.630355879189312, grad_norm: 1.3974605707698702, ic: -0.02187018223555754
train 8, step: 500, loss: 2.750871374251995, grad_norm: 1.1729106432835619, ic: 0.030218910255574605
train 8, step: 1000, loss: 3.10492774852808, grad_norm: 1.312630068442643, ic: 0.07604225664426922
train 8, step: 1500, loss: 0.7254767364991137, grad_norm: 0.005023100211690185, ic: 0.40667964837489906
train 8, step: 2000, loss: 1.0829131549996123, grad_norm: 0.43346890543329186, ic: 0.518301405346393
Epoch 8: 2022-05-04 15:34:15.239467: train loss: 1.6357146619022838
Eval step 0: eval loss: 0.8291891598475368
Eval: 2022-05-04 15:34:46.643809: total loss: 1.0752352924208386, mse:4.7490659730405635, ic :0.13220278629937005, sharpe5:11.611168123483658, irr5:397.0002136230469, ndcg5:0.8383572008147885, pnl5:3.190113067626953 
train 9, step: 0, loss: 5.41717285935008, grad_norm: 1.0872729437522353, ic: 0.14845055265082022
train 9, step: 500, loss: 1.3662254557736673, grad_norm: 1.2748140691068182, ic: 0.30211860315103367
train 9, step: 1000, loss: 0.9326582784918924, grad_norm: 0.007098811027652372, ic: 0.0076301772390929435
train 9, step: 1500, loss: 1.1001109599364876, grad_norm: 0.023896132237639766, ic: 0.4082767121534094
train 9, step: 2000, loss: 1.0831861721655256, grad_norm: 0.2824466972971835, ic: 0.18188079845900698
Epoch 9: 2022-05-04 15:43:04.423932: train loss: 1.6339614323508187
Eval step 0: eval loss: 0.8278346810376053
Eval: 2022-05-04 15:43:36.197442: total loss: 1.0721006793250378, mse:4.720501617784436, ic :0.14282208931894025, sharpe5:11.857211914658546, irr5:406.64727783203125, ndcg5:0.836424321868697, pnl5:3.2303552627563477 
train 10, step: 0, loss: 7.109256844478863, grad_norm: 1.5605708019866873, ic: 0.1745535761959746
train 10, step: 500, loss: 1.1399096334747842, grad_norm: 0.08369768993096198, ic: -0.03299839047586705
train 10, step: 1000, loss: 2.3735823368002302, grad_norm: 0.882532915569075, ic: -0.038037441196509536
train 10, step: 1500, loss: 1.094478260387074, grad_norm: 0.3115609696906748, ic: -0.027181769898378828
train 10, step: 2000, loss: 2.790750520932276, grad_norm: 0.35873584181823365, ic: 0.4336001632052491
Epoch 10: 2022-05-04 15:51:49.138300: train loss: 1.6343184537783317
Eval step 0: eval loss: 0.8292076826346811
Eval: 2022-05-04 15:52:20.725982: total loss: 1.0711761520306342, mse:4.718025160412933, ic :0.14327095946711313, sharpe5:11.852463367581366, irr5:400.5608215332031, ndcg5:0.8485686563577607, pnl5:3.0757360458374023 
train 11, step: 0, loss: 1.2625374613302398, grad_norm: 0.028174674609788786, ic: 0.140804059203897
train 11, step: 500, loss: 0.6952915337249522, grad_norm: 0.015886381836042504, ic: 0.45397023916718365
train 11, step: 1000, loss: 0.9434862757947754, grad_norm: 0.18364153949768355, ic: 0.061560268513250596
train 11, step: 1500, loss: 1.055288415206106, grad_norm: 0.06876847331644757, ic: 0.17079107064832075
train 11, step: 2000, loss: 0.7851560571559044, grad_norm: 0.004133520965011821, ic: 0.1356115370066594
Epoch 11: 2022-05-04 16:00:27.490456: train loss: 1.633607958533473
Eval step 0: eval loss: 0.8312387576972471
Eval: 2022-05-04 16:00:59.383465: total loss: 1.0718753327054462, mse:4.704818606241505, ic :0.1485355793092337, sharpe5:11.798712084889411, irr5:398.5964660644531, ndcg5:0.8542255237193978, pnl5:3.445380926132202 
train 12, step: 0, loss: 0.9849578539530436, grad_norm: 0.09454759225188777, ic: 0.30301544828529997
train 12, step: 500, loss: 0.9413788566620599, grad_norm: 0.0842279534939804, ic: 0.06340587837530658
train 12, step: 1000, loss: 2.9086437832777667, grad_norm: 0.9434034610564594, ic: 0.42487578028302225
train 12, step: 1500, loss: 0.925813789687262, grad_norm: 0.1490231231246138, ic: -0.10756315085680473
train 12, step: 2000, loss: 0.8729827051076284, grad_norm: 0.009714817746034948, ic: 0.21652232907226393
Epoch 12: 2022-05-04 16:09:18.751668: train loss: 1.6324573607310238
Eval step 0: eval loss: 0.8298140466527265
Eval: 2022-05-04 16:09:52.044779: total loss: 1.0710480094879085, mse:4.702147987691904, ic :0.15176012977232112, sharpe5:12.136527696847915, irr5:396.69427490234375, ndcg5:0.8563960344880656, pnl5:4.280674934387207 
train 13, step: 0, loss: 2.0784410680108234, grad_norm: 0.8848795592883085, ic: 0.34231252284553626
train 13, step: 500, loss: 0.8320949497196879, grad_norm: 0.061223122237894514, ic: 0.4930373185155713
train 13, step: 1000, loss: 0.9887589627464345, grad_norm: 0.5398938829304537, ic: 0.4523138137851409
train 13, step: 1500, loss: 2.4271469890466433, grad_norm: 0.5631203247681521, ic: -0.05744450133509185
train 13, step: 2000, loss: 1.4959671690062066, grad_norm: 0.11590724483672163, ic: 0.12281745043859493
Epoch 13: 2022-05-04 16:18:12.431249: train loss: 1.632295500575055
Eval step 0: eval loss: 0.8269953029298932
Eval: 2022-05-04 16:18:43.032091: total loss: 1.0723631953475, mse:4.73191357702265, ic :0.1415559323649339, sharpe5:11.325506125688552, irr5:388.58660888671875, ndcg5:0.8445599067601283, pnl5:2.7435719966888428 
train 14, step: 0, loss: 4.4903497800068255, grad_norm: 2.954730826892531, ic: 0.16473207295945655
train 14, step: 500, loss: 0.8248436604071101, grad_norm: 0.008011148988021445, ic: 0.136304699224173
train 14, step: 1000, loss: 1.8807781801679955, grad_norm: 0.24526485130139053, ic: 0.32901966899031876
train 14, step: 1500, loss: 1.1328957646009126, grad_norm: 0.09509768797041708, ic: -0.0730371372948689
train 14, step: 2000, loss: 1.1593967800627174, grad_norm: 0.35574749613745177, ic: 0.09100900769149656
Epoch 14: 2022-05-04 16:26:57.067616: train loss: 1.6317581700475199
Eval step 0: eval loss: 0.832431419380598
Eval: 2022-05-04 16:27:29.542484: total loss: 1.0715552205503334, mse:4.6999823642162, ic :0.1500862089423576, sharpe5:11.73918497979641, irr5:393.9671630859375, ndcg5:0.8574172986314449, pnl5:3.225135087966919 
train 15, step: 0, loss: 3.4317097823443583, grad_norm: 2.5162224185230215, ic: 0.06214614465825295
train 15, step: 500, loss: 1.2579876399496053, grad_norm: 0.00917650626793826, ic: -0.02180136348032857
train 15, step: 1000, loss: 1.3206952847116362, grad_norm: 0.256074963957769, ic: -0.08523878303589578
train 15, step: 1500, loss: 0.8522838682640256, grad_norm: 0.2077278976858474, ic: 0.01741635078848877
train 15, step: 2000, loss: 1.4949564910741244, grad_norm: 0.9151775801289965, ic: 0.031170132110214993
Epoch 15: 2022-05-04 16:35:44.560619: train loss: 1.6313806053379922
Eval step 0: eval loss: 0.8393967591296759
Eval: 2022-05-04 16:36:16.347017: total loss: 1.0760580864157918, mse:4.705239545509139, ic :0.14457586836760264, sharpe5:11.834299702048302, irr5:396.3032531738281, ndcg5:0.8547436860409473, pnl5:3.7740793228149414 
train 16, step: 0, loss: 0.6755823420798693, grad_norm: 0.3500139731886456, ic: -0.048544145075457124
train 16, step: 500, loss: 1.603253519553997, grad_norm: 0.7606139785398619, ic: 0.16891529228753843
train 16, step: 1000, loss: 0.8885479551373106, grad_norm: 0.010477704323237644, ic: -0.1643939695648176
train 16, step: 1500, loss: 0.8331301713947991, grad_norm: 0.2508630167116054, ic: 0.11942762365555212
train 16, step: 2000, loss: 3.3471659527250206, grad_norm: 1.6111517302407687, ic: -0.04253068458840906
Epoch 16: 2022-05-04 16:44:30.288288: train loss: 1.6309325190085653
Eval step 0: eval loss: 0.8286110945320732
Eval: 2022-05-04 16:45:03.167950: total loss: 1.0712286555173058, mse:4.697455364817783, ic :0.14908911056345436, sharpe5:12.717273112535477, irr5:423.2430725097656, ndcg5:0.8473808650530313, pnl5:4.151153564453125 
train 17, step: 0, loss: 1.2846726313826258, grad_norm: 0.41339149218981946, ic: -0.10660676219829937
train 17, step: 500, loss: 1.7692280921832655, grad_norm: 0.8115037726036562, ic: 0.19201491735004306
train 17, step: 1000, loss: 1.2834695308357635, grad_norm: 0.2218691744676533, ic: 0.15440912480726932
train 17, step: 1500, loss: 4.5259352325955176, grad_norm: 1.6175198069011372, ic: 0.141172741910089
train 17, step: 2000, loss: 1.290696707811257, grad_norm: 1.2473411762482896, ic: 0.030942943545185054
Epoch 17: 2022-05-04 16:53:06.810934: train loss: 1.6296840357160012
Eval step 0: eval loss: 0.8328411074157007
Eval: 2022-05-04 16:53:37.394166: total loss: 1.0714358119351908, mse:4.675952321035414, ic :0.16328026913109142, sharpe5:16.795402908325194, irr5:533.9786987304688, ndcg5:0.8518041917764158, pnl5:7.892667293548584 
train 18, step: 0, loss: 1.4272581951342447, grad_norm: 0.9563821760432101, ic: 0.047420738499194655
train 18, step: 500, loss: 1.464535963352502, grad_norm: 2.1686437338687408, ic: -0.039447508553712166
train 18, step: 1000, loss: 0.6658564319349315, grad_norm: 0.004974862282371513, ic: 0.534340844798263
train 18, step: 1500, loss: 1.4235158009572073, grad_norm: 0.057938241408106925, ic: 0.1613134990032232
train 18, step: 2000, loss: 0.9093429662619428, grad_norm: 0.008272756223574701, ic: -0.01079298995231475
Epoch 18: 2022-05-04 17:01:47.719879: train loss: 1.6265645813017862
Eval step 0: eval loss: 0.8222869133421693
Eval: 2022-05-04 17:02:18.813621: total loss: 1.068087690763723, mse:4.669184395788836, ic :0.1726522313159018, sharpe5:16.392973948717117, irr5:532.980224609375, ndcg5:0.8445376036231788, pnl5:6.156722068786621 
train 19, step: 0, loss: 1.5143265981522818, grad_norm: 1.2509694077015103, ic: 0.053710735520137415
train 19, step: 500, loss: 0.8509087739167389, grad_norm: 0.02068264215444661, ic: 0.25266795299953715
train 19, step: 1000, loss: 0.969011751376234, grad_norm: 0.0349947731969404, ic: 0.13691976097660744
train 19, step: 1500, loss: 3.9365090258257203, grad_norm: 1.642488492685683, ic: 0.10963941768793212
train 19, step: 2000, loss: 1.0224415940504807, grad_norm: 0.3131006494592688, ic: 0.16866503315280632
Epoch 19: 2022-05-04 17:10:34.460724: train loss: 1.623402060642518
Eval step 0: eval loss: 0.8258546722701527
Eval: 2022-05-04 17:11:06.141181: total loss: 1.068938504135955, mse:4.625726197146556, ic :0.18016699045353735, sharpe5:16.7763652074337, irr5:549.264404296875, ndcg5:0.8658547989549112, pnl5:11.243786811828613 
train 20, step: 0, loss: 2.2997672461709486, grad_norm: 2.006248222965156, ic: 0.02950019261078897
train 20, step: 500, loss: 3.2408955965909088, grad_norm: 1.538473092294227, ic: 0.11772966984155211
train 20, step: 1000, loss: 0.9789819717407227, grad_norm: 0.13742159250632308, ic: 0.11597200091192689
train 20, step: 1500, loss: 1.7445758159456355, grad_norm: 5.125663119916238, ic: 0.2514869765636414
train 20, step: 2000, loss: 1.030498759781556, grad_norm: 0.1453520820735898, ic: 0.02487889377326432
Epoch 20: 2022-05-04 17:19:25.124508: train loss: 1.6200276750171696
Eval step 0: eval loss: 0.8320931212542808
Eval: 2022-05-04 17:19:57.048287: total loss: 1.0686567070312845, mse:4.6027584221820215, ic :0.18707156730792698, sharpe5:17.580333956480025, irr5:576.9417114257812, ndcg5:0.8468247257389915, pnl5:19.753313064575195 
train 21, step: 0, loss: 0.9885819098314741, grad_norm: 0.46916719520970696, ic: 0.053312552158501274
train 21, step: 500, loss: 0.7618901075515072, grad_norm: 0.016493693013105402, ic: 0.21903794783252856
train 21, step: 1000, loss: 0.9526289555064419, grad_norm: 3.639486285369185, ic: 0.17498655519174902
train 21, step: 1500, loss: 0.9892368083209593, grad_norm: 0.7166205175163265, ic: 0.3051587774122391
train 21, step: 2000, loss: 0.942507451290836, grad_norm: 0.3272155143111678, ic: 0.030167491326677554
Epoch 21: 2022-05-04 17:28:10.454957: train loss: 1.6189251981064507
Eval step 0: eval loss: 0.8210995898231691
Eval: 2022-05-04 17:28:42.844743: total loss: 1.068025235893456, mse:4.614460410299475, ic :0.18234200348905394, sharpe5:17.33279087305069, irr5:561.2943115234375, ndcg5:0.8548412691788794, pnl5:10.472193717956543 
train 22, step: 0, loss: 1.040539585264389, grad_norm: 0.10251120478829759, ic: 0.22660609881024638
train 22, step: 500, loss: 3.278493394308943, grad_norm: 2.683089265529783, ic: -0.18900501115146337
train 22, step: 1000, loss: 1.191914852781792, grad_norm: 0.028285646798229615, ic: 0.457943459077505
train 22, step: 1500, loss: 0.9675132217721193, grad_norm: 0.23885081202278452, ic: 0.13605980626142622
train 22, step: 2000, loss: 1.7337231279230443, grad_norm: 3.2526712197005816, ic: 0.2024959011126934
Epoch 22: 2022-05-04 17:36:46.400568: train loss: 1.6185855520815715
Eval step 0: eval loss: 0.820179881989265
Eval: 2022-05-04 17:37:17.869016: total loss: 1.0666221708459387, mse:4.605893196572259, ic :0.18800099545856602, sharpe5:17.469234981536864, irr5:573.5775146484375, ndcg5:0.8614825482050066, pnl5:6.709388256072998 
train 23, step: 0, loss: 0.9687970691867795, grad_norm: 0.09038566786158295, ic: 0.21179061529879548
train 23, step: 500, loss: 1.4262515177344977, grad_norm: 0.264076512992922, ic: 0.05382107312874657
train 23, step: 1000, loss: 1.6451568603515625, grad_norm: 0.13582707583861559, ic: 0.2505529898649665
train 23, step: 1500, loss: 1.150839371582406, grad_norm: 1.8714529860271314, ic: 0.08969740111681183
train 23, step: 2000, loss: 1.8183648254065625, grad_norm: 8.760793402742504, ic: 0.44919430818433326
Epoch 23: 2022-05-04 17:45:31.826710: train loss: 1.616942774506537
Eval step 0: eval loss: 0.8237397301435722
Eval: 2022-05-04 17:46:04.208507: total loss: 1.065598624044652, mse:4.590740210357471, ic :0.1878655519618575, sharpe5:17.153475519418716, irr5:558.4312133789062, ndcg5:0.836481932049363, pnl5:8.657074928283691 
train 24, step: 0, loss: 2.1979596139430284, grad_norm: 0.17637337789520902, ic: 0.1331395418081403
train 24, step: 500, loss: 1.223579804535506, grad_norm: 0.4332933611419751, ic: 0.10633736349205318
train 24, step: 1000, loss: 0.9063889672835078, grad_norm: 0.11285658873111856, ic: 0.5302351723193599
train 24, step: 1500, loss: 2.627536003194559, grad_norm: 7.44170594416087, ic: 0.04205416405276037
train 24, step: 2000, loss: 0.9349857694070904, grad_norm: 0.2729021192619807, ic: 0.08236962462508017
Epoch 24: 2022-05-04 17:54:16.766524: train loss: 1.6147560654915736
Eval step 0: eval loss: 0.8208798889867623
Eval: 2022-05-04 17:54:48.729048: total loss: 1.0707087040074665, mse:4.6298054308636, ic :0.18228502686819922, sharpe5:17.010733437538146, irr5:549.1845092773438, ndcg5:0.8577044370108188, pnl5:7.027773857116699 
train 25, step: 0, loss: 0.8247270738756335, grad_norm: 0.10723090609803067, ic: 0.625171242071078
train 25, step: 500, loss: 0.8724599417602907, grad_norm: 0.009695767126062075, ic: 0.18236060543507618
train 25, step: 1000, loss: 2.082711794316451, grad_norm: 0.08192838859139266, ic: 0.26226745840911747
train 25, step: 1500, loss: 1.1275636417456023, grad_norm: 0.9738715612133754, ic: 0.534648726684733
train 25, step: 2000, loss: 1.0055931244849496, grad_norm: 0.5606053235588976, ic: 0.6055612205494028
Epoch 25: 2022-05-04 18:03:49.952711: train loss: 1.615939886781934
Eval step 0: eval loss: 0.8204669851900026
Eval: 2022-05-04 18:04:37.848271: total loss: 1.0670635464012537, mse:4.598825772369289, ic :0.18949818244301622, sharpe5:17.458368186950683, irr5:585.618896484375, ndcg5:0.8394857978780577, pnl5:5.540475368499756 
train 26, step: 0, loss: 6.5962046662839455, grad_norm: 10.243936896877656, ic: 0.1993986709893898
train 26, step: 500, loss: 3.982014910330336, grad_norm: 12.205162156002352, ic: 0.37540914330098224
train 26, step: 1000, loss: 1.295936955761872, grad_norm: 1.9981410524208885, ic: 0.014635891788874863
train 26, step: 1500, loss: 0.828412919999548, grad_norm: 0.4073449062989566, ic: 0.3264318594286906
train 26, step: 2000, loss: 0.9624689974060779, grad_norm: 1.2228842725090012, ic: 0.12992917840950433
Epoch 26: 2022-05-04 18:16:57.011747: train loss: 1.6154396038234988
Eval step 0: eval loss: 0.8209794489676633
Eval: 2022-05-04 18:17:44.675763: total loss: 1.0649214785464336, mse:4.593022897913041, ic :0.18864086837567043, sharpe5:17.39430063843727, irr5:568.9290771484375, ndcg5:0.8651419373085175, pnl5:5.895633697509766 
train 27, step: 0, loss: 0.8285133272058823, grad_norm: 0.03257186526195051, ic: 0.1120708784517858
train 27, step: 500, loss: 0.9054989250549358, grad_norm: 7.158948892106938, ic: 0.29069618668156694
train 27, step: 1000, loss: 0.7496886404558198, grad_norm: 0.8335874990194312, ic: 0.18922568363794218
train 27, step: 1500, loss: 0.6406308444980958, grad_norm: 0.09446914913729704, ic: 0.5248387714544304
train 27, step: 2000, loss: 1.39161824636959, grad_norm: 0.12199130840057099, ic: -0.058539297989308924
Epoch 27: 2022-05-04 18:29:54.673462: train loss: 1.6154062488907996
Eval step 0: eval loss: 0.8240809867706137
Eval: 2022-05-04 18:30:39.530143: total loss: 1.0661904977339067, mse:4.598874072568429, ic :0.185704366336968, sharpe5:16.847655844688415, irr5:549.2650756835938, ndcg5:0.840847449378057, pnl5:5.859100341796875 
train 28, step: 0, loss: 1.5696671769425676, grad_norm: 3.788187833383993, ic: 0.16985992064421657
train 28, step: 500, loss: 1.4133359462339297, grad_norm: 6.417687002619125, ic: 0.16443690258762944
train 28, step: 1000, loss: 0.9115380594723916, grad_norm: 0.2900702187091935, ic: 0.5895062831020085
train 28, step: 1500, loss: 1.035097159242667, grad_norm: 0.0414948490693087, ic: 0.0193224997123019
train 28, step: 2000, loss: 1.0530412358009011, grad_norm: 1.4115424859123504, ic: 0.04639433402338799
Epoch 28: 2022-05-04 18:43:14.871539: train loss: 1.6133014518016624
Eval step 0: eval loss: 0.8210318015674394
Eval: 2022-05-04 18:44:02.502666: total loss: 1.081124774357902, mse:4.718006024264689, ic :0.17847693493703892, sharpe5:18.19529539704323, irr5:601.2863159179688, ndcg5:0.8580794188513755, pnl5:12.177546501159668 
train 29, step: 0, loss: 0.9092758444785348, grad_norm: 0.2292845152884545, ic: 0.10039134944259502
train 29, step: 500, loss: 1.1062352913516589, grad_norm: 0.25263223286314285, ic: 0.6126526271207389
train 29, step: 1000, loss: 1.0669050238632003, grad_norm: 3.6469124890994857, ic: 0.08807162003988085
train 29, step: 1500, loss: 2.444936664471116, grad_norm: 6.6128191979833835, ic: -0.001592620555464495
train 29, step: 2000, loss: 3.9012775185667437, grad_norm: 25.43382102837717, ic: 0.23583373096912746
Epoch 29: 2022-05-04 18:56:08.572343: train loss: 1.6128337628783744
Eval step 0: eval loss: 0.8222967535728398
Eval: 2022-05-04 18:56:54.518291: total loss: 1.064866208955976, mse:4.589408942087868, ic :0.1928662859540186, sharpe5:17.776938940286634, irr5:589.6185913085938, ndcg5:0.8570875060831039, pnl5:14.859251022338867 
train 30, step: 0, loss: 1.0165963517145324, grad_norm: 0.5712373354671324, ic: 0.5035839074376873
train 30, step: 500, loss: 1.4283555416517126, grad_norm: 3.4296606521768167, ic: 0.10798498900130199
train 30, step: 1000, loss: 0.98624267578125, grad_norm: 0.21129572276395045, ic: -0.013261682281072638
train 30, step: 1500, loss: 1.4880802622439073, grad_norm: 8.741529750260336, ic: 0.16073312590997185
train 30, step: 2000, loss: 1.8504032523192424, grad_norm: 1.431960123445393, ic: 0.0923117210502767
Epoch 30: 2022-05-04 19:09:17.560280: train loss: 1.6130144440117589
Eval step 0: eval loss: 0.8225103444620983
Eval: 2022-05-04 19:10:03.035095: total loss: 1.0656244904801462, mse:4.59924342688601, ic :0.19087847861164484, sharpe5:17.106292744874953, irr5:573.5786743164062, ndcg5:0.8482577305155137, pnl5:10.587667465209961 
train 31, step: 0, loss: 1.0228854865950034, grad_norm: 0.721642112796429, ic: 0.3899437367324512
train 31, step: 500, loss: 1.505542896412037, grad_norm: 2.648585680823786, ic: 0.034037515832384124
train 31, step: 1000, loss: 4.486277047960578, grad_norm: 12.395689464329042, ic: 0.4810346114620301
train 31, step: 1500, loss: 0.7656222367572404, grad_norm: 0.03318528028795908, ic: 0.7132258383223352
train 31, step: 2000, loss: 1.2600190414846124, grad_norm: 6.794223319088459, ic: 0.12123969014106571
Epoch 31: 2022-05-04 19:22:08.445269: train loss: 1.6102606449487338
Eval step 0: eval loss: 0.8252270842251712
Eval: 2022-05-04 19:22:55.701506: total loss: 1.0644960727175043, mse:4.589543234050103, ic :0.18795496633492292, sharpe5:17.046279433965683, irr5:550.1082763671875, ndcg5:0.840517919245222, pnl5:11.826706886291504 
train 32, step: 0, loss: 1.1329898492303196, grad_norm: 0.3927360107031779, ic: 0.18950278412774652
train 32, step: 500, loss: 1.4984251968503937, grad_norm: 5.096006288069366, ic: 0.01825114944505351
train 32, step: 1000, loss: 1.0395788468595488, grad_norm: 0.2434623115702439, ic: 0.5056826768993881
train 32, step: 1500, loss: 0.9632608397692601, grad_norm: 3.48079187958251, ic: 0.058671752633310564
train 32, step: 2000, loss: 0.9423682032213911, grad_norm: 0.08537179496063652, ic: 0.5524658494032343
Epoch 32: 2022-05-04 19:34:55.911073: train loss: 1.611214466118553
Eval step 0: eval loss: 0.819330920911815
Eval: 2022-05-04 19:35:39.675177: total loss: 1.0655659743593218, mse:4.60677457673687, ic :0.19164582745126998, sharpe5:17.627679430246353, irr5:584.4559326171875, ndcg5:0.8366278625293194, pnl5:6.832533359527588 
train 33, step: 0, loss: 1.2784233120725341, grad_norm: 2.9123197160927843, ic: 0.1912267037448745
train 33, step: 500, loss: 0.9891125217842819, grad_norm: 0.0473197661909206, ic: 0.17316662337999178
train 33, step: 1000, loss: 1.0176093948871248, grad_norm: 11.626449565937829, ic: 0.2454246222941393
train 33, step: 1500, loss: 0.8783352128975757, grad_norm: 0.2207496686359641, ic: 0.5585691268023966
train 33, step: 2000, loss: 0.8042675084810816, grad_norm: 0.17048820566985728, ic: 0.2825993731126649
Epoch 33: 2022-05-04 19:47:43.346666: train loss: 1.6117007273068937
Eval step 0: eval loss: 0.8212639152438421
Eval: 2022-05-04 19:48:25.822840: total loss: 1.065296822128231, mse:4.584470638196146, ic :0.19435695187680216, sharpe5:17.78727601647377, irr5:591.0280151367188, ndcg5:0.8388399691352445, pnl5:7.749176502227783 
train 34, step: 0, loss: 0.9948081573126301, grad_norm: 0.7174351340438494, ic: 0.6077115040017549
train 34, step: 500, loss: 0.7944067158830276, grad_norm: 1.1867453380075321, ic: 0.30253846924859734
train 34, step: 1000, loss: 3.1278063046034945, grad_norm: 5.532157641382372, ic: 0.36016350281749254
train 34, step: 1500, loss: 0.8262494238397645, grad_norm: 1.9014713472598421, ic: 0.6960624308982247
train 34, step: 2000, loss: 5.595160001936484, grad_norm: 159.05338428382925, ic: 0.45252236468184526
Epoch 34: 2022-05-04 20:00:32.236668: train loss: 1.6090010809260278
Eval step 0: eval loss: 0.8190745603925184
Eval: 2022-05-04 20:01:17.683848: total loss: 1.0667658838902139, mse:4.613355081995365, ic :0.19408474578694335, sharpe5:17.667476416826247, irr5:588.2262573242188, ndcg5:0.8495125589349486, pnl5:8.205074310302734 
train 35, step: 0, loss: 1.14752197265625, grad_norm: 0.8383857806620125, ic: 0.5502223926811945
train 35, step: 500, loss: 1.1926327422322158, grad_norm: 3.2325075151492015, ic: 0.07099485039394404
train 35, step: 1000, loss: 1.826979975285297, grad_norm: 17.144673180571832, ic: 0.07585204920175473
train 35, step: 1500, loss: 1.5898740381226504, grad_norm: 4.792141695057481, ic: 0.041522331089243886
train 35, step: 2000, loss: 0.7777630811069101, grad_norm: 0.15718525823155544, ic: 0.5742375658470075
Epoch 35: 2022-05-04 20:13:33.982178: train loss: 1.6118493324467906
Eval step 0: eval loss: 0.8255476956623748
Eval: 2022-05-04 20:14:19.548714: total loss: 1.0659164053811767, mse:4.592564700011983, ic :0.18901052695927267, sharpe5:16.98706259250641, irr5:563.1326293945312, ndcg5:0.8554771714092445, pnl5:8.590126037597656 
train 36, step: 0, loss: 1.8706154336734695, grad_norm: 10.889404585103609, ic: 0.08068520718668473
train 36, step: 500, loss: 0.8395276561884358, grad_norm: 0.10517971142952746, ic: 0.13698249907009455
train 36, step: 1000, loss: 1.7679282670454544, grad_norm: 34.744274158486554, ic: 0.25125536764729267
train 36, step: 1500, loss: 0.7600228530854044, grad_norm: 0.09165027504045803, ic: 0.40359509806132093
train 36, step: 2000, loss: 1.1291517480391182, grad_norm: 5.494392387677724, ic: 0.7703742064013321
Epoch 36: 2022-05-04 20:26:44.747607: train loss: 1.6079814828439767
Eval step 0: eval loss: 0.8235402242903714
Eval: 2022-05-04 20:27:31.958971: total loss: 1.0651414218237827, mse:4.59191127543472, ic :0.19166727375581466, sharpe5:17.580929889678956, irr5:592.2971801757812, ndcg5:0.834160509071949, pnl5:4.522538661956787 
train 37, step: 0, loss: 1.990378877835471, grad_norm: 13.02009306514805, ic: 0.16323116171740149
train 37, step: 500, loss: 2.3540489109655383, grad_norm: 9.290291028480194, ic: -0.055095574843383094
train 37, step: 1000, loss: 1.0637164297350645, grad_norm: 0.7362235874934422, ic: 0.044908159954201575
train 37, step: 1500, loss: 2.0081256285567113, grad_norm: 9.887624758105066, ic: 0.611976805826335
train 37, step: 2000, loss: 1.3073242795823816, grad_norm: 0.8199401118658745, ic: 0.23904060109223252
Epoch 37: 2022-05-04 20:39:31.687639: train loss: 1.607580027331904
Eval step 0: eval loss: 0.8211051209332191
Eval: 2022-05-04 20:40:17.964109: total loss: 1.0673631708600395, mse:4.6140462117862935, ic :0.19064000529699612, sharpe5:17.949850376844406, irr5:599.37109375, ndcg5:0.8437633905653629, pnl5:7.24307107925415 
train 38, step: 0, loss: 1.3378703419755145, grad_norm: 2.110005287138906, ic: -0.07249930590614863
train 38, step: 500, loss: 0.9017000928337191, grad_norm: 0.15949020127451757, ic: 0.25880151474649377
train 38, step: 1000, loss: 0.9224831321022727, grad_norm: 0.40643465727821676, ic: 0.1277444857338419
train 38, step: 1500, loss: 0.9515586627099943, grad_norm: 0.08831278623666287, ic: 0.21121867886027132
train 38, step: 2000, loss: 2.3095629366503023, grad_norm: 21.462283429920497, ic: 0.050035257520627444
Epoch 38: 2022-05-04 20:52:23.273087: train loss: 1.607500219127976
Eval step 0: eval loss: 0.8198810091008627
Eval: 2022-05-04 20:53:05.983510: total loss: 1.0632886588455794, mse:4.593841876241469, ic :0.1966070432525833, sharpe5:17.89619368672371, irr5:601.1875, ndcg5:0.8523175949872783, pnl5:7.846424102783203 
train 39, step: 0, loss: 0.970047043733276, grad_norm: 0.014023830224552631, ic: 0.046513474909065364
train 39, step: 500, loss: 0.9017159464174193, grad_norm: 1.5722261115413325, ic: 0.23904221906466316
train 39, step: 1000, loss: 0.9376282993383473, grad_norm: 0.45291261223928175, ic: 0.21489615093901715
train 39, step: 1500, loss: 2.1068713011341127, grad_norm: 2.0377807866766884, ic: 0.22755144610220926
train 39, step: 2000, loss: 0.6140807816202607, grad_norm: 0.22992651074328274, ic: 0.025619186172931414
Epoch 39: 2022-05-04 21:05:25.056398: train loss: 1.6088628488663381
Eval step 0: eval loss: 0.8210226688043335
Eval: 2022-05-04 21:06:12.040036: total loss: 1.0660740709950032, mse:4.600215276756489, ic :0.1907259143397045, sharpe5:18.02682224392891, irr5:609.05029296875, ndcg5:0.8671632257173136, pnl5:11.962383270263672 
