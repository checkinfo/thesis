Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0001, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_20_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
25167
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795805286112197, grad_norm: 4.816275878622871, ic: 0.02289694486093561
train 0, step: 500, loss: 0.8600221474366523, grad_norm: 0.03505291767280737, ic: 0.04586044339926671
train 0, step: 1000, loss: 1.9660012301687089, grad_norm: 0.8908038897383317, ic: 0.03331408890299647
train 0, step: 1500, loss: 0.9542745838994565, grad_norm: 0.06433659791536443, ic: 0.03363674594856878
train 0, step: 2000, loss: 1.0008565135101692, grad_norm: 0.2237738310076355, ic: 0.0631514016004449
Epoch 0: 2022-05-05 04:49:54.108943: train loss: 1.6482328358595066
Eval step 0: eval loss: 0.8358272636903977
Eval: 2022-05-05 04:50:20.349022: total loss: 1.0793870684169409, mse:4.822911373350409, ic :0.007447747104197481, sharpe5:7.965384079813957, irr5:227.2235107421875, ndcg5:0.8636694874050589, pnl5:2.4921040534973145 
train 1, step: 0, loss: 2.7868404265372986, grad_norm: 1.1576680423580514, ic: 0.062226706114934076
train 1, step: 500, loss: 1.7618329076364854, grad_norm: 1.0873006496102755, ic: 0.09096668688356332
train 1, step: 1000, loss: 0.8790172230113636, grad_norm: 0.2287690929487304, ic: 0.08410249893616883
train 1, step: 1500, loss: 1.7139863112877154, grad_norm: 0.2637988549086548, ic: -0.037998487912809506
train 1, step: 2000, loss: 2.177279296875, grad_norm: 1.1671095595187166, ic: -0.03848606751741922
Epoch 1: 2022-05-05 04:58:32.032571: train loss: 1.6464587784515585
Eval step 0: eval loss: 0.8339633439187631
Eval: 2022-05-05 04:59:04.393231: total loss: 1.079375242461781, mse:4.825572081259892, ic :0.007493714386432202, sharpe5:7.738750211298465, irr5:221.32008361816406, ndcg5:0.8503292707506604, pnl5:2.5065839290618896 
train 2, step: 0, loss: 2.142096590909091, grad_norm: 0.01635820240744303, ic: 0.13306109128965285
train 2, step: 500, loss: 3.309958492273083, grad_norm: 0.40831695191965894, ic: 0.07934436916966527
train 2, step: 1000, loss: 2.070680301125479, grad_norm: 0.0009100168127639252, ic: 0.19244469503089118
train 2, step: 1500, loss: 1.4905096388955152, grad_norm: 0.08071866442806089, ic: -0.03796034615597917
train 2, step: 2000, loss: 3.250213341346154, grad_norm: 1.1016174831380168, ic: 0.17173957981234467
Epoch 2: 2022-05-05 05:07:19.997069: train loss: 1.6463431261859376
Eval step 0: eval loss: 0.835403812195403
Eval: 2022-05-05 05:07:53.158211: total loss: 1.0798452147007673, mse:4.824958597625366, ic :0.009368067306797142, sharpe5:7.839417044520378, irr5:223.69580078125, ndcg5:0.8543789923746694, pnl5:2.7058074474334717 
train 3, step: 0, loss: 1.5227124221925814, grad_norm: 0.6971670359111571, ic: -0.0021901983598413225
train 3, step: 500, loss: 1.5105976531910728, grad_norm: 0.445228666077207, ic: 0.10244418785219542
train 3, step: 1000, loss: 3.67914665191422, grad_norm: 0.9222056081360038, ic: -0.05711389237610513
train 3, step: 1500, loss: 1.97194780265446, grad_norm: 1.3499428531437465, ic: -0.05536034071206606
train 3, step: 2000, loss: 0.8979246067356419, grad_norm: 0.0013438167722687828, ic: 0.019325423495889445
Epoch 3: 2022-05-05 05:16:09.455099: train loss: 1.646239854345314
Eval step 0: eval loss: 0.8339805160860115
Eval: 2022-05-05 05:16:42.086004: total loss: 1.0798182093382862, mse:4.827775646945505, ic :0.008791463278446748, sharpe5:7.49505846440792, irr5:214.51109313964844, ndcg5:0.8635789495734378, pnl5:2.676572561264038 
train 4, step: 0, loss: 1.4307631138392858, grad_norm: 0.05669187946458406, ic: 0.14198280328092155
train 4, step: 500, loss: 1.6593734621062992, grad_norm: 0.7522858362906765, ic: 0.02550850021290514
train 4, step: 1000, loss: 2.9693596072313264, grad_norm: 0.8911283948049055, ic: 0.016054566410551013
train 4, step: 1500, loss: 2.1559492022679327, grad_norm: 0.6159493955387197, ic: -0.03008773066859401
train 4, step: 2000, loss: 1.0829579781966854, grad_norm: 0.4768014322164022, ic: 0.15042059069053657
Epoch 4: 2022-05-05 05:25:00.429614: train loss: 1.6457528103965096
Eval step 0: eval loss: 0.84277118646684
Eval: 2022-05-05 05:25:32.718577: total loss: 1.0822342944397436, mse:4.825246470872845, ic :0.01661735400834422, sharpe5:8.279446551203728, irr5:233.9592742919922, ndcg5:0.8446915716741276, pnl5:2.564913511276245 
train 5, step: 0, loss: 1.337471653471057, grad_norm: 0.12346688917991219, ic: 0.05736520138777937
train 5, step: 500, loss: 0.8886210527049465, grad_norm: 0.00761220843163284, ic: 0.03615043075772614
train 5, step: 1000, loss: 0.9799529678520116, grad_norm: 0.18315051766288226, ic: -0.031098590119172216
train 5, step: 1500, loss: 1.5361339341261486, grad_norm: 0.19350946852173917, ic: 0.025663882649396105
train 5, step: 2000, loss: 1.107135108894649, grad_norm: 0.03325478609269708, ic: 0.09528118386735548
Epoch 5: 2022-05-05 05:33:46.760694: train loss: 1.6453165610425424
Eval step 0: eval loss: 0.8382450019346022
Eval: 2022-05-05 05:34:20.123729: total loss: 1.081061537024025, mse:4.8307363269466785, ic :0.034623743000497614, sharpe5:9.596010552048682, irr5:276.20172119140625, ndcg5:0.8430910999828799, pnl5:2.9108035564422607 
train 6, step: 0, loss: 1.3426104823938396, grad_norm: 0.521909414616842, ic: 0.03945169550574351
train 6, step: 500, loss: 1.009143589831786, grad_norm: 0.05703705911566215, ic: 0.0425582790741707
train 6, step: 1000, loss: 1.1205932153041824, grad_norm: 0.11131628774130409, ic: 0.09153200825311407
train 6, step: 1500, loss: 1.5631061144111569, grad_norm: 0.8547805597818462, ic: 0.17385868132694748
train 6, step: 2000, loss: 0.8111492123386208, grad_norm: 0.05864540593621986, ic: 0.02406184300983213
Epoch 6: 2022-05-05 05:42:24.494799: train loss: 1.6438340218853953
Eval step 0: eval loss: 0.8343801709447444
Eval: 2022-05-05 05:42:57.277987: total loss: 1.0783485198859033, mse:4.825681503233122, ic :0.04282847866937966, sharpe5:10.837329946160317, irr5:321.44244384765625, ndcg5:0.8588829794160056, pnl5:2.420985460281372 
train 7, step: 0, loss: 0.9925104141235352, grad_norm: 0.06406825350696113, ic: 0.07119270526948829
train 7, step: 500, loss: 0.6472652168621771, grad_norm: 0.0021866591556627592, ic: 0.07268402527361999
train 7, step: 1000, loss: 1.035272649109863, grad_norm: 0.25200961322289195, ic: 0.09083633360149979
train 7, step: 1500, loss: 2.2673596282991695, grad_norm: 0.7938948346370133, ic: 0.3064298318359464
train 7, step: 2000, loss: 0.9149520947975566, grad_norm: 0.0584924758659647, ic: -0.06398608343736749
Epoch 7: 2022-05-05 05:51:09.642163: train loss: 1.64175233865688
Eval step 0: eval loss: 0.8341652937508232
Eval: 2022-05-05 05:51:41.160379: total loss: 1.0751774218748633, mse:4.747974721647387, ic :0.12560808222958544, sharpe5:12.263255256414412, irr5:413.2215576171875, ndcg5:0.8459186362255785, pnl5:3.374997854232788 
train 8, step: 0, loss: 3.632462565104167, grad_norm: 1.4017705973175985, ic: -0.02573870355651984
train 8, step: 500, loss: 2.7472565717610182, grad_norm: 1.186094434580101, ic: 0.02997971671305845
train 8, step: 1000, loss: 3.1087175894474637, grad_norm: 1.3051375565512089, ic: 0.07244591367878626
train 8, step: 1500, loss: 0.7253205737997072, grad_norm: 0.005561396450262243, ic: 0.40723162805802104
train 8, step: 2000, loss: 1.0858392076515975, grad_norm: 0.4340087516680703, ic: 0.5089974911732966
Epoch 8: 2022-05-05 05:59:48.115866: train loss: 1.6356869311017863
Eval step 0: eval loss: 0.8275232666787408
Eval: 2022-05-05 06:00:20.977679: total loss: 1.0748191202710835, mse:4.746507145260626, ic :0.13297578963170723, sharpe5:12.108972880840302, irr5:412.1216735839844, ndcg5:0.8343749349224452, pnl5:3.1964337825775146 
train 9, step: 0, loss: 5.4208727384868425, grad_norm: 1.0872730191939282, ic: 0.14393747500633858
train 9, step: 500, loss: 1.3664813612066389, grad_norm: 1.2917690536416093, ic: 0.3078570545499096
train 9, step: 1000, loss: 0.9278605544116892, grad_norm: 0.015711162145097317, ic: 0.043120701574080385
train 9, step: 1500, loss: 1.1031024916006995, grad_norm: 0.024343479310008888, ic: 0.4008833081585441
train 9, step: 2000, loss: 1.078130942570994, grad_norm: 0.257724055673015, ic: 0.18780699556802927
Epoch 9: 2022-05-05 06:08:41.983867: train loss: 1.6339344898750652
Eval step 0: eval loss: 0.8279371995192307
Eval: 2022-05-05 06:09:15.911857: total loss: 1.07231456401741, mse:4.720402860273173, ic :0.14282273359347178, sharpe5:12.170355892181396, irr5:412.1342468261719, ndcg5:0.8408664376332055, pnl5:3.046386957168579 
train 10, step: 0, loss: 7.107795559630102, grad_norm: 1.5699330964087848, ic: 0.1756007769722368
train 10, step: 500, loss: 1.1461527538449274, grad_norm: 0.1176717589208959, ic: -0.03264527282572782
train 10, step: 1000, loss: 2.3722311382644747, grad_norm: 0.8821319346232279, ic: -0.03056250538537207
train 10, step: 1500, loss: 1.0993761335100447, grad_norm: 0.31959162456793344, ic: -0.029450599065536435
train 10, step: 2000, loss: 2.7856883133073707, grad_norm: 0.3964944709781511, ic: 0.4392939679552217
Epoch 10: 2022-05-05 06:17:29.959668: train loss: 1.63420138116481
Eval step 0: eval loss: 0.8285090905723129
Eval: 2022-05-05 06:18:02.596428: total loss: 1.0713096431682605, mse:4.717590684082944, ic :0.14362966135528146, sharpe5:12.46683644235134, irr5:418.3614501953125, ndcg5:0.848812176901, pnl5:2.628523111343384 
train 11, step: 0, loss: 1.2654001187282484, grad_norm: 0.02710406463373445, ic: 0.13721996355175428
train 11, step: 500, loss: 0.6965121521377299, grad_norm: 0.01919782117452059, ic: 0.4481127754064783
train 11, step: 1000, loss: 0.9424176322264015, grad_norm: 0.18650188473638885, ic: 0.05863610638111605
train 11, step: 1500, loss: 1.0576632984897547, grad_norm: 0.08091629614220422, ic: 0.1680143182203213
train 11, step: 2000, loss: 0.7848877145969095, grad_norm: 0.004294811240468814, ic: 0.14002209981557084
Epoch 11: 2022-05-05 06:26:01.175654: train loss: 1.63343634234446
Eval step 0: eval loss: 0.8308143414737552
Eval: 2022-05-05 06:26:33.934183: total loss: 1.0721368085977487, mse:4.709423088825389, ic :0.14752564930539888, sharpe5:12.867482331991194, irr5:429.8131103515625, ndcg5:0.8516621212555605, pnl5:3.5707850456237793 
train 12, step: 0, loss: 0.984986941019694, grad_norm: 0.0966863889566063, ic: 0.29404833535210995
train 12, step: 500, loss: 0.9415184227195946, grad_norm: 0.08419939079640765, ic: 0.05465935968943897
train 12, step: 1000, loss: 2.9200282005747416, grad_norm: 0.8283333560866375, ic: 0.4109630206354078
train 12, step: 1500, loss: 0.9281260598640995, grad_norm: 0.1561280622368623, ic: -0.10217581717139973
train 12, step: 2000, loss: 0.8731206334969789, grad_norm: 0.008894293251999388, ic: 0.21166312773868307
Epoch 12: 2022-05-05 06:34:53.911314: train loss: 1.6322219748556364
Eval step 0: eval loss: 0.8288704135520942
Eval: 2022-05-05 06:35:26.956464: total loss: 1.0710363427207903, mse:4.702064417298991, ic :0.15205277370778641, sharpe5:14.103953856825829, irr5:462.4457092285156, ndcg5:0.8459346251358812, pnl5:5.7705512046813965 
train 13, step: 0, loss: 2.079400615430306, grad_norm: 0.897077165880069, ic: 0.3350063905594621
train 13, step: 500, loss: 0.8303305923857435, grad_norm: 0.056032475526754, ic: 0.4916950613713882
train 13, step: 1000, loss: 0.990562162463297, grad_norm: 0.5747588976969198, ic: 0.4537070469411615
train 13, step: 1500, loss: 2.419001725946526, grad_norm: 0.5084337275148345, ic: -0.039360355888296776
train 13, step: 2000, loss: 1.4836442696304848, grad_norm: 0.07451727007927979, ic: 0.18704148082386218
Epoch 13: 2022-05-05 06:43:39.800032: train loss: 1.6313708622112228
Eval step 0: eval loss: 0.8241135302785827
Eval: 2022-05-05 06:44:12.545145: total loss: 1.0711991645213115, mse:4.718807265384515, ic :0.14969642299614497, sharpe5:15.777652110457419, irr5:516.0467529296875, ndcg5:0.8496043622556017, pnl5:4.992067337036133 
train 14, step: 0, loss: 4.478541524473479, grad_norm: 2.5926318952467375, ic: 0.12343841207416181
train 14, step: 500, loss: 0.8274907570001911, grad_norm: 0.008335255454186492, ic: 0.1200448603229646
train 14, step: 1000, loss: 1.8661807211584094, grad_norm: 0.5056824932805986, ic: 0.36243783978745947
train 14, step: 1500, loss: 1.1323486519758144, grad_norm: 0.09903750223353623, ic: -0.06796927541343503
train 14, step: 2000, loss: 1.1552672065327725, grad_norm: 0.47883193032018484, ic: 0.07910452058060563
Epoch 14: 2022-05-05 06:52:20.889126: train loss: 1.628707391705794
Eval step 0: eval loss: 0.8296057296125856
Eval: 2022-05-05 06:52:54.067002: total loss: 1.0712402927710472, mse:4.674130116742637, ic :0.16846385120547627, sharpe5:16.967644629478453, irr5:545.590576171875, ndcg5:0.8645968811570127, pnl5:8.471668243408203 
train 15, step: 0, loss: 3.445481213521401, grad_norm: 2.661606307908383, ic: 0.10839826034776387
train 15, step: 500, loss: 1.2519813340996468, grad_norm: 0.04983745161708185, ic: 0.07224807975824946
train 15, step: 1000, loss: 1.3136028010670733, grad_norm: 0.2639253540678771, ic: 0.06259191913596768
train 15, step: 1500, loss: 0.8493948388287401, grad_norm: 0.23226655132821236, ic: 0.03615392236515272
train 15, step: 2000, loss: 1.4884916787754126, grad_norm: 1.0927342456785427, ic: 0.06794050661092019
Epoch 15: 2022-05-05 07:01:10.428454: train loss: 1.6251398071386975
Eval step 0: eval loss: 0.8339333087048867
Eval: 2022-05-05 07:01:42.593242: total loss: 1.0729507118944026, mse:4.664174608061168, ic :0.1719523139750927, sharpe5:17.150632066726683, irr5:563.5332641601562, ndcg5:0.8436525696666749, pnl5:5.051806449890137 
train 16, step: 0, loss: 0.6802542619259698, grad_norm: 0.5098516271627462, ic: -0.05210955561254179
train 16, step: 500, loss: 1.6037580854618847, grad_norm: 0.9622558217158723, ic: 0.18338180736157217
train 16, step: 1000, loss: 0.8786748712713068, grad_norm: 0.0055901160529564595, ic: -0.0887358316155115
train 16, step: 1500, loss: 0.8258559735766352, grad_norm: 0.2666514493688982, ic: 0.12258920269263085
train 16, step: 2000, loss: 3.338622442067506, grad_norm: 1.8139461335080038, ic: 0.01691144069418203
Epoch 16: 2022-05-05 07:09:52.845506: train loss: 1.6238048616968799
Eval step 0: eval loss: 0.8231589635883166
Eval: 2022-05-05 07:10:26.041751: total loss: 1.069170624903531, mse:4.659510867706568, ic :0.17282698572481384, sharpe5:17.382506836652755, irr5:575.5740356445312, ndcg5:0.8497107815671389, pnl5:10.678717613220215 
train 17, step: 0, loss: 1.2839079596319627, grad_norm: 0.5512305616371839, ic: -0.12046834896722691
train 17, step: 500, loss: 1.7597314850101626, grad_norm: 0.9211153445451655, ic: 0.1910948284158059
train 17, step: 1000, loss: 1.2836814386847164, grad_norm: 0.24898209342716082, ic: 0.14045685757639315
train 17, step: 1500, loss: 4.507104294865907, grad_norm: 1.6915521166029457, ic: 0.20698596166153152
train 17, step: 2000, loss: 1.2955534941825777, grad_norm: 1.2604801359668691, ic: 0.10402519589562485
Epoch 17: 2022-05-05 07:18:49.279308: train loss: 1.621759205118255
Eval step 0: eval loss: 0.8295380699873222
Eval: 2022-05-05 07:19:22.607226: total loss: 1.0690335562007534, mse:4.616732832631583, ic :0.18423669049576857, sharpe5:17.367381484508513, irr5:576.2024536132812, ndcg5:0.8612204858587649, pnl5:5.749947547912598 
train 18, step: 0, loss: 1.4239963918666405, grad_norm: 1.2552873813561742, ic: 0.16626349080123431
train 18, step: 500, loss: 1.4667679091631356, grad_norm: 1.8369385214388154, ic: 0.02763446538245099
train 18, step: 1000, loss: 0.656658015839041, grad_norm: 0.015096088458109402, ic: 0.576372045477422
train 18, step: 1500, loss: 1.4189863025312246, grad_norm: 0.1240390890534057, ic: 0.1828037464574345
train 18, step: 2000, loss: 0.9135893803493232, grad_norm: 0.09467721409417071, ic: -0.02970749290180528
Epoch 18: 2022-05-05 07:27:43.625865: train loss: 1.6204531484537146
Eval step 0: eval loss: 0.8208946814903846
Eval: 2022-05-05 07:28:17.186116: total loss: 1.0665260633607616, mse:4.6249517106760525, ic :0.18460153513328267, sharpe5:17.593474756479264, irr5:583.0647583007812, ndcg5:0.8324642183393776, pnl5:7.0768561363220215 
train 19, step: 0, loss: 1.5336786179315476, grad_norm: 1.3828667104448709, ic: 0.04901981084035045
train 19, step: 500, loss: 0.8537987603081597, grad_norm: 0.019470212576922597, ic: 0.24031049861369705
train 19, step: 1000, loss: 0.9611924851402335, grad_norm: 0.3442826808179437, ic: 0.20723222315468726
train 19, step: 1500, loss: 3.954966949226985, grad_norm: 1.8979415591510311, ic: 0.15162902066451278
train 19, step: 2000, loss: 1.0254860276442308, grad_norm: 0.36482435273087316, ic: 0.14438058770766968
Epoch 19: 2022-05-05 07:36:31.807767: train loss: 1.619660223445769
Eval step 0: eval loss: 0.8237459044059535
Eval: 2022-05-05 07:37:04.109505: total loss: 1.067001527113381, mse:4.592200000055146, ic :0.18930268456403931, sharpe5:17.558674149513244, irr5:586.081787109375, ndcg5:0.842392655021428, pnl5:18.28931999206543 
train 20, step: 0, loss: 2.3078453094120555, grad_norm: 1.9659679019627438, ic: 0.03964434870548994
train 20, step: 500, loss: 3.2445390625, grad_norm: 1.6408582767910542, ic: 0.09603909491918497
train 20, step: 1000, loss: 0.9710062980651856, grad_norm: 0.09724608591451112, ic: 0.17447240649340356
train 20, step: 1500, loss: 1.7441540845137826, grad_norm: 5.351878633947305, ic: 0.2594083252407078
train 20, step: 2000, loss: 1.034640985834268, grad_norm: 0.10868991916953509, ic: 0.014754381794444405
Epoch 20: 2022-05-05 07:45:15.760797: train loss: 1.6175078208796523
Eval step 0: eval loss: 0.828522403825573
Eval: 2022-05-05 07:45:47.792228: total loss: 1.0667295046529508, mse:4.587668380643661, ic :0.1908222473920502, sharpe5:17.92964540362358, irr5:595.3964233398438, ndcg5:0.8671177460541755, pnl5:7.3266425132751465 
train 21, step: 0, loss: 1.0014140043978834, grad_norm: 0.3971521401967165, ic: 0.07392578822299882
train 21, step: 500, loss: 0.762736092626521, grad_norm: 0.015717242166885685, ic: 0.2055911221913635
train 21, step: 1000, loss: 0.9555064418859649, grad_norm: 2.595261978567692, ic: 0.17510990790056055
train 21, step: 1500, loss: 0.9912636676565875, grad_norm: 0.5804919206770023, ic: 0.3054698468398756
train 21, step: 2000, loss: 0.9383197501384274, grad_norm: 0.2052162960851096, ic: 0.051586408468599805
Epoch 21: 2022-05-05 07:54:06.140120: train loss: 1.6176836203083407
Eval step 0: eval loss: 0.8193427549147128
Eval: 2022-05-05 07:54:38.100886: total loss: 1.0679728648770057, mse:4.6095802454500765, ic :0.1851335595755694, sharpe5:17.845149643421173, irr5:587.005126953125, ndcg5:0.8678692844074617, pnl5:15.020169258117676 
train 22, step: 0, loss: 1.0372805837857522, grad_norm: 0.03897075573491512, ic: 0.2321400279090124
train 22, step: 500, loss: 3.2743525311229673, grad_norm: 2.5046099256384826, ic: -0.1874196063182904
train 22, step: 1000, loss: 1.1867094359645955, grad_norm: 0.02346412251048861, ic: 0.46673910348156217
train 22, step: 1500, loss: 0.9682831187307098, grad_norm: 0.25139459481072723, ic: 0.12867903080644483
train 22, step: 2000, loss: 1.7555377294146826, grad_norm: 3.4590705788650458, ic: 0.1953813655964454
Epoch 22: 2022-05-05 08:02:45.471543: train loss: 1.6172824796498497
Eval step 0: eval loss: 0.8205880907740055
Eval: 2022-05-05 08:03:17.666610: total loss: 1.066851762710142, mse:4.604409126899732, ic :0.18866171493798722, sharpe5:17.632579325437543, irr5:582.3302612304688, ndcg5:0.8260618364748212, pnl5:11.814781188964844 
train 23, step: 0, loss: 0.9693415661023055, grad_norm: 0.07063188842410306, ic: 0.21212900976126647
train 23, step: 500, loss: 1.4278049918134321, grad_norm: 0.2504281425532454, ic: 0.05355398837031815
train 23, step: 1000, loss: 1.6498411051432293, grad_norm: 0.13063088045330798, ic: 0.2524967094154791
train 23, step: 1500, loss: 1.146624901069647, grad_norm: 1.804014755449698, ic: 0.09457144379809518
train 23, step: 2000, loss: 1.831250150356043, grad_norm: 9.779695615648574, ic: 0.4410350890514085
Epoch 23: 2022-05-05 08:11:34.398121: train loss: 1.6161342308175675
Eval step 0: eval loss: 0.82412330619402
Eval: 2022-05-05 08:12:08.076896: total loss: 1.0667436389295417, mse:4.59577939998073, ic :0.18402075299156925, sharpe5:17.467993927001952, irr5:558.6300659179688, ndcg5:0.8442081660719459, pnl5:8.659895896911621 
train 24, step: 0, loss: 2.1954318251030736, grad_norm: 0.13675190991442437, ic: 0.13686227894947406
train 24, step: 500, loss: 1.2241184338521403, grad_norm: 0.20947562553690285, ic: 0.06707011735413031
train 24, step: 1000, loss: 0.9060660290766375, grad_norm: 0.06247843219673711, ic: 0.5234173039957077
train 24, step: 1500, loss: 2.635028483743817, grad_norm: 6.580404340651234, ic: 0.052824089911509636
train 24, step: 2000, loss: 0.9356029855592909, grad_norm: 0.2848038690249945, ic: 0.09910870267860891
Epoch 24: 2022-05-05 08:20:25.569018: train loss: 1.6137908359500714
Eval step 0: eval loss: 0.8204031844787275
Eval: 2022-05-05 08:20:58.348187: total loss: 1.0703845502671738, mse:4.6276193230572105, ic :0.18394666560192494, sharpe5:16.963597959280012, irr5:566.360107421875, ndcg5:0.848963899042812, pnl5:6.946761131286621 
train 25, step: 0, loss: 0.8245489996832771, grad_norm: 0.07847170038641696, ic: 0.6231643116611155
train 25, step: 500, loss: 0.8752585235319109, grad_norm: 0.010687542061704167, ic: 0.14262612388674498
train 25, step: 1000, loss: 2.0840663750119, grad_norm: 0.06682869678908063, ic: 0.26111535774923234
train 25, step: 1500, loss: 1.128662109375, grad_norm: 0.7498210394272509, ic: 0.5337726805198548
train 25, step: 2000, loss: 1.007760656119882, grad_norm: 0.5178862917729954, ic: 0.6046648385026062
Epoch 25: 2022-05-05 08:29:26.588532: train loss: 1.6152131288139517
Eval step 0: eval loss: 0.8200200586349117
Eval: 2022-05-05 08:29:59.453255: total loss: 1.0651631038986764, mse:4.587107043815315, ic :0.19477463541592063, sharpe5:17.836677932739256, irr5:593.1997680664062, ndcg5:0.8385495396741846, pnl5:10.860654830932617 
train 26, step: 0, loss: 6.637647732378195, grad_norm: 10.154061275294211, ic: 0.1872608534206479
train 26, step: 500, loss: 3.977854402853792, grad_norm: 10.769714471866571, ic: 0.3778619148246185
train 26, step: 1000, loss: 1.2940610589187598, grad_norm: 1.9509148330630297, ic: 0.006537881578077477
train 26, step: 1500, loss: 0.8344262289294107, grad_norm: 0.3877403328818372, ic: 0.31339613834292057
train 26, step: 2000, loss: 0.9631121955829564, grad_norm: 1.1811273427247082, ic: 0.12575951824716636
Epoch 26: 2022-05-05 08:38:17.516781: train loss: 1.6152103812541128
Eval step 0: eval loss: 0.8208331961275026
Eval: 2022-05-05 08:38:50.470864: total loss: 1.0653590166198739, mse:4.594587398474978, ic :0.18866595076131687, sharpe5:17.682628254890442, irr5:584.1968994140625, ndcg5:0.8537014271529497, pnl5:7.97645378112793 
train 27, step: 0, loss: 0.8268292356004902, grad_norm: 0.026532342962632766, ic: 0.12925953120717468
train 27, step: 500, loss: 0.9201107167004733, grad_norm: 5.618857254126686, ic: 0.28264322495131544
train 27, step: 1000, loss: 0.7506472653228925, grad_norm: 0.7228918587445595, ic: 0.1934691491493755
train 27, step: 1500, loss: 0.6397384959024756, grad_norm: 0.08882576198548292, ic: 0.5201747118379176
train 27, step: 2000, loss: 1.3926743353146356, grad_norm: 0.10486723126269183, ic: -0.07078779895756789
Epoch 27: 2022-05-05 08:47:03.418988: train loss: 1.6151789782009924
Eval step 0: eval loss: 0.8233043160152134
Eval: 2022-05-05 08:47:36.395663: total loss: 1.0660858664238384, mse:4.596586026189197, ic :0.18771203724210503, sharpe5:17.41530775666237, irr5:573.6851806640625, ndcg5:0.8369610302676377, pnl5:8.525466918945312 
train 28, step: 0, loss: 1.5650030541143822, grad_norm: 2.7676257069654104, ic: 0.16094927491652725
train 28, step: 500, loss: 1.413162359383734, grad_norm: 4.621152125813606, ic: 0.16747659459330866
train 28, step: 1000, loss: 0.915502532710874, grad_norm: 0.29644076056143825, ic: 0.5900312119219002
train 28, step: 1500, loss: 1.0317101111778846, grad_norm: 0.04076301893823314, ic: 0.024768886815325367
train 28, step: 2000, loss: 1.048726742984327, grad_norm: 1.1622901680736972, ic: 0.07470146582939229
Epoch 28: 2022-05-05 08:55:53.717234: train loss: 1.6137514844659984
Eval step 0: eval loss: 0.8199328471787737
Eval: 2022-05-05 08:56:26.630387: total loss: 1.0765586001776328, mse:4.686091260692759, ic :0.18366485872135313, sharpe5:17.777907568216325, irr5:592.8876953125, ndcg5:0.8469675103638747, pnl5:13.78686809539795 
train 29, step: 0, loss: 0.9078168312286771, grad_norm: 0.19625820728567922, ic: 0.10825347166012247
train 29, step: 500, loss: 1.1014468808490532, grad_norm: 0.09702101847041711, ic: 0.615463294046036
train 29, step: 1000, loss: 1.0634715035614928, grad_norm: 2.9262981116730096, ic: 0.10371812288016447
train 29, step: 1500, loss: 2.4309608048521665, grad_norm: 6.199866057405799, ic: 0.011137651982569321
train 29, step: 2000, loss: 4.006483289930555, grad_norm: 19.820131494515465, ic: 0.23233705725931475
Epoch 29: 2022-05-05 09:04:51.070258: train loss: 1.6125753455694802
Eval step 0: eval loss: 0.8220414220972734
Eval: 2022-05-05 09:05:24.017972: total loss: 1.0647115878938975, mse:4.586037427985899, ic :0.19444523926386456, sharpe5:17.80323189139366, irr5:592.576904296875, ndcg5:0.8443777476332551, pnl5:9.415663719177246 
train 30, step: 0, loss: 1.0150453001167217, grad_norm: 0.3681109183787643, ic: 0.5043341155481936
train 30, step: 500, loss: 1.4209147602735337, grad_norm: 3.990262265276926, ic: 0.1234730192293569
train 30, step: 1000, loss: 0.9796376546223958, grad_norm: 0.25694352597764214, ic: 0.01220020399638955
train 30, step: 1500, loss: 1.4985979240593197, grad_norm: 12.041432779581626, ic: 0.1556238991364289
train 30, step: 2000, loss: 1.8490209718104438, grad_norm: 1.6350530772009486, ic: 0.08573555269156818
Epoch 30: 2022-05-05 09:13:39.683709: train loss: 1.614395851624614
Eval step 0: eval loss: 0.8194957608543532
Eval: 2022-05-05 09:14:13.135117: total loss: 1.0650368293579013, mse:4.597895094736536, ic :0.19454029785137014, sharpe5:18.277021486759185, irr5:613.3225708007812, ndcg5:0.846191852153997, pnl5:9.301119804382324 
train 31, step: 0, loss: 1.0292788673468303, grad_norm: 0.9915043025870696, ic: 0.3826909008309428
train 31, step: 500, loss: 1.5020328976980453, grad_norm: 2.699877820421269, ic: 0.03203024279242996
train 31, step: 1000, loss: 4.501370694281811, grad_norm: 14.1852436754663, ic: 0.48142647733311317
train 31, step: 1500, loss: 0.7646088581110603, grad_norm: 0.033510881350125477, ic: 0.7134495046679132
train 31, step: 2000, loss: 1.2677973135626426, grad_norm: 9.123543889245923, ic: 0.14577296562586908
Epoch 31: 2022-05-05 09:22:28.580737: train loss: 1.6089009446376095
Eval step 0: eval loss: 0.8239367920179136
Eval: 2022-05-05 09:23:01.131908: total loss: 1.0643706439510585, mse:4.5849380081345465, ic :0.1911335339701555, sharpe5:17.88131805896759, irr5:589.632080078125, ndcg5:0.8528018590605831, pnl5:9.589289665222168 
train 32, step: 0, loss: 1.1292792161925176, grad_norm: 0.1962832451363425, ic: 0.18770804996897764
train 32, step: 500, loss: 1.4961856391486221, grad_norm: 6.224676709288634, ic: 0.020127025345623696
train 32, step: 1000, loss: 1.0441476564914245, grad_norm: 0.23501419376483534, ic: 0.5043958727107467
train 32, step: 1500, loss: 0.9625891271572273, grad_norm: 3.988951955222035, ic: 0.06209945491761785
train 32, step: 2000, loss: 0.9414600934752854, grad_norm: 0.08767885782991096, ic: 0.5583389125988306
Epoch 32: 2022-05-05 09:31:05.520224: train loss: 1.6121382416424328
Eval step 0: eval loss: 0.8185545717325803
Eval: 2022-05-05 09:31:38.955689: total loss: 1.0654481645654406, mse:4.60484616704601, ic :0.19300578676294133, sharpe5:17.731025598049165, irr5:595.7841186523438, ndcg5:0.8587294631024035, pnl5:9.11232852935791 
train 33, step: 0, loss: 1.2792055639455093, grad_norm: 2.355378337128878, ic: 0.17814220911977946
train 33, step: 500, loss: 0.9920974160260695, grad_norm: 0.03652551742194457, ic: 0.1585929553736033
train 33, step: 1000, loss: 1.0187462319131833, grad_norm: 11.917876486927247, ic: 0.23624734333708702
train 33, step: 1500, loss: 0.8754488943876039, grad_norm: 0.11056467943236781, ic: 0.560095232920268
train 33, step: 2000, loss: 0.8062437513928784, grad_norm: 0.161710007112781, ic: 0.27375164056919765
Epoch 33: 2022-05-05 09:39:51.359712: train loss: 1.6111216818882683
Eval step 0: eval loss: 0.8204893668911354
Eval: 2022-05-05 09:40:23.620880: total loss: 1.065596969861655, mse:4.584302994217969, ic :0.19446417749672198, sharpe5:18.218317337036133, irr5:602.2147827148438, ndcg5:0.8454688005546229, pnl5:7.642181396484375 
train 34, step: 0, loss: 0.9921161723087265, grad_norm: 0.6390681193481627, ic: 0.6083485331211762
train 34, step: 500, loss: 0.7941197386575402, grad_norm: 1.2094485841111715, ic: 0.2949417440776517
train 34, step: 1000, loss: 3.148805398545507, grad_norm: 5.713832478856988, ic: 0.35537545941103643
train 34, step: 1500, loss: 0.8273762826839988, grad_norm: 1.1812810101938627, ic: 0.6968892215043899
train 34, step: 2000, loss: 5.454211246127033, grad_norm: 41.654987444348166, ic: 0.46003272712739285
Epoch 34: 2022-05-05 09:48:38.921749: train loss: 1.6086532940656402
Eval step 0: eval loss: 0.8186409470906875
Eval: 2022-05-05 09:49:11.742536: total loss: 1.0666322708330391, mse:4.612765430320709, ic :0.19529056924523117, sharpe5:18.128320072889327, irr5:611.0243530273438, ndcg5:0.8551490752707445, pnl5:7.948591709136963 
train 35, step: 0, loss: 1.1602192239200366, grad_norm: 0.8253270285789298, ic: 0.5483688875933771
train 35, step: 500, loss: 1.1961573104047425, grad_norm: 2.894226627264792, ic: 0.08637523359167643
train 35, step: 1000, loss: 1.8285150548201963, grad_norm: 16.551553639403615, ic: 0.06254930204252994
train 35, step: 1500, loss: 1.5919301427396617, grad_norm: 4.455101242794837, ic: 0.07871431470795852
train 35, step: 2000, loss: 0.773904402625752, grad_norm: 0.1248220685451709, ic: 0.5756886565099468
Epoch 35: 2022-05-05 09:57:30.179195: train loss: 1.6118089025197884
Eval step 0: eval loss: 0.8239371779093124
Eval: 2022-05-05 09:58:02.929231: total loss: 1.065228073117835, mse:4.58675915495371, ic :0.1918910199830646, sharpe5:17.806147234439848, irr5:590.367431640625, ndcg5:0.8585638126793905, pnl5:5.95632266998291 
train 36, step: 0, loss: 1.864240182250785, grad_norm: 10.116103681538693, ic: 0.13322718117198262
train 36, step: 500, loss: 0.8373896461288417, grad_norm: 0.17165643031489702, ic: 0.13102556725587156
train 36, step: 1000, loss: 1.7601052911931818, grad_norm: 38.633897586873196, ic: 0.2659703191877344
train 36, step: 1500, loss: 0.7604602915346815, grad_norm: 0.10154082716209989, ic: 0.4040911879685161
train 36, step: 2000, loss: 1.1318802473751488, grad_norm: 7.481893616275286, ic: 0.7673342467908507
Epoch 36: 2022-05-05 10:06:15.250705: train loss: 1.6065226841466964
Eval step 0: eval loss: 0.823076061252799
Eval: 2022-05-05 10:06:49.152241: total loss: 1.0654711813052713, mse:4.586997432962249, ic :0.1918470089585227, sharpe5:17.83862464785576, irr5:591.0534057617188, ndcg5:0.8514399058043695, pnl5:6.4102702140808105 
train 37, step: 0, loss: 1.9861873609183576, grad_norm: 14.37868611393628, ic: 0.17872321329613144
train 37, step: 500, loss: 2.348644362382749, grad_norm: 9.791486580574496, ic: -0.039078751207112315
train 37, step: 1000, loss: 1.066155234791191, grad_norm: 0.885310529631126, ic: 0.04900942292107457
train 37, step: 1500, loss: 2.010576999730181, grad_norm: 10.37933950013975, ic: 0.610835324664129
train 37, step: 2000, loss: 1.3162151349343334, grad_norm: 1.2971722372354775, ic: 0.15775493451659242
Epoch 37: 2022-05-05 10:14:27.143310: train loss: 1.6066829093040285
Eval step 0: eval loss: 0.8203095414992755
Eval: 2022-05-05 10:14:53.739688: total loss: 1.0663727883693945, mse:4.605936441797962, ic :0.19362228293363845, sharpe5:17.939715728759765, irr5:598.1016845703125, ndcg5:0.8370052503803956, pnl5:5.764917373657227 
train 38, step: 0, loss: 1.3307297869426449, grad_norm: 1.8991507473873428, ic: -0.07083639704291149
train 38, step: 500, loss: 0.9047446922019676, grad_norm: 0.1996739384430021, ic: 0.265297996101834
train 38, step: 1000, loss: 0.9224777281991107, grad_norm: 0.289637954882666, ic: 0.13768213197096768
train 38, step: 1500, loss: 0.9529898606563212, grad_norm: 0.10571915134267505, ic: 0.2016134904359279
