Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0001, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
832
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795775508209538, grad_norm: 4.816644294952649, ic: 0.02281513316069529
train 0, step: 500, loss: 0.8600190197873493, grad_norm: 0.03506950938422175, ic: 0.04586377011174172
train 0, step: 1000, loss: 1.9660215901037859, grad_norm: 0.8909643779227973, ic: 0.03330555094931244
train 0, step: 1500, loss: 0.9542279752346838, grad_norm: 0.06415675409314335, ic: 0.03355429852470855
train 0, step: 2000, loss: 1.0008678504938355, grad_norm: 0.22382601694455445, ic: 0.06336782805778617
Epoch 0: 2022-05-04 21:23:43.448014: train loss: 1.6482327097106744
Eval step 0: eval loss: 0.8358405126284246
Eval: 2022-05-04 21:24:27.853340: total loss: 1.0793854385021808, mse:4.8229034554318, ic :0.007435240572592474, sharpe5:8.08522435426712, irr5:231.1058349609375, ndcg5:0.8491831356316476, pnl5:2.844299077987671 
train 1, step: 0, loss: 2.7868565713205644, grad_norm: 1.1577289453975006, ic: 0.061462667639443753
train 1, step: 500, loss: 1.7618330612806798, grad_norm: 1.0877510171477924, ic: 0.09097358096508987
train 1, step: 1000, loss: 0.8790297241879449, grad_norm: 0.22876747575468254, ic: 0.08360118985712083
train 1, step: 1500, loss: 1.7139783135775861, grad_norm: 0.263768615049782, ic: -0.038119374715034654
train 1, step: 2000, loss: 2.1773226562500003, grad_norm: 1.1672014486586249, ic: -0.03905064864708772
Epoch 1: 2022-05-04 21:36:33.504965: train loss: 1.6464589443583202
Eval step 0: eval loss: 0.8339881052835221
Eval: 2022-05-04 21:37:19.507114: total loss: 1.0793614829940394, mse:4.825511385720753, ic :0.007489089420952881, sharpe5:8.114706966876984, irr5:231.91842651367188, ndcg5:0.8476585684907827, pnl5:2.7085797786712646 
train 2, step: 0, loss: 2.1420973011363635, grad_norm: 0.016362905680155127, ic: 0.13321938256162613
train 2, step: 500, loss: 3.309959256406495, grad_norm: 0.40840011090381767, ic: 0.07877398145407742
train 2, step: 1000, loss: 2.0707070536997128, grad_norm: 0.0009042457017570419, ic: 0.19216040560651862
train 2, step: 1500, loss: 1.4905168140207539, grad_norm: 0.08071852614560401, ic: -0.038121656216868686
train 2, step: 2000, loss: 3.250291466346154, grad_norm: 1.1018063652180392, ic: 0.170573217458872
Epoch 2: 2022-05-04 21:49:26.875869: train loss: 1.646343915175201
Eval step 0: eval loss: 0.8354487042281348
Eval: 2022-05-04 21:50:11.940213: total loss: 1.0798156567938868, mse:4.8248374870176525, ic :0.0093649055170814, sharpe5:7.928938035070896, irr5:226.48834228515625, ndcg5:0.8393886511229766, pnl5:2.9604246616363525 
train 3, step: 0, loss: 1.522660021277947, grad_norm: 0.6972248632001943, ic: -0.002730269189083338
train 3, step: 500, loss: 1.510660647972543, grad_norm: 0.4452279526733687, ic: 0.10217201420114685
train 3, step: 1000, loss: 3.6791247256404724, grad_norm: 0.9223094680420594, ic: -0.05679629454280201
train 3, step: 1500, loss: 1.9720439005600452, grad_norm: 1.3480857998989029, ic: -0.057458284794531536
train 3, step: 2000, loss: 0.8979610958614865, grad_norm: 0.0013374297719449097, ic: 0.018360418914603962
Epoch 3: 2022-05-04 22:02:24.032739: train loss: 1.6462414133387686
Eval step 0: eval loss: 0.8340425159707586
Eval: 2022-05-04 22:03:14.441614: total loss: 1.079762399970212, mse:4.827551781383232, ic :0.008806824526934409, sharpe5:7.779530014693736, irr5:220.38510131835938, ndcg5:0.8394682336925918, pnl5:2.3739616870880127 
train 4, step: 0, loss: 1.430803770727041, grad_norm: 0.0567299502659461, ic: 0.14216495002741147
train 4, step: 500, loss: 1.6596129890501967, grad_norm: 0.752115215531673, ic: 0.024410404247627367
train 4, step: 1000, loss: 2.9696037478563264, grad_norm: 0.8908058234283636, ic: 0.016117423431221543
train 4, step: 1500, loss: 2.1557606886207807, grad_norm: 0.616155189341254, ic: -0.030413288136704665
train 4, step: 2000, loss: 1.0829715521116836, grad_norm: 0.4767372464370272, ic: 0.14984599135716395
Epoch 4: 2022-05-04 22:15:03.740001: train loss: 1.6457580216179786
Eval step 0: eval loss: 0.8427611532904702
Eval: 2022-05-04 22:15:47.683747: total loss: 1.0821777789922173, mse:4.825142508415919, ic :0.016363031282493123, sharpe5:7.962809270024299, irr5:230.22518920898438, ndcg5:0.8381777243909788, pnl5:2.567824363708496 
train 5, step: 0, loss: 1.3372833866401006, grad_norm: 0.12298844895111562, ic: 0.057422088251519794
train 5, step: 500, loss: 0.8885529992317605, grad_norm: 0.00759255240895209, ic: 0.036302233277659135
train 5, step: 1000, loss: 0.9806933406669062, grad_norm: 0.18373258433592515, ic: -0.0340375678429574
train 5, step: 1500, loss: 1.5360371938708366, grad_norm: 0.1939908276228567, ic: 0.03337961593913653
train 5, step: 2000, loss: 1.1072719069478683, grad_norm: 0.03311430185152375, ic: 0.09586266037706104
Epoch 5: 2022-05-04 22:28:13.289910: train loss: 1.6453940714244626
Eval step 0: eval loss: 0.8385209142847734
Eval: 2022-05-04 22:28:59.364991: total loss: 1.0806215643739219, mse:4.8276491534954395, ic :0.03465749062393105, sharpe5:9.776511150598525, irr5:273.9793701171875, ndcg5:0.8401702985365644, pnl5:4.034184455871582 
train 6, step: 0, loss: 1.3421542275655403, grad_norm: 0.5217837046238387, ic: 0.040150781529690295
train 6, step: 500, loss: 1.0086948459035223, grad_norm: 0.0564709594866488, ic: 0.04591601521163136
train 6, step: 1000, loss: 1.121224917567728, grad_norm: 0.11142565152768774, ic: 0.09460121840215781
train 6, step: 1500, loss: 1.5639829020854854, grad_norm: 0.8548702338025415, ic: 0.16541679539956114
train 6, step: 2000, loss: 0.8115951745771617, grad_norm: 0.05935153942106599, ic: 0.023956680814891676
Epoch 6: 2022-05-04 22:41:12.066708: train loss: 1.6438871024116328
Eval step 0: eval loss: 0.8348814438718387
Eval: 2022-05-04 22:41:57.866806: total loss: 1.0780406494198913, mse:4.822938222851377, ic :0.043037418601242834, sharpe5:11.280235067009926, irr5:321.0423889160156, ndcg5:0.8475384204750616, pnl5:3.1676185131073 
train 7, step: 0, loss: 0.9929843902587892, grad_norm: 0.0642572652002666, ic: 0.07307206942000648
train 7, step: 500, loss: 0.6471969465354294, grad_norm: 0.0021651202300395542, ic: 0.07447478671678752
train 7, step: 1000, loss: 1.0331356283915651, grad_norm: 0.251878878685046, ic: 0.09822981982092681
train 7, step: 1500, loss: 2.267430018254287, grad_norm: 0.782331551356154, ic: 0.24401834060284686
train 7, step: 2000, loss: 0.9118897864814722, grad_norm: 0.05221007476289067, ic: -0.0672642036959897
Epoch 7: 2022-05-04 22:53:29.399576: train loss: 1.6422841785334399
Eval step 0: eval loss: 0.8378835503243546
Eval: 2022-05-04 22:54:15.399284: total loss: 1.0773136354733348, mse:4.7720569278000955, ic :0.10638226313693044, sharpe5:11.653186143636702, irr5:405.3675231933594, ndcg5:0.8636753520591989, pnl5:2.9112987518310547 
train 8, step: 0, loss: 3.6347995923913046, grad_norm: 1.343943961453219, ic: -0.029480266063660547
train 8, step: 500, loss: 2.7521393842610182, grad_norm: 1.1320641476499604, ic: 0.02323748433266886
train 8, step: 1000, loss: 3.114997310914855, grad_norm: 1.2926720046383324, ic: 0.07779184880143039
train 8, step: 1500, loss: 0.7242689026857275, grad_norm: 0.004366362028819264, ic: 0.40591548720316956
train 8, step: 2000, loss: 1.0877090340512563, grad_norm: 0.43735613382795835, ic: 0.4920964812825473
Epoch 8: 2022-05-04 23:06:33.994358: train loss: 1.6366302639587158
Eval step 0: eval loss: 0.8279669774721746
Eval: 2022-05-04 23:07:15.389710: total loss: 1.0738451336318844, mse:4.74108566352262, ic :0.13341554470595968, sharpe5:12.314755235910415, irr5:412.0758361816406, ndcg5:0.8414683186026795, pnl5:3.6302330493927 
train 9, step: 0, loss: 5.4351851419208534, grad_norm: 1.069593289961874, ic: 0.13881281966309061
train 9, step: 500, loss: 1.3686565156677206, grad_norm: 1.2837224571811223, ic: 0.31182350228222144
train 9, step: 1000, loss: 0.9241541469429905, grad_norm: 0.010709708069063132, ic: 0.07851570540243274
train 9, step: 1500, loss: 1.0976620928352816, grad_norm: 0.03421896385114746, ic: 0.3991674087892012
train 9, step: 2000, loss: 1.0782580475616972, grad_norm: 0.27785845690221944, ic: 0.18866307710575353
Epoch 9: 2022-05-04 23:19:10.753706: train loss: 1.633415888114443
Eval step 0: eval loss: 0.8278680606436051
Eval: 2022-05-04 23:19:53.912431: total loss: 1.0727916641688375, mse:4.718055394678493, ic :0.14515789858566516, sharpe5:15.370641086697578, irr5:495.1641845703125, ndcg5:0.8468625721685132, pnl5:3.612626075744629 
train 10, step: 0, loss: 7.096059015123907, grad_norm: 1.5650345103982575, ic: 0.21277981389138004
train 10, step: 500, loss: 1.141958670863177, grad_norm: 0.09399946023902776, ic: -0.034518773855895746
train 10, step: 1000, loss: 2.3634900052123275, grad_norm: 0.9611591447677211, ic: -0.025461870825578607
train 10, step: 1500, loss: 1.1147501561548803, grad_norm: 0.5326969251218985, ic: -0.025009275473232312
train 10, step: 2000, loss: 2.7357682341137917, grad_norm: 0.7798922281368255, ic: 0.451886818227593
Epoch 10: 2022-05-04 23:32:27.851165: train loss: 1.631284657722122
Eval step 0: eval loss: 0.8252965446769626
Eval: 2022-05-04 23:33:12.128381: total loss: 1.0707616432851959, mse:4.698180866694791, ic :0.15968665921402078, sharpe5:17.903817847967147, irr5:567.3308715820312, ndcg5:0.8454192764193061, pnl5:7.204148769378662 
train 11, step: 0, loss: 1.2471177056143659, grad_norm: 0.014303093680118374, ic: 0.2094063175388028
train 11, step: 500, loss: 0.6694149570610687, grad_norm: 0.02853583744807553, ic: 0.5191928282401537
train 11, step: 1000, loss: 0.9522985849907255, grad_norm: 0.24037375530769123, ic: 0.045843392276869864
train 11, step: 1500, loss: 1.0495951468484443, grad_norm: 0.07534242502009411, ic: 0.18603084822789226
train 11, step: 2000, loss: 0.7863255601735288, grad_norm: 0.03220136037533207, ic: 0.10749586029538509
Epoch 11: 2022-05-04 23:45:09.648176: train loss: 1.6273271852589337
Eval step 0: eval loss: 0.8270519646502897
Eval: 2022-05-04 23:45:58.034315: total loss: 1.0712601237767712, mse:4.682095240079223, ic :0.1660835158286563, sharpe5:16.963700119256973, irr5:545.4929809570312, ndcg5:0.8441343936241078, pnl5:5.7686448097229 
train 12, step: 0, loss: 0.9581917921702067, grad_norm: 0.129602283384767, ic: 0.3929316955272718
train 12, step: 500, loss: 0.9278865379939599, grad_norm: 0.12354108014203695, ic: 0.17468504326848255
train 12, step: 1000, loss: 2.921572545531449, grad_norm: 0.688100611595138, ic: 0.3928828344462896
train 12, step: 1500, loss: 0.9138629852318165, grad_norm: 0.15333131682547757, ic: -0.07256498520114056
train 12, step: 2000, loss: 0.8738261335205816, grad_norm: 0.013063030866244725, ic: 0.2187976297092742
Epoch 12: 2022-05-04 23:58:28.089935: train loss: 1.6250137231821684
Eval step 0: eval loss: 0.8253239429662802
Eval: 2022-05-04 23:59:12.802775: total loss: 1.0696785491462586, mse:4.672693815936765, ic :0.17158428490637886, sharpe5:17.174991545677184, irr5:547.7557373046875, ndcg5:0.8439221068757254, pnl5:4.3941330909729 
train 13, step: 0, loss: 2.074552219797716, grad_norm: 1.2321266007231464, ic: 0.33700087584071536
train 13, step: 500, loss: 0.8257983365130457, grad_norm: 0.09429878148284615, ic: 0.5254520918779281
train 13, step: 1000, loss: 0.971106694368708, grad_norm: 0.7143917957821877, ic: 0.4708226975327744
train 13, step: 1500, loss: 2.4291193632294106, grad_norm: 0.5935368349450167, ic: -0.05443655908115189
train 13, step: 2000, loss: 1.4642787871680139, grad_norm: 0.10782508324695068, ic: 0.22500056988016293
Epoch 13: 2022-05-05 00:11:16.088475: train loss: 1.624463007387837
Eval step 0: eval loss: 0.8214721036535167
Eval: 2022-05-05 00:12:00.585608: total loss: 1.0709057404171547, mse:4.695014262375113, ic :0.16492423147378493, sharpe5:17.552342122793196, irr5:558.9610595703125, ndcg5:0.8459086899080728, pnl5:8.74341106414795 
train 14, step: 0, loss: 4.490505938596919, grad_norm: 2.6116728656074963, ic: 0.17407058784371698
train 14, step: 500, loss: 0.8292307255829511, grad_norm: 0.011358063712201138, ic: 0.10499477376377764
train 14, step: 1000, loss: 1.8184806111190206, grad_norm: 0.45273013574205745, ic: 0.3945304339381337
train 14, step: 1500, loss: 1.133081936199961, grad_norm: 0.11793090824137403, ic: -0.06988061732477366
train 14, step: 2000, loss: 1.161155753788428, grad_norm: 0.5111463935664887, ic: 0.06684949381931515
Epoch 14: 2022-05-05 00:24:07.105031: train loss: 1.6234340458767402
Eval step 0: eval loss: 0.8314300312006059
Eval: 2022-05-05 00:24:58.169079: total loss: 1.070688023609807, mse:4.664060679207131, ic :0.17163302724957466, sharpe5:16.56293031334877, irr5:528.4815673828125, ndcg5:0.840813472972452, pnl5:6.527141094207764 
train 15, step: 0, loss: 3.457240621960117, grad_norm: 2.5647919283537606, ic: 0.11359640816855433
train 15, step: 500, loss: 1.2570942841464048, grad_norm: 0.1890425547930742, ic: 0.0782646534297311
train 15, step: 1000, loss: 1.3146788062118901, grad_norm: 0.24974346978988954, ic: 0.05451404985019939
train 15, step: 1500, loss: 0.845648529773622, grad_norm: 0.4382787185498952, ic: 0.04793749908137374
train 15, step: 2000, loss: 1.4724467057920692, grad_norm: 1.018008528880879, ic: 0.06949944261259768
Epoch 15: 2022-05-05 00:36:56.930586: train loss: 1.6224735221484725
Eval step 0: eval loss: 0.8333194841197971
Eval: 2022-05-05 00:37:44.393235: total loss: 1.0745954354644922, mse:4.656393663010938, ic :0.17027364168292894, sharpe5:17.497777343988417, irr5:565.9159545898438, ndcg5:0.8470723045430124, pnl5:6.703457832336426 
train 16, step: 0, loss: 0.6758855365140043, grad_norm: 0.6907867522149607, ic: -0.050978554023101225
train 16, step: 500, loss: 1.6065590466185813, grad_norm: 1.205460569103673, ic: 0.1917511718518909
train 16, step: 1000, loss: 0.8821327903053977, grad_norm: 0.007088602459096098, ic: -0.08356252980179557
train 16, step: 1500, loss: 0.8276442218897755, grad_norm: 0.28847097506226216, ic: 0.12283756921041405
train 16, step: 2000, loss: 3.340359045984723, grad_norm: 2.5785548059327104, ic: 0.001151400762571685
Epoch 16: 2022-05-05 00:49:53.838612: train loss: 1.621143858653541
Eval step 0: eval loss: 0.8264032812088382
Eval: 2022-05-05 00:50:39.183233: total loss: 1.0697683327159857, mse:4.621895828763713, ic :0.17623909110052888, sharpe5:16.771951518058778, irr5:543.7906494140625, ndcg5:0.8556047388805669, pnl5:7.424398899078369 
train 17, step: 0, loss: 1.2840716698441643, grad_norm: 0.6715286537059237, ic: -0.10925803602158349
train 17, step: 500, loss: 1.7571866001863143, grad_norm: 1.4380262170820328, ic: 0.1871267550365252
train 17, step: 1000, loss: 1.2833101792401247, grad_norm: 0.28248835768348524, ic: 0.14251268385882612
train 17, step: 1500, loss: 4.516749015544636, grad_norm: 1.9511774080894908, ic: 0.20079935526328996
train 17, step: 2000, loss: 1.287796251149811, grad_norm: 1.6030469570543582, ic: 0.05251415488512018
Epoch 17: 2022-05-05 01:02:58.621733: train loss: 1.6187478836933866
Eval step 0: eval loss: 0.8309321669808679
Eval: 2022-05-05 01:03:43.880803: total loss: 1.0692251800574766, mse:4.599222111418648, ic :0.18311832039278958, sharpe5:17.118909502029418, irr5:547.7981567382812, ndcg5:0.8446554003240487, pnl5:4.774323463439941 
train 18, step: 0, loss: 1.4192052041330645, grad_norm: 1.1426890946963209, ic: 0.19009417704690457
train 18, step: 500, loss: 1.4637271859551553, grad_norm: 2.6206681858480385, ic: 0.013727279576973901
train 18, step: 1000, loss: 0.6515349422089041, grad_norm: 0.012520757251103503, ic: 0.5719685065268195
train 18, step: 1500, loss: 1.421528684223997, grad_norm: 0.18881698310598888, ic: 0.19668674957861237
train 18, step: 2000, loss: 0.9134135641110172, grad_norm: 0.09290907519188531, ic: -0.023654729229934245
Epoch 18: 2022-05-05 01:15:56.833749: train loss: 1.6181701760212976
Eval step 0: eval loss: 0.8210492309956203
Eval: 2022-05-05 01:16:41.816373: total loss: 1.0675919054897842, mse:4.6147290780817025, ic :0.18312548178522087, sharpe5:17.421719241142274, irr5:555.8964233398438, ndcg5:0.8546764544708666, pnl5:7.571126937866211 
train 19, step: 0, loss: 1.5031439887152778, grad_norm: 1.313537424700842, ic: 0.04496884736170191
train 19, step: 500, loss: 0.8527930930808738, grad_norm: 0.016357044348779312, ic: 0.23393922162777156
train 19, step: 1000, loss: 0.9566395498172932, grad_norm: 0.027240741150989588, ic: 0.20896151280319747
train 19, step: 1500, loss: 3.955050674191848, grad_norm: 2.263004917970128, ic: 0.13414739732932884
train 19, step: 2000, loss: 1.023265850360577, grad_norm: 0.48342464513409417, ic: 0.1476907978948856
Epoch 19: 2022-05-05 01:29:15.398717: train loss: 1.6174850155526574
Eval step 0: eval loss: 0.8226850889505399
Eval: 2022-05-05 01:30:00.318800: total loss: 1.068709115413941, mse:4.608296092842526, ic :0.1852526658567481, sharpe5:18.177634963989256, irr5:592.4834594726562, ndcg5:0.8383760038974699, pnl5:5.638583660125732 
train 20, step: 0, loss: 2.320299376235178, grad_norm: 2.746518141046539, ic: 0.049259385445361445
train 20, step: 500, loss: 3.2511882102272724, grad_norm: 1.9982030368998767, ic: 0.09341851668911039
train 20, step: 1000, loss: 0.9703027725219727, grad_norm: 0.41502971592528337, ic: 0.18830963236185408
train 20, step: 1500, loss: 1.7561891330876724, grad_norm: 5.143083223336422, ic: 0.2615917211337106
train 20, step: 2000, loss: 1.0295508728525185, grad_norm: 0.1899050485781436, ic: 0.01372085604835585
Epoch 20: 2022-05-05 01:42:35.881954: train loss: 1.6168508104262602
Eval step 0: eval loss: 0.8288139447773972
Eval: 2022-05-05 01:43:19.070327: total loss: 1.0671486193384647, mse:4.5921597545042525, ic :0.1906205143301573, sharpe5:18.022225044965744, irr5:598.2263793945312, ndcg5:0.8445248473938424, pnl5:5.935958385467529 
train 21, step: 0, loss: 0.9989509029664856, grad_norm: 0.49686723747905664, ic: 0.07395172042161477
train 21, step: 500, loss: 0.7613379554410952, grad_norm: 0.031028472133855952, ic: 0.20774939986102547
train 21, step: 1000, loss: 0.9611385412383497, grad_norm: 3.151521627579304, ic: 0.17366934428777644
train 21, step: 1500, loss: 0.9933416752441054, grad_norm: 0.6573966759438117, ic: 0.3051328460500098
train 21, step: 2000, loss: 0.9386743353318798, grad_norm: 0.2698532964588859, ic: 0.0401829747568697
Epoch 21: 2022-05-05 01:55:19.570788: train loss: 1.6168095884033415
Eval step 0: eval loss: 0.8201319028253424
Eval: 2022-05-05 01:56:03.526501: total loss: 1.0687369039360082, mse:4.6144041910316185, ic :0.18177271022433963, sharpe5:17.477231837511063, irr5:568.1527099609375, ndcg5:0.8429470587034956, pnl5:5.631112575531006 
train 22, step: 0, loss: 1.038595253464866, grad_norm: 0.17626897683102588, ic: 0.23060677120915135
train 22, step: 500, loss: 3.2736681434197155, grad_norm: 3.214511629548438, ic: -0.1748285075050912
train 22, step: 1000, loss: 1.1925580857117053, grad_norm: 0.02388488986677809, ic: 0.460183186012559
train 22, step: 1500, loss: 0.96687212657536, grad_norm: 0.29948351077363516, ic: 0.1311472851629235
train 22, step: 2000, loss: 1.736946780665391, grad_norm: 5.321592084256871, ic: 0.19890935956903466
Epoch 22: 2022-05-05 02:07:56.851179: train loss: 1.617388332897696
Eval step 0: eval loss: 0.8210078763007113
Eval: 2022-05-05 02:08:40.249210: total loss: 1.0685078865548983, mse:4.608527249710159, ic :0.18436027817327558, sharpe5:17.189437723159788, irr5:563.9970092773438, ndcg5:0.8523450749464816, pnl5:4.2977094650268555 
train 23, step: 0, loss: 0.9680941974738834, grad_norm: 0.07629107758127429, ic: 0.21406180272126732
train 23, step: 500, loss: 1.4280482700892858, grad_norm: 0.3198185667359558, ic: 0.023947934046954843
train 23, step: 1000, loss: 1.6509887695312502, grad_norm: 0.1607053769016058, ic: 0.25609398194219674
train 23, step: 1500, loss: 1.145287373938747, grad_norm: 2.5327787120019303, ic: 0.09921345769042159
train 23, step: 2000, loss: 1.8214367873123556, grad_norm: 8.953088302914116, ic: 0.4452545804103602
Epoch 23: 2022-05-05 02:21:39.151142: train loss: 1.616135912372788
Eval step 0: eval loss: 0.8244531790247958
Eval: 2022-05-05 02:22:22.576300: total loss: 1.068081300248603, mse:4.610683844621341, ic :0.17959305285710137, sharpe5:16.61824237346649, irr5:538.1546020507812, ndcg5:0.867240084672819, pnl5:5.914361000061035 
train 24, step: 0, loss: 2.20016328505669, grad_norm: 0.26883426845804936, ic: 0.13421505927014377
train 24, step: 500, loss: 1.2216025504620622, grad_norm: 0.37716779665963884, ic: 0.07687051282255422
train 24, step: 1000, loss: 0.9051666156735314, grad_norm: 0.09996255507596383, ic: 0.533304991034958
train 24, step: 1500, loss: 2.6387501690668795, grad_norm: 8.064861890400607, ic: 0.043567742575426754
train 24, step: 2000, loss: 0.9328523743123471, grad_norm: 0.2925795544080048, ic: 0.09545583673302666
Epoch 24: 2022-05-05 02:34:28.939486: train loss: 1.611552854734404
Eval step 0: eval loss: 0.8219927997810195
Eval: 2022-05-05 02:35:13.098715: total loss: 1.0745674334417759, mse:4.654686738664779, ic :0.17713694347163433, sharpe5:16.648665236234663, irr5:531.88525390625, ndcg5:0.8516289723884118, pnl5:5.371240615844727 
train 25, step: 0, loss: 1.1417672647012247, grad_norm: 208.1223253797672, ic: 0.5784421660329964
train 25, step: 500, loss: 0.8738587716554947, grad_norm: 0.010477923503136803, ic: 0.16161066772990662
train 25, step: 1000, loss: 2.0861498447020184, grad_norm: 0.11253090526575893, ic: 0.2553212600027768
train 25, step: 1500, loss: 1.1237045515212705, grad_norm: 0.704460429953514, ic: 0.5328009719243475
train 25, step: 2000, loss: 1.0029050362432772, grad_norm: 0.4961518487223202, ic: 0.6029478688770222
Epoch 25: 2022-05-05 02:44:11.185191: train loss: 1.613484212223136
Eval step 0: eval loss: 0.8194926737231625
Eval: 2022-05-05 02:44:44.444837: total loss: 1.0652884197454655, mse:4.585560422383244, ic :0.19608807467959205, sharpe5:17.33509514808655, irr5:587.2608642578125, ndcg5:0.8603627473239092, pnl5:6.420901775360107 
train 26, step: 0, loss: 6.675957530451278, grad_norm: 9.621610476211025, ic: 0.18842779575970314
train 26, step: 500, loss: 3.9662387302091475, grad_norm: 8.29483676887612, ic: 0.38218170159754244
train 26, step: 1000, loss: 1.284234877845369, grad_norm: 1.8920002053004872, ic: -0.018522711869537828
train 26, step: 1500, loss: 0.8293968084835954, grad_norm: 0.46210156806481983, ic: 0.3199973581041398
train 26, step: 2000, loss: 0.954497354598987, grad_norm: 1.6312132890979703, ic: 0.15930130022890218
Epoch 26: 2022-05-05 02:53:07.626880: train loss: 1.614390882723992
Eval step 0: eval loss: 0.8204494914465884
Eval: 2022-05-05 02:53:40.068544: total loss: 1.0663485988972254, mse:4.5962375991534925, ic :0.18748199515420477, sharpe5:17.533866301774978, irr5:576.41064453125, ndcg5:0.8527410353639773, pnl5:5.461609840393066 
train 27, step: 0, loss: 0.8283317057291666, grad_norm: 0.060290918333812546, ic: 0.11732947412725361
train 27, step: 500, loss: 0.9213186267906948, grad_norm: 6.63512428472509, ic: 0.2935602779796942
train 27, step: 1000, loss: 0.7497005831430057, grad_norm: 1.017832388787677, ic: 0.19163076376986254
train 27, step: 1500, loss: 0.6420632114348137, grad_norm: 0.09394995971518633, ic: 0.5241303888829898
train 27, step: 2000, loss: 1.391480697133637, grad_norm: 0.15390892757957536, ic: -0.06490836118002853
Epoch 27: 2022-05-05 03:02:02.446666: train loss: 1.6141247628811584
Eval step 0: eval loss: 0.8235265894609457
Eval: 2022-05-05 03:02:35.681038: total loss: 1.0678235828407407, mse:4.601629208807286, ic :0.1837218065869829, sharpe5:17.370660063028335, irr5:568.016845703125, ndcg5:0.8501038729034052, pnl5:3.5092973709106445 
train 28, step: 0, loss: 1.572299559604247, grad_norm: 4.021865007760242, ic: 0.11322561323268052
train 28, step: 500, loss: 1.4175929180932085, grad_norm: 8.930883092048923, ic: 0.16708789257240347
train 28, step: 1000, loss: 0.9089709439574865, grad_norm: 0.3006124484316165, ic: 0.5939617395408934
train 28, step: 1500, loss: 1.0313744415306916, grad_norm: 0.05040682828064559, ic: 0.03212158187537632
train 28, step: 2000, loss: 1.0455736031561542, grad_norm: 1.3606258902036157, ic: 0.1190467421852282
Epoch 28: 2022-05-05 03:10:55.616029: train loss: 1.6121154545114909
Eval step 0: eval loss: 0.8208155737536221
Eval: 2022-05-05 03:11:27.631681: total loss: 1.078977183911548, mse:4.6969050526684395, ic :0.1808588203163262, sharpe5:17.526075657606125, irr5:583.771728515625, ndcg5:0.8409447850820767, pnl5:5.751341819763184 
train 29, step: 0, loss: 0.9107525343240618, grad_norm: 0.2428556751648056, ic: 0.08644661136282832
train 29, step: 500, loss: 1.0987275728952584, grad_norm: 0.11280306631469976, ic: 0.6179408148692479
train 29, step: 1000, loss: 1.0666472053674214, grad_norm: 4.950209879393077, ic: 0.08811316722754195
train 29, step: 1500, loss: 2.4396410427644417, grad_norm: 7.264308418239321, ic: 0.00527620166266651
train 29, step: 2000, loss: 3.9933362419222607, grad_norm: 20.64495974494421, ic: 0.23526053211079562
Epoch 29: 2022-05-05 03:19:48.916087: train loss: 1.6118343425897685
Eval step 0: eval loss: 0.8213383922838184
Eval: 2022-05-05 03:20:22.373178: total loss: 1.065260376867128, mse:4.586671790066044, ic :0.19520042134247334, sharpe5:17.28596755027771, irr5:582.1223754882812, ndcg5:0.8385794231635297, pnl5:5.438510894775391 
train 30, step: 0, loss: 1.0159010183713906, grad_norm: 0.48393534408076877, ic: 0.5013275619988593
train 30, step: 500, loss: 1.427856545452215, grad_norm: 5.705532926488297, ic: 0.12380660555276006
train 30, step: 1000, loss: 0.9827320445667613, grad_norm: 0.4247773101701785, ic: -0.03495410099444422
train 30, step: 1500, loss: 1.4703231182210765, grad_norm: 14.230999351185183, ic: 0.16647097160910088
train 30, step: 2000, loss: 1.846886655986993, grad_norm: 2.617033864887687, ic: 0.09607316163452692
Epoch 30: 2022-05-05 03:28:55.027605: train loss: 1.6129846705139848
Eval step 0: eval loss: 0.8214034792997562
Eval: 2022-05-05 03:29:23.996143: total loss: 1.06963427868289, mse:4.630136983607709, ic :0.18569803032586205, sharpe5:17.76701807141304, irr5:593.9306640625, ndcg5:0.8568163769197724, pnl5:3.2632312774658203 
train 31, step: 0, loss: 1.0214661183931, grad_norm: 1.1288965130246942, ic: 0.3941798025347417
train 31, step: 500, loss: 1.5002757884837963, grad_norm: 3.6267633979089258, ic: 0.03429286672922336
train 31, step: 1000, loss: 4.518640070013661, grad_norm: 15.073495707295383, ic: 0.4743682299855617
train 31, step: 1500, loss: 0.7626492313633906, grad_norm: 0.04411822172569624, ic: 0.7162037648323435
train 31, step: 2000, loss: 1.2623354091470365, grad_norm: 11.82583166041078, ic: 0.14457804536873903
Epoch 31: 2022-05-05 03:37:59.095750: train loss: 1.6085384872407023
Eval step 0: eval loss: 0.8255926520103398
Eval: 2022-05-05 03:38:30.300523: total loss: 1.0677105529988278, mse:4.600158785667404, ic :0.18096936596670146, sharpe5:17.486496233940123, irr5:574.1708984375, ndcg5:0.8385397765066035, pnl5:5.049559593200684 
train 32, step: 0, loss: 1.1285623980051638, grad_norm: 0.4320442906408222, ic: 0.20531088105546294
train 32, step: 500, loss: 1.5030312846026082, grad_norm: 8.234581135792606, ic: 0.0171780307182983
train 32, step: 1000, loss: 1.0311180463680083, grad_norm: 0.15138040038362588, ic: 0.5087185837649358
train 32, step: 1500, loss: 0.9596655850734173, grad_norm: 4.472449224425829, ic: 0.07147796636431608
train 32, step: 2000, loss: 0.9369749696367983, grad_norm: 0.08300931286262797, ic: 0.5623884341898239
Epoch 32: 2022-05-05 03:46:40.789987: train loss: 1.6094302848964908
Eval step 0: eval loss: 0.8187436585180123
Eval: 2022-05-05 03:47:13.928920: total loss: 1.0658840559784464, mse:4.605010586515706, ic :0.19170479270499124, sharpe5:17.055587342977525, irr5:582.1157836914062, ndcg5:0.8471611011003525, pnl5:7.191410541534424 
train 33, step: 0, loss: 1.2805708355831533, grad_norm: 3.8291055347383347, ic: 0.18115452869217644
train 33, step: 500, loss: 0.9897026370917685, grad_norm: 0.07307691144062632, ic: 0.16342255928124316
train 33, step: 1000, loss: 1.006939429051112, grad_norm: 17.478405685606557, ic: 0.23717506433218277
train 33, step: 1500, loss: 0.8772256473346679, grad_norm: 0.1184467460237145, ic: 0.5607485986364701
train 33, step: 2000, loss: 0.8074165743239897, grad_norm: 0.28671907511375594, ic: 0.27868349175070994
Epoch 33: 2022-05-05 03:55:43.293738: train loss: 1.6117942515194255
Eval step 0: eval loss: 0.820704887237388
Eval: 2022-05-05 03:56:16.231415: total loss: 1.0673792057543705, mse:4.596010545012687, ic :0.18746087796681152, sharpe5:17.852234627008436, irr5:597.387939453125, ndcg5:0.8328197805080813, pnl5:5.321403503417969 
train 34, step: 0, loss: 0.9926359665054649, grad_norm: 0.9091281167009486, ic: 0.6063382412247029
train 34, step: 500, loss: 0.7940875412127294, grad_norm: 2.466308045193922, ic: 0.32249069665899166
train 34, step: 1000, loss: 3.1370110812091974, grad_norm: 6.5511843182319085, ic: 0.3551509171277407
train 34, step: 1500, loss: 0.8333350307751043, grad_norm: 1.8474930863448267, ic: 0.6927632187550556
train 34, step: 2000, loss: 5.386786073053835, grad_norm: 42.49691546601319, ic: 0.4676548872245041
Epoch 34: 2022-05-05 04:04:45.464601: train loss: 1.608915624208388
Eval step 0: eval loss: 0.8204753461703108
Eval: 2022-05-05 04:05:17.139146: total loss: 1.0682694614959183, mse:4.618870289240122, ic :0.1927293308663726, sharpe5:16.98482831597328, irr5:579.6713256835938, ndcg5:0.8609717472341237, pnl5:5.601540565490723 
train 35, step: 0, loss: 1.1475017233455882, grad_norm: 0.7509766521439765, ic: 0.5525433881148861
train 35, step: 500, loss: 1.1900805444475675, grad_norm: 4.398091512473608, ic: 0.08915955877246518
train 35, step: 1000, loss: 1.831362584593949, grad_norm: 21.249844491097207, ic: 0.06496173121089827
train 35, step: 1500, loss: 1.5920226591870301, grad_norm: 6.08364636221724, ic: 0.06938477483271607
train 35, step: 2000, loss: 0.7797385639047877, grad_norm: 0.1717871635144193, ic: 0.5770538941777522
Epoch 35: 2022-05-05 04:13:21.910864: train loss: 1.6104492259017902
Eval step 0: eval loss: 0.8239760243101291
Eval: 2022-05-05 04:13:52.818378: total loss: 1.0665967391529432, mse:4.598016061183181, ic :0.18843061576286788, sharpe5:17.445866832733152, irr5:588.072998046875, ndcg5:0.8522870428998461, pnl5:5.635135173797607 
train 36, step: 0, loss: 1.850152846595369, grad_norm: 14.415296601965021, ic: 0.12231597052130594
train 36, step: 500, loss: 0.832377644952965, grad_norm: 0.1292915591200664, ic: 0.16045155283947585
train 36, step: 1000, loss: 1.7642895951704545, grad_norm: 45.39600656540703, ic: 0.25372259466923175
train 36, step: 1500, loss: 0.7631407620614035, grad_norm: 0.07399925560794779, ic: 0.39151855426140014
train 36, step: 2000, loss: 1.1295384171465448, grad_norm: 11.469832620424848, ic: 0.7730129043302715
Epoch 36: 2022-05-05 04:22:21.130579: train loss: 1.6074848155672987
Eval step 0: eval loss: 0.8226126699980242
Eval: 2022-05-05 04:22:52.754446: total loss: 1.0676592863450074, mse:4.604467392489644, ic :0.18660118783489593, sharpe5:17.656246386766433, irr5:586.4403076171875, ndcg5:0.8456790174044295, pnl5:5.725095748901367 
train 37, step: 0, loss: 1.9787952837863707, grad_norm: 22.54566032085185, ic: 0.17730776279554486
train 37, step: 500, loss: 2.358063690036195, grad_norm: 17.983169898256737, ic: -0.04816444877360711
train 37, step: 1000, loss: 1.0668672187143264, grad_norm: 1.3403673151177036, ic: 0.04769255557045768
train 37, step: 1500, loss: 2.010980579130691, grad_norm: 15.02391739535156, ic: 0.6120274952151853
train 37, step: 2000, loss: 1.3152569235361296, grad_norm: 1.3623089893897535, ic: 0.14642375801698046
Epoch 37: 2022-05-05 04:31:10.538370: train loss: 1.6040440543018657
Eval step 0: eval loss: 0.8205926571555584
Eval: 2022-05-05 04:31:41.109304: total loss: 1.0666617948268144, mse:4.607735182752673, ic :0.190128215305427, sharpe5:17.675948127508164, irr5:588.0357055664062, ndcg5:0.8458915186676074, pnl5:3.452310562133789 
train 38, step: 0, loss: 1.3315619491949315, grad_norm: 4.802409115324335, ic: -0.07626982570501006
train 38, step: 500, loss: 0.9054369514371142, grad_norm: 0.10693928910810943, ic: 0.26584037963026425
train 38, step: 1000, loss: 0.9194067672307312, grad_norm: 0.2807731045821623, ic: 0.1252479564579067
train 38, step: 1500, loss: 0.953280901492502, grad_norm: 0.14819802669714968, ic: 0.19912713607826302
train 38, step: 2000, loss: 2.317355153063633, grad_norm: 26.410273538925175, ic: 0.036813598966381325
Epoch 38: 2022-05-05 04:40:10.127672: train loss: 1.6053829908656911
Eval step 0: eval loss: 0.8193132342227014
Eval: 2022-05-05 04:40:41.664918: total loss: 1.0651973765357998, mse:4.602528110131786, ic :0.19600686741411746, sharpe5:17.602561318874358, irr5:598.0119018554688, ndcg5:0.8337914874069695, pnl5:3.6326961517333984 
train 39, step: 0, loss: 0.9720800557267776, grad_norm: 0.009112632561445712, ic: 0.028597867877147014
train 39, step: 500, loss: 0.8963488872742121, grad_norm: 1.65628540297894, ic: 0.22255179306696568
train 39, step: 1000, loss: 0.9369887550278465, grad_norm: 0.48335474896817543, ic: 0.19703327703194584
train 39, step: 1500, loss: 2.1107738754828644, grad_norm: 2.460064030872068, ic: 0.22297856643411312
train 39, step: 2000, loss: 0.6188520338109695, grad_norm: 0.27220835316676567, ic: -0.03964587928058045
Epoch 39: 2022-05-05 04:48:30.489226: train loss: 1.6033830270807468
Eval step 0: eval loss: 0.8206495118216544
Eval: 2022-05-05 04:49:03.018330: total loss: 1.0654676247260595, mse:4.589642805524197, ic :0.1932732896220169, sharpe5:17.827190297842026, irr5:611.66552734375, ndcg5:0.8428392371392313, pnl5:5.071766376495361 
