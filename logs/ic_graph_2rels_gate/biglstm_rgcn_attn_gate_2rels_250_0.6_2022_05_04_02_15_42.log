Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0001, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
66726
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795784288873143, grad_norm: 4.816807742217264, ic: 0.022723851365593503
train 0, step: 500, loss: 0.8600233433025621, grad_norm: 0.03507574464324688, ic: 0.04575750561160824
train 0, step: 1000, loss: 1.9660142676709949, grad_norm: 0.8908351459438129, ic: 0.0332315733214434
train 0, step: 1500, loss: 0.954252678791996, grad_norm: 0.06426114214448447, ic: 0.03357914852655455
train 0, step: 2000, loss: 1.0008594136687814, grad_norm: 0.2238214356449409, ic: 0.06324573265145371
Epoch 0: 2022-05-04 14:25:54.744166: train loss: 1.6482324596807985
Eval step 0: eval loss: 0.8358389047475961
Eval: 2022-05-04 14:26:26.886505: total loss: 1.0793873341150677, mse:4.822917216987235, ic :0.007394270280210212, sharpe5:8.022243674397469, irr5:229.5480499267578, ndcg5:0.8533973107952945, pnl5:2.566746473312378 
train 1, step: 0, loss: 2.7868394420992946, grad_norm: 1.1577585612386232, ic: 0.061922700931784765
train 1, step: 500, loss: 1.7618422799323474, grad_norm: 1.087437138247639, ic: 0.09091208362613994
train 1, step: 1000, loss: 0.8790249109007051, grad_norm: 0.22878680489560355, ic: 0.08363447658651323
train 1, step: 1500, loss: 1.7139980973868534, grad_norm: 0.2638125101468275, ic: -0.03808855727788614
train 1, step: 2000, loss: 2.1773175781250003, grad_norm: 1.1672691520740397, ic: -0.03896354412396147
Epoch 1: 2022-05-04 14:34:47.847794: train loss: 1.6464581328910717
Eval step 0: eval loss: 0.8339890700120192
Eval: 2022-05-04 14:35:20.274103: total loss: 1.0793766996562173, mse:4.8255991375076, ic :0.0074131717297486344, sharpe5:7.9542893171310425, irr5:225.6363525390625, ndcg5:0.8596701823967593, pnl5:2.719269037246704 
train 2, step: 0, loss: 2.142098899147727, grad_norm: 0.016379167096323297, ic: 0.13332654729931015
train 2, step: 500, loss: 3.309950850938967, grad_norm: 0.40843545471765497, ic: 0.07905680416679016
train 2, step: 1000, loss: 2.0706892810105364, grad_norm: 0.0009089531569704695, ic: 0.19205535648233413
train 2, step: 1500, loss: 1.4905176526717556, grad_norm: 0.08072806424939101, ic: -0.03841014548138909
train 2, step: 2000, loss: 3.2502306189903845, grad_norm: 1.1015613548656793, ic: 0.17198238419643103
Epoch 2: 2022-05-04 14:43:37.195844: train loss: 1.6463421785286312
Eval step 0: eval loss: 0.8354660693410827
Eval: 2022-05-04 14:44:09.268347: total loss: 1.0798460147650448, mse:4.8249801690903835, ic :0.009255452076300722, sharpe5:7.974789419174194, irr5:225.9036102294922, ndcg5:0.86789635257105, pnl5:2.6402366161346436 
train 3, step: 0, loss: 1.522682251969004, grad_norm: 0.6971193954200222, ic: -0.0021242296336445084
train 3, step: 500, loss: 1.510612183095145, grad_norm: 0.4451952959667602, ic: 0.10179290328417284
train 3, step: 1000, loss: 3.6790774998200924, grad_norm: 0.9222384417614486, ic: -0.0571634361939712
train 3, step: 1500, loss: 1.9720215243325492, grad_norm: 1.3513088104519808, ic: -0.05613910961414448
train 3, step: 2000, loss: 0.8979438740498311, grad_norm: 0.0013663535814954653, ic: 0.01891331699332243
Epoch 3: 2022-05-04 14:52:26.150103: train loss: 1.6462354889893145
Eval step 0: eval loss: 0.8340746092720955
Eval: 2022-05-04 14:52:57.037921: total loss: 1.0798113215285663, mse:4.827785439762498, ic :0.00868329078609508, sharpe5:7.8373431023955344, irr5:220.9543914794922, ndcg5:0.8780022046543249, pnl5:2.7497000694274902 
train 4, step: 0, loss: 1.430778659119898, grad_norm: 0.056746060355283984, ic: 0.14197377406990125
train 4, step: 500, loss: 1.659308870570866, grad_norm: 0.7525275803594322, ic: 0.025205155346967573
train 4, step: 1000, loss: 2.9689094729539827, grad_norm: 0.891970315963207, ic: 0.015587741652311426
train 4, step: 1500, loss: 2.15583547600211, grad_norm: 0.6160169578445495, ic: -0.029896314627492905
train 4, step: 2000, loss: 1.0829193447463064, grad_norm: 0.4767001846977341, ic: 0.15072220344700554
Epoch 4: 2022-05-04 15:01:04.942160: train loss: 1.6457505585743564
Eval step 0: eval loss: 0.8428935783555057
Eval: 2022-05-04 15:01:36.889109: total loss: 1.082247804303437, mse:4.8252762188316005, ic :0.016609930180994693, sharpe5:8.466157152056693, irr5:242.96200561523438, ndcg5:0.8394139438994427, pnl5:2.885401487350464 
train 5, step: 0, loss: 1.3375788131816275, grad_norm: 0.12355620030302453, ic: 0.05719306753123039
train 5, step: 500, loss: 0.8885696495836638, grad_norm: 0.0076481998999993125, ic: 0.036705015423656506
train 5, step: 1000, loss: 0.9799048880507664, grad_norm: 0.18304857762947768, ic: -0.030648339814558747
train 5, step: 1500, loss: 1.536084395637921, grad_norm: 0.19338417566790495, ic: 0.025427559316037934
train 5, step: 2000, loss: 1.1071962856637905, grad_norm: 0.03331922400457776, ic: 0.09456533907832554
Epoch 5: 2022-05-04 15:09:58.876868: train loss: 1.6452940989940417
Eval step 0: eval loss: 0.839140269979913
Eval: 2022-05-04 15:10:30.753399: total loss: 1.081055528265444, mse:4.830808586255205, ic :0.03443992348578884, sharpe5:10.511341243386267, irr5:306.8473205566406, ndcg5:0.8493644833779218, pnl5:4.639152526855469 
train 6, step: 0, loss: 1.342044130656898, grad_norm: 0.5209807483222539, ic: 0.04165994600246048
train 6, step: 500, loss: 1.008954689294602, grad_norm: 0.05672538940027182, ic: 0.041661637307945656
train 6, step: 1000, loss: 1.1204541572599809, grad_norm: 0.11094137207781292, ic: 0.0942975610547385
train 6, step: 1500, loss: 1.563297189759814, grad_norm: 0.8545898600484467, ic: 0.17277663705381074
train 6, step: 2000, loss: 0.8112552053361438, grad_norm: 0.05897517413290157, ic: 0.026416311082083505
Epoch 6: 2022-05-04 15:18:52.257488: train loss: 1.643810954588117
Eval step 0: eval loss: 0.8353832313207982
Eval: 2022-05-04 15:19:24.981180: total loss: 1.0782298989795158, mse:4.822466002471194, ic :0.04696562176158656, sharpe5:11.634742484092712, irr5:369.7907409667969, ndcg5:0.8405927919405864, pnl5:3.0557751655578613 
train 7, step: 0, loss: 0.9921989440917969, grad_norm: 0.06397600062076414, ic: 0.07295421444910817
train 7, step: 500, loss: 0.6467909885394899, grad_norm: 0.002348190331886312, ic: 0.07838419599022704
train 7, step: 1000, loss: 1.0308450203493906, grad_norm: 0.24818655270845685, ic: 0.09510110219809423
train 7, step: 1500, loss: 2.265183819835209, grad_norm: 0.7936905112178047, ic: 0.3276104622914659
train 7, step: 2000, loss: 0.9198968298793417, grad_norm: 0.07371917646242845, ic: -0.049373687356742677
Epoch 7: 2022-05-04 15:27:46.903489: train loss: 1.6413944801312426
Eval step 0: eval loss: 0.8340455387867162
Eval: 2022-05-04 15:28:18.955749: total loss: 1.0748165412974464, mse:4.742048460271981, ic :0.12945572345688183, sharpe5:12.281663916707037, irr5:414.414306640625, ndcg5:0.8465965368299735, pnl5:3.013561248779297 
train 8, step: 0, loss: 3.6312436311141303, grad_norm: 1.4130323695252032, ic: -0.022135766483129083
train 8, step: 500, loss: 2.7459193638392856, grad_norm: 1.1798179896633185, ic: 0.027513474823312224
train 8, step: 1000, loss: 3.106627887228261, grad_norm: 1.3122302481090506, ic: 0.08137918822478984
train 8, step: 1500, loss: 0.7303122110049322, grad_norm: 0.00694550107059563, ic: 0.3821286333312057
train 8, step: 2000, loss: 1.0841047793404932, grad_norm: 0.43435808029237677, ic: 0.5093255851629841
Epoch 8: 2022-05-04 15:36:38.358081: train loss: 1.6357713248335637
Eval step 0: eval loss: 0.8275727894082586
Eval: 2022-05-04 15:37:10.592359: total loss: 1.0748483969132268, mse:4.744694412463531, ic :0.13416595673069173, sharpe5:12.511966558098793, irr5:416.63665771484375, ndcg5:0.845328177467893, pnl5:3.947216033935547 
train 9, step: 0, loss: 5.413365511612839, grad_norm: 1.0825491759216583, ic: 0.1496487834048777
train 9, step: 500, loss: 1.3641319040231974, grad_norm: 1.262633549915612, ic: 0.30550552091212685
train 9, step: 1000, loss: 0.9341566965889265, grad_norm: 0.009735776419792858, ic: 0.004765698564701307
train 9, step: 1500, loss: 1.101803134923831, grad_norm: 0.01617760676745803, ic: 0.40071409297759747
train 9, step: 2000, loss: 1.0811129577100236, grad_norm: 0.2787463555753145, ic: 0.1832278073700625
Epoch 9: 2022-05-04 15:45:28.839318: train loss: 1.6340112375736622
Eval step 0: eval loss: 0.8275338786922088
Eval: 2022-05-04 15:46:01.238447: total loss: 1.0720973696479517, mse:4.719261279035899, ic :0.14340932359052427, sharpe5:12.307083778381347, irr5:418.40087890625, ndcg5:0.8609841858639232, pnl5:3.117737293243408 
train 10, step: 0, loss: 7.111078290133017, grad_norm: 1.5584847858948847, ic: 0.17321009231522144
train 10, step: 500, loss: 1.1435564121982928, grad_norm: 0.09765880234225079, ic: -0.02426572453359147
train 10, step: 1000, loss: 2.37102709811158, grad_norm: 0.8942618300470673, ic: -0.039029842138588546
train 10, step: 1500, loss: 1.0993387792017553, grad_norm: 0.33097185231787013, ic: -0.02416571712618395
train 10, step: 2000, loss: 2.7901065907579787, grad_norm: 0.3657392182749468, ic: 0.4336334841936709
Epoch 10: 2022-05-04 15:54:24.907660: train loss: 1.6341901142564847
Eval step 0: eval loss: 0.8283284933976554
Eval: 2022-05-04 15:54:56.536798: total loss: 1.0712471591262065, mse:4.715728784088106, ic :0.14401172345385183, sharpe5:12.316510874032973, irr5:413.4104919433594, ndcg5:0.859823571633791, pnl5:2.651413917541504 
train 11, step: 0, loss: 1.2633057584710945, grad_norm: 0.023139242035228935, ic: 0.14031294712807074
train 11, step: 500, loss: 0.6951556129640875, grad_norm: 0.017971428267318335, ic: 0.4525959463955057
train 11, step: 1000, loss: 0.942019117438685, grad_norm: 0.18276574065242784, ic: 0.060189966460160685
train 11, step: 1500, loss: 1.0561692087273848, grad_norm: 0.07698336799111091, ic: 0.16725262607914776
train 11, step: 2000, loss: 0.7848134214090886, grad_norm: 0.004311762113251618, ic: 0.13399522552992835
Epoch 11: 2022-05-04 16:03:17.513878: train loss: 1.6334215568746981
Eval step 0: eval loss: 0.830397578763007
Eval: 2022-05-04 16:03:50.131424: total loss: 1.0719444069605053, mse:4.7049494833846826, ic :0.15031519466892443, sharpe5:12.620769771337509, irr5:427.9849853515625, ndcg5:0.8325653172214288, pnl5:3.6111769676208496 
train 12, step: 0, loss: 0.9861007531483967, grad_norm: 0.09260983284959155, ic: 0.29222204868892376
train 12, step: 500, loss: 0.9416395932509214, grad_norm: 0.08446067602165369, ic: 0.0591185618944271
train 12, step: 1000, loss: 2.912849183295183, grad_norm: 0.844342532015004, ic: 0.4258831393554162
train 12, step: 1500, loss: 0.9267283408403941, grad_norm: 0.1554823501437463, ic: -0.09641879726584401
train 12, step: 2000, loss: 0.872939648585017, grad_norm: 0.009155634682991238, ic: 0.21564286220665652
Epoch 12: 2022-05-04 16:12:11.502106: train loss: 1.6322554754545289
Eval step 0: eval loss: 0.8290922367911946
Eval: 2022-05-04 16:12:43.418885: total loss: 1.0712442126166792, mse:4.7016414483251925, ic :0.15189561000697968, sharpe5:13.383581292629241, irr5:445.14044189453125, ndcg5:0.8367536829419565, pnl5:4.359232425689697 
train 13, step: 0, loss: 2.0815299496353616, grad_norm: 0.8734479397603705, ic: 0.34686751802321425
train 13, step: 500, loss: 0.831237043560283, grad_norm: 0.06059600813975481, ic: 0.4880447597092934
train 13, step: 1000, loss: 0.9901193503565435, grad_norm: 0.5436550517668223, ic: 0.4445358177274413
train 13, step: 1500, loss: 2.419490388368462, grad_norm: 0.5108161176667134, ic: -0.03023201620718548
train 13, step: 2000, loss: 1.4927488918759622, grad_norm: 0.08608749438964246, ic: 0.14985931719837278
Epoch 13: 2022-05-04 16:21:07.088945: train loss: 1.631728995695771
Eval step 0: eval loss: 0.8248822902603068
Eval: 2022-05-04 16:21:39.767005: total loss: 1.0719264555420334, mse:4.727129019412794, ic :0.14379924121077511, sharpe5:14.220539200901985, irr5:470.5661315917969, ndcg5:0.8413469747671964, pnl5:4.187032699584961 
train 14, step: 0, loss: 4.479465526277301, grad_norm: 2.9202633035897305, ic: 0.16507705737409503
train 14, step: 500, loss: 0.8250387115945146, grad_norm: 0.007844170935938633, ic: 0.13634065865604192
train 14, step: 1000, loss: 1.8752519264308087, grad_norm: 0.2048496481588864, ic: 0.3469906302629485
train 14, step: 1500, loss: 1.1336754240458202, grad_norm: 0.09759594799906045, ic: -0.07272556146892409
train 14, step: 2000, loss: 1.1561041387217228, grad_norm: 0.41053823902030184, ic: 0.08556378613193848
Epoch 14: 2022-05-04 16:29:59.988437: train loss: 1.630326884402021
Eval step 0: eval loss: 0.8302169815883496
Eval: 2022-05-04 16:30:32.633981: total loss: 1.0716744662830804, mse:4.684142205267436, ic :0.16215815576474546, sharpe5:16.8203715634346, irr5:543.8055419921875, ndcg5:0.8509970196195076, pnl5:8.900068283081055 
train 15, step: 0, loss: 3.437597276264592, grad_norm: 2.2823593900929326, ic: 0.10919196760953137
train 15, step: 500, loss: 1.258213109803616, grad_norm: 0.019216737424874918, ic: 0.00953233521275117
train 15, step: 1000, loss: 1.3193653137703252, grad_norm: 0.26724418122947713, ic: 0.019765330599959566
train 15, step: 1500, loss: 0.8461700679749016, grad_norm: 0.21309945721547124, ic: 0.026981612941920387
train 15, step: 2000, loss: 1.4761266048019828, grad_norm: 1.0035057423139055, ic: 0.06785391850716399
Epoch 15: 2022-05-04 16:38:53.327777: train loss: 1.6269121541151959
Eval step 0: eval loss: 0.8323165523742097
Eval: 2022-05-04 16:39:25.750843: total loss: 1.0729815366260405, mse:4.6761813351659685, ic :0.16683376057131472, sharpe5:17.06593198657036, irr5:558.4226684570312, ndcg5:0.8417352053979582, pnl5:6.962925434112549 
train 16, step: 0, loss: 0.6749876479921318, grad_norm: 0.4280413597964435, ic: -0.07961545920797966
train 16, step: 500, loss: 1.6038767308264956, grad_norm: 0.9253137128245569, ic: 0.19557189666309727
train 16, step: 1000, loss: 0.8812566583806818, grad_norm: 0.008655248580093212, ic: -0.10167466987314137
train 16, step: 1500, loss: 0.8254699466546002, grad_norm: 0.27377911319974946, ic: 0.11028827802421853
train 16, step: 2000, loss: 3.3332021573080097, grad_norm: 1.7507800150435455, ic: -0.01028185987625211
Epoch 16: 2022-05-04 16:47:41.088980: train loss: 1.6244421486476304
Eval step 0: eval loss: 0.8242916834743809
Eval: 2022-05-04 16:48:13.563279: total loss: 1.0696125607715006, mse:4.673663912620879, ic :0.16857018348610814, sharpe5:17.02916385412216, irr5:563.0734252929688, ndcg5:0.8461170593652624, pnl5:9.687774658203125 
train 17, step: 0, loss: 1.284372668683687, grad_norm: 0.46967952915235267, ic: -0.12388602368853799
train 17, step: 500, loss: 1.7651181931741193, grad_norm: 0.8419692645406462, ic: 0.2148330248622451
train 17, step: 1000, loss: 1.279727939836294, grad_norm: 0.2038487957351235, ic: 0.14376582796135906
train 17, step: 1500, loss: 4.508014844324026, grad_norm: 1.7355280037625178, ic: 0.20510010505695028
train 17, step: 2000, loss: 1.3011487233989658, grad_norm: 1.197218124363272, ic: 0.0751262271921468
Epoch 17: 2022-05-04 16:56:19.526567: train loss: 1.622367196053934
Eval step 0: eval loss: 0.830395263414614
Eval: 2022-05-04 16:56:52.083847: total loss: 1.07019923289898, mse:4.652156771764429, ic :0.1770223641240618, sharpe5:17.402513165473938, irr5:566.6846923828125, ndcg5:0.8566958593912093, pnl5:8.227734565734863 
train 18, step: 0, loss: 1.4184751833263671, grad_norm: 1.0426243499310675, ic: 0.17296599864178422
train 18, step: 500, loss: 1.4784377916288336, grad_norm: 1.535166133083172, ic: 0.032078763591522175
train 18, step: 1000, loss: 0.6630187152183219, grad_norm: 0.008005334230289766, ic: 0.5578945488487306
train 18, step: 1500, loss: 1.4205907123387593, grad_norm: 0.06773075770173492, ic: 0.1898519764536529
train 18, step: 2000, loss: 0.9146734347009355, grad_norm: 0.11143148781323474, ic: -0.0342495478709387
Epoch 18: 2022-05-04 17:05:16.494078: train loss: 1.6213499691548803
Eval step 0: eval loss: 0.8201252140410958
Eval: 2022-05-04 17:05:48.851927: total loss: 1.0669651393705042, mse:4.64776499125664, ic :0.18264269062700175, sharpe5:17.24565030455589, irr5:569.0196533203125, ndcg5:0.8492379663008018, pnl5:5.901061058044434 
train 19, step: 0, loss: 1.5151292588975696, grad_norm: 1.263715451886728, ic: 0.04632703874103127
train 19, step: 500, loss: 0.8535982061315466, grad_norm: 0.012652159998580445, ic: 0.2324305468016602
train 19, step: 1000, loss: 0.9598237590167047, grad_norm: 0.14820111739851305, ic: 0.21814871251141188
train 19, step: 1500, loss: 3.9762553940838017, grad_norm: 1.8849405691838825, ic: 0.10230384098431942
train 19, step: 2000, loss: 1.0211632361778846, grad_norm: 0.27307764223378295, ic: 0.15980640467960391
Epoch 19: 2022-05-04 17:14:11.459440: train loss: 1.6204758101281387
Eval step 0: eval loss: 0.8229492959282797
Eval: 2022-05-04 17:14:43.667766: total loss: 1.0673443384095562, mse:4.609149744836894, ic :0.18770226836784043, sharpe5:17.903505692481993, irr5:586.6451416015625, ndcg5:0.857928036087814, pnl5:6.265388488769531 
train 20, step: 0, loss: 2.2974462697628457, grad_norm: 1.766589882945137, ic: 0.05029468343366342
train 20, step: 500, loss: 3.259121448863636, grad_norm: 1.443549380277474, ic: 0.07291633407758712
train 20, step: 1000, loss: 0.9707876205444337, grad_norm: 0.08824069633910164, ic: 0.16509949929677598
train 20, step: 1500, loss: 1.7453142198267613, grad_norm: 3.3831439641663046, ic: 0.26356022672912094
train 20, step: 2000, loss: 1.0260685832037624, grad_norm: 0.10887197859283128, ic: 0.013921201071305959
Epoch 20: 2022-05-04 17:23:11.900985: train loss: 1.618126678617088
Eval step 0: eval loss: 0.8315643214074024
Eval: 2022-05-04 17:23:43.353346: total loss: 1.067793066652205, mse:4.5931437723720645, ic :0.18940672850069543, sharpe5:17.720605280399322, irr5:583.3735961914062, ndcg5:0.8586054092894889, pnl5:9.285517692565918 
train 21, step: 0, loss: 0.9936278551976069, grad_norm: 0.45652721065842006, ic: 0.07444058166201134
train 21, step: 500, loss: 0.7611152176308421, grad_norm: 0.024026572762649013, ic: 0.21365283783506847
train 21, step: 1000, loss: 0.9573599832099781, grad_norm: 2.5081759290308234, ic: 0.1729232813911194
train 21, step: 1500, loss: 0.997249507148803, grad_norm: 0.46480147852788917, ic: 0.30593354111976134
train 21, step: 2000, loss: 0.9375390679073228, grad_norm: 0.20437447737928616, ic: 0.04963727653203891
Epoch 21: 2022-05-04 17:31:58.698177: train loss: 1.6175448127583392
Eval step 0: eval loss: 0.819877921969672
Eval: 2022-05-04 17:32:30.753219: total loss: 1.0676704068880107, mse:4.614593414622509, ic :0.18554858216248557, sharpe5:17.465267769098283, irr5:574.973388671875, ndcg5:0.8564693845843128, pnl5:6.150016784667969 
train 22, step: 0, loss: 1.0388464631333862, grad_norm: 0.13759746198263725, ic: 0.22310318131487444
train 22, step: 500, loss: 3.2695634051067075, grad_norm: 1.9624483151645076, ic: -0.1484615233103561
train 22, step: 1000, loss: 1.1901996590498556, grad_norm: 0.03150468228682762, ic: 0.4638956623438993
train 22, step: 1500, loss: 0.9709116994598765, grad_norm: 0.2134774276923619, ic: 0.1265765179252582
train 22, step: 2000, loss: 1.7449400333049887, grad_norm: 2.675305404903771, ic: 0.19759451018698848
Epoch 22: 2022-05-04 17:40:43.602294: train loss: 1.617313242986744
Eval step 0: eval loss: 0.818695743669323
Eval: 2022-05-04 17:41:14.479663: total loss: 1.0671522764321955, mse:4.6067058388025215, ic :0.18961197936895882, sharpe5:17.794658020734786, irr5:586.9487915039062, ndcg5:0.8410208509076377, pnl5:7.599528789520264 
train 23, step: 0, loss: 0.9674578136257205, grad_norm: 0.04330435555986191, ic: 0.21002472255551213
train 23, step: 500, loss: 1.4239482789810636, grad_norm: 0.18183719090634143, ic: 0.052974282760938074
train 23, step: 1000, loss: 1.6442171223958335, grad_norm: 0.11130888966992372, ic: 0.25010344806338447
train 23, step: 1500, loss: 1.1413197608883345, grad_norm: 1.0531273067681948, ic: 0.09061288284380081
train 23, step: 2000, loss: 1.8430940717619322, grad_norm: 6.265674997446343, ic: 0.44685147687798643
Epoch 23: 2022-05-04 17:49:21.080184: train loss: 1.6158669173800944
Eval step 0: eval loss: 0.8226958939097075
Eval: 2022-05-04 17:49:52.985435: total loss: 1.0658880878931423, mse:4.589813511342797, ic :0.1869988187199812, sharpe5:16.72555386185646, irr5:550.807373046875, ndcg5:0.8390444295119583, pnl5:6.5827202796936035 
train 24, step: 0, loss: 2.1950866607711768, grad_norm: 0.13320878949543355, ic: 0.13958377173525394
train 24, step: 500, loss: 1.21850984922179, grad_norm: 0.17279986771614647, ic: 0.1107095188064625
train 24, step: 1000, loss: 0.9104087978983796, grad_norm: 0.06551373741558073, ic: 0.520189998308286
train 24, step: 1500, loss: 2.6232871914931986, grad_norm: 4.370187709176493, ic: 0.03985763374652439
train 24, step: 2000, loss: 0.9327122968877343, grad_norm: 0.10916855179062518, ic: 0.07902983637614694
Epoch 24: 2022-05-04 17:58:18.223087: train loss: 1.6137664095664472
Eval step 0: eval loss: 0.8197318620752106
Eval: 2022-05-04 17:58:51.205819: total loss: 1.0707931155553239, mse:4.62881901604832, ic :0.18342575509163883, sharpe5:17.128387677669526, irr5:552.4259643554688, ndcg5:0.8625080259409812, pnl5:7.369350910186768 
train 25, step: 0, loss: 0.8278714566617399, grad_norm: 0.10858957677421315, ic: 0.6239883824828193
train 25, step: 500, loss: 0.8724680551949585, grad_norm: 0.010457068123830758, ic: 0.19855225594627465
train 25, step: 1000, loss: 2.0866143254950495, grad_norm: 0.07488794851533098, ic: 0.25768840154503814
train 25, step: 1500, loss: 1.1307870248012517, grad_norm: 0.6441278307935748, ic: 0.5349779585950224
train 25, step: 2000, loss: 1.0094430408570436, grad_norm: 0.5101924449122356, ic: 0.6018345968685188
Epoch 25: 2022-05-04 18:09:23.955009: train loss: 1.614497964009509
Eval step 0: eval loss: 0.8186352873501712
Eval: 2022-05-04 18:10:12.068640: total loss: 1.0656733406555956, mse:4.59358846131269, ic :0.1936122806714529, sharpe5:18.295991080999375, irr5:598.272216796875, ndcg5:0.8507856454817647, pnl5:7.8111162185668945 
train 26, step: 0, loss: 6.636670389876198, grad_norm: 10.671600257160737, ic: 0.19235247844889075
train 26, step: 500, loss: 3.978895865910868, grad_norm: 7.463362820140973, ic: 0.3743073794162549
train 26, step: 1000, loss: 1.2992983927344977, grad_norm: 1.9395114984852349, ic: 0.023418428790685945
train 26, step: 1500, loss: 0.8420430457169649, grad_norm: 0.39026304269858364, ic: 0.31573229927846624
train 26, step: 2000, loss: 0.961277971485297, grad_norm: 0.8920826542134204, ic: 0.13444832896831369
Epoch 26: 2022-05-04 18:22:25.692691: train loss: 1.6146308941474263
Eval step 0: eval loss: 0.8194489393646271
Eval: 2022-05-04 18:23:09.315300: total loss: 1.0647829118850654, mse:4.5906932075263205, ic :0.19140458051596862, sharpe5:17.715169991254804, irr5:584.6112060546875, ndcg5:0.8375613695114302, pnl5:6.752983570098877 
train 27, step: 0, loss: 0.8291272212009804, grad_norm: 0.01653180604923103, ic: 0.11728878068540213
train 27, step: 500, loss: 0.8886645293219658, grad_norm: 2.785256373684547, ic: 0.2904826644693347
train 27, step: 1000, loss: 0.7508666708961718, grad_norm: 0.5756135787957157, ic: 0.1894675790981319
train 27, step: 1500, loss: 0.6408660191316988, grad_norm: 0.10233346643408966, ic: 0.5247566726652279
train 27, step: 2000, loss: 1.3903094894884207, grad_norm: 0.07960846477736923, ic: -0.05643242759595099
Epoch 27: 2022-05-04 18:35:42.601909: train loss: 1.61415010883129
Eval step 0: eval loss: 0.8221934633084167
Eval: 2022-05-04 18:36:26.817971: total loss: 1.066136580329867, mse:4.595702283183431, ic :0.18721997384741138, sharpe5:17.14986586689949, irr5:562.3685302734375, ndcg5:0.8500882460049898, pnl5:7.221302509307861 
train 28, step: 0, loss: 1.5628858175977316, grad_norm: 2.045463330707388, ic: 0.17546248038792675
train 28, step: 500, loss: 1.4221341520664477, grad_norm: 3.698571786462381, ic: 0.16998708548702962
train 28, step: 1000, loss: 0.9155774620490346, grad_norm: 0.30679508612674977, ic: 0.5864073677464239
train 28, step: 1500, loss: 1.0302177612968628, grad_norm: 0.022445643704670143, ic: 0.021582433135258594
train 28, step: 2000, loss: 1.0443744308378067, grad_norm: 1.0804534400383186, ic: 0.0965521989619518
Epoch 28: 2022-05-04 18:48:45.054297: train loss: 1.6124346687203857
Eval step 0: eval loss: 0.8198179158571522
Eval: 2022-05-04 18:49:30.579044: total loss: 1.0817359022629767, mse:4.7244526500452615, ic :0.1798067617701691, sharpe5:18.41754821062088, irr5:607.3873291015625, ndcg5:0.8472597313388665, pnl5:5.069154739379883 
train 29, step: 0, loss: 0.9104592509121493, grad_norm: 0.13743699604264903, ic: 0.09654258430248427
train 29, step: 500, loss: 1.1043463912488989, grad_norm: 0.10033683155248511, ic: 0.6171028419629311
train 29, step: 1000, loss: 1.0592836393179201, grad_norm: 3.3824567031961497, ic: 0.09683628347832775
train 29, step: 1500, loss: 2.4363904084943404, grad_norm: 5.539753106809995, ic: 0.02353279391045736
train 29, step: 2000, loss: 3.9386314109519676, grad_norm: 22.711210034808524, ic: 0.23397001811582338
Epoch 29: 2022-05-04 19:01:17.979861: train loss: 1.612398668519199
Eval step 0: eval loss: 0.8212114983288329
Eval: 2022-05-04 19:02:03.948574: total loss: 1.0644924521098038, mse:4.5845842280202085, ic :0.19506350904827746, sharpe5:17.546477383375166, irr5:576.6465454101562, ndcg5:0.8350602083808221, pnl5:6.701328277587891 
train 30, step: 0, loss: 1.0132481921639596, grad_norm: 0.22726428601141097, ic: 0.503243871752295
train 30, step: 500, loss: 1.4338607224800042, grad_norm: 3.230015090557453, ic: 0.06846282857931833
train 30, step: 1000, loss: 0.9820486357717803, grad_norm: 0.2655490729402974, ic: -0.005742304059349085
train 30, step: 1500, loss: 1.493018911940948, grad_norm: 10.031268016985987, ic: 0.15295999605845928
train 30, step: 2000, loss: 1.8502680129829763, grad_norm: 1.4148579805740977, ic: 0.08603300323194342
Epoch 30: 2022-05-04 19:13:56.611937: train loss: 1.6143444583356392
Eval step 0: eval loss: 0.8190824068509615
Eval: 2022-05-04 19:14:41.762356: total loss: 1.0643711915252754, mse:4.591352249135261, ic :0.19592582000456923, sharpe5:17.886698484420776, irr5:604.3779907226562, ndcg5:0.8366050549252276, pnl5:4.987795829772949 
train 31, step: 0, loss: 1.0330333385558292, grad_norm: 0.9190064749973419, ic: 0.378202909351666
train 31, step: 500, loss: 1.5015604906121398, grad_norm: 2.3929442273463506, ic: 0.028520890236374583
train 31, step: 1000, loss: 4.50287746694477, grad_norm: 10.82276231801498, ic: 0.4783815536550413
train 31, step: 1500, loss: 0.7709955248470789, grad_norm: 0.04990804838922264, ic: 0.7118581416345718
train 31, step: 2000, loss: 1.2633056640625, grad_norm: 6.285790255689589, ic: 0.16345768822007897
Epoch 31: 2022-05-04 19:26:55.095229: train loss: 1.6070736317295757
Eval step 0: eval loss: 0.8239486260208113
Eval: 2022-05-04 19:27:42.121216: total loss: 1.0654600430706256, mse:4.588518502808198, ic :0.1886863889050686, sharpe5:17.7616187274456, irr5:583.3033447265625, ndcg5:0.8409104687748743, pnl5:6.341406345367432 
train 32, step: 0, loss: 1.1305197093603858, grad_norm: 0.20413431331614038, ic: 0.1766304816662598
train 32, step: 500, loss: 1.4948360413078248, grad_norm: 5.276865696591284, ic: -0.012618607305865722
train 32, step: 1000, loss: 1.0403880719348733, grad_norm: 0.182924601193896, ic: 0.5085848295365977
train 32, step: 1500, loss: 0.9611101303275172, grad_norm: 2.614153213348632, ic: 0.07185322656734022
train 32, step: 2000, loss: 0.9426051145126465, grad_norm: 0.12871001730916548, ic: 0.5562798345720713
Epoch 32: 2022-05-04 19:40:18.614263: train loss: 1.6103469238287913
Eval step 0: eval loss: 0.8182915867442703
Eval: 2022-05-04 19:41:03.078424: total loss: 1.0653102665411784, mse:4.602086205478941, ic :0.19448626061813035, sharpe5:17.836865226030348, irr5:590.6722412109375, ndcg5:0.84333689521134, pnl5:8.817496299743652 
train 33, step: 0, loss: 1.2818219464317855, grad_norm: 2.067311202579867, ic: 0.1929359548818151
train 33, step: 500, loss: 0.9872861298283517, grad_norm: 0.05486091841448798, ic: 0.1743636304681287
train 33, step: 1000, loss: 1.0124059024986602, grad_norm: 9.895628654654296, ic: 0.24512840817878306
train 33, step: 1500, loss: 0.8778472936591786, grad_norm: 0.05524019983052247, ic: 0.560120221440581
train 33, step: 2000, loss: 0.8057250009285856, grad_norm: 0.14192622941267813, ic: 0.27219853708197017
Epoch 33: 2022-05-04 19:52:47.583277: train loss: 1.6104538761550844
Eval step 0: eval loss: 0.8202604046611565
Eval: 2022-05-04 19:53:33.024753: total loss: 1.06518106949674, mse:4.582602679418785, ic :0.19483767273680944, sharpe5:17.73568144440651, irr5:589.9185791015625, ndcg5:0.8323485892198699, pnl5:6.919926643371582 
train 34, step: 0, loss: 0.996857600109516, grad_norm: 0.6010199140199473, ic: 0.6065874135717645
train 34, step: 500, loss: 0.7957470628464258, grad_norm: 1.225388621868173, ic: 0.3165159642692143
train 34, step: 1000, loss: 3.133822064612135, grad_norm: 4.731801201456774, ic: 0.35723192692578015
train 34, step: 1500, loss: 0.8319169636704782, grad_norm: 1.0190775980053999, ic: 0.6961029318136144
train 34, step: 2000, loss: 5.28726747071069, grad_norm: 52.330347928433724, ic: 0.45050921157069673
Epoch 34: 2022-05-04 20:05:23.869151: train loss: 1.6079712414524314
Eval step 0: eval loss: 0.8191333445156085
Eval: 2022-05-04 20:06:08.807892: total loss: 1.0667163793505987, mse:4.610960580489709, ic :0.1946248881016093, sharpe5:17.50652148127556, irr5:581.6253662109375, ndcg5:0.8595612155674387, pnl5:5.276888370513916 
train 35, step: 0, loss: 1.1540911506204043, grad_norm: 0.8724623976897011, ic: 0.5488511500939242
train 35, step: 500, loss: 1.1944104972084526, grad_norm: 3.1222177721055067, ic: 0.10077469746889568
train 35, step: 1000, loss: 1.8251594170896364, grad_norm: 15.207657953140963, ic: 0.08455842680924966
train 35, step: 1500, loss: 1.5881266887922934, grad_norm: 3.049751904378931, ic: 0.07224417705441265
train 35, step: 2000, loss: 0.7757689940100685, grad_norm: 0.11566435917157313, ic: 0.5727780829016345
Epoch 35: 2022-05-04 20:18:14.369866: train loss: 1.61072089606464
Eval step 0: eval loss: 0.82246757483206
Eval: 2022-05-04 20:18:59.689838: total loss: 1.065180427692132, mse:4.590756554268413, ic :0.19178100381567342, sharpe5:17.24230929493904, irr5:583.255615234375, ndcg5:0.8561821967011394, pnl5:7.958500862121582 
train 36, step: 0, loss: 1.867131926388344, grad_norm: 9.051242727439549, ic: 0.11886291263004725
train 36, step: 500, loss: 0.8307882298438732, grad_norm: 0.21395169652275509, ic: 0.19999420180224936
train 36, step: 1000, loss: 1.7824708806818181, grad_norm: 30.78425979743437, ic: 0.25271525070043044
train 36, step: 1500, loss: 0.7615348910108024, grad_norm: 0.0797672830327912, ic: 0.39891727222753337
train 36, step: 2000, loss: 1.123935108624156, grad_norm: 3.8733444399808974, ic: 0.7682757307149737
Epoch 36: 2022-05-04 20:31:22.363143: train loss: 1.6072107695227011
Eval step 0: eval loss: 0.8213628320724117
Eval: 2022-05-04 20:32:10.611723: total loss: 1.0652365782646602, mse:4.592017622224949, ic :0.19334179040039748, sharpe5:18.641880168914795, irr5:618.8255615234375, ndcg5:0.8398459377688811, pnl5:7.659971237182617 
train 37, step: 0, loss: 1.9986480666156203, grad_norm: 11.939152157638759, ic: 0.16692802912711419
train 37, step: 500, loss: 2.358272981877039, grad_norm: 9.372874197673994, ic: -0.07399811137391614
train 37, step: 1000, loss: 1.0645625609422564, grad_norm: 0.6831736364638432, ic: 0.03209230805253954
train 37, step: 1500, loss: 1.9992698777227238, grad_norm: 8.099505432463648, ic: 0.6117716901375813
train 37, step: 2000, loss: 1.3140621755606312, grad_norm: 0.8800611560072256, ic: 0.19978618517212254
Epoch 37: 2022-05-04 20:44:18.497752: train loss: 1.6051397746031995
Eval step 0: eval loss: 0.8198854468519493
Eval: 2022-05-04 20:45:05.717794: total loss: 1.0652105404239602, mse:4.5984735125862795, ic :0.1956968429393046, sharpe5:17.55668959736824, irr5:590.7095336914062, ndcg5:0.8515454531944219, pnl5:5.8658623695373535 
train 38, step: 0, loss: 1.3335359154677973, grad_norm: 2.8445452774156283, ic: -0.07293467013094176
train 38, step: 500, loss: 0.8975467634789738, grad_norm: 0.6095136997331809, ic: 0.266708471811294
train 38, step: 1000, loss: 0.9220143435029644, grad_norm: 0.2802736672608005, ic: 0.1513344740872285
train 38, step: 1500, loss: 0.9506520797741078, grad_norm: 0.0635939220606918, ic: 0.2141099792691641
train 38, step: 2000, loss: 2.310849284837613, grad_norm: 16.533046528485226, ic: 0.046381821948286564
Epoch 38: 2022-05-04 20:56:55.010966: train loss: 1.604069462482411
Eval step 0: eval loss: 0.8199384426040568
Eval: 2022-05-04 20:57:41.072311: total loss: 1.0635032261664028, mse:4.590812828327508, ic :0.19798765903999446, sharpe5:17.875593316555022, irr5:594.6669921875, ndcg5:0.8307888216962386, pnl5:7.707827091217041 
train 39, step: 0, loss: 0.972755175482607, grad_norm: 0.011138009889704745, ic: 0.015913984213231938
train 39, step: 500, loss: 0.895062879816007, grad_norm: 1.2931774195703334, ic: 0.2425472153030905
train 39, step: 1000, loss: 0.9345911378998477, grad_norm: 0.41855458010783925, ic: 0.2244009939942993
train 39, step: 1500, loss: 2.112264061880943, grad_norm: 1.9802636656646173, ic: 0.22929585147549722
train 39, step: 2000, loss: 0.6125880718984499, grad_norm: 0.1781192464295221, ic: 0.03357291222504134
Epoch 39: 2022-05-04 21:09:16.964442: train loss: 1.6044644880843117
Eval step 0: eval loss: 0.8188283616800579
Eval: 2022-05-04 21:10:02.317293: total loss: 1.0645877685395777, mse:4.588343922052392, ic :0.19434072408256461, sharpe5:18.328667138814925, irr5:613.4007568359375, ndcg5:0.8479930131749369, pnl5:9.975700378417969 
