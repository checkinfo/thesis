Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=40, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0001, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_750_0.6/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
59608
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
          (lin_gate): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.795774744673573, grad_norm: 4.817218221298496, ic: 0.02270863337888009
train 0, step: 500, loss: 0.8600217794779107, grad_norm: 0.03508456266869433, ic: 0.045807456016294434
train 0, step: 1000, loss: 1.9660053378749087, grad_norm: 0.8908691058139939, ic: 0.03337864155198105
train 0, step: 1500, loss: 0.9542561527297431, grad_norm: 0.06426101137163798, ic: 0.03351066361263952
train 0, step: 2000, loss: 1.000860731922696, grad_norm: 0.2238108384613832, ic: 0.06331163497723526
Epoch 0: 2022-05-04 18:14:29.716827: train loss: 1.648232966346276
Eval step 0: eval loss: 0.8358433424986828
Eval: 2022-05-04 18:15:15.886244: total loss: 1.0793780565515765, mse:4.822853193808201, ic :0.0078018840460713, sharpe5:7.983817334175109, irr5:228.95144653320312, ndcg5:0.8448393623604319, pnl5:2.4993979930877686 
train 1, step: 0, loss: 2.7868522397933466, grad_norm: 1.1577954781629585, ic: 0.06217323504048311
train 1, step: 500, loss: 1.7618335222132633, grad_norm: 1.0874829977550353, ic: 0.09104052790544608
train 1, step: 1000, loss: 0.8790229053643551, grad_norm: 0.22875300489536315, ic: 0.08391758887806627
train 1, step: 1500, loss: 1.7139731220815373, grad_norm: 0.2637640424018356, ic: -0.038052878476930924
train 1, step: 2000, loss: 2.1773650390625, grad_norm: 1.167500261340647, ic: -0.03908591367988036
Epoch 1: 2022-05-04 18:27:17.452561: train loss: 1.6464590634650513
Eval step 0: eval loss: 0.8339864330874605
Eval: 2022-05-04 18:28:02.365912: total loss: 1.0793334210247343, mse:4.825341475921777, ic :0.007947590641486448, sharpe5:7.8692264744639395, irr5:224.57293701171875, ndcg5:0.844668274982715, pnl5:2.469789505004883 
train 2, step: 0, loss: 2.142087002840909, grad_norm: 0.016353425259721117, ic: 0.13341114828776496
train 2, step: 500, loss: 3.30991608286874, grad_norm: 0.4082869475037351, ic: 0.07901871998379945
train 2, step: 1000, loss: 2.0707027508381226, grad_norm: 0.0009049786723537775, ic: 0.19220938935333975
train 2, step: 1500, loss: 1.4905150435353054, grad_norm: 0.08071881632129047, ic: -0.03823679853430735
train 2, step: 2000, loss: 3.2502152193509617, grad_norm: 1.1013480875032002, ic: 0.1710237950507585
Epoch 2: 2022-05-04 18:39:44.541042: train loss: 1.6463436399704852
Eval step 0: eval loss: 0.8354517913593256
Eval: 2022-05-04 18:40:28.664657: total loss: 1.0797634818708, mse:4.824501780078188, ic :0.010237503751384179, sharpe5:7.898206327557563, irr5:223.8694610595703, ndcg5:0.8433601072128917, pnl5:2.7183072566986084 
train 3, step: 0, loss: 1.522628858612805, grad_norm: 0.6972405127103453, ic: -0.0020750173497006714
train 3, step: 500, loss: 1.5106194480471808, grad_norm: 0.4452084341502156, ic: 0.1018761439776514
train 3, step: 1000, loss: 3.6791213523675883, grad_norm: 0.922215352617209, ic: -0.057083597806769554
train 3, step: 1500, loss: 1.9720806615052169, grad_norm: 1.352134582148115, ic: -0.055674863903751655
train 3, step: 2000, loss: 0.8979634712837838, grad_norm: 0.0013339504607567014, ic: 0.01874046786083637
Epoch 3: 2022-05-04 18:52:53.967375: train loss: 1.6462424726272893
Eval step 0: eval loss: 0.8340221280418532
Eval: 2022-05-04 18:53:40.387903: total loss: 1.079671438393787, mse:4.826998121363029, ic :0.009876998296514693, sharpe5:7.6699889793992035, irr5:218.27169799804688, ndcg5:0.8655921787283808, pnl5:2.92792010307312 
train 4, step: 0, loss: 1.4309149792729592, grad_norm: 0.05674276972589819, ic: 0.14089737724253693
train 4, step: 500, loss: 1.6593819205216536, grad_norm: 0.7524913015441368, ic: 0.0246914807330835
train 4, step: 1000, loss: 2.969229907524295, grad_norm: 0.8917571466596563, ic: 0.015723741392217775
train 4, step: 1500, loss: 2.155739055907173, grad_norm: 0.6162012107143819, ic: -0.02949169680879567
train 4, step: 2000, loss: 1.0829258943975992, grad_norm: 0.47667815395871027, ic: 0.1493311787460112
Epoch 4: 2022-05-04 19:05:44.810373: train loss: 1.6457610773983973
Eval step 0: eval loss: 0.8428761489273248
Eval: 2022-05-04 19:06:33.658372: total loss: 1.0821703656615342, mse:4.824901615879585, ic :0.01808000883922896, sharpe5:7.818487492203712, irr5:224.59938049316406, ndcg5:0.8523395934487169, pnl5:3.0901362895965576 
train 5, step: 0, loss: 1.3374080785968958, grad_norm: 0.12321419658599322, ic: 0.05731813400778547
train 5, step: 500, loss: 0.8885953995464909, grad_norm: 0.0075900928442253195, ic: 0.03706043828783327
train 5, step: 1000, loss: 0.980863397240182, grad_norm: 0.18391765900535478, ic: -0.03592763173603623
train 5, step: 1500, loss: 1.5354529201162903, grad_norm: 0.1928995080826934, ic: 0.03691125624871832
train 5, step: 2000, loss: 1.1072458501758264, grad_norm: 0.033241517387768565, ic: 0.09424597157169007
Epoch 5: 2022-05-04 19:18:53.159026: train loss: 1.645413132642469
Eval step 0: eval loss: 0.8393384895284509
Eval: 2022-05-04 19:19:39.271448: total loss: 1.0798400333822433, mse:4.823485703136355, ic :0.03992634171926209, sharpe5:10.284774062633513, irr5:303.606201171875, ndcg5:0.8527401930356336, pnl5:3.51979660987854 
train 6, step: 0, loss: 1.3424313680597089, grad_norm: 0.5218987263630411, ic: 0.04013757533794669
train 6, step: 500, loss: 1.0093057582790965, grad_norm: 0.05692624250789104, ic: 0.04211938082317792
train 6, step: 1000, loss: 1.1208401408032318, grad_norm: 0.11160043221091298, ic: 0.09849284224773468
train 6, step: 1500, loss: 1.5634251113765496, grad_norm: 0.8549005734929083, ic: 0.17355803742654796
train 6, step: 2000, loss: 0.8112997722396667, grad_norm: 0.05882937576591158, ic: 0.025984374153549507
Epoch 6: 2022-05-04 19:31:38.890039: train loss: 1.6438933749466518
Eval step 0: eval loss: 0.8355235671595099
Eval: 2022-05-04 19:32:24.791291: total loss: 1.0771632095308643, mse:4.819205693120268, ic :0.04859466752709366, sharpe5:11.469504396915434, irr5:358.50506591796875, ndcg5:0.85388676173901, pnl5:3.2208919525146484 
train 7, step: 0, loss: 0.9926207542419434, grad_norm: 0.06453488177344902, ic: 0.07534757663398364
train 7, step: 500, loss: 0.6467737354406108, grad_norm: 0.0021834654690319455, ic: 0.07805853939862747
train 7, step: 1000, loss: 1.0352219801682692, grad_norm: 0.25353040027786095, ic: 0.09464785127296818
train 7, step: 1500, loss: 2.2676958777130225, grad_norm: 0.7852769703172172, ic: 0.21753573542102594
train 7, step: 2000, loss: 0.913799230424576, grad_norm: 0.05482962049200927, ic: -0.08279647996386202
Epoch 7: 2022-05-04 19:44:45.004747: train loss: 1.6424366830551183
Eval step 0: eval loss: 0.8356334175777134
Eval: 2022-05-04 19:45:31.607004: total loss: 1.0754763156651788, mse:4.763744376557572, ic :0.11550576845439753, sharpe5:12.539503401517868, irr5:424.83538818359375, ndcg5:0.8395537281203156, pnl5:3.77994704246521 
train 8, step: 0, loss: 3.635934669384058, grad_norm: 1.339704999282237, ic: -0.02629516203850607
train 8, step: 500, loss: 2.7577189993351063, grad_norm: 1.1394550069354163, ic: 0.02483812275837171
train 8, step: 1000, loss: 3.115385105298913, grad_norm: 1.3008269047541927, ic: 0.07522797082103214
train 8, step: 1500, loss: 0.7249293768303021, grad_norm: 0.0057152889446711345, ic: 0.4073783597420858
train 8, step: 2000, loss: 1.0864270685920052, grad_norm: 0.4393424347383079, ic: 0.49462407679601184
Epoch 8: 2022-05-04 19:57:58.832096: train loss: 1.6363345030206722
Eval step 0: eval loss: 0.8271097197296495
Eval: 2022-05-04 19:58:46.327083: total loss: 1.0735674703095335, mse:4.7395622443957155, ic :0.1365893636182228, sharpe5:14.180783835053443, irr5:458.6446838378906, ndcg5:0.8352421255891945, pnl5:6.765773773193359 
train 9, step: 0, loss: 5.427306058114035, grad_norm: 1.068764531002554, ic: 0.1360518348877699
train 9, step: 500, loss: 1.3687891826512304, grad_norm: 1.2783164633890887, ic: 0.3071285222837971
train 9, step: 1000, loss: 0.9276874711360837, grad_norm: 0.0186179062263011, ic: 0.04511755756319727
train 9, step: 1500, loss: 1.099812435998481, grad_norm: 0.023882860325271118, ic: 0.3917121106555591
train 9, step: 2000, loss: 1.0754548873024425, grad_norm: 0.2638020056155432, ic: 0.19318226718974477
Epoch 9: 2022-05-04 20:11:13.957841: train loss: 1.6337071822763551
Eval step 0: eval loss: 0.8277901748962724
Eval: 2022-05-04 20:12:02.788372: total loss: 1.0723739327304609, mse:4.719790427140159, ic :0.14397716961367402, sharpe5:14.175163144469261, irr5:473.7523193359375, ndcg5:0.8405753523424133, pnl5:3.168588161468506 
train 10, step: 0, loss: 7.105443125911078, grad_norm: 1.5288280665317278, ic: 0.19359627520202088
train 10, step: 500, loss: 1.147435258658752, grad_norm: 0.12859417205036058, ic: -0.027432921439895227
train 10, step: 1000, loss: 2.3692059663175806, grad_norm: 0.9141445888823071, ic: -0.02332508743086935
train 10, step: 1500, loss: 1.1030418098746957, grad_norm: 0.3454661898801733, ic: -0.022299728972636546
train 10, step: 2000, loss: 2.7751999133263676, grad_norm: 0.48667711687384, ic: 0.46155306148744196
Epoch 10: 2022-05-04 20:23:54.179389: train loss: 1.6330508163423467
Eval step 0: eval loss: 0.8268323924443493
Eval: 2022-05-04 20:24:44.375609: total loss: 1.0708582410866776, mse:4.714197417149729, ic :0.14966743315969785, sharpe5:16.96766165614128, irr5:541.4586181640625, ndcg5:0.8517083637138897, pnl5:3.800640344619751 
train 11, step: 0, loss: 1.263103912896365, grad_norm: 0.02048751575932173, ic: 0.16758618979871368
train 11, step: 500, loss: 0.6848002638277455, grad_norm: 0.017650248211046153, ic: 0.47482634742334257
train 11, step: 1000, loss: 0.9541538927246497, grad_norm: 0.23182846035582327, ic: 0.014084837717797616
train 11, step: 1500, loss: 1.0484790467379386, grad_norm: 0.066863345020159, ic: 0.18518552100581376
train 11, step: 2000, loss: 0.7850896223649783, grad_norm: 0.007509537594767094, ic: 0.12373967389816312
Epoch 11: 2022-05-04 20:36:51.695562: train loss: 1.6296521422130599
Eval step 0: eval loss: 0.8270464335402397
Eval: 2022-05-04 20:37:39.128038: total loss: 1.0700905410946098, mse:4.69222932429228, ic :0.16466475288945523, sharpe5:17.55720228910446, irr5:544.4215698242188, ndcg5:0.8529571966887945, pnl5:7.692133903503418 
train 12, step: 0, loss: 0.956207275390625, grad_norm: 0.1306968666912881, ic: 0.40043660141478127
train 12, step: 500, loss: 0.9221690284922708, grad_norm: 0.1322326977126037, ic: 0.16023776850731494
train 12, step: 1000, loss: 2.920785114264033, grad_norm: 0.6707391374757844, ic: 0.37988193971640094
train 12, step: 1500, loss: 0.9159597497084444, grad_norm: 0.15580961212966665, ic: -0.0842253674164535
train 12, step: 2000, loss: 0.8729541236900491, grad_norm: 0.013412240857187615, ic: 0.2303129702777797
Epoch 12: 2022-05-04 20:49:41.862592: train loss: 1.6267317020330805
Eval step 0: eval loss: 0.8256890605448168
Eval: 2022-05-04 20:50:29.912444: total loss: 1.068986854237702, mse:4.673469347103718, ic :0.1740474183271803, sharpe5:17.061953423023223, irr5:554.2227783203125, ndcg5:0.8343597504496798, pnl5:5.2091522216796875 
train 13, step: 0, loss: 2.0780642870990627, grad_norm: 1.1207720319352843, ic: 0.3442308324978055
train 13, step: 500, loss: 0.8251393566678036, grad_norm: 0.0656533687171239, ic: 0.5174114436645078
train 13, step: 1000, loss: 0.9698610036965184, grad_norm: 0.5701393859793398, ic: 0.4583195126842797
train 13, step: 1500, loss: 2.43011593725605, grad_norm: 0.5377684892120757, ic: -0.05889944363620628
train 13, step: 2000, loss: 1.4596885111443898, grad_norm: 0.10067313395681719, ic: 0.23072844047034718
Epoch 13: 2022-05-04 21:03:04.808405: train loss: 1.6248034399247313
Eval step 0: eval loss: 0.8220311959752041
Eval: 2022-05-04 21:03:54.030243: total loss: 1.0691141944435927, mse:4.692189628092315, ic :0.16825439722418992, sharpe5:17.80095977783203, irr5:563.3397216796875, ndcg5:0.8453052276201785, pnl5:8.856602668762207 
train 14, step: 0, loss: 4.487807061110569, grad_norm: 2.476174806885818, ic: 0.17155099928720696
train 14, step: 500, loss: 0.8271114804329128, grad_norm: 0.012559745078851114, ic: 0.1122866894681774
train 14, step: 1000, loss: 1.8175885802249432, grad_norm: 0.8186540818562948, ic: 0.3894301538664886
train 14, step: 1500, loss: 1.132701736042975, grad_norm: 0.10670390338493149, ic: -0.06587777284331763
train 14, step: 2000, loss: 1.1541405343677493, grad_norm: 0.41334732089903725, ic: 0.06256674026370243
Epoch 14: 2022-05-04 21:14:14.130356: train loss: 1.6239946343834049
Eval step 0: eval loss: 0.8304674894214304
Eval: 2022-05-04 21:14:59.488163: total loss: 1.0706007984622004, mse:4.665854069148538, ic :0.17410728115590152, sharpe5:16.65677560329437, irr5:534.7232666015625, ndcg5:0.8440552605764664, pnl5:5.526691913604736 
train 15, step: 0, loss: 3.4643459691147864, grad_norm: 2.2501168483240965, ic: 0.11532492950913066
train 15, step: 500, loss: 1.2550567347906276, grad_norm: 0.035425838286099076, ic: 0.07540420082575805
train 15, step: 1000, loss: 1.312598053226626, grad_norm: 0.2756548059997328, ic: 0.059623199319782966
train 15, step: 1500, loss: 0.839894500492126, grad_norm: 0.26521099568812945, ic: 0.05998505287265332
train 15, step: 2000, loss: 1.4729855056738124, grad_norm: 0.9259111618263502, ic: 0.06725480973571202
Epoch 15: 2022-05-04 21:26:13.307283: train loss: 1.6231002400221082
Eval step 0: eval loss: 0.8350630057749934
Eval: 2022-05-04 21:26:59.280744: total loss: 1.0737220550540503, mse:4.673469582300931, ic :0.16835655074787736, sharpe5:17.22507263660431, irr5:553.4839477539062, ndcg5:0.8372800539221164, pnl5:7.181112289428711 
train 16, step: 0, loss: 0.6711520982562599, grad_norm: 0.5023682894125912, ic: -0.084744281621593
train 16, step: 500, loss: 1.6006828699874274, grad_norm: 0.815354415776187, ic: 0.1951969927300568
train 16, step: 1000, loss: 0.8816029866536458, grad_norm: 0.007285356956240897, ic: -0.10576964787950917
train 16, step: 1500, loss: 0.8279024068533295, grad_norm: 0.28423431414070527, ic: 0.13099833254276677
train 16, step: 2000, loss: 3.345484184687242, grad_norm: 2.1133066181691014, ic: 0.013633946041533913
Epoch 16: 2022-05-04 21:38:51.682012: train loss: 1.6222678024453225
Eval step 0: eval loss: 0.8251616113178345
Eval: 2022-05-04 21:39:40.971029: total loss: 1.0685549963929204, mse:4.645330292741976, ic :0.1780904865562904, sharpe5:17.118063844442368, irr5:550.117431640625, ndcg5:0.8667967347183687, pnl5:6.401889801025391 
train 17, step: 0, loss: 1.2852744995440981, grad_norm: 0.47348353738000004, ic: -0.1131505521894325
train 17, step: 500, loss: 1.7562687902269647, grad_norm: 1.0062394732576547, ic: 0.20180825032739427
train 17, step: 1000, loss: 1.2819290889282875, grad_norm: 0.21438344246328656, ic: 0.14178582371570475
train 17, step: 1500, loss: 4.514190651405216, grad_norm: 1.7320305176511412, ic: 0.2000781805528131
train 17, step: 2000, loss: 1.2866608127299624, grad_norm: 1.2097737044889536, ic: 0.06766291030022678
Epoch 17: 2022-05-04 21:51:55.994223: train loss: 1.6204856373102954
Eval step 0: eval loss: 0.8305316117088711
Eval: 2022-05-04 21:52:44.237676: total loss: 1.0688595093987454, mse:4.619083095344508, ic :0.18488490239325803, sharpe5:17.432852786779403, irr5:560.4707641601562, ndcg5:0.8579966633880168, pnl5:10.963188171386719 
train 18, step: 0, loss: 1.4226353703100414, grad_norm: 0.9478842537375616, ic: 0.17493113869806348
train 18, step: 500, loss: 1.4689485243581013, grad_norm: 1.736154671452561, ic: -0.012515988774342342
train 18, step: 1000, loss: 0.6611441165453766, grad_norm: 0.012419714382504839, ic: 0.5595192133393578
train 18, step: 1500, loss: 1.4310195760391073, grad_norm: 0.2581384373791707, ic: 0.10682643997556585
train 18, step: 2000, loss: 0.911341770439391, grad_norm: 0.013920753194792572, ic: -0.02392134728288591
Epoch 18: 2022-05-04 22:05:02.252486: train loss: 1.6198059553944253
Eval step 0: eval loss: 0.8206894515814344
Eval: 2022-05-04 22:05:51.602303: total loss: 1.0662368253288184, mse:4.614514163893042, ic :0.18963932916682516, sharpe5:18.106794587373734, irr5:584.458251953125, ndcg5:0.8359886104253599, pnl5:21.295846939086914 
train 19, step: 0, loss: 1.5038566468253969, grad_norm: 1.2336611081880478, ic: 0.058131986333348026
train 19, step: 500, loss: 0.8535034744827835, grad_norm: 0.01852806089633271, ic: 0.23402714818631676
train 19, step: 1000, loss: 0.9580124469082195, grad_norm: 0.035014500885686105, ic: 0.20832646318126868
train 19, step: 1500, loss: 3.9596253513703443, grad_norm: 1.9196762564018606, ic: 0.1198137945126028
train 19, step: 2000, loss: 1.02461181640625, grad_norm: 0.3171660311528056, ic: 0.15336646275394644
Epoch 19: 2022-05-04 22:18:29.457519: train loss: 1.6182507620291353
Eval step 0: eval loss: 0.8236168880482745
Eval: 2022-05-04 22:19:13.331617: total loss: 1.0668089728619452, mse:4.596958805326813, ic :0.19206908979393278, sharpe5:18.26025589942932, irr5:587.856689453125, ndcg5:0.8704451392363569, pnl5:13.408477783203125 
train 20, step: 0, loss: 2.3189279428112646, grad_norm: 1.7235417177491945, ic: 0.04769507606682814
train 20, step: 500, loss: 3.2557720170454543, grad_norm: 1.262359144746019, ic: 0.08306178843577738
train 20, step: 1000, loss: 0.9680549621582032, grad_norm: 0.38400935848169593, ic: 0.20251577898441459
train 20, step: 1500, loss: 1.7601748316065275, grad_norm: 2.4964147225349174, ic: 0.25735135351214655
train 20, step: 2000, loss: 1.030469188099001, grad_norm: 0.09209297771705195, ic: 0.004329715421479877
Epoch 20: 2022-05-04 22:31:29.731480: train loss: 1.6170947412168883
Eval step 0: eval loss: 0.8297356463835287
Eval: 2022-05-04 22:32:17.340725: total loss: 1.06691287250662, mse:4.590608653871945, ic :0.1915670678574088, sharpe5:18.34106822490692, irr5:591.9664916992188, ndcg5:0.8457970538843398, pnl5:6.280040740966797 
train 21, step: 0, loss: 0.9948837547971492, grad_norm: 0.5754444669759544, ic: 0.0767101545343502
train 21, step: 500, loss: 0.7610530346895741, grad_norm: 0.027515253651220505, ic: 0.21152274663863993
train 21, step: 1000, loss: 0.9622731794390762, grad_norm: 2.1374870783020734, ic: 0.16919958410404787
train 21, step: 1500, loss: 0.9938230136901548, grad_norm: 0.4103222784928104, ic: 0.30640455199172406
train 21, step: 2000, loss: 0.9355145662461932, grad_norm: 0.17950835572386203, ic: 0.045917373785075315
Epoch 21: 2022-05-04 22:44:04.879126: train loss: 1.6169167111857423
Eval step 0: eval loss: 0.8198105196053411
Eval: 2022-05-04 22:44:51.785828: total loss: 1.0669841997836078, mse:4.605061956157288, ic :0.18928936650432454, sharpe5:17.445974668264387, irr5:582.7634887695312, ndcg5:0.8344258782001228, pnl5:9.979100227355957 
train 22, step: 0, loss: 1.0372544628078655, grad_norm: 0.04048233861900154, ic: 0.23070665270448049
train 22, step: 500, loss: 3.264334428988821, grad_norm: 1.7537028777376686, ic: -0.18067841665546197
train 22, step: 1000, loss: 1.1852479791365607, grad_norm: 0.01938201270503075, ic: 0.4680680422773093
train 22, step: 1500, loss: 0.9662692097479424, grad_norm: 0.18293145966650826, ic: 0.1311068539197496
train 22, step: 2000, loss: 1.7374477948731577, grad_norm: 2.5486343177447894, ic: 0.1967783954168572
Epoch 22: 2022-05-04 22:57:14.047392: train loss: 1.617367111295688
Eval step 0: eval loss: 0.8204591387315595
Eval: 2022-05-04 22:58:03.414679: total loss: 1.0673297337358079, mse:4.606882234540242, ic :0.1881550637220381, sharpe5:17.29388494849205, irr5:566.8075561523438, ndcg5:0.847908117059502, pnl5:4.404631614685059 
train 23, step: 0, loss: 0.9683118133105187, grad_norm: 0.04538110622772171, ic: 0.21448113172123487
train 23, step: 500, loss: 1.4260106348729396, grad_norm: 0.1975580338037479, ic: 0.05999627765631891
train 23, step: 1000, loss: 1.649871826171875, grad_norm: 0.10484967201420879, ic: 0.2573893441859128
train 23, step: 1500, loss: 1.1450674974817727, grad_norm: 0.9327830903425133, ic: 0.09064863961391721
train 23, step: 2000, loss: 1.8045630803743262, grad_norm: 4.259365506990528, ic: 0.4465697691381256
Epoch 23: 2022-05-04 23:10:26.360995: train loss: 1.6168695394332668
Eval step 0: eval loss: 0.8197363641415305
Eval: 2022-05-04 23:11:11.315350: total loss: 1.0666209997589993, mse:4.612029066076779, ic :0.1904860881442077, sharpe5:17.648945732116697, irr5:579.8865966796875, ndcg5:0.8453417504353322, pnl5:6.425687789916992 
train 24, step: 0, loss: 2.1769920703710643, grad_norm: 0.2341501970089597, ic: 0.16529061949244087
train 24, step: 500, loss: 1.2109694187743192, grad_norm: 0.07730408292190637, ic: 0.10069295431989009
train 24, step: 1000, loss: 0.9147231303542792, grad_norm: 0.043169063430967956, ic: 0.5099344541081682
train 24, step: 1500, loss: 2.6282480967899837, grad_norm: 3.502406543175473, ic: 0.04228402004275783
train 24, step: 2000, loss: 0.9297116752941625, grad_norm: 0.0753727447030628, ic: 0.11510299352098216
Epoch 24: 2022-05-04 23:23:39.359946: train loss: 1.6131919130235644
Eval step 0: eval loss: 0.8211946477377502
Eval: 2022-05-04 23:24:27.001232: total loss: 1.0723443718469576, mse:4.638244145545347, ic :0.18093738223354294, sharpe5:17.09846237182617, irr5:548.9401245117188, ndcg5:0.8476333665472962, pnl5:7.955521583557129 
train 25, step: 0, loss: 0.9077395052523227, grad_norm: 42.54656381817035, ic: 0.6159892655493154
train 25, step: 500, loss: 0.8727222454151718, grad_norm: 0.01033053398814399, ic: 0.19098660167840054
train 25, step: 1000, loss: 2.0839512774895277, grad_norm: 0.03520746606384247, ic: 0.2616497715393048
train 25, step: 1500, loss: 1.1231855462142677, grad_norm: 0.4712318857729456, ic: 0.5331326525435282
train 25, step: 2000, loss: 1.0063739170389485, grad_norm: 0.4528030926066179, ic: 0.6024501477704166
Epoch 25: 2022-05-04 23:36:38.605016: train loss: 1.615710633256661
Eval step 0: eval loss: 0.8200342723014357
Eval: 2022-05-04 23:37:24.438472: total loss: 1.0648282934974, mse:4.58740265268523, ic :0.19703782320599836, sharpe5:17.81342140316963, irr5:593.7747802734375, ndcg5:0.853637440521549, pnl5:5.48094367980957 
train 26, step: 0, loss: 6.653374912639776, grad_norm: 3.253021431757297, ic: 0.16458983375148287
train 26, step: 500, loss: 3.9571725041536356, grad_norm: 3.0763868810220703, ic: 0.3740518562290943
train 26, step: 1000, loss: 1.2849531188677394, grad_norm: 1.625053580119276, ic: 0.0011650985268670706
train 26, step: 1500, loss: 0.8340682266246384, grad_norm: 0.32465901368349687, ic: 0.32417944981185365
train 26, step: 2000, loss: 0.9565888771759441, grad_norm: 0.7298272666227261, ic: 0.14382490911317916
Epoch 26: 2022-05-04 23:49:43.716219: train loss: 1.6138297869148646
Eval step 0: eval loss: 0.8213381350228859
Eval: 2022-05-04 23:50:26.813909: total loss: 1.065941842869163, mse:4.5970312531953414, ic :0.18827151216078056, sharpe5:17.488019174337385, irr5:574.7258911132812, ndcg5:0.8424825411632738, pnl5:7.5620317459106445 
train 27, step: 0, loss: 0.8278085746017156, grad_norm: 0.039672936446751544, ic: 0.11855019835846
train 27, step: 500, loss: 0.9101726746059415, grad_norm: 3.920787828911422, ic: 0.2896239502592727
train 27, step: 1000, loss: 0.7554530404098995, grad_norm: 0.42516657422680765, ic: 0.18503621393543201
train 27, step: 1500, loss: 0.6404784890590995, grad_norm: 0.09860996551630313, ic: 0.518096706103323
train 27, step: 2000, loss: 1.389881826297931, grad_norm: 0.05837697764992554, ic: -0.0603803336724202
Epoch 27: 2022-05-05 00:01:57.540943: train loss: 1.6148676043863441
Eval step 0: eval loss: 0.8244733096927687
Eval: 2022-05-05 00:02:45.801749: total loss: 1.0669715690921415, mse:4.602153773891958, ic :0.18608584855589624, sharpe5:17.426516976356506, irr5:572.4365234375, ndcg5:0.8478897919778683, pnl5:6.4900031089782715 
train 28, step: 0, loss: 1.5585171143520753, grad_norm: 1.3059195110930835, ic: 0.15010205238343483
train 28, step: 500, loss: 1.4178440185683343, grad_norm: 3.3516017282473465, ic: 0.1589456241200398
train 28, step: 1000, loss: 0.9150996014354674, grad_norm: 0.3046261775966689, ic: 0.5931639764066791
train 28, step: 1500, loss: 1.029703396646756, grad_norm: 0.028777630782479635, ic: 0.034873894972492867
train 28, step: 2000, loss: 1.045520244200537, grad_norm: 0.6802633912233986, ic: 0.1189672951303629
Epoch 28: 2022-05-05 00:14:39.375185: train loss: 1.6126447432845328
Eval step 0: eval loss: 0.8212367099002239
Eval: 2022-05-05 00:15:28.149593: total loss: 1.0806028953194036, mse:4.711804063040163, ic :0.18079266666522048, sharpe5:18.880807760953903, irr5:614.7827758789062, ndcg5:0.8541863141383417, pnl5:19.866714477539062 
train 29, step: 0, loss: 0.9106782185841547, grad_norm: 0.10417101755660733, ic: 0.08771066245333528
train 29, step: 500, loss: 1.102493904369128, grad_norm: 0.08215409312200767, ic: 0.6169982260197501
train 29, step: 1000, loss: 1.0793707167653972, grad_norm: 1.7342861421979188, ic: 0.03981214302332533
train 29, step: 1500, loss: 2.4177705405932866, grad_norm: 2.6049905985902937, ic: -0.0032855225244884257
train 29, step: 2000, loss: 4.20477294921875, grad_norm: 6.8224239687439825, ic: 0.22128640717836368
Epoch 29: 2022-05-05 00:27:46.414503: train loss: 1.6126094943154223
Eval step 0: eval loss: 0.8231531109021009
Eval: 2022-05-05 00:28:31.883071: total loss: 1.0651588331809805, mse:4.588102932188528, ic :0.1932226793267811, sharpe5:17.634329288005826, irr5:582.1940307617188, ndcg5:0.848745860403386, pnl5:7.488107681274414 
train 30, step: 0, loss: 1.0095499904366763, grad_norm: 0.19665435854493685, ic: 0.5073555646867084
train 30, step: 500, loss: 1.4280355952625103, grad_norm: 2.508566475270645, ic: 0.15733588371214868
train 30, step: 1000, loss: 0.9785579796993371, grad_norm: 0.14618294305039675, ic: 0.004592761605206573
train 30, step: 1500, loss: 1.4826020709192556, grad_norm: 7.548214794126102, ic: 0.16954868909965543
train 30, step: 2000, loss: 1.846760569589231, grad_norm: 0.8539128283701003, ic: 0.08650912356366802
Epoch 30: 2022-05-05 00:40:29.799516: train loss: 1.6130447008877793
Eval step 0: eval loss: 0.8208986047196061
Eval: 2022-05-05 00:41:15.245810: total loss: 1.0674868958662058, mse:4.605515138541986, ic :0.19090695355053378, sharpe5:18.470213570594787, irr5:605.5866088867188, ndcg5:0.8392430927181466, pnl5:12.646308898925781 
train 31, step: 0, loss: 1.025699659564072, grad_norm: 0.7357273459910552, ic: 0.3884325708543803
train 31, step: 500, loss: 1.4992997283307612, grad_norm: 1.730333903539444, ic: 0.037093857927747886
train 31, step: 1000, loss: 4.488021290739656, grad_norm: 5.081949764773554, ic: 0.4842407232162891
train 31, step: 1500, loss: 0.7636084829602198, grad_norm: 0.04268096064427848, ic: 0.7147156010434964
train 31, step: 2000, loss: 1.2575467466221029, grad_norm: 4.153843506379814, ic: 0.14116305232576587
Epoch 31: 2022-05-05 00:53:01.598232: train loss: 1.6090649028362582
Eval step 0: eval loss: 0.8253298599677291
Eval: 2022-05-05 00:53:45.232114: total loss: 1.0656493041840607, mse:4.591915599888113, ic :0.18665654902782014, sharpe5:17.53333658337593, irr5:566.712646484375, ndcg5:0.8366688285974463, pnl5:8.42022705078125 
train 32, step: 0, loss: 1.126446956595869, grad_norm: 0.1260554416544048, ic: 0.20583772129165803
train 32, step: 500, loss: 1.5004090797244094, grad_norm: 3.985307203920633, ic: 0.030542889009186207
train 32, step: 1000, loss: 1.048942726238991, grad_norm: 0.2767809070718151, ic: 0.4986038952120561
train 32, step: 1500, loss: 0.9684999001835431, grad_norm: 2.066629912761573, ic: 0.05406567325412185
train 32, step: 2000, loss: 0.9425042803680214, grad_norm: 0.08981094140500898, ic: 0.5616815849817547
Epoch 32: 2022-05-05 01:06:11.927941: train loss: 1.6122456610223026
Eval step 0: eval loss: 0.8184175159707586
Eval: 2022-05-05 01:06:56.481480: total loss: 1.0656476942851594, mse:4.606550477531223, ic :0.19433799469161714, sharpe5:18.260717511177063, irr5:602.1105346679688, ndcg5:0.8466689353934783, pnl5:12.052352905273438 
train 33, step: 0, loss: 1.2857498157959864, grad_norm: 1.783999195820568, ic: 0.1839874947942704
train 33, step: 500, loss: 0.9922777704831932, grad_norm: 0.0521641831635564, ic: 0.16123179212998875
train 33, step: 1000, loss: 1.0224619187726085, grad_norm: 7.979287028720045, ic: 0.23393764918119292
train 33, step: 1500, loss: 0.8800380430753263, grad_norm: 0.09886334598296523, ic: 0.5576163866703963
train 33, step: 2000, loss: 0.8072744813849545, grad_norm: 0.11596176435385552, ic: 0.25645404206883443
Epoch 33: 2022-05-05 01:18:39.123041: train loss: 1.6123629424768695
Eval step 0: eval loss: 0.8210513533983139
Eval: 2022-05-05 01:19:28.025310: total loss: 1.0654534809390395, mse:4.582693112210113, ic :0.19545121692292028, sharpe5:18.53836184263229, irr5:617.8630981445312, ndcg5:0.8398688052692292, pnl5:9.173776626586914 
train 34, step: 0, loss: 0.9931762610600278, grad_norm: 0.5418596546408461, ic: 0.6077320212304292
train 34, step: 500, loss: 0.7955334397995508, grad_norm: 0.9980668190488408, ic: 0.292082020776132
train 34, step: 1000, loss: 3.1517497119815667, grad_norm: 4.084278658853114, ic: 0.34540958609503625
train 34, step: 1500, loss: 0.831278240581235, grad_norm: 0.9855383065461549, ic: 0.6982323735936676
train 34, step: 2000, loss: 5.85325767936677, grad_norm: 25.725519880107395, ic: 0.4662291376557126
Epoch 34: 2022-05-05 01:31:32.195757: train loss: 1.6080104833368862
Eval step 0: eval loss: 0.8187561999884746
Eval: 2022-05-05 01:32:16.884445: total loss: 1.066473151216039, mse:4.609107910251659, ic :0.1958463393842765, sharpe5:18.190703873634337, irr5:604.257568359375, ndcg5:0.8440236958519131, pnl5:7.978137969970703 
train 35, step: 0, loss: 1.1617972340303309, grad_norm: 0.816203201643263, ic: 0.5502076084922879
train 35, step: 500, loss: 1.193025203776574, grad_norm: 1.794925925570839, ic: 0.05359211180274208
train 35, step: 1000, loss: 1.8393908458150876, grad_norm: 9.41587762372014, ic: 0.06282848091250313
train 35, step: 1500, loss: 1.5924052073543233, grad_norm: 3.6767006588962956, ic: 0.06958202319498223
train 35, step: 2000, loss: 0.7771348596256684, grad_norm: 0.20529898614812653, ic: 0.5786141744947212
Epoch 35: 2022-05-05 01:44:25.849209: train loss: 1.6106415854955907
Eval step 0: eval loss: 0.8242869884623616
Eval: 2022-05-05 01:45:09.759014: total loss: 1.0655052601950372, mse:4.5894257212304685, ic :0.19245158535424356, sharpe5:17.657281229496, irr5:579.2184448242188, ndcg5:0.8650910587024523, pnl5:8.840448379516602 
train 36, step: 0, loss: 1.859141590831044, grad_norm: 6.412456188348761, ic: 0.13173712651359373
train 36, step: 500, loss: 0.8285453682156226, grad_norm: 0.05956348358622539, ic: 0.20118212591867518
train 36, step: 1000, loss: 1.7725973011363636, grad_norm: 23.759839381209176, ic: 0.2617331683958144
train 36, step: 1500, loss: 0.7614523210333414, grad_norm: 0.06201161466050444, ic: 0.3968388673234703
train 36, step: 2000, loss: 1.1381735101022636, grad_norm: 4.4225289848161955, ic: 0.7629562050375936
Epoch 36: 2022-05-05 01:57:01.637435: train loss: 1.606598598997927
Eval step 0: eval loss: 0.8217174662679465
Eval: 2022-05-05 01:57:45.254047: total loss: 1.0654026957373557, mse:4.586976333546782, ic :0.1943182631553089, sharpe5:18.644472005367277, irr5:617.8182983398438, ndcg5:0.8419800864146577, pnl5:5.727710247039795 
train 37, step: 0, loss: 2.0048895422568913, grad_norm: 7.33228090105165, ic: 0.17329454603099032
train 37, step: 500, loss: 2.349213891083809, grad_norm: 6.496628447328835, ic: 0.006693158424786599
train 37, step: 1000, loss: 1.0683315050656392, grad_norm: 0.6127625506391436, ic: 0.0431841990802042
train 37, step: 1500, loss: 2.0081298444858713, grad_norm: 8.543279218017336, ic: 0.6091959721211363
train 37, step: 2000, loss: 1.310479655218283, grad_norm: 0.6281060399812567, ic: 0.17061017288967426
Epoch 37: 2022-05-05 02:10:12.940041: train loss: 1.6037959000963626
Eval step 0: eval loss: 0.8197808702828635
Eval: 2022-05-05 02:10:57.081194: total loss: 1.0661421119803678, mse:4.601254439231854, ic :0.19402599743368792, sharpe5:18.38496485352516, irr5:598.7711181640625, ndcg5:0.8440611684787753, pnl5:13.659379959106445 
train 38, step: 0, loss: 1.32952313306855, grad_norm: 1.6998910466611286, ic: -0.07316762834915402
train 38, step: 500, loss: 0.9068036868248457, grad_norm: 0.15175295479874668, ic: 0.25835887186498957
train 38, step: 1000, loss: 0.921085644145257, grad_norm: 0.2969565338217491, ic: 0.1402902053317513
train 38, step: 1500, loss: 0.9535018700764047, grad_norm: 0.05442488642907256, ic: 0.19453630933284705
train 38, step: 2000, loss: 2.3098985378115557, grad_norm: 13.847949479982177, ic: 0.06604118630126755
Epoch 38: 2022-05-05 02:23:08.007887: train loss: 1.606934553609188
Eval step 0: eval loss: 0.819918762142716
Eval: 2022-05-05 02:23:51.287836: total loss: 1.0642515036882023, mse:4.593685869220371, ic :0.19658216918007265, sharpe5:17.64880005955696, irr5:593.3986206054688, ndcg5:0.8445214513614784, pnl5:5.605744361877441 
train 39, step: 0, loss: 0.9706904326739296, grad_norm: 0.017304468036110954, ic: 0.04373730783609843
train 39, step: 500, loss: 0.8977698194898155, grad_norm: 0.8852045274224956, ic: 0.24926355913137424
train 39, step: 1000, loss: 0.9347080948329208, grad_norm: 0.3612401485166566, ic: 0.21739248928523514
train 39, step: 1500, loss: 2.122138408775753, grad_norm: 1.6734479590377158, ic: 0.2170988407463473
train 39, step: 2000, loss: 0.6179995664864731, grad_norm: 0.11782182132886981, ic: -0.05977545349924497
Epoch 39: 2022-05-05 02:36:18.596091: train loss: 1.60554330014143
Eval step 0: eval loss: 0.8210597143786221
Eval: 2022-05-05 02:37:03.658439: total loss: 1.0653027936100279, mse:4.591847202178327, ic :0.19341127616919393, sharpe5:18.6028947865963, irr5:619.612060546875, ndcg5:0.8604823648689379, pnl5:10.51366901397705 
