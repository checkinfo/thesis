Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=256, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='MlpModel', dataset_type='TimeDataset', seed=10086, num_days=1, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=3, num_heads=1, gnn_layers=2, print_inteval=500, input_graph=False, mask_type='soft')
293997
MlpModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): ReLU()
)
train 0, step: 0, loss: 1.5951852705382832, grad_norm: 0.0013699726758069635, ic: 0.04077930529103216
Epoch 0: train loss: 1.5776409013854489
Eval step 0: eval loss: 1.084648025510225
Eval: total loss: 1.084648025510225, ic :-0.00017631642980903562 
train 1, step: 0, loss: 1.5769161936187466, grad_norm: 0.0008633662626480745, ic: 0.04214977901632805
Epoch 1: train loss: 1.6059878875982498
Eval step 0: eval loss: 1.084480765119976
Eval: total loss: 1.084480765119976, ic :0.0015801501850111334 
train 2, step: 0, loss: 1.4811000691587903, grad_norm: 0.0041818822669627215, ic: 0.04095165483712938
Epoch 2: train loss: 1.593432299886081
Eval step 0: eval loss: 1.0869536117348695
Eval: total loss: 1.0869536117348695, ic :0.009289215102181319 
train 3, step: 0, loss: 1.6472024149668314, grad_norm: 0.00023913279083549217, ic: 0.04133969593433741
Epoch 3: train loss: 1.5576746089180018
Eval step 0: eval loss: 1.0908390595750508
Eval: total loss: 1.0908390595750508, ic :0.02156190770971303 
train 4, step: 0, loss: 1.614071438333431, grad_norm: 0.0019857939506859688, ic: 0.04735415404980521
Epoch 4: train loss: 1.689135944709399
Eval step 0: eval loss: 1.0858829895858841
Eval: total loss: 1.0858829895858841, ic :0.009695134258797486 
train 5, step: 0, loss: 1.5716586457920867, grad_norm: 0.00010788313332049314, ic: 0.039324004975623425
Epoch 5: train loss: 1.72550223419248
Eval step 0: eval loss: 1.0842927512109677
Eval: total loss: 1.0842927512109677, ic :0.008562276206210617 
train 6, step: 0, loss: 1.6831416516006854, grad_norm: 0.002457368053158034, ic: 0.04232180019135327
Epoch 6: train loss: 1.5806953240767956
Eval step 0: eval loss: 1.0855168845440244
Eval: total loss: 1.0855168845440244, ic :0.01053509513944912 
train 7, step: 0, loss: 1.5556532791472866, grad_norm: 9.884016242349105e-05, ic: 0.04044254609277094
Epoch 7: train loss: 1.5737655897846985
Eval step 0: eval loss: 1.0853793924822452
Eval: total loss: 1.0853793924822452, ic :0.011601878878879356 
train 8, step: 0, loss: 1.4993266077526286, grad_norm: 0.0024250232937689763, ic: 0.05465779393851229
Epoch 8: train loss: 2.0139763143459146
Eval step 0: eval loss: 1.0848820862889803
Eval: total loss: 1.0848820862889803, ic :0.011158817916218202 
train 9, step: 0, loss: 1.6243937488773128, grad_norm: 0.006705101901030784, ic: 0.048142670605201575
Epoch 9: train loss: 1.5280663521369033
Eval step 0: eval loss: 1.0867268146876803
Eval: total loss: 1.0867268146876803, ic :0.024059014346136234 
train 10, step: 0, loss: 1.58100222100364, grad_norm: 0.00045072455645554267, ic: 0.04441548004824611
Epoch 10: train loss: 1.560148355707053
Eval step 0: eval loss: 1.0872800905268487
Eval: total loss: 1.0872800905268487, ic :0.02534457184840668 
train 11, step: 0, loss: 1.5447350080384736, grad_norm: 0.0004733572136041515, ic: 0.05209729067665335
Epoch 11: train loss: 1.58106820689825
Eval step 0: eval loss: 1.0859501791028667
Eval: total loss: 1.0859501791028667, ic :0.02629319605204882 
train 12, step: 0, loss: 1.493774573601868, grad_norm: 0.0026477996006503933, ic: 0.04200628906271458
Epoch 12: train loss: 1.7149813172637955
Eval step 0: eval loss: 1.0847156041556845
Eval: total loss: 1.0847156041556845, ic :0.03272136543076094 
train 13, step: 0, loss: 1.5671009549418724, grad_norm: 0.0008703832281073427, ic: 0.0619822780730637
Epoch 13: train loss: 1.6469267257449165
Eval step 0: eval loss: 1.0837992714477238
Eval: total loss: 1.0837992714477238, ic :0.02977365803354874 
train 14, step: 0, loss: 1.6107085368677414, grad_norm: 0.0005335430894041236, ic: 0.054955509253302504
Epoch 14: train loss: 1.567425500365434
Eval step 0: eval loss: 1.0836519863192804
Eval: total loss: 1.0836519863192804, ic :0.028587762797927675 
train 15, step: 0, loss: 1.6089565284007021, grad_norm: 0.0010265411887787634, ic: 0.05208610006595516
Epoch 15: train loss: 1.5931622137069816
Eval step 0: eval loss: 1.0840740610070436
Eval: total loss: 1.0840740610070436, ic :0.03555265157542796 
train 16, step: 0, loss: 1.6605055456002493, grad_norm: 0.0009975955695519755, ic: 0.045168776480714115
Epoch 16: train loss: 1.6475055934756888
Eval step 0: eval loss: 1.0856425081872632
Eval: total loss: 1.0856425081872632, ic :0.03741186588695014 
train 17, step: 0, loss: 1.5863662662743025, grad_norm: 0.0005990184432894334, ic: 0.04659592308233069
Epoch 17: train loss: 1.5676340305701673
Eval step 0: eval loss: 1.0884853511287838
Eval: total loss: 1.0884853511287838, ic :0.031168295837647134 
train 18, step: 0, loss: 1.6264370399207246, grad_norm: 0.001905552417005955, ic: 0.0463431393865943
Epoch 18: train loss: 1.5285036387360345
Eval step 0: eval loss: 1.0905787326241163
Eval: total loss: 1.0905787326241163, ic :0.03105328344708359 
train 19, step: 0, loss: 1.5235150729045532, grad_norm: 0.005369689355801477, ic: 0.043901919642075876
Epoch 19: train loss: 1.5371197434098351
Eval step 0: eval loss: 1.086802629885897
Eval: total loss: 1.086802629885897, ic :0.03722281181551606 
