Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='BiGLSTM', dataset_type='AdjTimeDataset', seed=10086, num_days=8, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=1, num_heads=1, gnn_layers=2, print_inteval=500, relation_num=1, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=False)
853115
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8224811888577644, grad_norm: 0.12863429554628292, ic: -0.027793866204674597
train 0, step: 500, loss: 1.0989442005108938, grad_norm: 0.014860083687053806, ic: 0.004937155423159468
train 0, step: 1000, loss: 0.6801176165590191, grad_norm: 6.797249127266673e-05, ic: 0.029470025979062987
train 0, step: 1500, loss: 0.8241052155447478, grad_norm: 0.16953929769646084, ic: 0.08716579231325876
train 0, step: 2000, loss: 2.349588068181818, grad_norm: 0.7178220076024239, ic: 0.016939026557456695
Epoch 0: train loss: 1.630245534733341
Eval step 0: eval loss: 0.835088407447176
Eval: total loss: 1.0736065724446962, mse:4.628853438408187, ic :0.0017248647973941655, sharpe5:1.3418630205094815, irr5:16.995677947998047, ndcg5:0.8391471067588718 
train 1, step: 0, loss: 2.2736104648315045, grad_norm: 0.042268859889627496, ic: 0.025164551623060312
train 1, step: 500, loss: 0.5958239697069528, grad_norm: 0.01429436708641241, ic: -0.061340697948850026
train 1, step: 1000, loss: 1.1878147773591896, grad_norm: 0.09705209752002393, ic: 0.07271479478533803
train 1, step: 1500, loss: 2.221684999346234, grad_norm: 0.8575573124048138, ic: 0.049305913234815375
train 1, step: 2000, loss: 1.1321810283358136, grad_norm: 0.012764781757409308, ic: -0.014630812852810874
Epoch 1: train loss: 1.6285381236839944
Eval step 0: eval loss: 0.8359885394039626
Eval: total loss: 1.0738596913234932, mse:4.628380681258466, ic :0.00500272352348804, sharpe5:2.7800508387386795, irr5:36.91748046875, ndcg5:0.8448781531664015 
train 2, step: 0, loss: 1.9866810021033654, grad_norm: 0.23851182969889612, ic: 0.07548216779637187
train 2, step: 500, loss: 1.0903939603965547, grad_norm: 0.4302718842297033, ic: 0.15953458249099087
train 2, step: 1000, loss: 1.0937203092075556, grad_norm: 0.05754715052692238, ic: 0.09350846032043658
train 2, step: 1500, loss: 1.2178377778091156, grad_norm: 0.27743449497490613, ic: 0.250340610181212
train 2, step: 2000, loss: 2.677919019170168, grad_norm: 4.735352006337428, ic: 0.09937020733494671
Epoch 2: train loss: 1.6285317728166775
Eval step 0: eval loss: 0.8376994522199183
Eval: total loss: 1.0744371428919912, mse:4.6283606867328775, ic :0.006168932674281818, sharpe5:4.437127995193005, irr5:65.05821228027344, ndcg5:0.8560888419091308 
train 3, step: 0, loss: 0.8178273569239248, grad_norm: 0.15021443871459852, ic: 0.1366901943175907
train 3, step: 500, loss: 1.2291330208629936, grad_norm: 0.06952323325115188, ic: 0.023212053643031902
train 3, step: 1000, loss: 1.3660631092316513, grad_norm: 0.3250261098487495, ic: 0.003856832124428711
train 3, step: 1500, loss: 1.629379587676932, grad_norm: 0.2656390280691273, ic: 0.06828397136449572
train 3, step: 2000, loss: 1.1091874162716406, grad_norm: 0.27490962660294344, ic: -0.06780408567325046
Epoch 3: train loss: 1.6276794158878238
Eval step 0: eval loss: 0.8416851539872958
Eval: total loss: 1.0762630279922203, mse:4.621765258881277, ic :0.07472754949727244, sharpe5:9.98783284664154, irr5:201.42947387695312, ndcg5:0.8324240598807336 
train 4, step: 0, loss: 1.088687885619904, grad_norm: 0.3674132528919808, ic: -0.005927143809801846
train 4, step: 500, loss: 0.9761853540619287, grad_norm: 0.037203008461406194, ic: 0.23815772519626577
train 4, step: 1000, loss: 1.1007514635293993, grad_norm: 0.02056738293868718, ic: 0.25048721132898155
train 4, step: 1500, loss: 1.263838047517878, grad_norm: 0.24554592560443583, ic: 0.020614451590155094
train 4, step: 2000, loss: 1.3360702831805715, grad_norm: 0.1428095173134778, ic: -0.0021270393139099122
Epoch 4: train loss: 1.6245004834293992
Eval step 0: eval loss: 0.831890795217055
Eval: total loss: 1.0716618120269765, mse:4.610768513334261, ic :0.07379521973162369, sharpe5:10.065666452050209, irr5:199.2624969482422, ndcg5:0.8276065539167713 
train 5, step: 0, loss: 2.261127752199343, grad_norm: 0.0037492591929946827, ic: -0.019721584796403114
train 5, step: 500, loss: 1.8002667004549051, grad_norm: 0.6511123223335898, ic: 0.004731673784409647
train 5, step: 1000, loss: 4.458718997124371, grad_norm: 0.7411603290457714, ic: -0.0093792887403886
train 5, step: 1500, loss: 0.9282416389604662, grad_norm: 0.022560406320518735, ic: 0.11730262514004455
train 5, step: 2000, loss: 2.2532401767127177, grad_norm: 0.6657926981698332, ic: 0.014987618960660263
Epoch 5: train loss: 1.6243040266179651
Eval step 0: eval loss: 0.8342564137374934
Eval: total loss: 1.0725659585828975, mse:4.607383281942857, ic :0.07483418013750766, sharpe5:10.032504188418388, irr5:204.6549530029297, ndcg5:0.8341627544442158 
train 6, step: 0, loss: 1.9850659640330188, grad_norm: 0.10487146242494987, ic: 0.26315126314974
train 6, step: 500, loss: 1.0061942159130588, grad_norm: 0.00011755384354663444, ic: -0.015532002102307025
train 6, step: 1000, loss: 1.56918193825815, grad_norm: 0.5183612443647574, ic: 0.019562170064637868
train 6, step: 1500, loss: 1.0850626627604167, grad_norm: 0.0037791631180762167, ic: 0.11606073717768169
train 6, step: 2000, loss: 1.0734222379609293, grad_norm: 0.14373294154193947, ic: 0.17842172568162362
Epoch 6: train loss: 1.623859416235351
Eval step 0: eval loss: 0.8299417842244272
Eval: total loss: 1.0714275664010906, mse:4.612733728231147, ic :0.07043498333741716, sharpe5:10.374557547569275, irr5:209.32449340820312, ndcg5:0.8218690741565018 
train 7, step: 0, loss: 1.1529305671470054, grad_norm: 0.13177053994844265, ic: 0.21855786385933015
train 7, step: 500, loss: 1.416601982075725, grad_norm: 0.03388422969358779, ic: 0.11289344176470033
train 7, step: 1000, loss: 1.0402065834980236, grad_norm: 0.09525705908164124, ic: -0.11788328575958033
train 7, step: 1500, loss: 1.8728435889500807, grad_norm: 0.6304999984168462, ic: -0.027092235866698938
train 7, step: 2000, loss: 2.963048500130604, grad_norm: 0.6829642500910198, ic: 0.039690682480921495
Epoch 7: train loss: 1.6247857956829705
Eval step 0: eval loss: 0.8376243715853738
Eval: total loss: 1.0743370580967195, mse:4.627771816588854, ic :0.020938429145972905, sharpe5:5.567633138000965, irr5:61.50519943237305, ndcg5:0.840144099456153 
train 8, step: 0, loss: 1.2646527403615901, grad_norm: 0.01961863302566507, ic: -0.004508582405827419
train 8, step: 500, loss: 0.6404359069051622, grad_norm: 0.07766870876705616, ic: -0.002226642214100136
train 8, step: 1000, loss: 1.9417805865053557, grad_norm: 0.5407754730926541, ic: 0.20133272037885078
train 8, step: 1500, loss: 1.4252498208543065, grad_norm: 0.2061763017390389, ic: 0.12468951570080318
train 8, step: 2000, loss: 3.898722296333474, grad_norm: 1.0121995912148012, ic: -0.04724996209610172
Epoch 8: train loss: 1.6258601973900195
Eval step 0: eval loss: 0.8318318492051737
Eval: total loss: 1.0710684156494374, mse:4.602598941500163, ic :0.07696897230394006, sharpe5:9.832383198738098, irr5:197.82424926757812, ndcg5:0.8457746443649024 
train 9, step: 0, loss: 0.7936520222897177, grad_norm: 0.009409605622945728, ic: 0.23193513973366345
train 9, step: 500, loss: 1.0679832987882654, grad_norm: 0.19068633156625578, ic: -0.039870496457134245
train 9, step: 1000, loss: 0.8009136400418888, grad_norm: 0.030912199068206154, ic: -0.043381304823369674
train 9, step: 1500, loss: 1.05303955078125, grad_norm: 0.10054490293620971, ic: 0.03364142534357795
train 9, step: 2000, loss: 6.699200396436962, grad_norm: 0.2807093023107554, ic: 0.1057461779811924
Epoch 9: train loss: 1.6225228885596783
Eval step 0: eval loss: 0.8329497021458662
Eval: total loss: 1.0712703908306538, mse:4.594879552079588, ic :0.07249601211650557, sharpe5:9.302894659638405, irr5:172.67665100097656, ndcg5:0.8573526423571104 
train 10, step: 0, loss: 0.9182784817230081, grad_norm: 0.01836216717011061, ic: 0.24497581910204103
train 10, step: 500, loss: 3.9430665473717483, grad_norm: 1.007894288560563, ic: 0.10062719593454979
train 10, step: 1000, loss: 1.3266664379020257, grad_norm: 0.5627265091275782, ic: 0.3183088624784999
train 10, step: 1500, loss: 1.4282738979325094, grad_norm: 0.005117706649536334, ic: -0.032777920967894905
train 10, step: 2000, loss: 1.3083034521458443, grad_norm: 0.5924245654620783, ic: -0.03193536410560577
Epoch 10: train loss: 1.6228822892644539
Eval step 0: eval loss: 0.8297177636667324
Eval: total loss: 1.0712438514530178, mse:4.605186049114449, ic :0.07309152580742428, sharpe5:9.965422920584679, irr5:200.18797302246094, ndcg5:0.8208735019269026 
train 11, step: 0, loss: 2.2739674217476913, grad_norm: 0.10372468582096327, ic: 0.03362019618123396
train 11, step: 500, loss: 1.8744291303519356, grad_norm: 0.2659993174426543, ic: 0.13423865227050433
train 11, step: 1000, loss: 2.9979824764075125, grad_norm: 1.045142976616924, ic: 0.10134979432803266
train 11, step: 1500, loss: 1.003415816869491, grad_norm: 0.12599908149295108, ic: -0.008022572180309264
train 11, step: 2000, loss: 1.1717221454139357, grad_norm: 0.4711688670828839, ic: 0.18196943141746863
Epoch 11: train loss: 1.622783839685398
Eval step 0: eval loss: 0.8295458752962085
Eval: total loss: 1.0694857299591694, mse:4.591194369125761, ic :0.07996077564178077, sharpe5:10.728609974384307, irr5:218.0637664794922, ndcg5:0.8373961475830488 
train 12, step: 0, loss: 1.3377454762293846, grad_norm: 0.6395533828845851, ic: -0.06865574007900278
train 12, step: 500, loss: 1.9096672169697844, grad_norm: 0.228485035413193, ic: 0.05852014476699273
train 12, step: 1000, loss: 1.3410305236437305, grad_norm: 0.17494698130174602, ic: 0.24181596576319972
train 12, step: 1500, loss: 1.1254455148038842, grad_norm: 0.2520872948442787, ic: 0.1223016185849758
train 12, step: 2000, loss: 2.803177604266367, grad_norm: 0.2974356281919271, ic: 0.05042320546940374
Epoch 12: train loss: 1.6219388071503893
Eval step 0: eval loss: 0.8288641071369799
Eval: total loss: 1.0682493236337873, mse:4.589912916093077, ic :0.09527479923599066, sharpe5:10.78588388442993, irr5:210.10147094726562, ndcg5:0.8358556977045316 
train 13, step: 0, loss: 1.9063068125982705, grad_norm: 0.07366184763909692, ic: 0.07177491866834515
train 13, step: 500, loss: 2.8153014991050123, grad_norm: 1.0132693660419536, ic: -0.023629103842210415
train 13, step: 1000, loss: 0.9797657064448471, grad_norm: 0.00867047464097777, ic: 0.033786728168178744
train 13, step: 1500, loss: 0.9573316450807984, grad_norm: 0.0003696732163249852, ic: 0.08180909045900273
train 13, step: 2000, loss: 0.7425982353650487, grad_norm: 0.007377744323295068, ic: -0.05383425337055793
Epoch 13: train loss: 1.6210392180914746
Eval step 0: eval loss: 0.8823744867775802
Eval: total loss: 1.1497750565652611, mse:5.099284653973493, ic :0.06155129587599181, sharpe5:10.318265508413313, irr5:176.27865600585938, ndcg5:0.8208669409181923 
train 14, step: 0, loss: 1.8528335723994418, grad_norm: 2.811263799770473, ic: 0.0719940862723555
train 14, step: 500, loss: 0.7727703498939265, grad_norm: 0.0015667101065756413, ic: -0.05794269875707833
train 14, step: 1000, loss: 3.9520039461096936, grad_norm: 0.8972039724998708, ic: 0.1956037059448713
train 14, step: 1500, loss: 1.434526149238939, grad_norm: 0.3142897009884012, ic: 0.06830338322380478
train 14, step: 2000, loss: 1.2992382231212798, grad_norm: 0.06701003085612052, ic: 0.19068602166163895
Epoch 14: train loss: 1.6209831192074557
Eval step 0: eval loss: 0.8342444574035676
Eval: total loss: 1.0694165896550192, mse:4.582401321267567, ic :0.10118475306344811, sharpe5:11.816396166086196, irr5:209.68670654296875, ndcg5:0.8361794349907671 
train 15, step: 0, loss: 0.9208236860795453, grad_norm: 0.09277703946992291, ic: 0.20650512425985473
train 15, step: 500, loss: 2.3470211506976084, grad_norm: 0.021718170614839733, ic: 0.018800149895461853
train 15, step: 1000, loss: 1.150649773486443, grad_norm: 0.209740696806286, ic: 0.14022726510581646
train 15, step: 1500, loss: 0.8614392154431217, grad_norm: 0.0032581038989857028, ic: 0.12929013816071472
train 15, step: 2000, loss: 2.854126560010621, grad_norm: 0.5649372938655317, ic: 0.2478252129776109
Epoch 15: train loss: 1.6207886972621013
Eval step 0: eval loss: 0.8335513757240652
Eval: total loss: 1.069893172757087, mse:4.583598944991435, ic :0.1005569362843115, sharpe5:11.314544738531112, irr5:212.6635284423828, ndcg5:0.8374888869310544 
train 16, step: 0, loss: 0.9141507835865279, grad_norm: 0.009789297863116964, ic: -0.007189606225361304
train 16, step: 500, loss: 1.4343985613313877, grad_norm: 0.004098392369748494, ic: 0.015467087212830521
train 16, step: 1000, loss: 0.9660237310065893, grad_norm: 0.12806371706420003, ic: -0.00022897774978913768
train 16, step: 1500, loss: 2.72354867906668, grad_norm: 0.502723156634849, ic: -0.029680230075489263
train 16, step: 2000, loss: 1.8982298719179465, grad_norm: 0.6567725499415623, ic: -0.013438806317224211
Epoch 16: train loss: 1.6204616519665573
Eval step 0: eval loss: 0.8282873747284755
Eval: total loss: 1.0676098878867208, mse:4.584484662762371, ic :0.102393924986887, sharpe5:11.509788535237313, irr5:199.78794860839844, ndcg5:0.8271221189784359 
train 17, step: 0, loss: 0.7653322537236282, grad_norm: 0.11258664449439193, ic: 0.07008989103230394
train 17, step: 500, loss: 1.8146593283397199, grad_norm: 0.3028369378596055, ic: 0.13170430999450167
train 17, step: 1000, loss: 2.0262172596201466, grad_norm: 0.774853816641875, ic: 0.02434441715036608
train 17, step: 1500, loss: 1.1272109427103183, grad_norm: 0.20016857062452378, ic: 0.05587079900554398
train 17, step: 2000, loss: 1.5325273282266076, grad_norm: 0.7364577253822849, ic: 0.12408710935588267
Epoch 17: train loss: 1.6221420892157281
Eval step 0: eval loss: 0.8390492966075894
Eval: total loss: 1.0721404521816842, mse:4.595029663664819, ic :0.09427433311183435, sharpe5:10.822333239912986, irr5:212.01498413085938, ndcg5:0.8385885483233687 
train 18, step: 0, loss: 1.016998384341552, grad_norm: 0.04735334755757617, ic: 0.055593738362364066
train 18, step: 500, loss: 1.5726023314647641, grad_norm: 0.28015743803564047, ic: 0.0630435596787506
train 18, step: 1000, loss: 1.758461136340312, grad_norm: 0.36134096410627603, ic: 0.005274282612633371
train 18, step: 1500, loss: 0.7641460379683812, grad_norm: 0.02666112642563384, ic: 0.016567418833347277
train 18, step: 2000, loss: 3.24561589183087, grad_norm: 0.8138771176885585, ic: 0.004208809919820919
Epoch 18: train loss: 1.6201869767555217
Eval step 0: eval loss: 0.8356364060854397
Eval: total loss: 1.0695177168357621, mse:4.580782970553434, ic :0.10215502688118186, sharpe5:11.326087870001793, irr5:202.04550170898438, ndcg5:0.8568498593134328 
train 19, step: 0, loss: 2.3476722842552777, grad_norm: 0.1262262739787518, ic: 0.048077752601459314
train 19, step: 500, loss: 9.22515573038928, grad_norm: 1.313019594416854, ic: -0.04894037978611402
train 19, step: 1000, loss: 1.2697392422408884, grad_norm: 0.38984687150651487, ic: -0.003152482907058061
train 19, step: 1500, loss: 0.7275846427980914, grad_norm: 0.029151648898761824, ic: 0.2108766408729582
train 19, step: 2000, loss: 1.3059912555293949, grad_norm: 0.20169766916700851, ic: -0.025546758239573916
Epoch 19: train loss: 1.6204944893363604
Eval step 0: eval loss: 0.8378802114188388
Eval: total loss: 1.0700265482960145, mse:4.582729665442601, ic :0.09991594822676116, sharpe5:11.843452479243277, irr5:212.92050170898438, ndcg5:0.8350633639160752 
