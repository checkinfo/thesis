Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='BiGLSTM', dataset_type='AdjTimeDataset', seed=10086, num_days=8, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=1, num_heads=1, gnn_layers=2, print_inteval=500, relation_num=1, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=True)
816241
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8224946406849644, grad_norm: 0.12882063193078525, ic: -0.027690082036425437
train 0, step: 500, loss: 1.0990338119235052, grad_norm: 0.015086501105145497, ic: 0.002420934192703096
train 0, step: 1000, loss: 0.6800818112817141, grad_norm: 6.238120292754446e-05, ic: 0.03000583870502569
train 0, step: 1500, loss: 0.8241476682153078, grad_norm: 0.1699456970769887, ic: 0.07567155270867251
train 0, step: 2000, loss: 2.3493607954545452, grad_norm: 0.7193954545087312, ic: 0.017851970245608664
Epoch 0: train loss: 1.6302727866565891
Eval step 0: eval loss: 0.8350788938051277
Eval: total loss: 1.0736034541629076, mse:4.628852837744158, ic :0.002923370502891804, sharpe5:2.174989693164825, irr5:28.530662536621094, ndcg5:0.8547619759635573 
train 1, step: 0, loss: 2.273624240791536, grad_norm: 0.04233484390947533, ic: 0.02594526340137595
train 1, step: 500, loss: 0.5955083517664408, grad_norm: 0.01385850155936003, ic: -0.038919752467571046
train 1, step: 1000, loss: 1.1877144191576088, grad_norm: 0.09695869442178638, ic: 0.07820327873666572
train 1, step: 1500, loss: 2.2216903111924684, grad_norm: 0.8576798249136417, ic: 0.05585622562449973
train 1, step: 2000, loss: 1.1322565956721231, grad_norm: 0.012730713738327855, ic: -0.026151395409583025
Epoch 1: train loss: 1.628536019535886
Eval step 0: eval loss: 0.8360811688512045
Eval: total loss: 1.073885170699247, mse:4.628372554625008, ic :0.004185715930027713, sharpe5:1.3423247504979372, irr5:17.094575881958008, ndcg5:0.8469047738524084 
train 2, step: 0, loss: 1.9861767578125, grad_norm: 0.23969924191532113, ic: 0.06822505238060891
train 2, step: 500, loss: 1.0854254873710054, grad_norm: 0.4190495801599876, ic: 0.13164231176268323
train 2, step: 1000, loss: 1.0883700872027393, grad_norm: 0.05861235965225457, ic: 0.10198888373400586
train 2, step: 1500, loss: 1.2176867690470818, grad_norm: 0.2769923226386343, ic: 0.2785892100172973
train 2, step: 2000, loss: 2.49856052150974, grad_norm: 1.832464459554223, ic: 0.124597373064686
Epoch 2: train loss: 1.628557278734657
Eval step 0: eval loss: 0.8379376789593206
Eval: total loss: 1.0744897139754235, mse:4.627896148769224, ic :0.028811299781034045, sharpe5:10.333994361162185, irr5:207.9779510498047, ndcg5:0.8152333586422216 
train 3, step: 0, loss: 0.8172178362855816, grad_norm: 0.14898074301495096, ic: 0.17245008443797927
train 3, step: 500, loss: 1.2289727630267655, grad_norm: 0.06948532692708403, ic: 0.009343198267637751
train 3, step: 1000, loss: 1.368643197809155, grad_norm: 0.3171148794246371, ic: 0.013659831646833764
train 3, step: 1500, loss: 1.6316539994978958, grad_norm: 0.274321289011212, ic: -0.014430637591239052
train 3, step: 2000, loss: 1.1268784537948269, grad_norm: 0.2781469276564795, ic: -0.06875169580147236
Epoch 3: train loss: 1.6266053053607554
Eval step 0: eval loss: 0.8387874786071616
Eval: total loss: 1.0746336300475454, mse:4.616881184632952, ic :0.07529507511942822, sharpe5:10.030256668925285, irr5:209.5050811767578, ndcg5:0.8278883008503342 
train 4, step: 0, loss: 1.1014787499544991, grad_norm: 0.36170747721077456, ic: -0.008370213378214678
train 4, step: 500, loss: 0.9806602984793953, grad_norm: 0.03150266482775038, ic: 0.2286104242847383
train 4, step: 1000, loss: 1.0980446406039703, grad_norm: 0.01398277274844297, ic: 0.26848260396741597
train 4, step: 1500, loss: 1.267102471713362, grad_norm: 0.2441509989735668, ic: 0.0392681893475233
train 4, step: 2000, loss: 1.3384750401450636, grad_norm: 0.14117652948884873, ic: -0.0009743468147274492
Epoch 4: train loss: 1.6241137522432905
Eval step 0: eval loss: 0.831952312483544
Eval: total loss: 1.0717235729642325, mse:4.611524848574522, ic :0.07150213987948677, sharpe5:10.13124464392662, irr5:206.0633087158203, ndcg5:0.8326121710291322 
train 5, step: 0, loss: 2.26112812983372, grad_norm: 0.003691193426934529, ic: -0.013671233762559078
train 5, step: 500, loss: 1.8024183828619462, grad_norm: 0.6434989396781902, ic: 0.004262421117463118
train 5, step: 1000, loss: 4.4610698379088785, grad_norm: 0.7405681137165779, ic: 0.025329304658507745
train 5, step: 1500, loss: 0.9275675162994603, grad_norm: 0.022889797798203798, ic: 0.11872290157632048
train 5, step: 2000, loss: 2.248487843608062, grad_norm: 0.644049375677522, ic: 0.02364747540563889
Epoch 5: train loss: 1.6244957568191825
Eval step 0: eval loss: 0.8348162401469523
Eval: total loss: 1.0727599120140463, mse:4.608202450934169, ic :0.07343078949410863, sharpe5:10.206569654345511, irr5:208.17608642578125, ndcg5:0.8407115707018801 
train 6, step: 0, loss: 1.9877428508254715, grad_norm: 0.13915952638808793, ic: 0.2640832247112899
train 6, step: 500, loss: 1.006345707801277, grad_norm: 0.0001619343254945966, ic: -0.006584313654815257
train 6, step: 1000, loss: 1.5671666212566338, grad_norm: 0.5097704336809564, ic: 0.00851424104886982
train 6, step: 1500, loss: 1.0820719401041667, grad_norm: 0.004388242328118698, ic: 0.11748072096063086
train 6, step: 2000, loss: 1.0745362108126997, grad_norm: 0.14228276872119017, ic: 0.1742621704850003
Epoch 6: train loss: 1.6238264329256347
Eval step 0: eval loss: 0.8306547288457741
Eval: total loss: 1.0713849886714824, mse:4.607863408888308, ic :0.07550614356617145, sharpe5:9.788971830010414, irr5:191.67420959472656, ndcg5:0.8369223779161118 
train 7, step: 0, loss: 1.1485625796312027, grad_norm: 0.1394530501792749, ic: 0.2367906492372787
train 7, step: 500, loss: 1.4204791240098011, grad_norm: 0.03722093310596079, ic: 0.11337424921612335
train 7, step: 1000, loss: 1.0434966920392788, grad_norm: 0.0976739267743287, ic: -0.11793843454504847
train 7, step: 1500, loss: 1.881705084440683, grad_norm: 0.6439130321863783, ic: 0.02935365334280559
train 7, step: 2000, loss: 2.969637143213336, grad_norm: 0.6915915054744631, ic: 0.050899583750106635
Epoch 7: train loss: 1.62379317323169
Eval step 0: eval loss: 0.8412269564162058
Eval: total loss: 1.0760664576117143, mse:4.607166661336607, ic :0.07051912225126837, sharpe5:10.242818473577499, irr5:211.9651641845703, ndcg5:0.8429722196675666 
train 8, step: 0, loss: 1.2624188068726054, grad_norm: 0.00036704421580353283, ic: -0.04933944299442151
train 8, step: 500, loss: 0.6370538156243819, grad_norm: 0.0768104787644363, ic: 0.016181562344232188
train 8, step: 1000, loss: 1.9138392803772952, grad_norm: 0.5251961188903242, ic: 0.20264365984536978
train 8, step: 1500, loss: 1.4236000965638982, grad_norm: 0.20251012269088184, ic: 0.1254807779118968
train 8, step: 2000, loss: 3.901218598108622, grad_norm: 1.0169315713857177, ic: -0.06342178416733799
Epoch 8: train loss: 1.6220964857117248
Eval step 0: eval loss: 0.8312433533068391
Eval: total loss: 1.0707317580020395, mse:4.5945452011296375, ic :0.07199324465528102, sharpe5:11.120903339982032, irr5:184.19241333007812, ndcg5:0.8384272391227436 
train 9, step: 0, loss: 0.7933543331492551, grad_norm: 0.021667062476652506, ic: 0.23831454522944695
train 9, step: 500, loss: 1.0598713528380102, grad_norm: 0.19517309348895373, ic: 0.004205445571444659
train 9, step: 1000, loss: 0.7996849414508759, grad_norm: 0.029905778536587276, ic: -0.04786729050707027
train 9, step: 1500, loss: 1.0477365270837562, grad_norm: 0.10331754418956375, ic: 0.050060670394469715
train 9, step: 2000, loss: 6.691246421055207, grad_norm: 0.2940897697544733, ic: 0.13062119933729655
Epoch 9: train loss: 1.6218455474506244
Eval step 0: eval loss: 0.8325754560377171
Eval: total loss: 1.0710401402940957, mse:4.593575270913464, ic :0.07690160022296469, sharpe5:9.958260371088981, irr5:198.42332458496094, ndcg5:0.8152517210415836 
train 10, step: 0, loss: 0.9157937578680392, grad_norm: 0.015036623614076577, ic: 0.255985853929057
train 10, step: 500, loss: 3.948334975049675, grad_norm: 1.0004009387652268, ic: 0.076184321043419
train 10, step: 1000, loss: 1.3158591837377591, grad_norm: 0.4282011113855686, ic: 0.31393978118606874
train 10, step: 1500, loss: 1.429199125920865, grad_norm: 0.005142949212183445, ic: -0.08485276944813502
train 10, step: 2000, loss: 1.306253085792286, grad_norm: 0.5980966364954962, ic: 0.04500071476326703
Epoch 10: train loss: 1.6213140216757738
Eval step 0: eval loss: 0.8286681775358741
Eval: total loss: 1.069478808751328, mse:4.590266027274547, ic :0.07613345807510838, sharpe5:10.915891914367675, irr5:205.30287170410156, ndcg5:0.8298528550501436 
train 11, step: 0, loss: 2.2768793390989948, grad_norm: 0.10356940858996778, ic: -0.16572449132666991
train 11, step: 500, loss: 1.8756735226255967, grad_norm: 0.2639342721768554, ic: 0.13220994087488566
train 11, step: 1000, loss: 3.015257623071271, grad_norm: 1.0061695945948834, ic: 0.09679409098638299
train 11, step: 1500, loss: 1.0117044693384414, grad_norm: 0.14142785074172776, ic: -0.044601325287741724
train 11, step: 2000, loss: 1.1707498500409643, grad_norm: 0.46993475637945403, ic: 0.1843020461105808
Epoch 11: train loss: 1.621347572044356
Eval step 0: eval loss: 0.8306729204721234
Eval: total loss: 1.0707628954019435, mse:4.59819417566909, ic :0.06953740894353824, sharpe5:10.807953277230263, irr5:177.7728271484375, ndcg5:0.8258314495079626 
train 12, step: 0, loss: 1.3212218894559533, grad_norm: 0.6243090376702239, ic: 0.07719941593081937
train 12, step: 500, loss: 1.914282771603156, grad_norm: 0.2394527392678315, ic: 0.03799602285670031
train 12, step: 1000, loss: 1.347200192279697, grad_norm: 0.13113990037522083, ic: 0.24010182506611416
train 12, step: 1500, loss: 1.1260752972082064, grad_norm: 0.2457017925669469, ic: 0.08526094104354699
train 12, step: 2000, loss: 2.803150311363419, grad_norm: 0.29169643780129073, ic: -0.012635102561764748
Epoch 12: train loss: 1.6211505154484105
Eval step 0: eval loss: 0.8300108866920088
Eval: total loss: 1.070119625274528, mse:4.593327835235439, ic :0.07461474039597235, sharpe5:11.181995006203651, irr5:193.49990844726562, ndcg5:0.8305688196723741 
train 13, step: 0, loss: 1.915259211318298, grad_norm: 0.07736176069636307, ic: 0.10946767600909936
train 13, step: 500, loss: 2.8035439041368337, grad_norm: 1.0928896746046108, ic: -0.027640466962940497
train 13, step: 1000, loss: 0.9788181344320889, grad_norm: 0.0015587403008162095, ic: -0.051087505497118255
train 13, step: 1500, loss: 0.9585722136406843, grad_norm: 0.00039983842939083665, ic: 0.07024740939375704
train 13, step: 2000, loss: 0.7406821975274167, grad_norm: 0.007820032640119382, ic: 0.041666937508497245
Epoch 13: train loss: 1.6210733719719879
Eval step 0: eval loss: 0.8297065144278238
Eval: total loss: 1.069863603128484, mse:4.5939479564559536, ic :0.07629508581200477, sharpe5:11.899875056147575, irr5:209.0834503173828, ndcg5:0.8392078701284769 
train 14, step: 0, loss: 2.2933374981957275, grad_norm: 0.5090339899326303, ic: 0.01788957253608208
train 14, step: 500, loss: 0.7709948367330521, grad_norm: 0.0007931384456124398, ic: 0.03307392718389923
train 14, step: 1000, loss: 3.944395989275644, grad_norm: 0.8963722239426779, ic: 0.17321693327791465
train 14, step: 1500, loss: 1.4110936199955952, grad_norm: 0.327370893632691, ic: 0.13038807724643048
train 14, step: 2000, loss: 1.3085374620225694, grad_norm: 0.06446622569088836, ic: 0.06528936400542007
Epoch 14: train loss: 1.6210219717569863
Eval step 0: eval loss: 0.8331897930448591
Eval: total loss: 1.0709620228462593, mse:4.590604877492417, ic :0.07705114001668416, sharpe5:11.698518687486647, irr5:211.1965789794922, ndcg5:0.8448869122695242 
train 15, step: 0, loss: 0.9217317116477273, grad_norm: 0.08326930318138355, ic: 0.20055160367615651
train 15, step: 500, loss: 2.340274564588079, grad_norm: 0.026155116821890572, ic: 0.07178271815491184
train 15, step: 1000, loss: 1.1488154686888215, grad_norm: 0.23382680905223507, ic: 0.1780061824640124
train 15, step: 1500, loss: 0.8649215871244331, grad_norm: 0.0025140537678367374, ic: 0.0074328885208791535
train 15, step: 2000, loss: 2.945843048825013, grad_norm: 0.7182316762018487, ic: -0.08027257138228905
Epoch 15: train loss: 1.6208602764874631
Eval step 0: eval loss: 0.8273891069312795
Eval: total loss: 1.0694424549394148, mse:4.579043584724702, ic :0.09320362715886246, sharpe5:14.248249148726462, irr5:271.6849670410156, ndcg5:0.8365306455254962 
train 16, step: 0, loss: 0.9136000400395935, grad_norm: 0.010392244746444769, ic: -0.0479843844561757
train 16, step: 500, loss: 1.4345337006750665, grad_norm: 0.005723385814643867, ic: 0.02023633516638127
train 16, step: 1000, loss: 0.9807945269472856, grad_norm: 0.15390203267091712, ic: 0.06684867175830263
train 16, step: 1500, loss: 2.705952498033045, grad_norm: 0.4516350099153169, ic: 0.029458473101605223
train 16, step: 2000, loss: 1.8908764431335892, grad_norm: 0.7023855401547943, ic: -0.008189770743131309
Epoch 16: train loss: 1.6183570683257449
Eval step 0: eval loss: 0.8233307029398697
Eval: total loss: 1.0692712263056525, mse:4.588213988849015, ic :0.08746570762392443, sharpe5:14.079333302378654, irr5:284.4529113769531, ndcg5:0.8340835326707953 
train 17, step: 0, loss: 0.7597471016522633, grad_norm: 0.11167001289707333, ic: 0.05196199499478988
train 17, step: 500, loss: 1.806628753468552, grad_norm: 0.292343731506399, ic: 0.10432095467933872
train 17, step: 1000, loss: 2.03522821538233, grad_norm: 0.7810599363444484, ic: -0.018654836030640824
train 17, step: 1500, loss: 1.1250548944240664, grad_norm: 0.1586146021721983, ic: -0.01480360555103435
train 17, step: 2000, loss: 1.529807559327583, grad_norm: 0.45990852342712196, ic: 0.12913588215931637
Epoch 17: train loss: 1.6175750728880949
Eval step 0: eval loss: 0.8316652319065955
Eval: total loss: 1.0701143163321911, mse:4.575507879311871, ic :0.10983853623898183, sharpe5:16.21993954181671, irr5:314.4988098144531, ndcg5:0.8277859781221495 
train 18, step: 0, loss: 1.0203301199350152, grad_norm: 0.055011195945569843, ic: 0.034604121208381994
train 18, step: 500, loss: 1.577563206401063, grad_norm: 0.3063928656125425, ic: 0.09805828169944338
train 18, step: 1000, loss: 1.7586898427878694, grad_norm: 0.263290261339541, ic: -0.027826343186106803
train 18, step: 1500, loss: 0.7649466021618067, grad_norm: 0.029153687400549626, ic: -0.020093846392145242
train 18, step: 2000, loss: 3.250481265714799, grad_norm: 0.7850742251341577, ic: 0.058737573846163224
Epoch 18: train loss: 1.6177308410079603
Eval step 0: eval loss: 0.8297668103483741
Eval: total loss: 1.0689814705057834, mse:4.574140522082472, ic :0.10684169637553091, sharpe5:17.49155126094818, irr5:310.6344299316406, ndcg5:0.8319223432142726 
train 19, step: 0, loss: 2.314793852924648, grad_norm: 0.0948725270713367, ic: 0.15526024904023275
train 19, step: 500, loss: 9.214279819542254, grad_norm: 1.2072132599480492, ic: -0.0008800862372560521
train 19, step: 1000, loss: 1.254605767576642, grad_norm: 0.1910348623875568, ic: 0.02061411162571054
train 19, step: 1500, loss: 0.7469426138510961, grad_norm: 0.031002240000364206, ic: 0.0612315675991584
train 19, step: 2000, loss: 1.3032386350123668, grad_norm: 0.20534012886677025, ic: 0.020423784816093516
Epoch 19: train loss: 1.6168134484298158
Eval step 0: eval loss: 0.8304076955593404
Eval: total loss: 1.0699672786049952, mse:4.574714758867064, ic :0.10640842037955027, sharpe5:17.379865812063215, irr5:310.4892883300781, ndcg5:0.8300618501078614 
