Namespace(train_path='./data/train_2305_1931_12.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', test_mask_path='./data/test_mask_126_1931.npy', label_cnt=3, batch_size=1, lr=0.001, adj_path='./data/concepts_graph_1931_233_3.npy', model_type='GNNModel', dataset_type='AdjTimeDataset', seed=10086, num_days=1, epochs=20, hidden_dim=128, input_dim=9, dout=0.3, lstm_layers=3, num_heads=1, gnn_layers=2, print_inteval=500, mask_type='soft', shuffle=True, input_graph=True, use_adj=False, mask_adj=True)
450012
GNNModel(
  (fc1): Linear(in_features=9, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
  (gnns): ModuleList(
    (0): GraphConv(128, 128)
    (1): GraphConv(128, 128)
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (relu): LeakyReLU(negative_slope=0.01)
)
train 0, step: 0, loss: 0.846193612339985, grad_norm: 0.00010983532791725252, ic: -0.04420470280196351
train 0, step: 500, loss: 0.9536256130674985, grad_norm: 0.14605472777887307, ic: -0.031730671754279204
train 0, step: 1000, loss: 1.015875154646261, grad_norm: 0.02308058110131486, ic: -0.11939903689396941
train 0, step: 1500, loss: 0.9279013465876318, grad_norm: 0.1035652960992805, ic: 0.054694030856998505
train 0, step: 2000, loss: 1.0579190159622958, grad_norm: 0.3265775548836272, ic: 0.014441747818247865
Epoch 0: train loss: 1.6294835563118806
Eval step 0: eval loss: 1.0092554238044693
Eval: total loss: 1.0857726183998886, ic :0.015120801756580541 
train 1, step: 0, loss: 0.9466753433607652, grad_norm: 0.07826664083341446, ic: 0.0761674839230579
train 1, step: 500, loss: 1.679684912589894, grad_norm: 0.10385325233498405, ic: 0.0004989623443280775
train 1, step: 1000, loss: 1.1068824149154188, grad_norm: 0.3560614361729359, ic: 0.05582618675423285
train 1, step: 1500, loss: 0.8807523738478665, grad_norm: 0.12158202112837024, ic: -0.02171950890948284
train 1, step: 2000, loss: 4.807170234130094, grad_norm: 0.5609864052709741, ic: 0.05115040737702832
Epoch 1: train loss: 1.6281676244700551
Eval step 0: eval loss: 0.9981838585892245
Eval: total loss: 1.083515897961138, ic :0.051922415915887254 
train 2, step: 0, loss: 0.9497835224572243, grad_norm: 0.10827123840650373, ic: 0.005173413460710234
train 2, step: 500, loss: 1.2850636498327515, grad_norm: 0.09475421372921283, ic: 0.19124472956856864
train 2, step: 1000, loss: 1.0490896224975585, grad_norm: 0.01011382452176955, ic: 0.0048338455227322705
train 2, step: 1500, loss: 0.9403798359322177, grad_norm: 0.2512311808128754, ic: -0.011708037064772749
train 2, step: 2000, loss: 1.7030387247415415, grad_norm: 0.0766865881761211, ic: 0.08756869820494415
Epoch 2: train loss: 1.6266910443978375
Eval step 0: eval loss: 0.9918201319979264
Eval: total loss: 1.084644211624156, ic :0.04541857840825007 
train 3, step: 0, loss: 0.8328132659313725, grad_norm: 0.06214522512523002, ic: 0.23254080899909133
train 3, step: 500, loss: 1.182093131941738, grad_norm: 0.0039080909486121935, ic: 0.10243474477936369
train 3, step: 1000, loss: 0.9080774182471159, grad_norm: 0.029126957281752103, ic: 0.21086715203521483
train 3, step: 1500, loss: 4.376616405055429, grad_norm: 0.7261971808800397, ic: 0.08488240699007973
train 3, step: 2000, loss: 1.6399790282418107, grad_norm: 0.38086772524195056, ic: -0.0107375639905728
Epoch 3: train loss: 1.6256156075780421
Eval step 0: eval loss: 0.9969610341791731
Eval: total loss: 1.0869531804770016, ic :0.04581920812501043 
train 4, step: 0, loss: 1.2748737306173896, grad_norm: 0.2644621030525172, ic: 0.03657325510636877
train 4, step: 500, loss: 0.8296257679149499, grad_norm: 0.03936740089095522, ic: -0.03974526405908117
train 4, step: 1000, loss: 1.8650070578471083, grad_norm: 0.18969395641892842, ic: -0.05386005951539865
train 4, step: 1500, loss: 1.1727029245349856, grad_norm: 0.011053968113051166, ic: 0.1700651734673743
train 4, step: 2000, loss: 6.638163768796993, grad_norm: 0.9416110168518983, ic: -0.04330495281632446
Epoch 4: train loss: 1.625859321120322
Eval step 0: eval loss: 0.9998583238711163
Eval: total loss: 1.0845759726564401, ic :0.054646049886587604 
train 5, step: 0, loss: 3.7372563788231385, grad_norm: 0.5032783232844649, ic: 0.1696969485000812
train 5, step: 500, loss: 3.4380074221752497, grad_norm: 0.720437930734509, ic: -0.0451829123573262
train 5, step: 1000, loss: 1.0416163600909365, grad_norm: 0.04743404021440081, ic: 0.07432086535087362
train 5, step: 1500, loss: 1.489296795712111, grad_norm: 0.9626116401193925, ic: -0.06639118875842862
train 5, step: 2000, loss: 1.7468457153396655, grad_norm: 0.48352071037475797, ic: 0.037310580076687846
Epoch 5: train loss: 1.6262888841138023
Eval step 0: eval loss: 0.998291722720017
Eval: total loss: 1.0856051155194752, ic :0.051800067150381206 
train 6, step: 0, loss: 1.1065098792053258, grad_norm: 0.030558612278025126, ic: -0.05637561588079803
train 6, step: 500, loss: 1.848883670221561, grad_norm: 0.10472118385618091, ic: 0.18543149313791465
train 6, step: 1000, loss: 0.890363667725954, grad_norm: 0.194324389863973, ic: 0.056514585246324106
train 6, step: 1500, loss: 0.8444487756039916, grad_norm: 0.0027411681690638415, ic: 0.0933330606748773
train 6, step: 2000, loss: 1.9333395397616733, grad_norm: 0.2344221858975758, ic: 0.07080418189825231
Epoch 6: train loss: 1.6252813894396307
Eval step 0: eval loss: 0.9903324683632833
Eval: total loss: 1.081419048994014, ic :0.0534477563442068 
train 7, step: 0, loss: 0.8172724487219858, grad_norm: 0.07840554986803885, ic: -0.0420908026460328
train 7, step: 500, loss: 1.1255182115333215, grad_norm: 0.015155346758988886, ic: 0.039290469816622424
train 7, step: 1000, loss: 1.1027470067842284, grad_norm: 0.02695733906845714, ic: -0.07586058649860977
train 7, step: 1500, loss: 1.072110054993783, grad_norm: 0.06301943560564771, ic: 0.17090798918837502
train 7, step: 2000, loss: 0.7097945934872898, grad_norm: 0.015154477857532532, ic: -0.028830252939169403
Epoch 7: train loss: 1.623701775696795
Eval step 0: eval loss: 1.0015162688421537
Eval: total loss: 1.0946175570497803, ic :0.0339255392246419 
train 8, step: 0, loss: 2.02200897897848, grad_norm: 0.34583661526638626, ic: -0.10576125511599443
train 8, step: 500, loss: 0.9305688476562499, grad_norm: 0.2408685999219647, ic: 0.32635424169142835
train 8, step: 1000, loss: 0.7432541025613444, grad_norm: 0.033534510832895825, ic: 0.09120207402873017
train 8, step: 1500, loss: 0.8419647575827205, grad_norm: 0.04123476156848804, ic: 0.19608242999949302
train 8, step: 2000, loss: 1.4428625727634803, grad_norm: 0.8608875227900016, ic: 0.13311394852179684
Epoch 8: train loss: 1.6203327398059881
Eval step 0: eval loss: 1.0007992744947998
Eval: total loss: 1.0853855858880201, ic :0.062250884000492564 
train 9, step: 0, loss: 0.7766706457511153, grad_norm: 0.04969341566048881, ic: 0.20102500191415493
train 9, step: 500, loss: 1.7464133047389827, grad_norm: 0.05119778057104098, ic: -0.12183821067635248
train 9, step: 1000, loss: 0.7732900828618148, grad_norm: 0.01030504643444553, ic: 0.17643092965732077
train 9, step: 1500, loss: 0.9850228893898221, grad_norm: 0.05085209772022595, ic: 0.09343790152365614
train 9, step: 2000, loss: 4.492843903186274, grad_norm: 1.6389407571449441, ic: 0.23671850320456025
Epoch 9: train loss: 1.6166451869545084
Eval step 0: eval loss: 0.9945501615776394
Eval: total loss: 1.081882577996383, ic :0.09367304921789284 
train 10, step: 0, loss: 0.7781404341774426, grad_norm: 0.010556239093216251, ic: 0.12169147040881814
train 10, step: 500, loss: 0.7866288197191456, grad_norm: 0.07502062649929565, ic: -0.016747327778872834
train 10, step: 1000, loss: 0.9975464870112603, grad_norm: 0.4177403700000674, ic: -0.020158581378871446
train 10, step: 1500, loss: 1.2968135579427085, grad_norm: 0.13949797719527057, ic: 0.08805571122584649
train 10, step: 2000, loss: 1.267920276845214, grad_norm: 0.5305530511766382, ic: -0.17536530277481818
Epoch 10: train loss: 1.6182346983033868
Eval step 0: eval loss: 0.9975918914971694
Eval: total loss: 1.0786582989131257, ic :0.10503879883423488 
train 11, step: 0, loss: 1.5244752922422953, grad_norm: 0.021081831888977703, ic: 0.18307193523020965
train 11, step: 500, loss: 1.3476813101443463, grad_norm: 0.19333713364642857, ic: 0.1539002206545334
train 11, step: 1000, loss: 2.2149673563564027, grad_norm: 0.8026194220340175, ic: 0.2566301735929102
train 11, step: 1500, loss: 1.3979169663856084, grad_norm: 1.598608449546893, ic: 0.022965756814705165
train 11, step: 2000, loss: 3.6726715894731727, grad_norm: 2.5551211556061872, ic: 0.21565002629521549
Epoch 11: train loss: 1.6135609581957246
Eval step 0: eval loss: 1.0086362014135728
Eval: total loss: 1.0878308825254916, ic :0.08617456270675829 
train 12, step: 0, loss: 1.002481649210165, grad_norm: 0.10128886929726814, ic: 0.05680969151019394
train 12, step: 500, loss: 1.1949020483384234, grad_norm: 0.08514343389339547, ic: 0.10592433413851544
train 12, step: 1000, loss: 1.209479474199718, grad_norm: 0.3983394150269971, ic: 0.23982966878485787
train 12, step: 1500, loss: 0.7420423353040541, grad_norm: 0.3012047719523241, ic: 0.40125278363294914
train 12, step: 2000, loss: 2.6831018248600746, grad_norm: 0.6064477100868733, ic: -0.02931921502725855
Epoch 12: train loss: 1.6159597104376249
Eval step 0: eval loss: 0.9971562566852619
Eval: total loss: 1.0828331384348129, ic :0.07596386857881476 
train 13, step: 0, loss: 1.3374513956861154, grad_norm: 0.5887936524100588, ic: 0.11353301934712212
train 13, step: 500, loss: 1.429893644614488, grad_norm: 0.13018370635398205, ic: -0.07224775209681045
train 13, step: 1000, loss: 2.1503529428564176, grad_norm: 0.8390899633487666, ic: 0.17105276357586124
train 13, step: 1500, loss: 1.5742220103225648, grad_norm: 1.2082793366187399, ic: -0.11493166933956307
train 13, step: 2000, loss: 0.7171757914143815, grad_norm: 0.026849037012886583, ic: -0.057510085658992474
Epoch 13: train loss: 1.6142026592555285
Eval step 0: eval loss: 1.0041928163260267
Eval: total loss: 1.0810233292843945, ic :0.10066596464858858 
train 14, step: 0, loss: 1.9773366851921628, grad_norm: 0.7038215486494016, ic: 0.1259014945290269
train 14, step: 500, loss: 2.7593894911689993, grad_norm: 2.3551371495220703, ic: -0.147424701526222
train 14, step: 1000, loss: 1.055596610208745, grad_norm: 0.08979362069384685, ic: 0.17200919472561232
train 14, step: 1500, loss: 2.9638628176678017, grad_norm: 0.6623455525698075, ic: 0.1601837713261342
train 14, step: 2000, loss: 0.7896082370406512, grad_norm: 0.02825482120782997, ic: -0.0544729270221211
Epoch 14: train loss: 1.6143344186169462
Eval step 0: eval loss: 0.9996278108955371
Eval: total loss: 1.0803600920652827, ic :0.11424210527833292 
train 15, step: 0, loss: 1.2426491320972712, grad_norm: 0.23367419126608802, ic: 0.3291079676677556
train 15, step: 500, loss: 1.8052085455209932, grad_norm: 0.8979285413721629, ic: 0.010639892838213259
train 15, step: 1000, loss: 1.7369076563691839, grad_norm: 0.5843731190289404, ic: 0.17889318567998552
train 15, step: 1500, loss: 1.397684007641562, grad_norm: 0.44041977925770254, ic: -0.08095614451635189
train 15, step: 2000, loss: 3.10307183666764, grad_norm: 0.16498470330591847, ic: 0.21309395936455078
Epoch 15: train loss: 1.6137914549907317
Eval step 0: eval loss: 1.0000154918090112
Eval: total loss: 1.0799048931118158, ic :0.10888288197788125 
train 16, step: 0, loss: 1.8066308329098917, grad_norm: 0.6083179919443996, ic: 0.18645756813529543
train 16, step: 500, loss: 1.3458242475616264, grad_norm: 0.5171148268239882, ic: 0.3248313045202734
train 16, step: 1000, loss: 0.9073225642114855, grad_norm: 0.012925603256470793, ic: -0.04786605494532507
train 16, step: 1500, loss: 1.3374979919169372, grad_norm: 0.8227593119665539, ic: 0.22948481283012562
train 16, step: 2000, loss: 1.243409529976223, grad_norm: 0.09834901539778435, ic: 0.16373307482420524
Epoch 16: train loss: 1.6132097388708222
Eval step 0: eval loss: 1.003497934768299
Eval: total loss: 1.0819219621599798, ic :0.11035819385110873 
train 17, step: 0, loss: 1.56077617424962, grad_norm: 1.0613121691775622, ic: 0.13539205561260662
train 17, step: 500, loss: 2.2272800579100074, grad_norm: 0.8784325186186508, ic: 0.1590402638541446
train 17, step: 1000, loss: 0.9621775669044748, grad_norm: 0.039215879746013205, ic: 0.1337819030666122
train 17, step: 1500, loss: 0.8312681540464744, grad_norm: 0.11771179272524124, ic: 0.2752094469443956
train 17, step: 2000, loss: 3.158451676649647, grad_norm: 1.1578189165042452, ic: 0.023344890999506082
Epoch 17: train loss: 1.6139628969667956
Eval step 0: eval loss: 1.0034095478911598
Eval: total loss: 1.0818425752692784, ic :0.10158282708000684 
train 18, step: 0, loss: 1.0707318322971535, grad_norm: 0.2807440066768816, ic: -0.012646711220069471
train 18, step: 500, loss: 0.7948890396311313, grad_norm: 0.12556294006177282, ic: -0.0364510963282538
train 18, step: 1000, loss: 1.1638021079566945, grad_norm: 0.7113859238192491, ic: 0.05129650436037033
train 18, step: 1500, loss: 0.8995557651398288, grad_norm: 0.015879958137909416, ic: 0.16065765166742538
train 18, step: 2000, loss: 1.7650889374243603, grad_norm: 0.973904453268214, ic: 0.08908468866015794
Epoch 18: train loss: 1.6119112559842645
Eval step 0: eval loss: 0.9944419117586558
Eval: total loss: 1.0780231065678318, ic :0.11022157066952507 
train 19, step: 0, loss: 1.0775691645805825, grad_norm: 0.6396658417858173, ic: 0.10049545887774897
train 19, step: 500, loss: 2.221493232540968, grad_norm: 1.4015056163038364, ic: 0.23555765109933563
train 19, step: 1000, loss: 1.2993468656994047, grad_norm: 0.04328512935318894, ic: 0.17351845898497004
train 19, step: 1500, loss: 1.4915699455308618, grad_norm: 0.26041823418542437, ic: 0.20306654519430256
train 19, step: 2000, loss: 1.1369089374105439, grad_norm: 0.29592257183403936, ic: 0.19976547071669234
Epoch 19: train loss: 1.6127069100106206
Eval step 0: eval loss: 1.0051286244404949
Eval: total loss: 1.0816529641797363, ic :0.09860915417223755 
