Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', batch_size=1, dataset_type='AdjTimeDataset', dout=0.3, epochs=20, gnn_layers=2, hidden_dim=128, input_dim=9, input_graph=True, label_cnt=3, lr=0.001, lstm_layers=1, mask_adj=False, mask_type='soft', model_type='BiGLSTM', num_days=8, num_heads=1, print_inteval=500, relation_num=1, seed=10086, shuffle=True, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=False)
21923
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (backward_cell): GLSTMCell(
    (dropout): Dropout(p=0.3, inplace=False)
    (Wh): Linear(in_features=128, out_features=640, bias=False)
    (Wn): Linear(in_features=128, out_features=640, bias=False)
    (Wt): Linear(in_features=128, out_features=640, bias=False)
    (U): Linear(in_features=128, out_features=640, bias=False)
    (V): Linear(in_features=128, out_features=640, bias=True)
    (relu): LeakyReLU(negative_slope=0.01)
    (gnn): ModuleList(
      (0): GraphConv(128, 128)
      (1): GraphConv(128, 128)
    )
  )
  (fc0): Linear(in_features=256, out_features=128, bias=True)
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 0.8208533365081, grad_norm: 0.1314592324682835, ic: -0.023116689749083992
train 0, step: 500, loss: 1.1520923607272582, grad_norm: 0.01415952116233683, ic: 0.07140571760534396
train 0, step: 1000, loss: 0.7122352524559097, grad_norm: 9.693249185534356e-05, ic: 0.09220394097274205
train 0, step: 1500, loss: 0.8559511392423422, grad_norm: 0.1679003499124421, ic: 0.04019694192069157
train 0, step: 2000, loss: 2.3858856845752916, grad_norm: 0.7234095010660343, ic: 0.10086696160406988
Epoch 0: train loss: 1.6484653649697023
Eval step 0: eval loss: 0.8353286920030953
Eval: total loss: 1.0790347809653833, mse:4.823634341110539, ic :0.009395440463467906, sharpe5:7.596405891180038, irr5:212.32955932617188, ndcg5:0.8437891836894714 
train 1, step: 0, loss: 2.3075998577052457, grad_norm: 0.04316597140629832, ic: 0.05194504726986245
train 1, step: 500, loss: 0.6237497907929971, grad_norm: 0.037588912542268135, ic: -0.061331796897718435
train 1, step: 1000, loss: 1.1953568277479727, grad_norm: 0.09917456588221166, ic: 0.053113056429457677
train 1, step: 1500, loss: 2.2696574126124287, grad_norm: 0.8639003822889422, ic: 0.06969096819939166
train 1, step: 2000, loss: 1.1667858673917888, grad_norm: 0.012274545700745845, ic: -0.03779688177293989
Epoch 1: train loss: 1.6468367101117516
Eval step 0: eval loss: 0.8362812006059009
Eval: total loss: 1.0792943592404913, mse:4.8229487781615035, ic :0.023623761506936715, sharpe5:9.672494321465491, irr5:275.8263854980469, ndcg5:0.841160508632377 
train 2, step: 0, loss: 2.0056466938536843, grad_norm: 0.23835699229798932, ic: 0.0807472080754742
train 2, step: 500, loss: 1.148412793358855, grad_norm: 0.4230466869879348, ic: 0.3846343857143775
train 2, step: 1000, loss: 1.1328113255411907, grad_norm: 0.05752079492620853, ic: 0.3545249248020561
train 2, step: 1500, loss: 1.2788706162934391, grad_norm: 0.27084968654019786, ic: 0.5459640859252174
train 2, step: 2000, loss: 2.569795056470138, grad_norm: 1.556743070307074, ic: -0.06714058631063079
Epoch 2: train loss: 1.6460914457740585
Eval step 0: eval loss: 0.8363529120908522
Eval: total loss: 1.079071352072681, mse:4.809090997214814, ic :0.0810734138733927, sharpe5:11.203564950227737, irr5:393.389404296875, ndcg5:0.8485882389716899 
train 3, step: 0, loss: 0.8285994405640102, grad_norm: 0.14631856395587797, ic: 0.161089907061635
train 3, step: 500, loss: 1.2541347050973168, grad_norm: 0.07227563379926123, ic: 0.010261328276470997
train 3, step: 1000, loss: 1.3728622284752774, grad_norm: 0.3201060685984741, ic: 0.007184699605713702
train 3, step: 1500, loss: 1.6992064403886555, grad_norm: 0.2703865282107659, ic: 0.706546723156123
train 3, step: 2000, loss: 1.1171342073594523, grad_norm: 0.2730186274130783, ic: -0.06061263928668915
Epoch 3: train loss: 1.6421455162204868
Eval step 0: eval loss: 0.8394500764579491
Eval: total loss: 1.0796920415737352, mse:4.725138472801948, ic :0.12574898235373907, sharpe5:11.599938093423843, irr5:402.7041015625, ndcg5:0.8426388390780822 
train 4, step: 0, loss: 1.110568631652903, grad_norm: 0.36690112512591616, ic: 0.0038184567003018283
train 4, step: 500, loss: 1.0154658036800488, grad_norm: 0.05250776568569463, ic: 0.5665688892823064
train 4, step: 1000, loss: 1.1251801912144839, grad_norm: 0.009918063183991046, ic: 0.4563430626583983
train 4, step: 1500, loss: 1.274362976648217, grad_norm: 0.26894521706988256, ic: 0.05506272262068895
train 4, step: 2000, loss: 1.3499292639850058, grad_norm: 0.1571691631882487, ic: 0.39202020368777607
Epoch 4: train loss: 1.6391486027022664
Eval step 0: eval loss: 0.8326767176797945
Eval: total loss: 1.0756681888315651, mse:4.722592149396662, ic :0.1318787042145746, sharpe5:11.55406826376915, irr5:400.8482971191406, ndcg5:0.8493974602796601 
train 5, step: 0, loss: 2.279727522433727, grad_norm: 0.004161569317042473, ic: -0.01888848104515571
train 5, step: 500, loss: 1.7885035261323181, grad_norm: 0.6928446084471431, ic: -0.025158960556680703
train 5, step: 1000, loss: 4.464617687365205, grad_norm: 0.7823116736267982, ic: -0.04572715100150289
train 5, step: 1500, loss: 0.949363125519103, grad_norm: 0.08983140304338844, ic: 0.10854739237761712
train 5, step: 2000, loss: 2.2543740632670883, grad_norm: 0.9178332181794855, ic: 0.050509346205835026
Epoch 5: train loss: 1.639517525230868
Eval step 0: eval loss: 0.8367100545804794
Eval: total loss: 1.0774032543794156, mse:4.668480870401748, ic :0.13080085984128043, sharpe5:11.463274530172347, irr5:398.1220397949219, ndcg5:0.8457903023619527 
train 6, step: 0, loss: 1.9759018406723485, grad_norm: 0.04442021226667809, ic: 0.26601509257813016
train 6, step: 500, loss: 1.0158049593266754, grad_norm: 0.0002216884105625489, ic: -0.03353283149112573
train 6, step: 1000, loss: 1.5749487692799016, grad_norm: 0.5622008214030385, ic: 0.007117537455993693
train 6, step: 1500, loss: 1.0898236146907216, grad_norm: 0.05491072424972118, ic: 0.48101710001365056
train 6, step: 2000, loss: 1.1051206837706447, grad_norm: 0.17908793661385952, ic: -0.017500417886256822
Epoch 6: train loss: 1.6378625773921716
Eval step 0: eval loss: 0.8313186372168071
Eval: total loss: 1.074081496433659, mse:4.638304805731108, ic :0.13771167666486486, sharpe5:11.495471003651618, irr5:398.9326477050781, ndcg5:0.865447888488328 
train 7, step: 0, loss: 1.2080225438010685, grad_norm: 0.3469756899728596, ic: 0.5882667278998871
train 7, step: 500, loss: 1.415915844647892, grad_norm: 0.038365196725456276, ic: 0.10926503264127715
train 7, step: 1000, loss: 1.076026904526835, grad_norm: 0.0976185437337998, ic: -0.10913167152129252
train 7, step: 1500, loss: 1.9489348206339003, grad_norm: 0.8497216485175724, ic: -0.09969170404354387
train 7, step: 2000, loss: 2.993110470655488, grad_norm: 0.7236392404306006, ic: 0.04904070943445106
Epoch 7: train loss: 1.636778013258015
Eval step 0: eval loss: 0.8430150698309075
Eval: total loss: 1.0784537720399907, mse:4.653477984178654, ic :0.13634625585022017, sharpe5:11.665802900791167, irr5:404.539794921875, ndcg5:0.8549901825655276 
train 8, step: 0, loss: 1.2662541244261667, grad_norm: 0.0003555133870958857, ic: -0.020348016032495725
train 8, step: 500, loss: 0.6346747009954438, grad_norm: 0.083996814941524, ic: 0.007348239275700037
train 8, step: 1000, loss: 1.8973564400500385, grad_norm: 0.7155085672899567, ic: 0.4461165787258475
train 8, step: 1500, loss: 1.4291045102012299, grad_norm: 0.20777531621361572, ic: 0.12200712933786409
train 8, step: 2000, loss: 4.077668624638131, grad_norm: 1.0531947964230872, ic: -0.028625209941818632
Epoch 8: train loss: 1.636534073219414
Eval step 0: eval loss: 0.8319192128638698
Eval: total loss: 1.0736434657996852, mse:4.6316041546887705, ic :0.13741661892537452, sharpe5:11.576740211248397, irr5:402.6239013671875, ndcg5:0.8452751811129197 
train 9, step: 0, loss: 0.7926538611883253, grad_norm: 0.015160401729542538, ic: 0.23572763742295547
train 9, step: 500, loss: 1.072494903168121, grad_norm: 0.19425480892269145, ic: -0.03486112840266865
train 9, step: 1000, loss: 0.801489537362834, grad_norm: 0.03467006087502773, ic: 0.006924207190951232
train 9, step: 1500, loss: 1.0519325678822273, grad_norm: 0.10203836111520533, ic: -0.04551177680780908
train 9, step: 2000, loss: 6.686702057957268, grad_norm: 0.24991854964126875, ic: 0.09860581310708491
Epoch 9: train loss: 1.6361450372791495
Eval step 0: eval loss: 0.8333080360082982
Eval: total loss: 1.0739972408230918, mse:4.638796802471054, ic :0.136180296205092, sharpe5:11.616358417868614, irr5:401.3306884765625, ndcg5:0.848316169652913 
train 10, step: 0, loss: 0.9156250868827848, grad_norm: 0.023189718827527187, ic: 0.5205817887124988
train 10, step: 500, loss: 3.971196345511199, grad_norm: 1.0672126650040874, ic: 0.1002425351267276
train 10, step: 1000, loss: 1.3330393598123562, grad_norm: 0.4986627211264802, ic: 0.3216612812756703
train 10, step: 1500, loss: 1.4165796997163425, grad_norm: 0.005289693593201811, ic: -0.017206470909848753
train 10, step: 2000, loss: 1.3065750195001327, grad_norm: 0.6091172266432997, ic: -0.035266643866287604
Epoch 10: train loss: 1.6357220331827367
Eval step 0: eval loss: 0.8280900125131717
Eval: total loss: 1.0726633107961348, mse:4.6411848916778755, ic :0.13939433417933647, sharpe5:11.504118656516075, irr5:395.2812805175781, ndcg5:0.8544175823041311 
train 11, step: 0, loss: 2.2832012724424118, grad_norm: 0.10464467294996946, ic: 0.030672502300359325
train 11, step: 500, loss: 1.876927480520557, grad_norm: 0.27331206942804404, ic: 0.1330109394040151
train 11, step: 1000, loss: 2.97041181266132, grad_norm: 1.3492541279331254, ic: 0.10084743388262128
train 11, step: 1500, loss: 1.0119390864222402, grad_norm: 0.20245155798287148, ic: 0.032797791186637136
train 11, step: 2000, loss: 1.1823118976343459, grad_norm: 0.5086654026745268, ic: 0.6800867638142756
Epoch 11: train loss: 1.6367718820261483
Eval step 0: eval loss: 0.8293048629519559
Eval: total loss: 1.0711238587730743, mse:4.6158466669842735, ic :0.1485278164189734, sharpe5:11.59756665766239, irr5:402.86431884765625, ndcg5:0.849268927247808 
train 12, step: 0, loss: 1.385026290856668, grad_norm: 0.7687974767256713, ic: -0.17891236938763472
train 12, step: 500, loss: 1.9344705904447115, grad_norm: 0.24193769580699967, ic: 0.04929165873731098
train 12, step: 1000, loss: 1.377393088721195, grad_norm: 0.14316108229360397, ic: 0.4456376861624931
train 12, step: 1500, loss: 1.1335968389743711, grad_norm: 0.2636925476936658, ic: 0.13651019624051958
train 12, step: 2000, loss: 2.804859143905057, grad_norm: 0.32239613703618397, ic: 0.010156545494602152
Epoch 12: train loss: 1.6353853512237682
Eval step 0: eval loss: 0.8288659758010076
Eval: total loss: 1.0711872977108217, mse:4.622552902672707, ic :0.14962681091355898, sharpe5:11.329445906281471, irr5:393.2540588378906, ndcg5:0.8631238030721409 
train 13, step: 0, loss: 1.9163933011049723, grad_norm: 0.07886808003694969, ic: 0.08748508372068693
train 13, step: 500, loss: 2.8942046998517785, grad_norm: 0.9333433761601586, ic: 0.26111484547090014
train 13, step: 1000, loss: 1.0007523497572457, grad_norm: 0.09486905582391607, ic: 0.38703504598333727
train 13, step: 1500, loss: 0.9699063775389143, grad_norm: 0.0007362333157836106, ic: 0.10534364708034794
train 13, step: 2000, loss: 0.7996362284692363, grad_norm: 0.0806904540329277, ic: 0.9310481797832029
Epoch 13: train loss: 1.6338705420949682
Eval step 0: eval loss: 0.8278998966840094
Eval: total loss: 1.0710473359682273, mse:4.626246793646562, ic :0.14904121126683031, sharpe5:11.878796993494033, irr5:404.7056884765625, ndcg5:0.8567685282571761 
train 14, step: 0, loss: 2.2828004807692306, grad_norm: 0.597923946991135, ic: 0.039785083683782134
train 14, step: 500, loss: 0.7757142737791112, grad_norm: 0.0027667112169152573, ic: 0.04165805328181118
train 14, step: 1000, loss: 3.9415729418276975, grad_norm: 1.1416722737369485, ic: 0.19159776564107595
train 14, step: 1500, loss: 1.4450618438481162, grad_norm: 0.3747006699831341, ic: 0.04388334860332602
train 14, step: 2000, loss: 1.3201566449618358, grad_norm: 0.07170144067418985, ic: 0.2876538320720249
Epoch 14: train loss: 1.633425328392234
Eval step 0: eval loss: 0.8339563335583509
Eval: total loss: 1.072247588156706, mse:4.6198304524931695, ic :0.15316491220541112, sharpe5:11.55584660410881, irr5:398.2606201171875, ndcg5:0.8662450816757286 
train 15, step: 0, loss: 0.9297511041250907, grad_norm: 0.11821243590610751, ic: 0.5429753073322082
train 15, step: 500, loss: 2.3578391267612338, grad_norm: 0.14948943647841656, ic: 0.4099602422102655
train 15, step: 1000, loss: 1.159688377679798, grad_norm: 0.27756247537709383, ic: 0.5126076633466484
train 15, step: 1500, loss: 0.8644045565990691, grad_norm: 0.006590876634398642, ic: -0.006352319281985586
train 15, step: 2000, loss: 2.97267984723513, grad_norm: 1.5780182099266986, ic: 0.028167353425428306
Epoch 15: train loss: 1.6349212182014266
Eval step 0: eval loss: 0.8288480318509615
Eval: total loss: 1.0713377503696069, mse:4.629822535669301, ic :0.14935148414878793, sharpe5:11.992055516839027, irr5:406.2488098144531, ndcg5:0.8634102419355818 
train 16, step: 0, loss: 0.9259218861394961, grad_norm: 0.036353538909615575, ic: 0.020515700770497046
train 16, step: 500, loss: 1.4343088128267585, grad_norm: 0.005455366225667362, ic: 0.02748982703372794
train 16, step: 1000, loss: 0.9717216310907879, grad_norm: 0.14000879674331013, ic: 0.03227188515163695
train 16, step: 1500, loss: 2.7098160985868924, grad_norm: 0.5036627706071346, ic: 0.01209745909144689
train 16, step: 2000, loss: 1.964360297480716, grad_norm: 0.9492922941279054, ic: -0.10292009052918469
Epoch 16: train loss: 1.633159269315409
Eval step 0: eval loss: 0.8270540227377502
Eval: total loss: 1.07045732766673, mse:4.621955718025578, ic :0.1499119861012568, sharpe5:11.96104618012905, irr5:408.9344787597656, ndcg5:0.8538846086899897 
train 17, step: 0, loss: 0.7742396073028008, grad_norm: 0.1200350166338643, ic: 0.04688782802640237
train 17, step: 500, loss: 1.8097773602931446, grad_norm: 0.3280767803141856, ic: 0.108038395543716
train 17, step: 1000, loss: 2.033017113095238, grad_norm: 0.8732059465427138, ic: -0.013848634686828168
train 17, step: 1500, loss: 1.1194230403348553, grad_norm: 0.16992151959749657, ic: 0.03030103571555491
train 17, step: 2000, loss: 1.5455203451694304, grad_norm: 0.4448244571486406, ic: 0.12823624401707184
Epoch 17: train loss: 1.633615786487939
Eval step 0: eval loss: 0.8367279342152923
Eval: total loss: 1.073408713097321, mse:4.622704657682428, ic :0.15331925656251805, sharpe5:11.841799947023391, irr5:405.526611328125, ndcg5:0.845737232050733 
train 18, step: 0, loss: 1.0139702275555131, grad_norm: 0.05625210733492848, ic: 0.027406495858062603
train 18, step: 500, loss: 1.6205510262412892, grad_norm: 0.4985241682654127, ic: 0.27312230819491995
train 18, step: 1000, loss: 1.780328404247124, grad_norm: 0.28994751334942814, ic: 2.3301045466822892e-05
train 18, step: 1500, loss: 0.7958036433234166, grad_norm: 0.036507781264163874, ic: -0.03228920704052806
train 18, step: 2000, loss: 3.3005730649918634, grad_norm: 0.9247603512293969, ic: 0.0029737202805204766
Epoch 18: train loss: 1.633435759086241
Eval step 0: eval loss: 0.8338576739907139
Eval: total loss: 1.0728496173849875, mse:4.621585726905106, ic :0.14850155293066117, sharpe5:11.699992439746856, irr5:401.0189208984375, ndcg5:0.843175847345119 
train 19, step: 0, loss: 2.370582562671032, grad_norm: 0.0865422120345012, ic: 0.0859687977873779
train 19, step: 500, loss: 9.202544279016104, grad_norm: 1.3756206686327146, ic: 0.03902671876008933
train 19, step: 1000, loss: 1.2621952777020944, grad_norm: 0.16715607611999433, ic: 0.09541851854959366
train 19, step: 1500, loss: 0.75079299534323, grad_norm: 0.0307007887819574, ic: 0.03981263429596165
train 19, step: 2000, loss: 1.3135744788693873, grad_norm: 0.22326745919439872, ic: 0.0431510537830869
Epoch 19: train loss: 1.633647875206983
Eval step 0: eval loss: 0.8353290778944942
Eval: total loss: 1.0732808631437392, mse:4.617775464700456, ic :0.1510155492569994, sharpe5:11.744143522381782, irr5:404.35528564453125, ndcg5:0.8560801303534601 
