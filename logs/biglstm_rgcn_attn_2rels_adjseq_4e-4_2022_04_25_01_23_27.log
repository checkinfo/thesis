Namespace(adj_path='./data/concepts_graph_1931_233_3.npy', ann_embed_dim=128, ann_embed_num=89, ann_path=None, batch_size=1, dataset_type='RGCNSeqTimeDataset', dout=0.3, epochs=60, glstm_layers=1, gnn_layers=1, gpu=0, graph_attn=True, hidden_dim=128, inner_prod=False, input_dim=9, input_graph=True, label_cnt=3, lr=0.0004, lstm_layers=1, market=None, mask_adj=True, mask_type='soft', model_type='BiGLSTM', normalize_adj=True, num_days=8, num_heads=1, print_inteval=500, rank_loss=False, relation_num=2, rsr_data_path=None, seed=10086, shuffle=True, side_info=False, sparse_adj_path='../data/icgraph_window_250_0.8/', stock_num=1931, test_mask_path='./data/test_mask_126_1931.npy', test_path='./data/test_126_1931_12.npy', top_stocks=5, train_mask_path='./data/train_mask_2305_1931.npy', train_path='./data/train_2305_1931_12.npy', use_adj=True)
load 2305 train graphs successful!
load 126 test graphs successful!
2517
BiGLSTM(
  (input_to_hidden): Linear(in_features=9, out_features=128, bias=True)
  (forward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (backward_cells): ModuleList(
    (0): GLSTMCell(
      (dropout): Dropout(p=0.3, inplace=False)
      (Wh): Linear(in_features=128, out_features=640, bias=False)
      (Wn): Linear(in_features=128, out_features=640, bias=False)
      (Wt): Linear(in_features=128, out_features=640, bias=False)
      (U): Linear(in_features=128, out_features=640, bias=False)
      (V): Linear(in_features=128, out_features=640, bias=True)
      (relu): LeakyReLU(negative_slope=0.01)
      (gnn): ModuleList(
        (0): RGCN(
          (attention): ModuleList(
            (0): GraphAttentionLayer (128 -> 128)
            (1): GraphAttentionLayer (128 -> 128)
          )
          (lin_rel): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Linear(in_features=128, out_features=128, bias=True)
          )
          (lin_root): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=False)
            (1): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (fc0): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
  (w_out): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
train 0, step: 0, loss: 4.931181358116692, grad_norm: 4.941579925469582, ic: -0.03934702255174804
train 0, step: 500, loss: 0.8647283397407216, grad_norm: 0.026175876613327684, ic: 0.03614061190865891
train 0, step: 1000, loss: 1.9479735792337236, grad_norm: 0.5075208383850753, ic: 0.0366094044807793
train 0, step: 1500, loss: 0.9573278856842885, grad_norm: 0.053536183296322565, ic: 0.027250224528147255
train 0, step: 2000, loss: 1.0013043683067855, grad_norm: 0.15401850781445628, ic: 0.007092585491823299
Epoch 0: 2022-04-25 13:31:55.914665: train loss: 1.6486740136407043
Eval step 0: eval loss: 0.8361741800579556
Eval: 2022-04-25 13:32:26.784327: total loss: 1.0792990730044212, mse:4.822932138507077, ic :0.008104010530046134, sharpe5:8.279790868163108, irr5:235.18467712402344, ndcg5:0.8630661462734066, pnl5:2.84899640083313 
train 1, step: 0, loss: 2.772035857169859, grad_norm: 0.8635760214679601, ic: 0.05749831848762091
train 1, step: 500, loss: 1.754440009931561, grad_norm: 0.7542049935481133, ic: 0.12037187686334132
train 1, step: 1000, loss: 0.8775988408534707, grad_norm: 0.17398103648110153, ic: 0.06760831568292697
train 1, step: 1500, loss: 1.712982388200431, grad_norm: 0.20786453774955066, ic: -0.014772672378500553
train 1, step: 2000, loss: 2.1784666015625, grad_norm: 0.8865598672944269, ic: -0.03763179698844819
Epoch 1: 2022-04-25 13:40:44.663585: train loss: 1.6467048719490733
Eval step 0: eval loss: 0.834353351492525
Eval: 2022-04-25 13:41:14.768613: total loss: 1.0789934659485438, mse:4.823833719052189, ic :0.006895584125472916, sharpe5:8.12517363488674, irr5:233.8961944580078, ndcg5:0.8529958498971929, pnl5:2.710139513015747 
